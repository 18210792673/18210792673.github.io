<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[问题日志]]></title>
    <url>%2F2020%2F02%2F25%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E9%97%AE%E9%A2%98%E6%97%A5%E5%BF%97%2F%E9%97%AE%E9%A2%98%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[2020-2-25-周二-zookeeper占用磁盘存储问题问题描述： zk的事务及快照日志目录数量巨多，并且文件的更新频率很快，每一个事务日志的大小都是64M，这会导致短时间内会有几千上万个文件，并且快速消耗储存空间。 问题分析: 根据特征，问题应该是有客户端对zk进行频繁的更新操作。那么现在我们需要知道这几个信息： 客户端 具体的操作 具体操作为： 1）使用zk自带的事务日志可视化功能，把事务日志解析出来，获取具体的客户端session id： zookeeper的事务日志为二进制文件，不能通过vim等工具直接访问。可以通过zookeeper自带的jar包读取事务日志文件。 首先将libs中的slf4j-api-1.6.1.jar文件和zookeeper根目录下的zookeeper-3.4.8.jar文件复制到临时文件夹tmplibs中，然后执行如下命令,将日志内容输出至a.txt文件中： 1# java -classpath .:slf4j-api-1.6.1.jar:zookeeper-3.4.8.jar org.apache.zookeeper.server.LogFormatter /home/zookeeper/2181/data/version-2/log.2f019aaf7f &gt; a.txt 2）查看文件。文件中都是同一个客户端在进行频繁的create以及delete操作 内容格式为： 1232/25/20 9:33:06 AM CST session 0x2706083ada50136 cxid 0xffffffffdac9b87f zxid 0x4972915de7 delete &apos;/app_name/leader/_c_c9d6577b-dd38-4513-a32b-403b40a7fd93-latch-09176630642/25/20 9:33:06 AM CST session 0x2706083ada50136 cxid 0xffffffffdac9b880 zxid 0x4972915de8 create &apos;/app_name/leader/_c_92759f69-7d68-4fbf-8924-1e786b3f5f9b-latch-0917663066,,v&#123;s&#123;31,s&#123;&apos;world,&apos;anyone&#125;&#125;&#125;,T,917663067 到这里可以定位到问题的原因，就是因为有人在频繁的进行更新操作，接下来就是解决这个问题 解决问题 1）根据session id找到对应的客户端ip 使用zk的cons命令 12[zookeeper@common001-dev.novalocal version-2]$ echo cons | nc 127.0.0.1 2181 | grep -i &quot;0x2706083ada50136&quot; /192.168.1.107:44194[1](queued=0,recved=3726396779,sent=3726396753,sid=0x2706083ada50136,lop=GETD,est=1582293664630,to=40000,lcxid=0x7fffffff,lzxid=0x49743acabe,lresp=1582598304375,llat=1,minlat=0,avglat=1,maxlat=791) 可以看到，这个对应的客户端信息为：192.168.1.107:44194 2）根据客户端信息定位具体的pod，找到应用 因为使用的k8s网络问题，这里显示的并不是真正的容器实例，而是宿主机的信息，因此还需要再进行一层解析 使用命令如下，根据端口确定： 12[root@app028-dev.novalocal ~]# cat /proc/net/nf_conntrack* | grep 44194ipv4 2 tcp 6 86398 ESTABLISHED src=10.201.3.140 dst=192.168.11.29 sport=44194 dport=2181 src=192.168.11.29 dst=192.168.1.107 sport=2181 dport=44194 [ASSURED] mark=0 zone=0 use=2 到这里，找到真正的客户端ip: 10.201.3.140。打开我们的容器终端界面，找到了对应的应用。 3）找到应用对应的开发，进行沟通，了解问题产生的真正原因 zk在这个应用中的作用是被拿来进行实例选主的。 选主逻辑： 每个应用实例在启动的时候都会在这个znode（这里是/app_name/leader）下注册一个临时节点，然后对所有节点名排序，排第一个的就是主，同时所有实例还要监听这个目录下面的节点变动，如果节点变动了，再重新进行排序选主。 个人问题：如果新加入了一个实例，经过计算，这个实例的值是排第一的，那么就算现有的主是正常的，也会触发重新选主吗? 开发解答：好像可能会被顶掉的，所以应用要check，自己当前还是不是主了。实例创建的node名称用了uuid，而uuid前几位是根据系统时间算出来的，所以只要不同实例之间的系统时间差别不大，一般不会被顶掉 4）重启应用，问题暂时解决，不再输出相关的事务日志 5）在这个过程中，发现follower中的数据和leader中的不一致。 follower中这个znode下多了一个实例。在删除时删除不掉，因为所有的删除操作都会转到主上执行再同步下去，主上又没有，所以就没办法删。没有办法只好重启这个follower，重启之后，数据恢复一致。 6）到这里，问题算是解决了。 但是为什么日志中会不断的create然后再delete这些znode还不明确。 以及zk主从节点之间数据为什么会不同步的原因也还是没有明确，我猜想大概率是版本的bug，可以看这些：https://www.jianshu.com/p/9fc4f6fdd44f以及https://issues.apache.org/jira/browse/ZOOKEEPER-2845 在3.4.12版本之后，修复了这个事务日志编号问题]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>问题日志</category>
      </categories>
      <tags>
        <tag>问题日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[saltstack]]></title>
    <url>%2F2019%2F12%2F20%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96%2Fsaltstack%2F</url>
    <content type="text"><![CDATA[基础知识官方文档 简介：https://docs.saltstack.com/en/getstarted/ 详细文档：https://docs.saltstack.com/en/latest/contents.html saltstack的版本安装部署yum方式安装 安装yum源 1sudo yum install https://repo.saltstack.com/yum/redhat/salt-repo-2019.2.el6.noarch.rpm 如果需要清除yum的缓存，install之后可以执行： 1sudo yum clean expire-cache 安装salt相关组件 123456 sudo yum install salt-master sudo yum install salt-minionsudo yum install salt-ssh sudo yum install salt-syndic sudo yum install salt-cloud sudo yum install salt-api 启动 1sudo service salt-minion restart python pip方式安装在salt被加入pypi之后，我们就可以使用pip的方式进行安装 在这里以python2.7为例，安装salt的2016.3.2版本（这里目的是和生产环境保持一致） 在这里要注意的是，使用pip安装之后，salt相关的配置目录（例如/etc/salt）等是不会自动创建的，需要我们人为手动去的生成。 依赖 依赖 安装依赖软件包 Install the group ‘Development Tools’, yum groupinstall &#39;Development Tools&#39; Install the ‘zeromq-devel’ package if it fails on linking against that afterwards as well. pip install 12最新版本：./pip install salt指定版本：./pip install salt==2016.3.2 如果没有事先解决好依赖关系，那么会报错： 123Fatal: Cython-generated file &apos;zmq/devices/monitoredqueue.c&apos; not found. Cython &gt;= 0.20 is required to compile pyzmq from a development branch. Please install Cython or download a release package of pyzmq. 需要安装zmq 1./pip install pyzmq==15.4.0 配置salt的配置非常简单，master使用默认配置就可以工作，而minion只需要修改一下minion配置文件中的master地址即可。 master配置默认情况下，salt-master在所有接口上监听4505和4506端口 如果想要指定监控的接口，那么设置： 1interface: 10.0.0.1 minion设置启停常用命令 查看当前使用的salt版本 1salt &apos;*&apos; test.version 实例案例salt api调用常见问题salt执行特别慢在默认情况下，我们安装salt之后可能会出现获取返回结果耗时非常久的情况，例如： 123456[root@shidc-1 ~]# time salt --versionsalt 2016.3.2 (Boron)real 0m5.390suser 0m0.236ssys 0m0.142s 简单的一个命令，就需要5秒。 问题解决：]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维自动化</category>
      </categories>
      <tags>
        <tag>saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix trapper方式监控]]></title>
    <url>%2F2019%2F07%2F28%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E8%BF%90%E7%BB%B4%E7%9B%91%E6%8E%A7%E4%BD%93%E7%B3%BB%2Fzabbix%2Fzabbix-trapper%E6%96%B9%E5%BC%8F%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[简介 zabbix获取数据时有时会出现超时，如果一些数据需要执行比较长的时间才能获取的话，那么zabbix会出现异常，考虑到这种情况，zabbix增加了Trapper功能，客户端自己提交数据给zabbix。 使用场景： 例如自定义的网络监控。需要添加ping丢包、延迟、抖动等情况的监控，如果通过server去获取数据，因为存在ping的时间，所以很容易就会导致超时，从而拿不到数据 监控项比较多，并且比较耗时的情况。例如zk的数据、es的数据等，使用trapper的方式，会比server去获取的效率高很多。 具体实现实现逻辑 客户端自己实现获取获取数据的方式，可以是shell脚本、python程序等 通过zabbix_sender，把数据推送到zabbix server 客户端上配置定时任务，控制发送的频率 配置流程具体到配置，流程是： zabbix server添加客户端，客户端的配置文件中要配置主机名 配置监控项，设置监控类型为：zabbix trapper item的key是 zabbix_sender语法语法如下： 1zabbix_sender -z &lt;server IP address&gt; -p 10051 -s &quot;New host&quot; -k trap -o &quot;test value&quot; 参数说明： -z to specify Zabbix server IP address -p to specify Zabbix server port number (10051 by default) -s to specify the host (make sure to use the ‘technical’ host name here, instead of the ‘visible’ name) -k to specify the key of the item we just defined -o to specify the actual value to send 执行的命令是： 1./zabbix_sender -s common010-dev.novalocal -c /usr/local/zabbix/etc/zabbix_agentd.conf -k trapper-test -o &quot;5&quot; -p 10051 或者直接指定zabbix server的地址： 1./zabbix_sender -s common010-dev.novalocal -z 192.168.1.82 -k trapper-test -o &quot;6&quot; 注意事项 trapper是被监控主机主动发送数据给zabbix server，与主动模式的区别是可以不需要安装客户端； trapper方式发送数据是以主机名处理，不是IP地址，所以主机名要唯一。 接上面一点，-s 后面加的必须是主机名，而不是ip地址 指定server地址，可以使用配置文件，也可以直接使用ip 如果使用的是默认端口，那么端口参数可以省略 实际案例-es监控使用zabbix来做，整体步骤如下： zabbix上创建模板 主机上使用zabbix_send定时发送数据-上传脚本 设置定时任务 123[root@es042.ecs.east1-e ~]# crontab -l# send es monitor data every 30s* * * * * for i in 30 30;do sleep $i; /application/python/bin/python /usr/local/zabbix/scripts/es_status_monitor.py 9200;done discovery-自动发现整体流程分为2个部分 动态发现要监控的对象 根据上一步输出的对象，添加预先设置好的监控项和触发器 也就是说，分为2个逻辑 获取监控对象的逻辑 根据上面输出变量的值，添加监控项和触发器 官方说明： 首先，用户在“配置”→“模板”→“发现”列中创建一个发现规则。发现规则包括（1）发现必要实体（例如，文件系统或网络接口）的项目和（2）应该根据该项目的值创建的监控项，触发器和图形的原型 发现必要实体的项目就像其他地方所看到的常规项目：服务器向该项目的值询问Zabbix agent（或者该项目的任何类型的设置），agent以文本值进行响应。区别在于agent响应的值应该包含特定JSON格式的发现实体的列表。这种格式的自定义检查者发现的细节才是最重要的，因为返回值必须包含宏→值对。例如，项目“net.if.discovery”可能会返回两对键值： 1“&#123;#IFNAME&#125;”→“lo”和“&#123;#IFNAME&#125;”→“eth0”。 这些宏用于名称，键值和其他原型字段中，然后用接收到的值为每个发现的实体创建实际的监控项，触发器，图形甚至主机。请参阅使用LLD宏的选项的完整列表。 当服务器接收到发现项目的值时，它会查看宏→值对，每对都根据原型生成实际监控项，触发器和图形。在上面的“net.if.discovery”示例中，服务器将生成环路接口“lo”的一组监控项，触发器和图表，另一组用于界面“eth0”。 以下部分将详细说明上述过程，并作为一个指导上述类型的所有发现。最后一节描述了发现项目的JSON格式，并给出了文件系统发现实现的Perl脚本的示例。 具体实现创建模板和应用集创建discovery rules实例案例-zookeeper根据端口号，定位pid，然后根据pid去获取相应的jvm信息 监控脚本12345678910111213141516171819202122232425262728293031323334353637383940#!/application/python/bin/python# -*- coding:utf-8 -*-import subprocess,json,sys#base_path=os.path.dirname(os.path.dirname(os.path.abspath(__file__)))#sys.path.append(base_path)args = sys.argv[1:]def get_zk_node_path(): get_path_cmd = &quot;&quot;&quot;sudo /bin/ps -elf | grep zookeeper| grep -E -v &apos;grep|sleep|sugar_java|scripts|sudo&apos; | awk &apos;&#123;print $NF&#125;&apos; | awk -F &quot;/&quot; &apos;&#123;print &quot;/&quot;$2&quot;/&quot;$3&quot;/&quot;$4&#125;&apos;&quot;&quot;&quot; zk_node_path_list=subprocess.Popen(get_path_cmd,shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE) macros_keys = &#123;&quot;data&quot;: []&#125; for zk_node_path in zk_node_path_list.stdout.readlines(): zk_node_path = zk_node_path.strip() macros_keys[&apos;data&apos;].append(&#123;&apos;&#123;&amp;#35ZK_NODE&#125;&apos;:zk_node_path&#125;) return json.dumps(macros_keys)def get_zk_value(zk_node_path,key): get_pid_cmd = &quot;&quot;&quot;sudo /bin/ps -elf | grep %s | grep -Ev &quot;grep|python&quot; | awk &apos;&#123;print $4&#125;&apos;&quot;&quot;&quot; % (zk_node_path) zk_pid_tmp = subprocess.Popen(get_pid_cmd,shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE) zk_pid = zk_pid_tmp.stdout.read().strip() values=subprocess.Popen(&apos;sudo /usr/local/jdk/bin/jstat -gcutil &#123;&#125;&apos;.format(zk_pid),shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE) jstat_keys = values.stdout.readline().strip().split() jstat_values = values.stdout.readline().strip().split() try: index_num=jstat_keys.index(key) except ValueError: pass else: return jstat_values[index_num]if __name__==&apos;__main__&apos;: result=None if len(args)==1: if args[0]==&apos;discovery&apos;: result = get_zk_node_path() if len(args)==2: result = get_zk_value(args[0],args[1]) if result: print (result) 配置zabbix上创建模板 修改sudo 123[root@common001-dev.novalocal ~]# cat /etc/sudoers 在文件末尾添加zabbix ALL = (ALL) NOPASSWD: /usr/local/jdk/bin/jps, /usr/local/jdk/bin/jstat, /sbin/blockdev, /bin/ps 脚本上传 修改配置文件 123456[root@common001-dev.novalocal zabbix_agentd.conf.d]# pwd/usr/local/zabbix/etc/zabbix_agentd.conf.d[root@common001-dev.novalocal zabbix_agentd.conf.d]# cat zk_jvm.confUserParameter=zkjvm.discovery, /usr/local/zabbix/scripts/get_zk_jvm.py discoveryUserParameter=zkjvm.status[*], /usr/local/zabbix/scripts/get_zk_jvm.py $1 $2 重启agent]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维监控体系</category>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python从入门到实践]]></title>
    <url>%2F2019%2F06%2F18%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%2FPython%2Fpython%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[参考文献： https://wiki.python.org/moin/BeginnersGuide/Overview 学习编程的方法人学习东西，天然有一种特性，就是学到的知识不马上应用的话，就会很快忘记。因此采集学习大量的知识，再去应用的方式是不可取的，比较浪费时间，而实际情况是，很多人往往就会陷入这种误区。 特别是现在的IT培训机构，在短时间内，灌输大量的知识，只是做一些简单的案例练习，这样是无法真正掌握这些知识的，只是走马观花的看一遍。 看了网上的，以及结合自身的实际情况来看，比较合适的一个学习方式是：项目驱动式去学习。 也就是，由输出来决定输入。实现一个项目，在实现这个项目中，会涉及很多需要学习的知识，在这个时候，再去学习这些知识，学完之后马上就能得到应用。 把项目驱动式的方式进程拆解，其实就是一种从上到下的树形图。 根节点是项目，也就是要解决或者要实现的问题或者需求。往下的每一个部分都代表了学习过程中遇到的问题，每个问题其实就是单独的需要去学习的知识点。 它的好处就是让你知道了你应该去学什么，而不是先学一大堆有用的或者没用的知识，然后再来做项目。 存在的问题 这种学些方式，存在一个天然的问题，就是学习到的知识都是分散的，可能也就是只是学习了这个项目需要要就好了。每一个部分的知识都是非常零碎的，不成体系和结构。 python特性Some programming-language features of Python are: A variety of basic data types are available: numbers (floating point, complex, and unlimited-length long integers), strings (both ASCII and Unicode), lists, and dictionaries. Python supports object-oriented programming with classes and multiple inheritance. Code can be grouped into modules and packages. The language supports raising and catching exceptions, resulting in cleaner error handling. Data types are strongly and dynamically typed. Mixing incompatible types (e.g. attempting to add a string and a number) causes an exception to be raised, so errors are caught sooner. Python contains advanced programming features such as generators and list comprehensions. Python’s automatic memory management frees you from having to manually allocate and free memory in your code. 下面我们会把一些重要的知识点单独拿出来详细说明 强数据类型以及动态类型官方描述： 1Data types are strongly and dynamically typed. Mixing incompatible types (e.g. attempting to add a string and a number) causes an exception to be raised, so errors are caught sooner. Python是一个强数据类型以及动态类型的编程语言。 强数据类型： 一个变量在被赋值时会获得一个数据类型，如果不经过强制转换，那么它将永远会是这个类型、 测试代码： 123a = 1b = &quot;2&quot;print (a+b) 执行后输出如下： 1234Traceback (most recent call last): File &quot;/Users/wangxiaohua/PycharmProjects/private_project/test/test.py&quot;, line 152, in &lt;module&gt; print (a+b)TypeError: unsupported operand type(s) for +: &apos;int&apos; and &apos;str&apos; 弱数据类型： 不允许隐式转换的是强类型，允许隐式转换的是弱类型。 有强就有弱，弱语言类型的编程语言在执行时，数据类型可以被忽略，我们直接看一个例子 测试代码： 1234567➜ script cat test.sh#!/bin/basha=1c=dddecho $a+$c➜ script sh test.sh1+ddd 也就是说，它与强类型定义语言相反, 一个变量可以赋不同数据类型的值。 常见的强弱数据类型编程语言 强数据类型语言：Java/C#/python 弱数据类型语言：C/C++/PHP/Perl/JavaScript/Unix Shell 动态类型： 运行期间才会做数据类型检查。也就是说只会在程序运行时，执行到变量赋值的代码时，才将该变量的数据类型进行记录。 因此不用在变量定义的时候指定变量的数据类型 静态类型： 同样的，有静就有动，静态类型的编程语言，在编译期间就知道变量的类型，例如在C语言中要使用： 1int age = 18; 这样的方式来定义变量 总结 下图是常见的语言类型的划分： 由于强类型语言一般需要在运行时运行一套类型检查系统，因此强类型语言的速度一般比弱类型要慢，动态类型也比静态类型慢，因此常见的四种语言中执行的速度应该是 C &gt; Java &gt; JavaScript &gt; Python。 强类型，静态类型的语言写起来往往是最安全的。 生成器及列表解析官方描述 1Python contains advanced programming features such as generators and list comprehensions. python安装在centos 7中，默认安装的python版本为2.7,一般情况下，我们都需要对python进行升级 12345[root@master ~]# cat /etc/redhat-release CentOS Linux release 7.3.1611 (Core)[root@master ~]# python --version Python 2.7.5 环境说明12345678910[root@master ~]# which python /usr/bin/python[root@master ~]# ll /usr/bin/pythonlrwxrwxrwx 1 root root 7 Apr 13 16:50 /usr/bin/python -&gt; python2[root@master ~]# ll /usr/bin/python2lrwxrwxrwx 1 root root 9 Apr 13 16:50 /usr/bin/python2 -&gt; python2.7[root@master ~]# ll /usr/bin/python2.7-rwxr-xr-x 1 root root 7136 Aug 4 2017 /usr/bin/python2.7 我们知道我们的python命令是在/usr/bin目录下 可以看到，python指向的是python2，python2指向的是python2.7 因此我们可以装个python3，然后将python指向python3，然后python2指向python2.7，那么两个版本的python就能共存了。 正式安装下载python3的源码包 1wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz 解压编译安装 12[root@master software]# tar -zxvf Python-3.6.5.tgz [root@master software]# cd Python-3.6.5 ​ 12[root@master Python-3.6.5]# ./configure --prefix=/usr/local/python3[root@master Python-3.6.5]# make &amp;&amp; make install 添加软链接 1[root@master Python-3.6.5]# mv /usr/bin/python /usr/bin/python.bak ​ 1234[root@master Python-3.6.5]# ln -s /usr/local/python3/bin/python3.6 /usr/bin/python[root@master Python-3.6.5]# python --version Python 3.6.5 补充操作更改yum配置安装完毕之后，我们需要修改yum的配置，因为其要使用python2执行，此时我们修改了python的指向路径，不修改则会导致yum无法正常使用。 vim /usr/bin/yum 1把#! /usr/bin/python修改为#! /usr/bin/python2 vim /usr/libexec/urlgrabber-ext-down 1把#! /usr/bin/python 修改为#! /usr/bin/python2 安装pip默认情况下，安装python2之后，bin目录下是不会有pip的，需要我们额外进行安装 这里以安装pip 9.0.3版本为例 从pypi上找到对应的包，根据官方文档进行安装： 12# curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py# /application/python/bin/python get-pip.py pip==9.0.3 安装过程如下： 1234567891011121314[root@shidc-1 ~]# /application/python/bin/python get-pip.py pip==9.0.3DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won&apos;t be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-supportCollecting pip==9.0.3 Downloading https://files.pythonhosted.org/packages/ac/95/a05b56bb975efa78d3557efa36acaf9cf5d2fd0ee0062060493687432e03/pip-9.0.3-py2.py3-none-any.whl (1.4MB) |████████████████████████████████| 1.4MB 36kB/sCollecting setuptools Downloading https://files.pythonhosted.org/packages/54/28/c45d8b54c1339f9644b87663945e54a8503cfef59cf0f65b3ff5dd17cf64/setuptools-42.0.2-py2.py3-none-any.whl (583kB) |████████████████████████████████| 583kB 18kB/sCollecting wheel Downloading https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whlInstalling collected packages: pip, setuptools, wheelSuccessfully installed pip-9.0.3 setuptools-42.0.2 wheel-0.33.6WARNING: You are using pip version 9.0.3; however, version 19.3.1 is available.You should consider upgrading via the &apos;pip install --upgrade pip&apos; command. 安装完成的结果： 1234[root@shidc-1 ~]# ll /application/python/bin/pip*-rwxr-xr-x. 1 root root 216 12月 23 10:59 /application/python/bin/pip-rwxr-xr-x. 1 root root 216 12月 23 10:59 /application/python/bin/pip2-rwxr-xr-x. 1 root root 216 12月 23 10:59 /application/python/bin/pip2.7 unicode问题常用模块re-正则匹配模块正则表达式是一个特殊的字符序列，它能帮助你方便的检查一个字符串是否与某种模式匹配。 Python 自1.5版本起增加了re 模块，它提供 Perl 风格的正则表达式模式。 re.match函数re.match 尝试从字符串的起始位置匹配一个模式 如果没有匹配成功的话，match()就返回none。 如果匹配成功的话，返回匹配对象 函数语法： 1re.match(pattern, string, flags=0) 函数参数说明： 参数 描述 pattern 匹配的正则表达式 string 要匹配的字符串。 flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志 我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。 匹配对象方法 描述 group(num=0) 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组。 groups() 返回一个包含所有小组字符串的元组，从 1 到 所含的小组号。 例如： 12345678import retest_str = &quot;wo shi wangxiaohua,wangxiaohua shi wo&quot;a = re.match(&apos;wangxiaohua&apos;,test_str)b = re.match(&apos;w&apos;,test_str)print (a)print(b) 执行后的结果是： 12None&lt;re.Match object; span=(0, 1), match=&apos;w&apos;&gt; 当执行group方法是，会表现为： 12345678910print (a.group())输出为：Traceback (most recent call last): File &quot;/Users/wangxiaohua/PycharmProjects/private_project/learn_python/module-re.py&quot;, line 12, in &lt;module&gt; print (a.group())AttributeError: &apos;NoneType&apos; object has no attribute &apos;group&apos;print (b.group())输出为：w re.search方法re.search 扫描整个字符串并返回第一个成功的匹配。 函数语法： 1re.search(pattern, string, flags=0) 函数参数说明： 参数 描述 pattern 匹配的正则表达式 string 要匹配的字符串。 flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。 匹配成功re.search方法返回一个匹配的对象，否则返回None。 我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。 匹配对象方法 描述 group(num=0) 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组。 groups() 返回一个包含所有小组字符串的元组，从 1 到 所含的小组号。 re.match与re.search的区别re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。 检索和替换Python 的 re 模块提供了re.sub用于替换字符串中的匹配项。 语法： 1re.sub(pattern, repl, string, count=0, flags=0) 参数： pattern : 正则中的模式字符串。 repl : 替换的字符串，也可为一个函数。 string : 要被查找替换的原始字符串。 count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。 示例： 1234567891011121314#!/usr/bin/python# -*- coding: UTF-8 -*- import re phone = &quot;2004-959-559 # 这是一个国外电话号码&quot; # 删除字符串中的 Python注释 num = re.sub(r&apos;#.*$&apos;, &quot;&quot;, phone)print &quot;电话号码是: &quot;, num # 删除非数字(-)的字符串 num = re.sub(r&apos;\D&apos;, &quot;&quot;, phone)print &quot;电话号码是 : &quot;, num 执行后输出如下： 12电话号码是: 2004-959-559 电话号码是 : 2004959559 findall在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。 注意： match 和 search 是匹配一次，而 findall是匹配所有。 语法格式为： 1findall(string[, pos[, endpos]]) 参数： string : 待匹配的字符串。 pos : 可选参数，指定字符串的起始位置，默认为 0。 endpos : 可选参数，指定字符串的结束位置，默认为字符串的长度。 re.finditer和 findall 类似，在字符串中找到正则表达式所匹配的所有子串，并把它们作为一个迭代器返回。 1re.finditer(pattern, string, flags=0) 参数： 参数 描述 pattern 匹配的正则表达式 string 要匹配的字符串。 flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志 re.splitsplit 方法按照能够匹配的子串将字符串分割后返回列表，它的使用形式如下： 1re.split(pattern, string[, maxsplit=0, flags=0]) 参数： 参数 描述 pattern 匹配的正则表达式 string 要匹配的字符串。 maxsplit 分隔次数，maxsplit=1 分隔一次，默认为 0，不限制次数。 flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志 示例： 12345678910&gt;&gt;&gt;import re&gt;&gt;&gt; re.split(&apos;\W+&apos;, &apos;runoob, runoob, runoob.&apos;)[&apos;runoob&apos;, &apos;runoob&apos;, &apos;runoob&apos;, &apos;&apos;]&gt;&gt;&gt; re.split(&apos;(\W+)&apos;, &apos; runoob, runoob, runoob.&apos;) [&apos;&apos;, &apos; &apos;, &apos;runoob&apos;, &apos;, &apos;, &apos;runoob&apos;, &apos;, &apos;, &apos;runoob&apos;, &apos;.&apos;, &apos;&apos;]&gt;&gt;&gt; re.split(&apos;\W+&apos;, &apos; runoob, runoob, runoob.&apos;, 1) [&apos;&apos;, &apos;runoob, runoob, runoob.&apos;] &gt;&gt;&gt; re.split(&apos;a*&apos;, &apos;hello world&apos;) # 对于一个找不到匹配的字符串而言，split 不会对其作出分割[&apos;hello world&apos;] 正则表达式修饰符 - 可选标志正则表达式可以包含一些可选标志修饰符（属性）来控制匹配的模式。修饰符被指定为一个可选的标志。多个标志可以通过按位 OR(|) 它们来指定。如 re.I | re.M 被设置成 I 和 M 标志： 修饰符 描述 re.I 大小写不敏感 re.L 做本地化识别（locale-aware）匹配 re.M 多行匹配，影响 ^ 和 $ re.S 使 . 匹配包括换行在内的所有字符 re.U 根据Unicode字符集解析字符。这个标志影响 \w, \W, \b, \B. re.X 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。 正则表达式模式模式字符串使用特殊的语法来表示一个正则表达式： 字母和数字表示他们自身。一个正则表达式模式中的字母和数字匹配同样的字符串。 多数字母和数字前加一个反斜杠时会拥有不同的含义。 标点符号只有被转义时才匹配自身，否则它们表示特殊的含义。 反斜杠本身需要使用反斜杠转义。 由于正则表达式通常都包含反斜杠，所以你最好使用原始字符串来表示它们。模式元素(如 r’\t’，等价于 ‘\t’)匹配相应的特殊字符。 下表列出了正则表达式模式语法中的特殊元素。如果你使用模式的同时提供了可选的标志参数，某些模式元素的含义会改变。 模式 描述 ^ 匹配字符串的开头 $ 匹配字符串的末尾。 . 匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。 […] 用来表示一组字符,单独列出：[amk] 匹配 ‘a’，’m’或’k’ [^…] 不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符。 re* 匹配0个或多个的表达式。 re+ 匹配1个或多个的表达式。 re? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式 re{ n} 精确匹配 n 个前面表达式。例如， o{2} 不能匹配 “Bob” 中的 “o”，但是能匹配 “food” 中的两个 o。 re{ n,} 匹配 n 个前面表达式。例如， o{2,} 不能匹配”Bob”中的”o”，但能匹配 “foooood”中的所有 o。”o{1,}” 等价于 “o+”。”o{0,}” 则等价于 “o*”。 re{ n, m} 匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式 a\ b 匹配a或b (re) 匹配括号内的表达式，也表示一个组 (?imx) 正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域。 (?-imx) 正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域。 (?: re) 类似 (…), 但是不表示一个组 (?imx: re) 在括号中使用i, m, 或 x 可选标志 (?-imx: re) 在括号中不使用i, m, 或 x 可选标志 (?#…) 注释. (?= re) 前向肯定界定符。如果所含正则表达式，以 … 表示，在当前位置成功匹配时成功，否则失败。但一旦所含表达式已经尝试，匹配引擎根本没有提高；模式的剩余部分还要尝试界定符的右边。 (?! re) 前向否定界定符。与肯定界定符相反；当所含表达式不能在字符串当前位置匹配时成功 (?&gt; re) 匹配的独立模式，省去回溯。 \w 匹配字母数字及下划线 \W 匹配非字母数字及下划线 \s 匹配任意空白字符，等价于 [\t\n\r\f]. \S 匹配任意非空字符 \d 匹配任意数字，等价于 [0-9]. \D 匹配任意非数字 \A 匹配字符串开始 \Z 匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串。 \z 匹配字符串结束 \G 匹配最后匹配完成的位置。 \b 匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’。 \B 匹配非单词边界。’er\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’。 \n, \t, 等. 匹配一个换行符。匹配一个制表符。等 \1…\9 匹配第n个分组的内容。 \10 匹配第n个分组的内容，如果它经匹配。否则指的是八进制字符码的表达式。 正则表达式实例字符匹配 实例 描述 python 匹配 “python”. 字符类 实例 描述 [Pp]ython 匹配 “Python” 或 “python” rub[ye] 匹配 “ruby” 或 “rube” [aeiou] 匹配中括号内的任意一个字母 [0-9] 匹配任何数字。类似于 [0123456789] [a-z] 匹配任何小写字母 [A-Z] 匹配任何大写字母 [a-zA-Z0-9] 匹配任何字母及数字 [^aeiou] 除了aeiou字母以外的所有字符 [^0-9] 匹配除了数字外的字符 特殊字符类 实例 描述 . 匹配除 “\n” 之外的任何单个字符。要匹配包括 ‘\n’ 在内的任何字符，请使用象 ‘[.\n]’ 的模式。 \d 匹配一个数字字符。等价于 [0-9]。 \D 匹配一个非数字字符。等价于 [^0-9]。 \s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。 \S 匹配任何非空白字符。等价于 [^ \f\n\r\t\v]。 \w 匹配包括下划线的任何单词字符。等价于’[A-Za-z0-9_]’。 \W 匹配任何非单词字符。等价于 ‘[^A-Za-z0-9_]’。 json模块JSON(JavaScript Object Notation) 是一种基于文本的轻量级数据交换格式，易于人阅读和编写。 主谓宾重点：json是一种数据的描述表达和使用的格式、规范。 json模块的功能作用使用该模块，实现： 解码：解释json字符串数据以供程序使用。将字符串或者文件中的数据内容（内容格式必须是json规范)，转换为python对象 编码：生成json格式数据。将json对象（字典、列表等）转换成为字符串 函数 描述 json.dumps 将 Python 对象编码成 JSON 字符串 json.loads 将已编码的 JSON 字符串解码为 Python 对象 码：数据、信息 特点JSON 文本格式在语法上与创建 JavaScript 对象的代码相同。 由于这种相似性，无需解析器，JavaScript 程序能够使用内建的 eval() 函数，用 JSON 数据来生成原生的 JavaScript 对象。 json.dumps-编码json.dumps 用于将 Python 对象编码成 JSON 字符串。 语法 1json.dumps(obj, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding=&quot;utf-8&quot;, default=None, sort_keys=False, **kw) 示例代码： 123456789101112# 编码data_list = [ &#123; &apos;a&apos; : 1, &apos;b&apos; : 2, &apos;c&apos; : 3, &apos;d&apos; : 4, &apos;e&apos; : 5 &#125; ]data_dict = &#123; &apos;a&apos; : 1, &apos;b&apos; : 2, &apos;c&apos; : 3, &apos;d&apos; : 4, &apos;e&apos; : 5 &#125;a = json.dumps(data_list)b = json.dumps(data_dict)print (type(a),a)print (type(b),b)执行后输出为：&lt;class &apos;str&apos;&gt; [&#123;&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3, &quot;d&quot;: 4, &quot;e&quot;: 5&#125;]&lt;class &apos;str&apos;&gt; &#123;&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3, &quot;d&quot;: 4, &quot;e&quot;: 5&#125; python 原始类型向 json 类型的转化对照表： Python JSON dict object list, tuple array str, unicode string int, long, float number True true False false None null json.loads-解码json.loads 用于解码 JSON 数据。该函数返回 Python 字段的数据类型。 语法 1json.loads(s[, encoding[, cls[, object_hook[, parse_float[, parse_int[, parse_constant[, object_pairs_hook[, **kw]]]]]]]]) 示例代码： 12345678# 解码data = &apos;[ &#123;&quot;a&quot;:1,&quot;b&quot;:2&#125;]&apos;a = json.loads(data)print (type(a),a)执行后输出如下：&lt;class &apos;list&apos;&gt; [&#123;&apos;a&apos;: 1, &apos;b&apos;: 2&#125;]&lt;class &apos;list&apos;&gt; json 类型转换到 python 的类型对照表： JSON Python object dict array list string unicode number (int) int, long number (real) float true True false False null None json对象json格式的字符串需要转换成为一个对象之后，才可以被调用。这个步骤也就是loads解码 例如： 123json_string = &apos;&#123;&quot;first_name&quot;: &quot;Guido&quot;, &quot;last_name&quot;:&quot;Rossum&quot;&#125;&apos;json_string = json.loads(json_string)print (json_string[&apos;first_name&apos;]) 如果没有使用json.loads进行实例化对象，那么print是会报错的，报错为： 1234Traceback (most recent call last): File &quot;/Users/wangxiaohua/PycharmProjects/dnamed/lib/error.py&quot;, line 18, in &lt;module&gt; print (json_string[&apos;first_name&apos;])TypeError: string indices must be integers dumps() 和dump()等的区别通过前面的内容，我们知道dumps是json编码的过程，将python对象编码成为字符串，loads是解码过程，将字符串编程python对象，但是上面的例子都是的数据来源全都是变量，也就是将结果写到变量里面或者从变量里面获取信息。 但是在实际情况中，我们可能会涉及到文件的操作，例如将编码后的数据写入到文件当中，或者从文件当中获取数据去生产python对象，这个时候就涉及到dump和load。 dump和load也是类似的功能，只是与文件操作结合起来了。在使用的时候，要多加一个文件对象参数 requests模块实现功能及解决问题request为python的内置模块，实现功能： 发送HTTP请求（get、post、delete等等） 获取一个返回对象，然后针对这个对象做一系列的操作，例如获取参数及数据等 语法语法部分也就是使用这个语法的逻辑思路和规则 我们先看一个案例： 一开始要导入 Requests 模块： 1&gt;&gt;&gt; import requests 然后，尝试获取某个网页。本例子中，我们来获取 Github 的公共时间线： 1&gt;&gt;&gt; r = requests.get(&apos;https://api.github.com/events&apos;) 现在，我们有一个名为 r 的 Response 对象。我们可以从这个对象中获取所有我们想要的信息。 Requests 简便的 API 意味着所有 HTTP 请求类型都是显而易见的。例如，你可以这样发送一个 HTTP POST 请求： 1&gt;&gt;&gt; r = requests.post(&apos;http://httpbin.org/post&apos;, data = &#123;&apos;key&apos;:&apos;value&apos;&#125;) 那么其他 HTTP 请求类型：PUT，DELETE，HEAD 以及 OPTIONS 又是如何的呢？都是一样的简单： 1234&gt;&gt;&gt; r = requests.put(&apos;http://httpbin.org/put&apos;, data = &#123;&apos;key&apos;:&apos;value&apos;&#125;)&gt;&gt;&gt; r = requests.delete(&apos;http://httpbin.org/delete&apos;)&gt;&gt;&gt; r = requests.head(&apos;http://httpbin.org/get&apos;)&gt;&gt;&gt; r = requests.options(&apos;http://httpbin.org/get&apos;) 传递URL参数你也许经常想为 URL 的查询字符串(query string)传递某种数据。如果你是手工构建 URL，那么数据会以键/值对的形式置于 URL 中，跟在一个问号的后面。例如， httpbin.org/get?key=val。 Requests 允许你使用 params 关键字参数，以一个字符串字典来提供这些参数。举例来说，如果你想传递 key1=value1 和 key2=value2 到 httpbin.org/get ，那么你可以使用如下代码： 12&gt;&gt;&gt; payload = &#123;&apos;key1&apos;: &apos;value1&apos;, &apos;key2&apos;: &apos;value2&apos;&#125;&gt;&gt;&gt; r = requests.get(&quot;http://httpbin.org/get&quot;, params=payload) 通过打印输出该 URL，你能看到 URL 已被正确编码： 12&gt;&gt;&gt; print(r.url)http://httpbin.org/get?key2=value2&amp;key1=value1 注意字典里值为 None 的键都不会被添加到 URL 的查询字符串里。 你还可以将一个列表作为值传入： 12345&gt;&gt;&gt; payload = &#123;&apos;key1&apos;: &apos;value1&apos;, &apos;key2&apos;: [&apos;value2&apos;, &apos;value3&apos;]&#125;&gt;&gt;&gt; r = requests.get(&apos;http://httpbin.org/get&apos;, params=payload)&gt;&gt;&gt; print(r.url)http://httpbin.org/get?key1=value1&amp;key2=value2&amp;key2=value3 响应内容我们能读取服务器响应的内容。再次以 GitHub 时间线为例： 1234&gt;&gt;&gt; import requests&gt;&gt;&gt; r = requests.get(&apos;https://api.github.com/events&apos;)&gt;&gt;&gt; r.textu&apos;[&#123;&quot;repository&quot;:&#123;&quot;open_issues&quot;:0,&quot;url&quot;:&quot;https://github.com/... Requests 会自动解码来自服务器的内容。大多数 unicode 字符集都能被无缝地解码。 请求发出后，Requests 会基于 HTTP 头部对响应的编码作出有根据的推测。当你访问 r.text 之时，Requests 会使用其推测的文本编码。你可以找出 Requests 使用了什么编码，并且能够使用r.encoding 属性来改变它： 123&gt;&gt;&gt; r.encoding&apos;utf-8&apos;&gt;&gt;&gt; r.encoding = &apos;ISO-8859-1&apos; 如果你改变了编码，每当你访问 r.text ，Request 都将会使用 r.encoding 的新值。你可能希望在使用特殊逻辑计算出文本的编码的情况下来修改编码。比如 HTTP 和 XML 自身可以指定编码。这样的话，你应该使用 r.content 来找到编码，然后设置 r.encoding 为相应的编码。这样就能使用正确的编码解析 r.text 了。 在你需要的情况下，Requests 也可以使用定制的编码。如果你创建了自己的编码，并使用 codecs模块进行注册，你就可以轻松地使用这个解码器名称作为 r.encoding 的值， 然后由 Requests 来为你处理编码。 二进制响应内容你也能以字节的方式访问请求响应体，对于非文本请求： 12&gt;&gt;&gt; r.contentb&apos;[&#123;&quot;repository&quot;:&#123;&quot;open_issues&quot;:0,&quot;url&quot;:&quot;https://github.com/... Requests 会自动为你解码 gzip 和 deflate 传输编码的响应数据。 例如，以请求返回的二进制数据创建一张图片，你可以使用如下代码： 1234&gt;&gt;&gt; from PIL import Image&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; i = Image.open(BytesIO(r.content)) JSON 响应内容Requests 中也有一个内置的 JSON 解码器，助你处理 JSON 数据： 12345&gt;&gt;&gt; import requests&gt;&gt;&gt; r = requests.get(&apos;https://api.github.com/events&apos;)&gt;&gt;&gt; r.json()[&#123;u&apos;repository&apos;: &#123;u&apos;open_issues&apos;: 0, u&apos;url&apos;: &apos;https://github.com/... 如果 JSON 解码失败， r.json() 就会抛出一个异常。例如，响应内容是 401 (Unauthorized)，尝试访问 r.json() 将会抛出 ValueError: No JSON object could be decoded 异常。 需要注意的是，成功调用 r.json() 并不意味着响应的成功。有的服务器会在失败的响应中包含一个 JSON 对象（比如 HTTP 500 的错误细节）。这种 JSON 会被解码返回。要检查请求是否成功，请使用 r.raise_for_status() 或者检查 r.status_code 是否和你的期望相同。 原始响应内容在罕见的情况下，你可能想获取来自服务器的原始套接字响应，那么你可以访问 r.raw。 如果你确实想这么干，那请你确保在初始请求中设置了 stream=True。具体你可以这么做： 12345&gt;&gt;&gt; r = requests.get(&apos;https://api.github.com/events&apos;, stream=True)&gt;&gt;&gt; r.raw&lt;requests.packages.urllib3.response.HTTPResponse object at 0x101194810&gt;&gt;&gt;&gt; r.raw.read(10)&apos;\x1f\x8b\x08\x00\x00\x00\x00\x00\x00\x03&apos; 但一般情况下，你应该以下面的模式将文本流保存到文件： 123with open(filename, &apos;wb&apos;) as fd: for chunk in r.iter_content(chunk_size): fd.write(chunk) 使用 Response.iter_content 将会处理大量你直接使用 Response.raw 不得不处理的。 当流下载时，上面是优先推荐的获取内容方式。 Note that chunk_size can be freely adjusted to a number that may better fit your use cases. 定制请求头如果你想为请求添加 HTTP 头部，只要简单地传递一个 dict 给 headers 参数就可以了。 例如，在前一个示例中我们没有指定 content-type: 1234&gt;&gt;&gt; url = &apos;https://api.github.com/some/endpoint&apos;&gt;&gt;&gt; headers = &#123;&apos;user-agent&apos;: &apos;my-app/0.0.1&apos;&#125;&gt;&gt;&gt; r = requests.get(url, headers=headers) 注意: 定制 header 的优先级低于某些特定的信息源，例如： 如果在 .netrc 中设置了用户认证信息，使用 headers= 设置的授权就不会生效。而如果设置了 auth= 参数，.netrc 的设置就无效了。 如果被重定向到别的主机，授权 header 就会被删除。 代理授权 header 会被 URL 中提供的代理身份覆盖掉。 在我们能判断内容长度的情况下，header 的 Content-Length 会被改写。 更进一步讲，Requests 不会基于定制 header 的具体情况改变自己的行为。只不过在最后的请求中，所有的 header 信息都会被传递进去。 注意: 所有的 header 值必须是 string、bytestring 或者 unicode。尽管传递 unicode header 也是允许的，但不建议这样做。 更加复杂的 POST 请求通常，你想要发送一些编码为表单形式的数据——非常像一个 HTML 表单。要实现这个，只需简单地传递一个字典给 data 参数。你的数据字典在发出请求时会自动编码为表单形式： 123456789101112&gt;&gt;&gt; payload = &#123;&apos;key1&apos;: &apos;value1&apos;, &apos;key2&apos;: &apos;value2&apos;&#125;&gt;&gt;&gt; r = requests.post(&quot;http://httpbin.org/post&quot;, data=payload)&gt;&gt;&gt; print(r.text)&#123; ... &quot;form&quot;: &#123; &quot;key2&quot;: &quot;value2&quot;, &quot;key1&quot;: &quot;value1&quot; &#125;, ...&#125; 你还可以为 data 参数传入一个元组列表。在表单中多个元素使用同一 key 的时候，这种方式尤其有效： 12345678910111213&gt;&gt;&gt; payload = ((&apos;key1&apos;, &apos;value1&apos;), (&apos;key1&apos;, &apos;value2&apos;))&gt;&gt;&gt; r = requests.post(&apos;http://httpbin.org/post&apos;, data=payload)&gt;&gt;&gt; print(r.text)&#123; ... &quot;form&quot;: &#123; &quot;key1&quot;: [ &quot;value1&quot;, &quot;value2&quot; ] &#125;, ...&#125; 很多时候你想要发送的数据并非编码为表单形式的。如果你传递一个 string 而不是一个 dict，那么数据会被直接发布出去。 例如，Github API v3 接受编码为 JSON 的 POST/PATCH 数据： 123456&gt;&gt;&gt; import json&gt;&gt;&gt; url = &apos;https://api.github.com/some/endpoint&apos;&gt;&gt;&gt; payload = &#123;&apos;some&apos;: &apos;data&apos;&#125;&gt;&gt;&gt; r = requests.post(url, data=json.dumps(payload)) 此处除了可以自行对 dict 进行编码，你还可以使用 json 参数直接传递，然后它就会被自动编码。这是 2.4.2 版的新加功能： 1234&gt;&gt;&gt; url = &apos;https://api.github.com/some/endpoint&apos;&gt;&gt;&gt; payload = &#123;&apos;some&apos;: &apos;data&apos;&#125;&gt;&gt;&gt; r = requests.post(url, json=payload) 注意事项 在request中向服务端传入参数及数据时，数据主体部分的数据类型根据服务端的要求进行转化，如果服务端接收字典，那么直接传输字典即可，如果是为json，那么使用json.dumps(data)进行转化。将字典进行转换成为json字符串的形式。由于发送json格式数据太常见了，所以在Requests模块的高版本中，又加入了json这个关键字参数，可以直接发送json数据给post请求而不用再使用json模块了，直接使用json=upload_data_dict，它会自动转换 在向服务端传输参数及数据时，get等请求的参数关键字是params；POST的关键字是data time模块在平常的代码中，我们常常需要与时间打交道。在Python中，与时间处理有关的模块包括：time，datetime以及calendar。 在开始之前，首先要说明这几点： 在Python中，通常有这几种方式来表示时间： 1）时间戳 2）格式化的时间字符串 3）元组（struct_time）共九个元素。由于Python的time模块实现主要调用C库，所以各个平台可能有所不同。 UTC（Coordinated Universal Time，世界协调时）亦即格林威治天文时间，世界标准时间。在中国为UTC+8。DST（Daylight Saving Time）即夏令时。 时间戳（timestamp）的方式：通常来说，时间戳表示的是从1970年1月1日00:00:00开始按秒计算的偏移量。我们运行“type(time.time())”，返回的是float类型。返回时间戳方式的函数主要有time()，clock()等。 元组（struct_time）方式：struct_time元组共有9个元素，返回struct_time的函数主要有gmtime()，localtime()，strptime()。下面列出这种方式元组中的几个元素： 索引（Index） 属性（Attribute） 值（Values） 0 tm_year（年） 比如2011 1 tm_mon（月） 1 - 12 2 tm_mday（日） 1 - 31 3 tm_hour（时） 0 - 23 4 tm_min（分） 0 - 59 5 tm_sec（秒） 0 - 61 6 tm_wday（weekday） 0 - 6（0表示周日） 7 tm_yday（一年中的第几天） 1 - 366 8 tm_isdst（是否是夏令时） 默认为-1 获取当前时间戳123print (time.time())输出如下：1531317129.0039742 时间元祖123print (time.localtime())输出如下：time.struct_time(tm_year=2018, tm_mon=7, tm_mday=11, tm_hour=22, tm_min=4, tm_sec=18, tm_wday=2, tm_yday=192, tm_isdst=0) 格式化时间最简单的获取可读模式的方法是asctime() 123print (time.asctime())输出如下：Wed Jul 11 21:52:09 2018 使用指定的格式输出，使用strftime方法 123print (time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;,time.localtime()))输入如下：2018-07-11 22:02:27 python中时间日期格式化符号 %y 两位数的年份表示（00-99） %Y 四位数的年份表示（000-9999） %m 月份（01-12） %d 月内中的一天（0-31） %H 24小时制小时数（0-23） %I 12小时制小时数（01-12） %M 分钟数（00=59） %S 秒（00-59） %a 本地简化星期名称 %A 本地完整星期名称 %b 本地简化的月份名称 %B 本地完整的月份名称 %c 本地相应的日期表示和时间表示 %j 年内的一天（001-366） %p 本地A.M.或P.M.的等价符 %U 一年中的星期数（00-53）星期天为星期的开始 %w 星期（0-6），星期天为星期的开始 %W 一年中的星期数（00-53）星期一为星期的开始 %x 本地相应的日期表示 %X 本地相应的时间表示 %Z 当前时区的名称 %% %号本身 sleepPython time sleep() 方法推迟调用线程的运行，可通过参数secs指秒数，表示进程挂起的时间。 语法如下： 1time.sleep(t) commands模块-3.x已废弃用Python写运维脚本时，经常需要执行linux shell的命令，Python中的commands模块专门用于调用Linux shell命令，并返回状态和结果。 下面是commands模块的几个主要方法： commands.getoutput(‘shell command’)执行shell命令，返回结果（string类型） 案例如下： 12345678910111213输出指定进程的pidwxh@wxh-virtual-machine:~/python_files$ cat tt.py#!/usr/bin/env python2import sys,commandscmdline = sys.argv[1]cmdline1 = sys.argv[2]cmd = &quot;ps -ef|grep &quot; + cmdline + &quot;|grep &quot; + cmdline1 + &quot;|grep -v grep|grep -v python|awk &apos;&#123;print $2&#125;&apos;&quot;c1 = commands.getoutput(cmd)print (c1)print (type(c1)) 执行后输出如下所示： 1234wxh@wxh-virtual-machine:~/python_files$ python2 ./tt.py unity-panel-service lockscreen-mode126263&lt;type &apos;str&apos;&gt; commands.getstatusoutput(‘shell command’)执行shell命令, 返回两个元素的元组tuple(status, result)，status为int类型，result为string类型。 因为cmd的执行方式是{ cmd ; } 2&gt;&amp;1, 故返回结果包含标准输出和标准错误. 第一个值为命令执行的返回状态码，执行成功则返回的是0，不成功则返回的是非0 案例如下： 123456789wxh@wxh-virtual-machine:~/python_files$ cat tt.py #!/usr/bin/env python2import sys,commandscmdline = sys.argv[1]cmdline1 = sys.argv[2]cmd = &quot;ps -ef|grep &quot; + cmdline + &quot;|grep &quot; + cmdline1 + &quot;|grep -v grep|grep -v python|awk &apos;&#123;print $2&#125;&apos;&quot;res = commands.getstatusoutput(cmd)print (res) 执行后输出如下所示： 12wxh@wxh-virtual-machine:~/python_files$ python2 ./tt.py unity-panel-service lockscreen-mode(0, &apos;126263&apos;) subprocess模块从Python 2.4开始，Python引入subprocess模块来管理子进程，以取代一些旧模块的方法：如 os.system、os.spawn*、os.popen*、popen2.*、commands.*等不但可以调用外部的命令作为子进程，而且可以连接到子进程的input/output/error管道，获取相关的返回信息。 运行python的时候，我们都是在创建并运行一个进程。像Linux进程那样，一个进程可以fork一个子进程，并让这个子进程exec另外一个程序。在Python中，我们通过标准库中的subprocess包来fork一个子进程，并运行一个外部的程序。 subprocess包中定义有数个创建子进程的函数，这些函数分别以不同的方式创建子进程，所以我们可以根据需要来从中选取一个使用。另外subprocess还提供了一些管理标准流(standard stream)和管道(pipe)的工具，从而在进程间使用文本通信。 shell命令格式默认情况下，我们传入的参数（一般是shell命令）是需要使用列表的方式，以空格分开的每一个单独的命令或者参数，是列表的每一个元素 因为subprocess模块中的各个类和函数的shell参数默认为False，在Linux下，shell=False时, Popen调用os.execvp()执行args指定的程序； shell=True时，如果args是字符串，Popen直接调用系统的Shell来执行args指定的程序，如果args是一个序列，则args的第一项是定义程序命令字符串，其它项是调用系统Shell时的附加参数。 call(),check_call()和check_output()函数subprocess.call() 父进程等待子进程完成 返回退出信息(returncode，相当于Linux exit code) subprocess.check_call() 父进程等待子进程完成 返回0 检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性，可用try…except…来检查 subprocess.check_output() 父进程等待子进程完成 返回子进程向标准输出的输出结果 检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性和output属性，output属性为标准输出的输出结果，可用try…except…来检查。 这三个函数的使用方法相类似，下面来举例说明: call()函数 12345p = subprocess.call(&apos;pdd&apos;,shell=True)print (p)执行后输出如下：127 check_call函数 12345678910p = subprocess.check_call(&apos;pdd&apos;,shell=True)print (p)执行后输出如下：Traceback (most recent call last): File &quot;/Users/wangxiaohua/PycharmProjects/private_project/test/test.py&quot;, line 31, in &lt;module&gt; p = subprocess.check_call(&apos;pdd&apos;,shell=True) File &quot;/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py&quot;, line 328, in check_call raise CalledProcessError(retcode, cmd)subprocess.CalledProcessError: Command &apos;pdd&apos; returned non-zero exit status 127. check_output() 123456789101112p = subprocess.check_output(&apos;pdd&apos;,shell=True)print (p)执行后输出如下：Traceback (most recent call last): File &quot;/Users/wangxiaohua/PycharmProjects/private_project/test/test.py&quot;, line 31, in &lt;module&gt; p = subprocess.check_output(&apos;pdd&apos;,shell=True) File &quot;/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py&quot;, line 376, in check_output **kwargs).stdout File &quot;/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py&quot;, line 468, in run output=stdout, stderr=stderr)subprocess.CalledProcessError: Command &apos;pdd&apos; returned non-zero exit status 127. subprocess.Popen类上面的几个函数都是基于Popen()的封装(wrapper)。这些封装的目的在于让我们容易使用子进程，也就是说，我们不需要关心父子进程通信和回收等问题。当我们想要更个性化我们的需求的时候，就要转向Popen类，该类生成的对象用来代表子进程。 与上面的封装不同，Popen对象创建后，主程序不会自动等待子进程完成。我们必须调用对象的wait()方法，父进程才会等待 (也就是阻塞block)，举例： 123child = subprocess.Popen(&quot;ping -c4 www.baidu.com&quot;,shell=True)# child.wait()print (111) 添加注释和不添加的2次结果分别为： 1234567891011121314111和PING www.baidu.com (115.239.211.112): 56 data bytes64 bytes from 115.239.211.112: icmp_seq=0 ttl=54 time=49.593 ms64 bytes from 115.239.211.112: icmp_seq=1 ttl=54 time=20.527 ms64 bytes from 115.239.211.112: icmp_seq=2 ttl=54 time=30.277 ms64 bytes from 115.239.211.112: icmp_seq=3 ttl=54 time=12.728 ms--- www.baidu.com ping statistics ---4 packets transmitted, 4 packets received, 0.0% packet lossround-trip min/avg/max/stddev = 12.728/28.281/49.593/13.786 ms111 一些方法和属性： 123456child.wait() # 等待子进程执行完毕child.poll() # 检查子进程状态child.kill() # 终止子进程child.send_signal() # 向子进程发送信号child.terminate() # 终止子进程child.pid # 子进程的pid 子进程的文本流控制子进程的标准输入、标准输出和标准错误如下属性分别表示: child.stdin child.stdout child.stderr 在pycharm中可以看到 可以在Popen()建立子进程的时候改变标准输入、标准输出和标准错误，并可以利用subprocess.PIPE将多个子进程的输入和输出连接在一起，构成管道(pipe)，如下2个例子： 案例1： 12345678import subprocesschild1 = subprocess.Popen([&quot;ls&quot;,&quot;-l&quot;], stdout=subprocess.PIPE)print (child1.stdout.read())#或者child1.communicate()执行后输出如下：b&apos;total 16\ndrwxr-xr-x 3 wangxiaohua staff 96 12 10 16:35 __pycache__\n-rw-r--r-- 1 wangxiaohua staff 185 12 10 16:33 custom_except.py\n-rw-r--r-- 1 wangxiaohua staff 1028 1 5 17:54 test.py\n&apos;可以看到，输出的是byte类型 案例2： 123456789import subprocesschild1 = subprocess.Popen([&quot;cat&quot;,&quot;/etc/passwd&quot;], stdout=subprocess.PIPE)child2 = subprocess.Popen([&quot;grep&quot;,&quot;0:0&quot;],stdin=child1.stdout, stdout=subprocess.PIPE)out = child2.communicate()注意: 最终的输出是从child2中获取的。 默认的输出是byte类型，如果要转换成为字符串类型，使用下面这种方式实现。out_str = str(out[0],encoding=&quot;utf-8&quot;) subprocess.PIPE实际上为文本流提供一个缓存区。child1的stdout将文本输出到缓存区，随后child2的stdin从该PIPE中将文本读取走。child2的输出文本也被存放在PIPE中，直到communicate()方法从PIPE中读取出PIPE中的文本。 注意： communicate()是Popen对象的一个方法，该方法会阻塞父进程，直到子进程完成。 communicate()函数返回一个tuple(标准输出和错误)。所以当你只是标准输出的时候就需要使用[0]获取 实际案例检测linux进程的状态1234567def redis_status(new_port): p = subprocess.Popen([&quot;netstat&quot;, &quot;-unptl&quot;], stdout=subprocess.PIPE) out, err = p.communicate() if (new_port in str(out) ): print (&quot;redis &#123;PORT&#125; instance is running...&quot;.format(PORT=new_port)) else: print (&quot;start redis &#123;PORT&#125; faild.please check again...&quot;.format(PORT=new_port)) 获取标准和错误输出在写项目的时候，难免会涉及到调用shell命令。一旦涉及到shell命令的执行，就可能会出现执行不成功的情况，这个时候，我们需要将这些输出信息打印到日志里，方便我们后续对问题进行排查 修改前的一般逻辑为： 1234567891011121314def reload_zone(self): &quot;&quot;&quot; 执行 rndc reload命令 &quot;&quot;&quot; # reload_cmd = &quot;&#123;rndc_cmd&#125; -c &#123;rndc_conf&#125; reload&quot;.format(rndc_cmd=self.rndc_cmd, rndc_conf=self.rndc_conf) try: reload_res = call(self.reload_cmd, shell=True, stdout=open(&apos;/dev/null&apos;, &apos;w&apos;), stderr=open(&apos;/dev/null&apos;, &apos;w&apos;)) if reload_res == 0: return &quot;success&quot; else: logger.error(&quot;reload zone失败，reload命令：&#123;reload_cmd&#125;&quot;.format(reload_cmd=self.reload_cmd)) return &quot;faild&quot; except Exception as e: logger.error(&quot;reload zone出现异常，reload命令：&#123;reload_cmd&#125;，异常信息：&#123;e&#125;&quot;.format(reload_cmd=self.reload_cmd,e=e)) 调用这个方法获取返回值之后，我们只能只能知道是成功还是失败。如果失败，具体的报错信息我们是看不到的。 修改之后： 12 os模块python文件处理Python open() 方法用于打开一个文件，并返回文件对象，在对文件进行处理过程都需要使用到这个函数，如果该文件无法被打开，会抛出 OSError。 注意： 使用 open() 方法一定要保证关闭文件对象，即调用 close() 方法。 open() 函数常用形式是接收两个参数：文件名(file)和模式(mode)。 语法open() Python open() 方法用于打开一个文件，并返回文件对象，在对文件进行处理过程都需要使用到这个函数，如果该文件无法被打开，会抛出 OSError。 注意： 使用 open() 方法一定要保证关闭文件对象，即调用 close() 方法。 open() 函数常用形式是接收两个参数：文件名(file)和模式(mode)。 语法格式： 1open(file, mode=&apos;r&apos;) 完整的语法格式为： 1open(file, mode=&apos;r&apos;, buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None) 参数说明: file: 必需，文件路径（相对或者绝对路径）。 mode: 可选，文件打开模式 buffering: 设置缓冲 encoding: 一般使用utf8 errors: 报错级别 newline: 区分换行符 closefd: 传入的file参数类型 opener: mode模式一共有一下几种： 模式 描述 t 文本模式 (默认)。 x 写模式，新建一个文件，如果该文件已存在则会报错。 b 二进制模式。 + 打开一个文件进行更新(可读可写)。 U 通用换行模式（不推荐）。 r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。 rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。一般用于非文本文件如图片等。 r+ 打开一个文件用于读写。文件指针将会放在文件的开头。 rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。一般用于非文本文件如图片等。 w 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 w+ 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。 ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。 默认为文本模式，如果要以二进制模式打开，加上 b 。 file 对象file 对象使用 open 函数来创建，下表列出了 file 对象常用的函数： 序号 方法及描述 1 file.close()关闭文件。关闭后文件不能再进行读写操作。 2 file.flush()刷新文件内部缓冲，直接把内部缓冲区的数据立刻写入文件, 而不是被动的等待输出缓冲区写入。 3 file.fileno()返回一个整型的文件描述符(file descriptor FD 整型), 可以用在如os模块的read方法等一些底层操作上。 4 file.isatty()如果文件连接到一个终端设备返回 True，否则返回 False。 5 file.next()返回文件下一行。 6 file.read([size])从文件读取指定的字节数，如果未给定或为负则读取所有。 7 file.readline([size])读取整行，包括 “\n” 字符。 8 file.readlines([sizeint])读取所有行并返回列表，若给定sizeint&gt;0，则是设置一次读多少字节，这是为了减轻读取压力。 9 file.seek(offset[, whence])设置文件当前位置 10 file.tell()返回文件当前位置。 11 file.truncate([size])截取文件，截取的字节通过size指定，默认为当前文件位置。 12 file.write(str)将字符串写入文件，返回的是写入的字符长度。 13 file.writelines(sequence)向文件写入一个序列字符串列表，如果需要换行则要自己加入每行的换行符。 关于 Buffering官方解释如下： 1The optional buffering argument specifies the file’s desired buffer size: 0 means unbuffered, 1 means line buffered, any other positive value means use a buffer of (approximately) that size. A negative buffering means to use the system default, which is usually line buffered for tty devices and fully buffered for other files. If omitted, the system default is used. 缓冲的目的：是为了减少系统的io调用。只有当符合一定条件(比如缓冲数量)时才调用io。 缓冲分以下几种： 全缓冲 : open函数的buffering设置大于1的整数n,n为缓冲区大小，linux默认为page的大小4096 满了n 个字节才会写入磁盘 。 1f=open(“demo.txt”,’w’,buffering=10) 行缓冲 : open 函数的buffering设置为1, 每写一行就会将缓冲区写入磁盘。 1f=open(“demo.txt”,’w’,buffering=1) 无缓冲 : open 函数的buffering设置为0 有输入就写入磁盘。 1f=open(“demo.txt”,’w,’,buffering=0) 系统缓存：open 函数的buffering设置小于0，由操作系统决定何时写入磁盘 1f=open(“demo.txt”,’w,’,buffering=-1) 如果没有设置，那么默认值就是使用系统缓存 python异常处理python提供了两个非常重要的功能来处理python程序在运行中出现的异常和错误。你可以使用它们来调试python程序。 异常处理 断言(Assertions) 方式/功能1-异常处理使用python自带异常进行处理python标准异常我们先了解下python定义了哪些标准异常 异常名称 描述 BaseException 所有异常的基类 SystemExit 解释器请求退出 KeyboardInterrupt 用户中断执行(通常是输入^C) Exception 常规错误的基类 StopIteration 迭代器没有更多的值 GeneratorExit 生成器(generator)发生异常来通知退出 StandardError 所有的内建标准异常的基类 ArithmeticError 所有数值计算错误的基类 FloatingPointError 浮点计算错误 OverflowError 数值运算超出最大限制 ZeroDivisionError 除(或取模)零 (所有数据类型) AssertionError 断言语句失败 AttributeError 对象没有这个属性 EOFError 没有内建输入,到达EOF 标记 EnvironmentError 操作系统错误的基类 IOError 输入/输出操作失败 OSError 操作系统错误 WindowsError 系统调用失败 ImportError 导入模块/对象失败 LookupError 无效数据查询的基类 IndexError 序列中没有此索引(index) KeyError 映射中没有这个键 MemoryError 内存溢出错误(对于Python 解释器不是致命的) NameError 未声明/初始化对象 (没有属性) UnboundLocalError 访问未初始化的本地变量 ReferenceError 弱引用(Weak reference)试图访问已经垃圾回收了的对象 RuntimeError 一般的运行时错误 NotImplementedError 尚未实现的方法 SyntaxError Python 语法错误 IndentationError 缩进错误 TabError Tab 和空格混用 SystemError 一般的解释器系统错误 TypeError 对类型无效的操作 ValueError 传入无效的参数 UnicodeError Unicode 相关的错误 UnicodeDecodeError Unicode 解码时的错误 UnicodeEncodeError Unicode 编码时错误 UnicodeTranslateError Unicode 转换时错误 Warning 警告的基类 DeprecationWarning 关于被弃用的特征的警告 FutureWarning 关于构造将来语义会有改变的警告 OverflowWarning 旧的关于自动提升为长整型(long)的警告 PendingDeprecationWarning 关于特性将会被废弃的警告 RuntimeWarning 可疑的运行时行为(runtime behavior)的警告 SyntaxWarning 可疑的语法的警告 UserWarning 用户代码生成的警告 try/except/else/finally语法首先说明下语法 try: 需要检测异常的代码块放在try中 except &lt;异常名称&gt;：匹配之后，指定该段中的代码。可以定义多个抓取异常的语句 except Exception：用户捕获我们没有发现的异常，这段需要放在except语句块的最后 else: 语句执行正常时执行的代码 finally：语句无论是否发生异常都将执行最后的代码 实际组合： 123456789101112try:&lt;语句&gt; #运行代码except &lt;名字1&gt;：&lt;语句&gt; #如果在try部份引发了&apos;name1&apos;异常，则执行这段代码except &lt;名字2&gt;:&lt;语句&gt; #如果在try部份引发了&apos;name2&apos;异常，则执行这段代码except Exception：&lt;语句&gt; # 匹配到未知异常时执行，也就是没有被&quot;except &lt;名字&gt;&quot;定义的异常else:&lt;语句&gt; #如果没有异常发生执行这段代码finally:&lt;语句&gt; # 无论异常与否都执行 光说没案例是理解不了的，下面我们看一个案例 实际案例我们先看一段会导致异常的代码 1234def test(par1,par2): print (par1,par2)test(1) 运行之后的结果是： 1234567/usr/local/bin/python3 /Users/wangxiaohua/PycharmProjects/private_project/test/test.pyTraceback (most recent call last): File &quot;/Users/wangxiaohua/PycharmProjects/private_project/test/test.py&quot;, line 6, in &lt;module&gt; test(1)TypeError: test() missing 1 required positional argument: &apos;par2&apos;Process finished with exit code 1 可以很明显的看到，因为我们少输入的一个参数，这里产生了TypeError这个异常，并且还给出了异常的相关信息 —— 分割线 —— 接下来我们对这段代码进行一下改造，添加上异常处理功能 修改之后的代码如下： 123456789101112def test(par1,par2): print (par1,par2)try: test(1)except TypeError as e: print (&quot;位置参数错误，详细信息为：&quot;,e)except Exception as e: print (&quot;未知错误：&quot;,e)else: print (&quot;执行正常时输出这段话&quot;)finally: print (&quot;不管成功与否，都会执行的代码段，可以不定义&quot;) 我们开始运行程序 第一次（错误案例，调用方式为：test(1)），结果如下 12345/usr/local/bin/python3 /Users/wangxiaohua/PycharmProjects/private_project/test/test.py位置参数错误，详细信息为： test() missing 1 required positional argument: &apos;par2&apos;不管成功与否，都会执行的代码段，可以不定义Process finished with exit code 0 第二次（成功案例，调用方式为：test(1,2)），结果如下 123456/usr/local/bin/python3 /Users/wangxiaohua/PycharmProjects/private_project/test/test.py1 2执行正常时输出这段话不管成功与否，都会执行的代码段，可以不定义Process finished with exit code 0 注意： 如果代码本身有错误，都没办法执行的话，是抓不到错误异常的，因为上面抓取错误异常指的是在代码执行时出现的异常 工作流try的工作原理是，当开始一个try语句后，python就在当前程序的上下文中作标记，这样当异常出现时就可以回到这里，try子句先执行，接下来会发生什么依赖于执行时是否出现异常。 如果当try后的语句执行时发生异常，python就跳回到try并执行第一个匹配该异常的except子句，异常处理完毕，控制流就通过整个try语句（除非在处理异常时又引发新的异常）。 如果在try后的语句里发生了异常，却没有匹配的except子句，异常将被递交到上层的try，或者到程序的最上层（这样将结束程序，并打印缺省的出错信息）。 如果在try子句执行时没有发生异常，python将执行else语句后的语句（如果有else的话），然后控制流通过整个try语句。 自定义触发异常在我们定义一些代码逻辑的时候，可能程序并不是产生异常，但是已经不符合我们定义的逻辑，这个时候，我们需要使用raise语句来强制引发抛出异常 语法raise语法格式如下： 1raise Exception(&quot;message&quot;, args) 语句中 Exception 是异常的类型（例如，NameError、IOError）参数，标准异常中任一种，args 是自已提供的参数。两个参数都可以省略 实例案例代码： 1234567891011def test (num): if num &lt; 10: raise ValueError(&quot;Invaild num:&quot;,num)try: test(8)except ValueError as e: print (e)except Exception as e: print (&quot;异常错误&quot;,e)else: print (&quot;ok&quot;) 执行后输出如下： 1(&apos;Invaild num:&apos;, 8) 自定义异常类有些时候，我们需要使用我们自己定义的异常类去做一些处理 123456789101112131415class wxherror(Exception): passdef test (num): if num &lt; 10: raise wxherror(num)try: test(8)except wxherror as e: print (&quot;错误,参数为：&#123;e&#125;&quot;.format(e=e))except Exception as e: print (&quot;异常错误&quot;,e)else: print (&quot;ok&quot;) 执行后输出如下： 1错误,参数为：8 总结通过上面的案例，我们可以总结一下使用方式 和python自带的异常处理不同，在使用自定义异常的时候，我们在具体的实现代码块中就要事先定义好，当出现某种情况时，需要抛出指定的异常（需要人为定义异常的内容） 其实自带异常处理，也是在实现代码中定义了这些异常，只不过已经内部集成，不为外部所见 自己写的异常，系统不知道它的存在，也就是说系统不知道走到哪一步应该触发它，因为所有的逻辑都是我们认为的去判断的，因此自己写的异常需要我们自己去触发，否则它不会自动触发，会自动触发的异常只有标准异常 方式/功能2-断言(Assertions)语法assert断言语句用来声明某个条件是真的，其作用是判断一个条件(condition)是否成立，如果不成立，则抛出异常。 assert一般用法： 1assert condition 如果condition为false，就raise一个AssertionError出来。逻辑上等同于： 12if not condition: raise AssertionError() 另一种使用方法： 1assert condition，expression 如果condition为false，就raise一个描述为 expression 的AssertionError出来。逻辑上等同于： 12if not condition: raise AssertionError(expression) 案例assert使用示例： 案例1 1assert isinstance(11,str) 执行后输出为： 1234Traceback (most recent call last): File &quot;/Users/wangxiaohua/PycharmProjects/private_project/test/test.py&quot;, line 26, in &lt;module&gt; assert isinstance(11,str)AssertionError 知道会抛出AssertionError之后，我们就可以做一些判断处理 案例2 12345678910def test (num): if not (num &lt; 10): raise AssertionError(&quot;&#123;num&#125; &gt;= 10&quot;.format(num=num))try: test(11)except AssertionError as e: print (&quot;输入数字错误&quot;,e)except Exception as e: print (&quot;异常错误&quot;,e) 执行后输出为： 1输入数字错误 11 &gt;= 10 案例3 1assert (1&gt;2),&quot;异常信息：1&gt;2&quot; 执行后输出为： 1234Traceback (most recent call last): File &quot;/Users/wangxiaohua/PycharmProjects/private_project/test/test.py&quot;, line 18, in &lt;module&gt; assert (1&gt;2),&quot;异常信息：1&gt;2&quot;AssertionError: 异常信息：1&gt;2 断言跟异常的区别断言是用来检查非法情况而不是错误情况的，用来帮开发者快速定位问题的位置。异常处理用于对程序发生异常情况的处理，增强程序的健壮性和容错性。 对一个函数而言，一般情况下，断言用于检查函数输入的合法性，要求输入满足一定的条件才能继续执行; 在函数执行过程中出现的异常情况使用异常来捕获。 python日志处理参考文献 Python之日志处理（logging模块） 官方文档 日志相关概念日志是一种可以追踪某些软件运行时所发生事件的方法。软件开发人员可以向他们的代码中调用日志记录相关的方法来表明发生了某些事情。一个事件可以用一个可包含可选变量数据的消息来描述。此外，事件也有重要性的概念，这个重要性也可以被称为严重性级别（level）。 总结重点： 追踪记录程序运行时发生的事件 实现：在代码中调用日志处理方法 事件有严重性级别 日志作用通过对log进行分析，可以 方便用户了解系统或软件、应用的运行情况； 如果你的应用log足够丰富，也可以分析以往用户的操作行为、类型喜好、地域分布或其他更多信息； 如果一个应用的log同时也分了多个级别，那么可以很轻易地分析得到该应用的健康状况，及时发现问题并快速定位、解决问题，补救损失。 简单来讲就是：我们通过记录和分析日志可以了解一个系统或软件程序运行情况是否正常，也可以在应用程序出现故障时快速定位问题。 比如，做运维的同学，在接收到报警或各种问题反馈后，进行问题排查时通常都会先去看各种日志，大部分问题都可以在日志中找到答案。 再比如，做开发的同学，可以通过IDE控制台上输出的各种日志进行程序调试。 对于运维老司机或者有经验的开发人员，可以快速的通过日志定位到问题的根源。可见，日志的重要性不可小觑。 如果应用的日志信息足够详细和丰富，还可以用来做用户行为分析，如：分析用户的操作行为、类型洗好、地域分布以及其它更多的信息，由此可以实现改进业务、提高商业利益。 日志作用简单总结： 程序调试 了解软件程序运行状况是否正常 软件程序运行故障时定位问题及分析问题 用户行为分析，并以此改进业务等 日志等级在软件开发阶段或部署开发环境时，为了尽可能详细的查看应用程序的运行状态来保证上线后的稳定性，我们需要把该应用程序所有的运行日志全部记录下来进行分析，这是非常耗费机器性能的。当应用程序在生产环境正式部署时，我们通常只记录应用程序的异常信息、错误信息等，这样既可以减小服务器的I/O压力，也可以避免我们在排查故障时被淹没在日志的海洋里。 那么，怎样才能在不改动应用程序代码的情况下实现在不同的环境记录不同详细程度的日志呢？这就是日志等级的作用了，我们通过配置文件指定我们需要的日志等级就可以了。 不同的应用程序所定义的日志等级可能会有所差别，分的详细点的会包含以下几个等级： DEBUG INFO NOTICE WARNING ERROR CRITICAL ALERT EMERGENCY 日志字段信息与日志格式一条日志信息对应的是一个需要关注的事件的发生，因此通常需要包括以下几个内容： 事件的严重程度（日志级别） 事件发生时间 事件发生位置 事件内容 上面这些都是一条日志记录中可能包含的字段信息，当然还可以包括一些其他信息，如进程ID、进程名称、线程ID、线程名称等。 日志格式就是用来定义一条日志记录中包含哪些字段及其组合顺序及方式，且日志格式通常都是可以自定义的。 注意：输出一条日志时，日志内容和日志级别是需要开发人员明确指定的。对于而其它字段信息，只需要是否显示在日志中就可以了。 日志功能实现几乎所有开发语言都会内置日志相关功能，或者会有比较优秀的第三方库来提供日志操作功能，比如：log4j，log4php等。它们功能强大、使用简单。Python自身也提供了一个用于记录日志的标准库模块-logging。 我们在Python中一般使用logging模块实现日志功能，因此我们在这里说的python日志处理实际上是logging模块相关内容。 logging模块是Python的一个标准库模块，由标准库模块提供日志记录API的关键好处是所有Python模块都可以使用这个日志记录功能。所以，你的应用日志可以将你自己的日志信息与来自第三方模块的信息整合起来。 logging模块logging模块支持日志级别 logging模块并不支持我们上面说的所有级别，它默认定义了以下几个日志等级 日志等级（level） 描述 DEBUG 最详细的日志信息，典型应用场景是 问题诊断 INFO 信息详细程度仅次于DEBUG，通常只记录关键节点信息，用于确认一切都是按照我们预期的那样进行工作 WARNING 当某些不期望的事情发生时记录的信息（如，磁盘可用空间较低），但是此时应用程序还是正常运行的 ERROR 由于一个更严重的问题导致某些功能不能正常运行时记录的信息 CRITICAL 当发生严重错误，导致应用程序不能继续运行时记录的信息 开发应用程序或部署开发环境时，可以使用DEBUG或INFO级别的日志获取尽可能详细的日志信息来进行开发或部署调试； 应用上线或部署生产环境时，应该使用WARNING或ERROR或CRITICAL级别的日志来降低机器的I/O压力和提高获取错误日志信息的效率。日志级别的指定通常都是在应用程序的配置文件中进行指定的。 说明： 上面列表中的日志等级是从上到下依次升高的，即：DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; CRITICAL，而日志的信息量是依次减少的； 当为某个应用程序指定一个日志级别后，应用程序会记录所有日志级别大于或等于指定日志级别的日志信息，而不是仅仅记录指定级别的日志信息，nginx、php等应用程序以及这里python的logging模块都是这样的。同样，logging模块也可以指定日志记录器的日志级别，只有级别大于或等于该指定日志级别的日志记录才会被输出，小于该等级的日志记录将会被丢弃。 logging模块的两种实现方式 logging模块提供了两种记录日志的方式： 第一种方式是使用logging提供的模块级别的函数 第二种方式是使用Logging日志系统的四大组件 logging模块定义的模块级别常用函数 函数 说明 logging.debug(msg, args, *kwargs) 创建一条严重级别为DEBUG的日志记录 logging.info(msg, args, *kwargs) 创建一条严重级别为INFO的日志记录 logging.warning(msg, args, *kwargs) 创建一条严重级别为WARNING的日志记录 logging.error(msg, args, *kwargs) 创建一条严重级别为ERROR的日志记录 logging.critical(msg, args, *kwargs) 创建一条严重级别为CRITICAL的日志记录 logging.log(level, args, *kwargs) 创建一条严重级别为level的日志记录 logging.basicConfig(**kwargs) 对root logger进行一次性配置 其中logging.basicConfig(**kwargs)函数用于指定“要记录的日志级别”、“日志格式”、“日志输出位置”、“日志文件的打开模式”等信息，其他几个都是用于记录各个级别日志的函数。 使用这个模块提供的这些函数，不需要进行额外的相关配置（其中涉及到配置的，其实也就是一个logging.basicConfig(**kwargs) 方法）就可以使用日志功能，其实就是相当于在linux中的立即生效以及永久生效。这些函数其实底层也是调用的四大组件去实现功能，只不过使用了一些默认值。 logging模块日志系统的四大组件 组件 说明 loggers 提供应用程序代码直接使用的接口，可以理解为入口，最外层的代理层 handlers 用于将日志记录发送到指定的目的位置进行输出 filters 提供更细粒度的日志过滤功能，用于决定哪些日志记录将会被输出（其它的日志记录将会被忽略） formatters 用于控制日志信息的最终输出格式 说明： logging模块提供的模块级别的那些函数实际上也是通过这几个组件的相关实现类来记录日志的，只是在创建这些类的实例时设置了一些默认值。 实现方式1-使用logging提供的模块级别函数记录日志代码： 12345logging.debug(&quot;debug log&quot;)logging.info(&quot;info log&quot;)logging.warning(&quot;warning log&quot;)logging.error(&quot;error log&quot;)logging.critical(&quot;critical log&quot;) 执行后输出如下： 123WARNING:root:warning logERROR:root:error logCRITICAL:root:critical log 这里需要注意的是：logging模块提供的日志记录函数所使用的日志器设置的日志级别是WARNING，因此只有WARNING级别的日志记录以及大于它的ERROR和CRITICAL级别的日志记录被输出了，而小于它的DEBUG和INFO级别的日志记录被丢弃了。 几个注意事项： 默认的输出格式为：日志级别:日志器名称:日志内容 之所以会这样输出，是因为logging模块提供的日志记录函数所使用的日志器设置的日志格式默认是BASIC_FORMAT，其值为： 1&quot;%(levelname)s:%(name)s:%(message)s&quot; 日志记录函数所使用的日志器设置的处理器所指定的日志输出位置默认为:sys.stderr 日志器（Logger）是有层级关系的，上面调用的logging模块级别的函数所使用的日志器是RootLogger类的实例，其名称为’root’，它是处于日志器层级关系最顶层的日志器，且该实例是以单例模式存在的。 源码实现： 查看这些日志记录函数的实现代码，可以发现：当我们没有提供任何配置信息的时候，这些函数都会去调用logging.basicConfig(**kwargs)方法，且不会向该方法传递任何参数。继续查看basicConfig()方法的代码就可以找到上面这些问题的答案了。 如何修改默认配置 在我们调用上面这些日志记录函数之前，手动调用一下basicConfig()方法，把我们想设置的内容以参数的形式传递进去就可以了 在我们需要将日志内容从控制台输出重定向到文件时需要修改配置 logging.basicConfig()函数该方法用于为logging日志系统做一些基本配置，方法定义如下： 1logging.basicConfig(**kwargs) 该函数可接收的关键字参数如下： 参数名称 描述 filename 指定日志输出目标文件的文件名，指定该设置项后日志就不会被输出到控制台了 filemode 指定日志文件的打开模式，默认为’a’。需要注意的是，该选项要在filename指定时才有效 format 指定日志格式字符串，即指定日志输出时所包含的字段信息以及它们的顺序。logging模块定义的格式字段下面会列出。 datefmt 指定日期/时间格式。需要注意的是，该选项要在format中包含时间字段%(asctime)s时才有效 level 指定日志器的日志级别 stream 指定日志输出目标stream，如sys.stdout、sys.stderr以及网络stream。需要说明的是，stream和filename不能同时提供，否则会引发 ValueError异常 style Python 3.2中新添加的配置项。指定format格式字符串的风格，可取值为’%’、’{‘和’$’，默认为’%’ handlers Python 3.3中新添加的配置项。该选项如果被指定，它应该是一个创建了多个Handler的可迭代对象，这些handler将会被添加到root logger。需要说明的是：filename、stream和handlers这三个配置项只能有一个存在，不能同时出现2个或3个，否则会引发ValueError异常。 logging模块中定义好的可以用于format格式字符串中的字段： 字段/属性名称 使用格式 描述 asctime %(asctime)s 日志事件发生的时间–人类可读时间，如：2003-07-08 16:49:45,896 created %(created)f 日志事件发生的时间–时间戳，就是当时调用time.time()函数返回的值 relativeCreated %(relativeCreated)d 日志事件发生的时间相对于logging模块加载时间的相对毫秒数（目前还不知道干嘛用的） msecs %(msecs)d 日志事件发生事件的毫秒部分 levelname %(levelname)s 该日志记录的文字形式的日志级别（’DEBUG’, ‘INFO’, ‘WARNING’, ‘ERROR’, ‘CRITICAL’） levelno %(levelno)s 该日志记录的数字形式的日志级别（10, 20, 30, 40, 50） name %(name)s 所使用的日志器名称，默认是’root’，因为默认使用的是 rootLogger message %(message)s 日志记录的文本内容，通过 msg % args计算得到的 pathname %(pathname)s 调用日志记录函数的源码文件的全路径 filename %(filename)s pathname的文件名部分，包含文件后缀 module %(module)s filename的名称部分，不包含后缀 lineno %(lineno)d 调用日志记录函数的源代码所在的行号 funcName %(funcName)s 调用日志记录函数的函数名 process %(process)d 进程ID processName %(processName)s 进程名称，Python 3.1新增 thread %(thread)d 线程ID threadName %(thread)s 线程名称 实际配置案例代码如下： 12345678910111213141516171819import loggingimport timefrom os import path# 定义日志文件名称格式base_log_name = path.abspath(path.dirname(path.dirname(__file__))) + &apos;/logs/&apos; + &quot;dcache.log&quot; + &quot;-&quot;info_log_filename = base_log_name + time.strftime(&apos;%Y-%m-%d-%H&apos;) + &quot;-&quot; + time.strftime(&apos;%H&apos;)error_log_filename = base_log_name + &quot;error.log&quot;warn_log_filename = base_log_name + &quot;warn.log&quot;# 定义日志的输出格式log_format = &quot;%(asctime)s - %(levelname)s - %(pathname)s[line:%(lineno)d] - %(message)s&quot;logging.basicConfig(filename=info_log_filename,level=logging.DEBUG,format=log_format)# 日志记录，第一种方式来源方式2，下面会将log = logging.getLogger(&apos;root&apos;)log.info(&quot;info log&quot;)或者logging.info(&quot;info log&quot;) 在这里使用了以下字段： asctime 事件发生的时间 levelname 事件的等级 pathname 产生事件的文件的绝对路径 lineno 调用日志记录函数的源代码所在的行号 message 日志记录的文本内容 日志的输出格式为： 12018-12-07 11:10:33,546 - INFO - /Users/wangxiaohua/PycharmProjects/dcache/lib/logger.py[line:35] - info log 实现方式2-使用四大组件记录日志logging模块的四大组件 组件名称 对应类名 功能描述 日志器 Logger 提供了应用程序可一直使用的接口 处理器 Handler 将logger创建的日志记录发送到合适的目的输出 过滤器 Filter 提供了更细粒度的控制工具来决定输出哪条日志记录，丢弃哪条日志记录 格式器 Formatter 决定日志记录的最终输出格式 logging模块就是通过这些组件来完成日志处理的，上面所使用的logging模块级别的函数也是通过这些组件对应的类来实现的。 这些组件之间的关系描述： 日志器/记录器（logger）需要通过处理器（handler）将日志信息输出到目标位置，如：文件、sys.stdout、网络等； 不同的处理器（handler）可以将日志输出到不同的位置； 日志器（logger）可以设置多个处理器（handler）将同一条日志记录输出到不同的位置；这里的意思也就是说，当日志器接收到一个请求之后，会把这个日志信息发送给所有的处理器，至于具体的输出情况则是由处理器去控制 每个处理器（handler）都可以设置自己的过滤器（filter）实现日志过滤，从而只保留感兴趣的日志； 每个处理器（handler）都可以设置自己的格式器（formatter）实现同一条日志以不同的格式输出到不同的地方。 简单点说就是：日志器（logger）是入口，真正干活儿的是处理器（handler），处理器（handler）还可以通过过滤器（filter）和格式器（formatter）对要输出的日志内容做过滤和格式化等处理操作。 总结：日志器logger是一个入口，它本身不处理任何的请求，只是将接受到的请求转发给后端的所有处理器，可以类比为4层转发的机制（它所能做的一些设置只是例如设置最低日志等级这种）。真正实现灵活控制的是处理器(handler)，handler接受到日志之后，根据自身的过滤规则和格式器，决定是否要记录，以及按照什么格式进行记录。 四大组件相关类及其常用方法下面介绍下与logging四大组件相关的类：Logger, Handler, Filter, Formatter。 logger类Logger对象，也就是日志器有3个任务要做： 向应用程序代码暴露几个方法，使应用程序可以在运行时记录日志消息； 基于日志严重等级（默认的过滤设施）或filter对象来决定要对哪些日志进行后续处理； 也就是说logger日志器这一层就会对日志做初步的过滤 将日志消息传送给所有感兴趣的日志handlers。 Logger对象最常用的方法分为两类：配置方法 和 消息发送方法 常用配置方法： 方法 描述 Logger.setLevel() 设置日志器将会处理的日志消息的最低严重级别 Logger.addHandler() 和 Logger.removeHandler() 为该logger对象添加 和 移除一个handler对象 Logger.addFilter() 和 Logger.removeFilter() 为该logger对象添加 和 移除一个filter对象 关于Logger.setLevel()方法的说明： 内建等级中，级别最低的是DEBUG，级别最高的是CRITICAL。例如setLevel(logging.INFO)，此时函数参数为INFO，那么该logger将只会处理INFO、WARNING、ERROR和CRITICAL级别的日志，而DEBUG级别的消息将会被忽略/丢弃。 logger对象配置完成后，可以使用下面的方法来创建日志记录： 常用消息发送方法 方法 描述 Logger.debug(), Logger.info(), Logger.warning(), Logger.error(), Logger.critical() 创建一个与它们的方法名对应等级的日志记录 Logger.exception() 创建一个类似于Logger.error()的日志消息 Logger.log() 需要获取一个明确的日志level参数来创建一个日志记录 说明： Logger.exception()与Logger.error()的区别在于：Logger.exception()将会输出堆栈追踪信息，另外通常只是在一个exception handler中调用该方法。 Logger.log()的用法为：logging.log(logging.ERROR,”log message”) Logger.log()与Logger.debug()、Logger.info()等方法相比，虽然需要多传一个level参数，显得不是那么方便，但是当需要记录自定义level的日志时还是需要该方法来完成。 如何得到一个Logger对象呢？一种方式是通过Logger类的实例化方法创建一个Logger类的实例，但是我们通常都是用第二种方式–logging.getLogger()方法。 logging.getLogger()方法有一个可选参数name，该参数表示将要返回的日志器的名称标识，如果不提供该参数，则其值为’root’。若以相同的name参数值多次调用getLogger()方法，将会返回指向同一个logger对象的引用。 关于logger的层级结构与有效等级的说明 logger的名称是一个以’.’分割的层级结构，每个’.’后面的logger都是’.’前面的logger的children，例如，有一个名称为 foo 的logger，其它名称分别为 foo.bar, foo.bar.baz 和 foo.bam都是 foo 的后代。 logger有一个”有效等级（effective level）”的概念。如果一个logger上没有被明确设置一个level，那么该logger就是使用它parent的level;如果它的parent也没有明确设置level则继续向上查找parent的parent的有效level，依次类推，直到找到个一个明确设置了level的祖先为止。需要说明的是，root logger总是会有一个明确的level设置（默认为 WARNING）。当决定是否去处理一个已发生的事件时，logger的有效等级将会被用来决定是否将该事件传递给该logger的handlers进行处理。 child loggers在完成对日志消息的处理后，默认会将日志消息传递给与它们的祖先loggers相关的handlers。因此，我们不必为一个应用程序中所使用的所有loggers定义和配置handlers，只需要为一个顶层的logger配置handlers，然后按照需要创建child loggers就可足够了。我们也可以通过将一个logger的propagate属性设置为False来关闭这种传递机制。 也就是说：四大组件是一种分层的架构，不管是父logger还是子logger，只要是logger，都是在handler层级之上的，所以child logger处理之后，会把消息传递给父logger的handler Handler类Handler对象的作用是（基于日志消息的level）将消息分发到handler指定的位置（文件、网络、邮件等）。Logger对象可以通过addHandler()方法为自己添加0个或者更多个handler对象。比如，一个应用程序可能想要实现以下几个日志需求： 1）把所有日志都发送到一个日志文件中； 2）把所有严重级别大于等于error的日志发送到stdout（标准输出）； 3）把所有严重级别为critical的日志发送到一个email邮件地址。这种场景就需要3个不同的handlers，每个handler负责发送一个特定严重级别的日志到一个特定的位置。 一个handler中只有非常少数的方法是需要应用开发人员去关心的。对于使用内建handler对象的应用开发人员来说，似乎唯一相关的handler方法就是下面这几个配置方法： 方法 描述 Handler.setLevel() 设置handler将会处理的日志消息的最低严重级别 Handler.setFormatter() 为handler设置一个格式器对象 Handler.addFilter() 和 Handler.removeFilter() 为handler添加 和 删除一个过滤器对象 需要说明的是，应用程序代码不应该直接实例化和使用Handler实例。因为Handler是一个基类，它只定义了所有handlers都应该有的接口，同时提供了一些子类可以直接使用或覆盖的默认行为。下面是一些常用的Handler： Handler 描述 logging.StreamHandler 将日志消息发送到输出到Stream，如std.out, std.err或任何file-like对象。 logging.FileHandler 将日志消息发送到磁盘文件，默认情况下文件大小会无限增长 logging.handlers.RotatingFileHandler 将日志消息发送到磁盘文件，并支持日志文件按大小切割 logging.hanlders.TimedRotatingFileHandler 将日志消息发送到磁盘文件，并支持日志文件按时间切割 logging.handlers.HTTPHandler 将日志消息以GET或POST的方式发送给一个HTTP服务器 logging.handlers.SMTPHandler 将日志消息发送给一个指定的email地址 logging.NullHandler 该Handler实例会忽略error messages，通常被想使用logging的library开发者使用来避免’No handlers could be found for logger XXX’信息的出现。 Formater类Formater对象用于配置日志信息的顺序、结构和内容。 与logging.Handler基类不同的是，应用代码可以直接实例化Formatter类。另外，如果你的应用程序需要一些特殊的处理行为，也可以实现一个Formatter的子类来完成。 Formatter类的构造方法定义如下： 1logging.Formatter.__init__(fmt=None, datefmt=None, style=&apos;%&apos;) 可见，该构造方法接收3个可选参数： fmt：指定消息格式化字符串，如果不指定该参数则默认使用message的原始值 datefmt：指定日期格式字符串，如果不指定该参数则默认使用”%Y-%m-%d %H:%M:%S” style：Python 3.2新增的参数，可取值为 ‘%’, ‘{‘和 ‘$’，如果不指定该参数则默认使用’%’ Filter类Filter可以被Handler和Logger用来做比level更细粒度的、更复杂的过滤功能。Filter是一个过滤器基类，它只允许某个logger层级下的日志事件通过过滤。该类定义如下： 12class logging.Filter(name=&apos;&apos;) filter(record) 比如，一个filter实例化时传递的name参数值为’A.B’，那么该filter实例将只允许名称为类似如下规则的loggers产生的日志记录通过过滤：’A.B’，’A.B,C’，’A.B.C.D’，’A.B.D’，而名称为’A.BB’, ‘B.A.B’的loggers产生的日志则会被过滤掉。如果name的值为空字符串，则允许所有的日志事件通过过滤。 filter方法用于具体控制传递的record记录是否能通过过滤，如果该方法返回值为0表示不能通过过滤，返回值为非0表示可以通过过滤。 说明： 如果有需要，也可以在filter(record)方法内部改变该record，比如添加、删除或修改一些属性。 我们还可以通过filter做一些统计工作，比如可以计算下被一个特殊的logger或handler所处理的record数量等。 案例演示需求： 生成2个日志文件 普通日志文件： 日志级别：INFO及以上级别 格式：dcache.log | 之前文件：dcache.log.2018-12-09 日志轮询：所有级别的日志相对来说会比较大，因此按天分割，每天输出一个日志文件，保留30天 error日志： 日志级别：Error及CRITICAL级别 格式：dcache-error.log | 之前文件：dcache-error.log.2018-12-09 日志轮询：error日志不会太大，因此每7天生成一个新的文件，保留4个文件 分析： 要记录INFO级别机器以上的日志，因此日志器的有效level需要设置为最低级别–INFO; 日志需要被发送到2个不同的目的地，因此需要为日志器设置2个handler，并且这3个目的地都是磁盘文件，因此这3个handler都是与FileHandler相关的 这里使用统一的内容格式，因此handler分别格式器设置一致，不需要额外区分 日志按照时间进行分割，因此需要用logging.handlers.TimedRotatingFileHandler; 而不是使用FileHandler; 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import loggingimport logging.handlersfrom os import pathimport datetime&quot;&quot;&quot;生成2个日志文件1. 普通日志文件： - 日志级别：INFO及以上级别 - 格式：dcache.log | 之前文件：dcache.log.2018-12-09 - 日志轮询：按天分割，每天输出一个日志文件，保留30天2. error日志： - 日志级别：Error及CRITICAL级别 - 格式：dcache-error.log | 之前文件：dcache-error.log.2018-12-09 - 日志轮询：为防止文件过大，每7天生成一个新的文件，保留7个文件 &quot;&quot;&quot;## 定义日志文件名称格式base_log_name = path.abspath(path.dirname(path.dirname(__file__))) + &apos;/logs/&apos; + &quot;dcache&quot;info_log_filename = base_log_name + &quot;.log&quot;error_log_filename = base_log_name + &quot;-error.log&quot;# 定义日志内容的输出格式log_format = &quot;%(asctime)s - %(levelname)s - %(pathname)s[line:%(lineno)d] - %(message)s&quot;# 定义日志处理器(实例化一个日志处理器对象)logger = logging.getLogger(&apos;root&apos;)logger.setLevel(logging.INFO)# 定义handlersinfo_handler = logging.handlers.TimedRotatingFileHandler(info_log_filename, when=&apos;midnight&apos;, interval=1,backupCount=30, atTime=datetime.time(0, 0, 0, 0))info_handler.suffix = &quot;%Y-%m-%d&quot;info_handler.setLevel(logging.INFO)info_handler.setFormatter(logging.Formatter(log_format))error_handler = logging.handlers.TimedRotatingFileHandler(error_log_filename, when=&apos;midnight&apos;, interval=7,backupCount=4, atTime=datetime.time(0, 0, 0, 0))error_handler.suffix = &quot;%Y-%m-%d&quot;error_handler.setLevel(logging.ERROR)error_handler.setFormatter(logging.Formatter(log_format))# 日志器添加handlerslogger.addHandler(info_handler)logger.addHandler(error_handler)# testlogger.debug(&apos;debug message&apos;)logger.info(&apos;info message&apos;)logger.warning(&apos;warning message&apos;)logger.error(&apos;error message&apos;)logger.critical(&apos;critical message&apos;) 注意： interval表示的是：多少个指定时间内，当前的日志文件没有新的内容被写入进来，再去创建新文件，而不是时间一到就去创建新文件。每次每隔一小时输出一个文件的功能使用TimedRotatingFileHandler的方式实现不了。 因此要每小时一个文件的这种功能，需要我们想其他办法去实现 补充-零碎知识记录python中方法与函数的区别定义: function(函数) —— A series of statements which returns some value toa caller. It can also be passed zero or more arguments which may beused in the execution of the body. method(方法) —— A function which is defined inside a class body. Ifcalled as an attribute of an instance of that class, the methodwill get the instance object as its first argument (which isusually called self). Function包含一个函数头和一个函数体, 支持0到n个形参 而Method则是在function的基础上, 多了一层类的关系, 正因为这一层类, 所以区分了 function 和 method.而这个过程是通过 PyMethod_New实现的 也就是说，函数可以脱离于类单独存在，在使用的时候，需要往函数中传入参数（实参） 而方法是与某个对象紧密联系的，不能脱离于类而存在方法的作用域只是在一个类中，只能在该类实例化后被该类使用 方法的绑定, 肯定是伴随着class的实例化而发生,我们都知道, 在class里定义方法, 需要显示传入self参数, 因为这个self是代表即将被实例化的对象。 定义角度： 从定义的角度上看，我们知道函数(function)就相当于一个数学公式，它理论上不与其它东西关系，它只需要相关的参数就可以。所以普通的在module中定义的称谓函数是很有道理的。 那么方法的意思就很明确了，它是与某个对象相互关联的，也就是说它的实现与某个对象有关联关系。这就是方法。虽然它的定义方式和函数是一样的。也就是说，在Class定义的函数就是方法。 总结： 123456函数是一段代码，通过名字来进行调用。它能将一些数据（参数）传递进去进行处理，然后返回一些数据（返回值），也可以没有返回值。所有传递给函数的数据都是显式传递的。方法也是一段代码，也通过名字来进行调用，但它跟一个对象相关联。方法和函数大致上是相同的，但有两个主要的不同之处：方法中的数据是隐式传递的；方法可以操作类内部的数据（请记住，对象是类的实例化–类定义了一个数据类型，而对象是该数据类型的一个实例化） 可变参数args基础概念如果我们在函数被调用前并不知道也不限制将来函数可以接收的参数数量。在这种情况下我们可以使用*args和**kwargs来进行定义函数 *args和**kwargs这两个是python中的可变参数。 args表示任何多个无名参数，它是一个tuple kwargs表示关键字参数，它是一个dict 特别注意： 同时使用*args和**kwargs时，*args参数要列在**kwargs前。 因此像foo(a=1, b=’2’, c=3, a’, 1, None, )这样调用的话，会提示语法错误“SyntaxError: non-keyword arg after keyword arg”。 当两者同时存在时，正确的调用方式应该像是：foo(‘a’,1,a=1,b=2) 实际上真正的Python参数传递语法是*和**。*args和**kwargs只是一种约定俗成的编程实践。我们也可以写成*vars和**kvars。 实际案例代码如下： 1234567def test(*args,**kwargs): print (&quot;args = &quot;,args) print (&quot;kwargs = &quot;,kwargs) passtest(1,2,3,4)test(&quot;11&quot;,&quot;2&quot;,a=1,b=2) 执行后输出如下： 1234args = (1, 2, 3, 4)kwargs = &#123;&#125;args = (&apos;11&apos;, &apos;2&apos;)kwargs = &#123;&apos;a&apos;: 1, &apos;b&apos;: 2&#125; python import相关问题参考文献： 关于python——import问题 python导入模块的几种姿势 在python中，每个py文件被称之为模块，每个具有__init__.py文件的目录被称为包。只要模块或者包所在的目录在sys.path中，就可以使用import 模块或import 包来使用。 作为一名新手Python程序员，你首先需要学习的内容之一就是如何导入模块或包。但是我注意到，那些许多年来不时使用Python的人并不是都知道Python的导入机制其实非常灵活。在本文中，我们将探讨以下话题： 常规导入（regular imports） 使用from语句导入 相对导入（relative imports） 可选导入（optional imports） 本地导入（local imports） 导入注意事项 常规导入常规导入应该是最常使用的导入方式，大概是这样的： 1import sys 你只需要使用import一词，然后指定你希望导入的模块或包即可。通过这种方式导入的好处是可以一次性导入多个包或模块： 1import os, sys, time 虽然这节省了空间，但是却违背了Python风格指南。Python风格指南建议将每个导入语句单独成行。 有时在导入模块时，你想要重命名这个模块。这个功能很容易实现： 123import sys as systemprint(system.platform) 上面的代码将我们导入的sys模块重命名为system。我们可以按照和以前一样的方式调用模块的方法，但是可以用一个新的模块名。也有某些子模块必须要使用点标记法才能导入。 1import urllib.error 这个情况不常见，但是对此有所了解总是没有坏处的。 注意： import导入为绝对导入 import 只能导入模块，不能导入模块中的对象（类、函数、变量等) Python 中所有加载到内存的模块都放在 sys.modules 。当 import 一个模块时首先会在这个列表中查找是否已经加载了此模块，如果没有加载则从 sys.path 目录中按照模块名称查找模块文件，找到后将模块载入内存，并加到 sys.modules 中。只有存在sys.path中的模块才会被正确导入。 一个模块不会重复载入。多个不同的模块都可以用 import 引入同一个模块到自己的 Local 名字空间，其实背后的 PyModuleObject 对象只有一个。 一个容易忽略的问题：import 只能导入模块，不能导入模块中的对象（类、函数、变量等）。例如：模块 A（A.py）中有个函数 getName，另一个模块不能通过 import A.getName 将 getName导入到本模块，只能用 from A import getName。 同级目录下，可以使用import直接导入所需模块 使用from语句导入很多时候你只想要导入一个模块或库中的某个部分。我们来看看在Python中如何实现这点： 1from functools import lru_cache 上面这行代码可以让你直接调用lru_cache。如果你按常规的import方式导入functools，那么你就必须像这样调用lru_cache： 1functools.lru_cache(*args) 根据你实际的使用场景，上面的做法可能是更好的。在复杂的代码库中，能够看出某个函数是从哪里导入的这点很有用的。不过，如果你的代码维护的很好，模块化程度高，那么只从某个模块中导入一部分内容也是非常方便和简洁的。 当然，你还可以使用from方法导入模块的全部内容，就像这样： 1from os import * 这种做法在少数情况下是挺方便的，但是这样也会打乱你的命名空间。问题在于，你可能定义了一个与导入模块中名称相同的变量或函数，这时如果你试图使用os模块中的同名变量或函数，实际使用的将是你自己定义的内容。因此，你最后可能会碰到一个相当让人困惑的逻辑错误。标准库中我唯一推荐全盘导入的模块只有Tkinter。 如果你正好要写自己的模块或包，有人会建议你在__init__.py文件中导入所有内容，让模块或者包使用起来更方便。我个人更喜欢显示地导入，而非隐式地导入。 你也可以采取折中方案，从一个包中导入多个项： 12from os import path, walk, unlinkfrom os import uname, remove 在上述代码中，我们从os模块中导入了5个函数。你可能注意到了，我们是通过多次从同一个模块中导入实现的。当然，如果你愿意的话，你也可以使用圆括号一次性导入多个项： 12from os import (path, walk, unlink, uname, remove, rename) 这是一个有用的技巧，不过你也可以换一种方式： 12from os import path, walk, unlink, uname, \ remove, rename 上面的反斜杠是Python中的续行符，告诉解释器这行代码延续至下一行。 相对导入PEP 328介绍了引入相对导入的原因，以及选择了哪种语法。具体来说，是使用句点来决定如何相对导入其他包或模块。这么做的原因是为了避免偶然情况下导入标准库中的模块产生冲突。这里我们以PEP 328中给出的文件夹结构为例，看看相对导入是如何工作的： 12345678910my_package/ __init__.py subpackage1/ __init__.py module_x.py module_y.py subpackage2/ __init__.py module_z.py module_a.py 在本地磁盘上找个地方创建上述文件和文件夹。在顶层的__init__.py文件中，输入以下代码： 12from . import subpackage1from . import subpackage2 接下来进入subpackage1文件夹，编辑其中的__init__.py文件，输入以下代码： 12from . import module_xfrom . import module_y 现在编辑module_x.py文件，输入以下代码： 1234from .module_y import spam as hamdef main(): ham() 最后编辑module_y.py文件，输入以下代码： 12def spam(): print(&apos;spam &apos; * 3) 打开终端，cd至my_package包所在的文件夹，但不要进入my_package。在这个文件夹下运行Python解释器。我使用的是IPython，因为它的自动补全功能非常方便： 1234567In [1]: import my_packageIn [2]: my_package.subpackage1.module_xOut[2]: &lt;module &apos;my_package.subpackage1.module_x&apos; from &apos;my_package/subpackage1/module_x.py&apos;&gt;In [3]: my_package.subpackage1.module_x.main()spam spam spam 相对导入适用于你最终要放入包中的代码。如果你编写了很多相关性强的代码，那么应该采用这种导入方式。你会发现PyPI上有很多流行的包也是采用了相对导入。还要注意一点，如果你想要跨越多个文件层级进行导入，只需要使用多个句点即可。不过，PEP 328建议相对导入的层级不要超过两层。 还要注意一点，如果你往module_x.py文件中添加了if __name__ == ‘__main__’，然后试图运行这个文件，你会碰到一个很难理解的错误。编辑一下文件，试试看吧！ 12345678from . module_y import spam as hamdef main(): ham()if __name__ == &apos;__main__&apos;: # This won&apos;t work! main() 现在从终端进入subpackage1文件夹，执行以下命令： 1python module_x.py 如果你使用的是Python 2，你应该会看到下面的错误信息： 1234Traceback (most recent call last): File &quot;module_x.py&quot;, line 1, in &lt;module&gt; from . module_y import spam as hamValueError: Attempted relative import in non-package 如果你使用的是Python 3，错误信息大概是这样的： 1234Traceback (most recent call last): File &quot;module_x.py&quot;, line 1, in &lt;module&gt; from . module_y import spam as hamSystemError: Parent module &apos;&apos; not loaded, cannot perform relative import 这指的是，module_x.py是某个包中的一个模块，而你试图以脚本模式执行，但是这种模式不支持相对导入。 如果你想在自己的代码中使用这个模块，那么你必须将其添加至Python的导入检索路径（import search path）。最简单的做法如下： 123import syssys.path.append(&apos;/path/to/folder/containing/my_package&apos;)import my_package 注意，你需要添加的是my_package的上一层文件夹路径，而不是my_package本身。原因是my_package就是我们想要使用的包，所以如果你添加它的路径，那么将无法使用这个包。 我们接下来谈谈可选导入。 可选导入（Optional imports）如果你希望优先使用某个模块或包，但是同时也想在没有这个模块或包的情况下有备选，你就可以使用可选导入这种方式。这样做可以导入支持某个软件的多种版本或者实现性能提升。以github2包中的代码为例： 123456789try: # For Python 3 from http.client import responsesexcept ImportError: # For Python 2.5-2.7 try: from httplib import responses # NOQA except ImportError: # For Python 2.4 from BaseHTTPServer import BaseHTTPRequestHandler as _BHRH responses = dict([(k, v[0]) for k, v in _BHRH.responses.items()]) lxml包也有使用可选导入方式： 1234567try: from urlparse import urljoin from urllib2 import urlopenexcept ImportError: # Python 3 from urllib.parse import urljoin from urllib.request import urlopen 正如以上示例所示，可选导入的使用很常见，是一个值得掌握的技巧。 局部导入当你在局部作用域中导入模块时，你执行的就是局部导入。如果你在Python脚本文件的顶部导入一个模块，那么你就是在将该模块导入至全局作用域，这意味着之后的任何函数或方法都可能访问该模块。例如： 12345678910111213import sys # global scopedef square_root(a): # This import is into the square_root functions local scope import math return math.sqrt(a)def my_pow(base_num, power): return math.pow(base_num, power)if __name__ == &apos;__main__&apos;: print(square_root(49)) print(my_pow(2, 3)) 这里，我们将sys模块导入至全局作用域，但我们并没有使用这个模块。然后，在square_root函数中，我们将math模块导入至该函数的局部作用域，这意味着math模块只能在square_root函数内部使用。如果我们试图在my_pow函数中使用math，会引发NameError。试着执行这个脚本，看看会发生什么。 使用局部作用域的好处之一，是你使用的模块可能需要很长时间才能导入，如果是这样的话，将其放在某个不经常调用的函数中或许更加合理，而不是直接在全局作用域中导入。老实说，我几乎从没有使用过局部导入，主要是因为如果模块内部到处都有导入语句，会很难分辨出这样做的原因和用途。根据约定，所有的导入语句都应该位于模块的顶部。 导入注意事项在导入模块方面，有几个程序员常犯的错误。这里我们介绍两个。 循环导入（circular imports） 覆盖导入（Shadowed imports，暂时翻译为覆盖导入） 先来看看循环导入。 循环导入如果你创建两个模块，二者相互导入对方，那么就会出现循环导入。例如： 12345678# a.pyimport bdef a_test(): print(&quot;in a_test&quot;) b.b_test()a_test() 然后在同个文件夹中创建另一个模块，将其命名为b.py。 1234567import adef b_test(): print(&apos;In test_b&quot;&apos;) a.a_test()b_test() 如果你运行任意一个模块，都会引发AttributeError。这是因为这两个模块都在试图导入对方。简单来说，模块a想要导入模块b，但是因为模块b也在试图导入模块a（这时正在执行），模块a将无法完成模块b的导入。我看过一些解决这个问题的破解方法（hack），但是一般来说，你应该做的是重构代码，避免发生这种情况。 覆盖导入当你创建的模块与标准库中的模块同名时，如果你导入这个模块，就会出现覆盖导入。举个例子，创建一个名叫math.py的文件，在其中写入如下代码： 123456import mathdef square_root(number): return math.sqrt(number)square_root(72) 现在打开终端，试着运行这个文件，你会得到以下回溯信息（traceback）： 12345678Traceback (most recent call last): File &quot;math.py&quot;, line 1, in &lt;module&gt; import math File &quot;/Users/michael/Desktop/math.py&quot;, line 6, in &lt;module&gt; square_root(72) File &quot;/Users/michael/Desktop/math.py&quot;, line 4, in square_root return math.sqrt(number)AttributeError: module &apos;math&apos; has no attribute &apos;sqrt&apos; 这到底是怎么回事？其实，你运行这个文件的时候，Python解释器首先在当前运行脚本所处的的文件夹中查找名叫math的模块。在这个例子中，解释器找到了我们正在执行的模块，试图导入它。但是我们的模块中并没有叫sqrt的函数或属性，所以就抛出了AttributeError。 import总结在本文中，我们讲了很多有关导入的内容，但是还有部分内容没有涉及。PEP 302中介绍了导入钩子（import hooks），支持实现一些非常酷的功能，比如说直接从github导入。Python标准库中还有一个importlib模块，值得查看学习。当然，你还可以多看看别人写的代码，不断挖掘更多好用的妙招。 删除字符串中指定符号源文件 1234567wxh:wxh123wsy:wsy123badou:badou123dabadou:dabadou123 此时我要根据冒号（:）将两边的内容都截取出来 传统的字符串截取方式是根据索引进行区分的，这种事不能实现这种需求，这个时候需要使用函数split() Python中有split()和os.path.split()两个函数，具体作用如下： split()：拆分字符串。通过指定分隔符对字符串进行切片，并返回分割后的字符串列表（list） os.path.split()：按照路径将文件名和路径分割开 如下方代码所示： 12345with open (r&apos;C:\Users\Administrator\PycharmProjects\files\login.txt&apos;,&apos;r&apos;) as file_object: for lines in file_object: lines = lines.strip(&apos;\n&apos;) file_username = lines.split(&apos;:&apos;, 1)[0] file_password = lines.split(&apos;:&apos;, 1)[1] 查看python执行过程-类似sh -x123#详细追踪 python -m trace --trace script.py #显示调用了哪些函数 python -m trace --trackcalls script.py 版本变化##django## 外键： Django2.0版本之后，创建外键时需要在后面加上on_delete 1topic = models.ForeignKey(Topic) 应该修改为： 1topic = models.ForeignKey(Topic,on_delete=models.CASCADE) django.core.urlresolvers变化 Django 2.0 removes the django.core.urlresolvers module, which was moved to django.urls in version 1.10. You should change any import to use django.urls instead. pycharm使用解决pycharm问题：module ‘pip’ has no attribute ‘main’ 1更新pip之后，Pycharm安装package出现报错：module &apos;pip&apos; has no attribute &apos;main&apos; 1找到安装目录下 helpers/packaging_tool.py文件，找到如下代码： 1234567891011121314def do_install(pkgs): try: import pip except ImportError: error_no_pip() return pip.main([&apos;install&apos;] + pkgs)def do_uninstall(pkgs): try: import pip except ImportError: error_no_pip() return pip.main([&apos;uninstall&apos;, &apos;-y&apos;] + pkgs) 修改为如下，保存即可。 12345678910111213141516171819202122def do_install(pkgs): try: # import pip try: from pip._internal import main except Exception: from pip import main except ImportError: error_no_pip() return main([&apos;install&apos;] + pkgs)def do_uninstall(pkgs): try: # import pip try: from pip._internal import main except Exception: from pip import main except ImportError: error_no_pip() return main([&apos;uninstall&apos;, &apos;-y&apos;] + pkgs) 创建directory和python package的区别对于python而言，有一点是要认识明确的，python作为一个相对而言轻量级的，易用的脚本语言（当然其功能并不仅限于此，在此只是讨论该特点），随着程序的增长，可能想要把它分成几个文件，以便逻辑更加清晰，更好维护，亦或想要在几个程序中均使用某个函数，而不必将其复制粘贴到所有程序中。 为了支持这一点，Python有一种方法将定义函数放在一个文件中，并在脚本中使用它们，这样的文件叫做模块，一个模块中的定义可以被导入到其他模块，或者主模块中。 简单来说在python中模块就是指一个py文件，如果我们将所有相关的代码都放在一个py文件中，则该py文件既是程序由是模块，但是程序和模块的设计目的是不同的，程序的目的是为了运行，而模块的目的是为了其他程序进行引 Dictionary Dictionary在pycharm中就是一个文件夹，放置资源文件，对应于在进行JavaWeb开发时用于放置css/js文件的目录，或者说在进行物体识别时，用来存储背景图像的文件夹。该文件夹其中并不包含 init.py 文件 Python package 对于Python package 文件夹而言，与Dictionary不同之处在于其会自动创建 init.py 文件。 简单的说，python package就是一个目录，其中包括一组模块和一个 init.py 文件。 Image/ _init _.py jpg.py tiff.py bmp.py 只要image目录是我们程序目录的子目录，我们就可以导入image目录下的任意模块来为我们所用，使用时可如下： init .py 该文件与Python的import机制有关，这关乎到你的哪些.py文件是对外可访问的。有些时候，如果一个包下有很多模块，在调用方import如此多模块是很费事，且不优雅的，此时可以通过修改 init .py来完成该任务。在 init_ .py中定义特殊变量_all_,将要包含的模块复制给该变量。 例如在Image/ init .py中定义 _all_=[‘tiff’,’bmp’,’jpg’],这里的all 对应的就是 from …import 中代指的模块，此时在引用方使用如下语句： 12from image import *tool = tiff.read(&apos;a.tiff&apos;) 其实 init_ .py可以为空，当其为空时，from Image import *将Image包下所有的模块都进行引用，如果想要控制引用的模块，则可以自行定义 all python的dict和json的区别工作中和其他语言的工程师交流，合作与联调中经常会涉及到数据的传输，这个数据的传输通常为json字符串，这个json格式数据和python自身的dict数据对象非常像，所以很自然的会思考这两者究竟区别在哪里？ 首先，两者不一样 区别 Python 的字典是一种数据结构，JSON 是一种数据格式。 json 就是一个根据某种约定格式编写的纯字符串，不具备任何数据结构的特征。而 python 的字典的字符串表现形式的规则看上去和 json 类似，但是字典本身是一个完整的数据结构，实现了一切自身该有的算法。 Python的字典key可以是任意可hash对象，json只能是字符串。 形式上有些相像，但JSON是纯文本的，无法直接操作。 1.python dict 字符串用单引号，json强制规定双引号。 2.python dict 里可以嵌套tuple,json里只有array。 json.dumps({1:2}) 的结果是 ｛”1”:2}； json.dumps((1,2)) 的结果是[1,2] 3.json key name 必须是字符串, python 是hashable, {(1,2):1} 在python里是合法的,因为tuple是hashable type；{[1,2]:1} 在python里TypeError: unhashable “list” 4.json: true false null ； python:True False None python {“me”: “我”} 是合法的； json 必须是 {“me”: “\u6211”} 联系dict 存在于内存中，可以被序列化成 json 格式的数据（string），之后这些数据就可以传输或者存储了。 JSON 是一种数据传输格式。 也就是说，这些字符串以 JSON 这样的格式来传输，至于你怎么 parse 这些信息，甚至是是否 parse，是否储存，都不是 JSON 的事情。 用 Python 举个例子: 某段程序可以把字符串 &quot;{A:1, B:2}&quot; parse 成 一对 tuple( (&quot;A&quot;, 1), (&quot;B&quot;, 2) )而不是 dictionary {&quot;A&quot;: 1, &quot;B&quot;: 2}。Python 的 dictionary 是对 Hash Table 这一数据结构的一种实现。它使用其内置的哈希函数来规划键对应的内容的储存位置，从而获得 O(1) 的数据读取速度。所以 JSON 是一种数据传输格式，它能被解析成 Python 的 Dictionary 或者其他形式，但解析成什么内容是和 JSON 这种格式无关的。Python 的 Dictionary 则是 Python 对 Hash Table 的实现，一套从存储到提取都封装好了的方案。 Python 2.7.12 源码安装pipwget https://bootstrap.pypa.io/get-pip.py ./python get-pip.py if __name__ == ‘main‘ 如何正确理解?这里的作用主要是作为整个程序的一个入口 对于很多编程语言来说，程序都必须要有一个入口，比如 C，C++，以及完全面向对象的编程语言 Java，C# 等。如果你接触过这些语言，对于程序入口这个概念应该很好理解，C 和 C++ 都需要有一个 main 函数来作为程序的入口，也就是程序的运行会从 main 函数开始。同样，Java 和 C# 必须要有一个包含 Main 方法的主类来作为程序入口。 而 Python 则有不同，它属于脚本语言，不像编译型语言那样先将程序编译成二进制再运行，而是动态的逐行解释运行。也就是从脚本第一行开始运行，没有统一的入口。 一个 Python 源码文件除了可以被直接运行外，还可以作为模块（也就是库）被导入。不管是导入还是直接运行，最顶层的代码都会被运行（Python 用缩进来区分代码层次）。而实际上在导入的时候，有一部分代码我们是不希望被运行的。 详细内容可以看这篇文章：http://blog.konghy.cn/2017/04/24/python-entry-program/ 获取当前py文件目录 当前目录 1path.dirname(__file__) 上级目录 1path.abspath(path.dirname(path.dirname(__file__))) byte类型和str类型之间的转换123456789101112131415161718# bytes objectb = b&quot;example&quot; # str objects = &quot;example&quot;# str to bytesbytes(s, encoding = &quot;utf8&quot;) # bytes to strstr(b, encoding = &quot;utf-8&quot;) # an alternative method# str to bytesstr.encode(s) # bytes to strbytes.decode(b 时间戳转换12345678now_time = time.strftime(&quot;%H:%M:%S&quot;)today = datetime.date.today()past_date = today - datetime.timedelta(days=days)today_time = str(today) + &quot; &quot; + now_timepast_day_time = str(past_date) + &quot; &quot; + now_time# 将时间转换成为时间戳start_time = int(time.mktime(time.strptime(today_time, &quot;%Y-%m-%d %H:%M:%S&quot;)))end_time = int(time.mktime(time.strptime(past_day_time, &quot;%Y-%m-%d %H:%M:%S&quot;))) Python requests.post方法中data与json参数区别在通过requests.post()进行POST请求时，传入报文的参数有两个，一个是data，一个是json。 1data与json既可以是str类型，也可以是dict类型。 区别： 1、不管json是str还是dict，如果不指定headers中的content-type，默认为application/json 2、data为dict时，如果不指定content-type，默认为application/x-www-form-urlencoded，相当于普通form表单提交的形式 3、data为str时，如果不指定content-type，默认为application/json 4、用data参数提交数据时，request.body的内容则为a=1&amp;b=2的这种形式，用json参数提交数据时，request.body的内容则为’{“a&quot;: 1, &quot;b&quot;: 2}&#39;的这种形式 python 添加项目目录代码如下： 12345import osimport syscurrent_dir = os.path.abspath(os.path.dirname(__file__))parent_dir = os.path.dirname(current_dir)sys.path.append(parent_dir) python列表-根据值获取索引下标代码为： 1index_num=list_name.index(value) 补充-git使用git输入用户名密码使用如下这种方式： 123git clone http://wangxiaohua:密码@192.168.1.66/wangxiaohua/es-monitor.git或者git clone http://wangxiaohua:密码@git.nidianwo.com/wangxiaohua/es-monitor.git .gitignore文件的编写示例： 12345➜ dnamed git:(master) cat .gitignore*.pyc*.loglogs/*db.sqlite3 注意： .gitignore只能忽略那些原来没有被 track 的文件，如果某些文件已经被纳入了版本管理中，则修改 .gitignore 是无效的。解决方法是先把本地缓存删除，然后再提交。 执行下面的命令： 1234git rm -r --cached .git add .git commit -m &quot;commit content&quot;git push -u origin master 补充-python操作mysql参考文献 菜鸟教程 实际案例在py3中，我们一般使用pymysql这个客户端去连接mysql数据库 读取数据修改数据写入数据]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>编程开发</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK部署]]></title>
    <url>%2F2019%2F06%2F18%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2FELK%2FELK%E9%83%A8%E7%BD%B2%2FELK%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[基础知识实现架构传统的ELK是使用elasticsearch+logstash+kibana的方式来部署日志收集系统 但是在数据量较大下，该架构过于单薄存在各种问题，因此，我们这里所使用的架构是： Filebeat—&gt;kafka—&gt;logstash—&gt;[elasticsearch、filesystem、kafka] 也就是说将客户端收集替换成了filebeat，将日志推送到kafka中，然后在用logstash收集kafaka中的数据，logstash的output可以是es、文件服务器等等 架构说明Filebeat部署filebeat工作原理Filebeat是本地文件的日志数据采集器，作为服务器上的代理安装。 Filebeat监视指定的日志目录或特定日志文件，使用tail -f file的机制，收集之后将它们转发给Elasticsearch或Logstash或kafka等进行存储。 Filebeat由两个主要组件组成： prospector 探勘器、查找器 harvester 采集器 工作过程概述： 启动Filebeat时，启动一个或多个prospector，查看指定的日志文件目录或者单个文件。 对于涉及的每个日志文件，prospector 启动对应数量的harvester， 每个harvester会读取到指定文件的新内容，将新数据发送到libbeat，libbeat将聚合事件并将聚合数据发送到你为Filebeat配置的输出。 prospectorprospector有2个职责：负责找到所有需要收集的文件对象，以及管理harvester 注意：prospector只能读取本地文件， 不能连接到远程主机来读取存储的文件或日志。 harvesterharvester只有1个职责：负责读取单个文件的内容，并将内容发送到指定的output prospector会为每一个要收集的文件都启动一个harvesterharvester 负责打开和关闭文件，这意味着在运行时文件描述符保持打开状态，如果文件在读取时被删除或重命名，Filebeat将继续读取文件。也就是说，这会存在一个问题，就是如果文件被删了了，但是在harvester关闭之前，磁盘上的空间将会被保留无法释放。默认情况下，Filebeat将文件保持打开状态，直到达到close_inactive状态 Filebeat如何保持文件的状态？prospector保存每个文件的状态并经常将状态刷新到磁盘上的注册文件(data/registry)中。 该状态的作用是用于harvester记住正在读取文件的最后偏移量，并确保发送所有日志行。如果输出（例如Elasticsearch或Logstash）无法访问，Filebeat会检测output，并在oputput再次可用时继续读取文件发送。 在Filebeat运行时，每个prospector内存中也会保存的文件状态信息，当重新启动Filebeat时，将使用注册文件的数据来重建文件状态，每个harvester基于注册文件中记录的最后偏移量继续读取。 因为文件可以被重命名或移动，因此文件名和路径不足以识别文件，对于每个文件，Filebeat存储唯一标识符以检测文件是否先前已采集过。 如果每天会创建大量新文件，注册文件增长可能过大。这个时候参阅注册表文件太大相关问题？ Filebeat如何确保至少一次分发Filebeat保证所有的事件至少会被传送到配置的output一次，并且不会丢失数据。 Filebeat之所以能够实现此行为，因为它将每个事件的传递状态存储在注册文件中。 如果输出（例如Elasticsearch或Logstash）无法访问，Filebeat会检测output，并在oputput再次可用时继续读取文件发送。 如果Filebeat在发送事件的过程中关闭，它不会等待output确认所有收到事件。发送到output但未确认的任何事件将会在在重新启动Filebeat后再次发送，这可以确保每个事件至少发送一次，但最终可能会将重复发送事件发送到output 介于这个特性，我们可以配置shutdown_timeout参数来指定，在手动关闭filebeat时，等待多久才停止进程 注意：Filebeat的至少一次交付保证在日志轮换和删除旧文件时有限制。如果将日志文件写入磁盘并且写入速度超过Filebeat可以处理的速度，或者在output不可用时删除了文件，则可能会丢失数据。在Linux上，Filebeat也可能因inode重用而跳过行 filebeat安装配置基础环境Before running Filebeat, you need to install and configure the Elastic stack. See Getting Started with Beats and the Elastic Stack. A regular Beats setup consists of: Elasticsearch for storage and indexing. See Install Elasticsearch. Logstash (optional) for inserting data into Elasticsearch. See Installing Logstash. Kibana for the UI. See Install Kibana. One or more Beats. You install the Beats on your servers to capture operational data. See Install Beats. Kibana dashboards for visualizing the data. 也就是说，在部署filebeat之前，我们需要ELK环境，相当于说filebeat是在ELK的基础之上的一个组件。 如果还没有ELK环境的，先跳转到ES部分。 下载在这里，我们使用源码包的方式安装： 1wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.2.3-linux-x86_64.tar.gz 配置默认的配置如下： 1234567891011121314[appdeploy@node001 filebeat6]$ cat filebeat.ymlfilebeat.prospectors:- type: log enabled: false paths: - /var/log/*.logfilebeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: falsesetup.template.settings: index.number_of_shards: 3setup.kibana:output.elasticsearch: hosts: [&quot;localhost:9200&quot;] 一个实际的案例配置文件如下： 123456789101112131415161718192021filebeat.shutdown_timeout: 10sfilebeat.config.prospectors: enabled: true path: conf/*.yml reload.enabled: true reload.period: 10soutput.kafka: hosts: [&quot;172.24.48.76:19092&quot;, &quot;172.24.48.77:19092&quot;, &quot;172.24.48.78:19092&quot;] topic: &apos;%&#123;[fields.log_topic]&#125;&apos; key: &apos;%&#123;[fields.app_name]&#125; %&#123;[beat.hostname]&#125;&apos; required_acks: 1 workers: 6 timeout: 120 compression: snappy max_message_bytes: 10240000 codec.format: string: &apos;%&#123;[fields.app_name]&#125; %&#123;[beat.hostname]&#125; %&#123;[message]&#125;&apos;xpack.monitoring: enabled: true elasticsearch: hosts: [&quot;http://172.24.48.76:9200&quot;, &quot;http://172.24.48.77:9200&quot;, &quot;http://172.24.48.78:9200&quot;] 启动nohup /home/appdeploy/deploy/tools/filebeat6/filebeat -e -c /home/appdeploy/deploy/tools/filebeat6/applogg.yml &gt;&gt;/home/appdeploy/deploy/tools/filebeat6/applogg.log &amp; filebeat的模块filebeat的模块简化了收集、解析以及可视化例如nginx、Redis、mysql等通用日志格式这一系列操作。 也就是说，通过使用这些现成的模块，我们可以快速的进行收集，而免去了比较繁琐的自定义设置。 filebeat的配置详解设置prospectors- 探勘器、查找器filebeat使用prospectors去定位和处理文件。 在yml配置文件中以”filebeat.prospectors:”开头，下面的列表代表每一个prospectors(一个type就是一个prospectors) 例如： 123456789filebeat.prospectors:- type: log paths: - /var/log/apache/httpd-*.log- type: log paths: - /var/log/messages - /var/log/*.log filebeat将会为每一个文件都启动一个harvester。 在prospectors这一栏中，有一些参数可以配置： type。下面是一些常用的二级参数，归属于type下 paths：文件的路径列表 exclude_lines Include_lines tags。tags可以用于后面的logstash过滤 fileds scan_frequency：在指定的path下，每隔多久去检测是否有新文件 harvester_buffer_size max_bytes。这个参数需要关注下。 tail_files 如果这个参数被设置为true， Multiline-多行合并管理参考链接：https://www.elastic.co/guide/en/beats/filebeat/6.2/multiline-examples.html 因为在应用的日志输出中，例如java的堆栈信息等，输出是多行的形式，例如： 12345[beat-logstash-some-name-832-2015.11.28] IndexNotFoundException[no such index] at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.resolve(IndexNameExpressionResolver.java:566) at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:133) at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:77) at org.elasticsearch.action.admin.indices.delete.TransportDeleteIndexAction.checkBlock(TransportDeleteIndexAction.java:75) 这几行其实是同一段，我们要放在一起展示，所以需要把这多行的信息进行归纳。这时，就使用到了多行合并的功能 配置案例： 123456- type: log paths: - /home/appdeploy/deploy/logs/rider-mission-card/rider-mission-card.log multiline.pattern: &apos;^[0-9]&#123;4&#125;-[0-9]&#123;2&#125;-[0-9]&#123;2&#125;&apos; multiline.negate: true multiline.match: after 上面配置的意思是：不以’^[0-9]{4}-[0-9]{2}-[0-9]{2}’开头的行都合并到上一行的末尾 配置项说明： pattern：正则表达式 negate：true 或 false；控制是否匹配上面的正则表达式规则 默认是false，匹配pattern的行，合并到上一行； true，匹配到pattern的行，不合并到上一行 match：after 或 before，控制如何合并。合并到上一行的末尾或者开头。 处理java等日志时，我们肯定是合并到上一行的后面。 加载外部的配置文件prospector config探勘器、查找器使用如下的方式去加载外部的配置文件： 123filebeat.config.prospectors: enabled: true path: configs/*.yml 被加载的外部文件的格式应该是： 123456789- type: log paths: - /var/log/mysql.log scan_frequency: 10s- type: log paths: - /var/log/apache.log scan_frequency: 5s 动态重加载当配置的，加载外部文件的路径下有文件更新时，filebeat需要能够感知，所以filebeat提供了一个reload的功能。 配置方式如下： 12345filebeat.config.prospectors: enabled: true path: configs/*.yml reload.enabled: true reload.period: 10s 注意：period的值不要设置少于1s，因为修改文件的操作往往在s级左右， output配置es当使用filebeat直接连接es或者kibana时，如果有密码配置，配置文件应该为： 12345678output.elasticsearch: hosts: [&quot;myEShost:9200&quot;] username: &quot;elastic&quot; password: &quot;elastic&quot;setup.kibana: host: &quot;mykibanahost:5601&quot; username: &quot;elastic&quot; password: &quot;elastic&quot; logstash在filebeat的output中指定logstash即可： 12345#----------------------------- Logstash output --------------------------------output.logstash: hosts: [&quot;127.0.0.1:5044&quot;] loadbalance: true worker: 4 这里有几个比较常用的参数： worker：指定为每个主机的工作进程数，例如设置为2，output主机为2，那么总数为4 loadbalance：当后端有多个logstash主机时，可以设置为true kafka配置格式如下： 终端控制台输出filebeat监控配置文件权限问题filebeat在启动的时候，可能会出现下面的问题： 12019-08-02T15:57:04.027+0800 ERROR cfgfile/reload.go:237 Error loading config: invalid config: config file (&quot;/home/appdeploy/deploy/tools/filebeat6/conf/httpdns.yml&quot;) can only be writable by the owner but the permissions are &quot;-rw-rw-r--&quot; (to fix the permissions use: &apos;chmod go-w /home/appdeploy/deploy/tools/filebeat6/conf/httpdns.yml&apos;) 文件的权限需要是644，在创建文件时，默认的权限是664 系统日志权限问题当收集系统日志时，例如cron、secure、message、kern.log等时，如果filebeat使用的时候非root用户启动，那么默认是没有权限读取这些文件的，这个时候，我们就需要对这些日志进行相应的配置，保证启动用户对系统日志有权限。 filebea案例自定义文件收集收集流程： filebeat–&gt;kafka–&gt;logstash–&gt;es 文件的内容如下： 1234567892019-12-18 03:13:22,299 run_train(129) INFO 7657813a084f Epoch 15 Batch 0 Loss 1.02172019-12-18 03:13:28,138 run_train(129) INFO 7657813a084f Epoch 15 Batch 100 Loss 1.09072019-12-18 03:13:32,126 run_train(134) INFO 7657813a084f Epoch 15 Loss 1.10822019-12-18 03:13:32,126 run_train(135) INFO 7657813a084f Time taken for 1 epoch 11.139458894729614 sec2019-12-18 03:13:34,025 run_train(129) INFO 7657813a084f Epoch 16 Batch 0 Loss 1.01272019-12-18 03:13:39,955 run_train(129) INFO 7657813a084f Epoch 16 Batch 100 Loss 1.09672019-12-18 03:13:44,099 run_train(134) INFO 7657813a084f Epoch 16 Loss 1.07112019-12-18 03:13:44,100 run_train(135) INFO 7657813a084f Time taken for 1 epoch 11.973908185958862 sec 启动filebeat: 1nohup /home/appdeploy/deploy/tools/filebeat6/filebeat -e -c /home/appdeploy/deploy/tools/filebeat6/applogg.yml &gt;&gt;/home/appdeploy/deploy/tools/filebeat6/applogg.log &amp; filebeat配置文件： 1234567891011121314151617181920212223[appdev@common006-dev.novalocal filebeat6]$ cat applogg.ymlfilebeat.shutdown_timeout: 10sfilebeat.registry_file_permissions: 0644filebeat.config.inputs: enabled: true path: conf/*.yml reload.enabled: true reload.period: 10soutput.kafka: hosts: [&quot;192.168.1.160:19092&quot;, &quot;192.168.1.188:19092&quot;, &quot;192.168.1.202:19092&quot;] topic: &apos;%&#123;[fields.log_topic]&#125;&apos; key: &apos;%&#123;[fields.app_name]&#125; %&#123;[beat.hostname]&#125;&apos; required_acks: 1 workers: 6 timeout: 120 compression: snappy max_message_bytes: 10240000 codec.format: string: &apos;%&#123;[fields.app_name]&#125; %&#123;[beat.hostname]&#125; %&#123;[fields.namespace]&#125; %&#123;[message]&#125;&apos;#xpack.monitoring:# enabled: true# elasticsearch:# hosts: [&quot;http://192.168.1.188:9200&quot;, &quot;http://192.168.1.160:9200&quot;, &quot;http://192.168.1.143:9200&quot;] logstash配置文件： 12345678910111213141516171819202122232425262728293031323334353637[elk@bigdata013-dev config]$ grep -v &apos;#&apos; algtoes.ymlinput &#123; kafka &#123; codec =&gt; &quot;plain&quot; group_id =&gt; &quot;kafkatoes&quot; bootstrap_servers =&gt; &quot;192.168.1.160:19092&quot; auto_offset_reset =&gt; &quot;latest&quot; consumer_threads =&gt; 3 topics =&gt; &quot;training-test&quot; &#125;&#125;filter &#123; dissect &#123; mapping =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;app_name&#125; %&#123;host&#125; %&#123;namespace&#125; %&#123;logtime&#125; %&#123;+logtime&#125; %&#123;module_line&#125; %&#123;level&#125; %&#123;podname&#125; %&#123;content&#125;&quot; &#125; &#125; if &quot;_dissectfailure&quot; in [tags] &#123; drop &#123; &#125; &#125; if [logtime] !~ &quot;^[0-9]&#123;4&#125;-[0-9]&#123;2&#125;-[0-9]&#123;2&#125;&quot; &#123; drop &#123;&#125; &#125; date &#123; match =&gt; [&quot;logtime&quot;, &quot;yyyy-MM-dd HH:mm:ss,SSS&quot;, &quot;yyyy-MM-dd HH:mm:ss&quot;, &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;, &quot;yyyy-MM-dd HH:mm:ss:SSS&quot;] remove_field =&gt; [&quot;message&quot;,&quot;logtime&quot;,&quot;@version&quot;] &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;192.168.1.143:9200&quot;,&quot;192.168.1.160:9200&quot;,&quot;192.168.1.188:9200&quot;] index =&gt; &quot;training-test-%&#123;+YYYY.MM.dd&#125;&quot; document_type =&gt; &quot;doc&quot; &#125; stdout &#123; codec =&gt; rubydebug &#125; &#125; Logstashlogstash概述logstash是一个具备实时pipelining流水线能力的数据收集引擎，它能动态的统一来自不同数据源的数据，规范化后发往output， logstash工作流logstash的工作流非常简单：inputs → filters → outputs input： logstash术语表-glossary of terms参考链接：https://www.elastic.co/guide/en/logstash/6.2/glossary.html 安装我们一般使用源码包的安装方式 1wget https://artifacts.elastic.co/downloads/logstash/logstash-6.2.3.tar.gz 下载下来之后，解压即可。 在安装之后，我们可以启动一个最简单的pipeline去测试功能： 12cd logstash-6.2.3bin/logstash -e &apos;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;&apos; -e参数可以让我们在命令行直接指定配置，而不是去编辑配置配置，这里的意思是input和output是使用标准输入和标准输出。 完整的输出如下： 1234567891011121314151617181920[elk@node002 logstash-6.2.3]$ bin/logstash -e &apos;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;&apos;Sending Logstash&apos;s logs to /home/elk/logstash-6.2.3/logs which is now configured via log4j2.properties[2019-07-29T10:53:06,777][INFO ][logstash.modules.scaffold] Initializing module &#123;:module_name=&gt;&quot;fb_apache&quot;, :directory=&gt;&quot;/home/elk/logstash-6.2.3/modules/fb_apache/configuration&quot;&#125;[2019-07-29T10:53:06,874][INFO ][logstash.modules.scaffold] Initializing module &#123;:module_name=&gt;&quot;netflow&quot;, :directory=&gt;&quot;/home/elk/logstash-6.2.3/modules/netflow/configuration&quot;&#125;[2019-07-29T10:53:08,037][INFO ][logstash.setting.writabledirectory] Creating directory &#123;:setting=&gt;&quot;path.queue&quot;, :path=&gt;&quot;/home/elk/logstash-6.2.3/data/queue&quot;&#125;[2019-07-29T10:53:08,088][INFO ][logstash.setting.writabledirectory] Creating directory &#123;:setting=&gt;&quot;path.dead_letter_queue&quot;, :path=&gt;&quot;/home/elk/logstash-6.2.3/data/dead_letter_queue&quot;&#125;[2019-07-29T10:53:11,772][WARN ][logstash.config.source.multilocal] Ignoring the &apos;pipelines.yml&apos; file because modules or command line options are specified[2019-07-29T10:53:12,855][INFO ][logstash.agent ] No persistent UUID file found. Generating new UUID &#123;:uuid=&gt;&quot;f9f6fb83-176e-4ba1-8065-ca5699df67fa&quot;, :path=&gt;&quot;/home/elk/logstash-6.2.3/data/uuid&quot;&#125;[2019-07-29T10:53:25,521][INFO ][logstash.runner ] Starting Logstash &#123;&quot;logstash.version&quot;=&gt;&quot;6.2.3&quot;&#125;[2019-07-29T10:53:34,693][INFO ][logstash.agent ] Successfully started Logstash API endpoint &#123;:port=&gt;9600&#125;[2019-07-29T10:54:13,765][INFO ][logstash.pipeline ] Starting pipeline &#123;:pipeline_id=&gt;&quot;main&quot;, &quot;pipeline.workers&quot;=&gt;4, &quot;pipeline.batch.size&quot;=&gt;125, &quot;pipeline.batch.delay&quot;=&gt;50&#125;[2019-07-29T10:54:16,628][INFO ][logstash.pipeline ] Pipeline started succesfully &#123;:pipeline_id=&gt;&quot;main&quot;, :thread=&gt;&quot;#&lt;Thread:0x7028647c run&gt;&quot;&#125;The stdin plugin is now waiting for input:[2019-07-29T10:54:19,144][INFO ][logstash.agent ] Pipelines running &#123;:count=&gt;1, :pipelines=&gt;[&quot;main&quot;]&#125;2019-07-29T02:54:20.429Z node0022019-07-29T02:54:20.463Z node0022019-07-29T02:54:34.399Z node002hello world2019-07-29T02:54:39.357Z node002 hello world 配置filebeat+logstash配置案例filebeat端配置： 12345678[appdeploy@node001 filebeat6]$ cat filebeat.ymlfilebeat.prospectors:- type: log paths: - /home/appdeploy/logstash-tutorial.logoutput.logstash: hosts: [&quot;192.168.101.172:5144&quot;][appdeploy@node001 filebeat6]$ 启动filebeat： 1./filebeat -e -c filebeat.yml -d "publish" logstash端配置： 12345678910[elk@node002 logstash-6.2.3]$ cat config/first-pipeline.confinput &#123; beats &#123; port =&gt; &quot;5144&quot;&#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125;&#125; 编辑完配置文件之后，我们可以检测一下配置文件的正确性： 1[elk@node002 logstash-6.2.3]$ ./bin/logstash -f config/first-pipeline.conf --config.test_and_exit 执行后的输出如下： 1234567Sending Logstash&apos;s logs to /home/elk/logstash-6.2.3/logs which is now configured via log4j2.properties[2019-07-29T11:22:44,111][INFO ][logstash.modules.scaffold] Initializing module &#123;:module_name=&gt;&quot;fb_apache&quot;, :directory=&gt;&quot;/home/elk/logstash-6.2.3/modules/fb_apache/configuration&quot;&#125;[2019-07-29T11:22:44,199][INFO ][logstash.modules.scaffold] Initializing module &#123;:module_name=&gt;&quot;netflow&quot;, :directory=&gt;&quot;/home/elk/logstash-6.2.3/modules/netflow/configuration&quot;&#125;[2019-07-29T11:22:49,059][WARN ][logstash.config.source.multilocal] Ignoring the &apos;pipelines.yml&apos; file because modules or command line options are specifiedConfiguration OK[2019-07-29T11:23:29,319][INFO ][logstash.runner ] Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash[elk@node002 logstash-6.2.3]$ 检测通过之后，我们可以启动logsatsh 1bin/logstash -f config/first-pipeline.conf --config.reload.automatic —config.reload.automatic参数的作用是，当我们修改了配置文件之后，进程能够自动的reload grok过滤查看当前logstash已安装的所有插件： 1[elk@node004 logstash-6.2.3]$ bin/logstash-plugin list grok插件能够帮助你解析结构凌乱的日志，转变成有结构的以及可查询的。 从源数据中拆分出我们需要的字段进行展示。 grok使用正则表达式去匹配源的数据 一个简单的案例： 我们使用logstash内置的%{COMBINEDAPACHELOG}进行匹配 12345678910111213141516[elk@node004 config]$ cat first-pipeline.confinput &#123; beats &#123; port =&gt; &quot;5144&quot;&#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;COMBINEDAPACHELOG&#125;&quot; &#125;&#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125;&#125; reload或者重启后，在filebeat节点上删除注册文件之后，在重新启动filebeat，logstash端的显示为： 123456789101112131415161718192021222324252627282930&#123; &quot;@timestamp&quot; =&gt; 2019-07-29T08:53:35.835Z, &quot;message&quot; =&gt; &quot;192.1.76.62 - - [04/Jan/2015:05:30:37 +0000] \&quot;GET /style2.css HTTP/1.1\&quot; 200 4877 \&quot;http://www.semicomplete.com/projects/xdotool/\&quot; \&quot;Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20140205 Firefox/24.0 Iceweasel/24.3.0\&quot;&quot;, &quot;ident&quot; =&gt; &quot;-&quot;, &quot;prospector&quot; =&gt; &#123; &quot;type&quot; =&gt; &quot;log&quot; &#125;, &quot;timestamp&quot; =&gt; &quot;04/Jan/2015:05:30:37 +0000&quot;, &quot;clientip&quot; =&gt; &quot;192.1.76.62&quot;, &quot;referrer&quot; =&gt; &quot;\&quot;http://www.semicomplete.com/projects/xdotool/\&quot;&quot;, &quot;verb&quot; =&gt; &quot;GET&quot;, &quot;source&quot; =&gt; &quot;/home/appdeploy/logstash-tutorial.log&quot;, &quot;beat&quot; =&gt; &#123; &quot;hostname&quot; =&gt; &quot;node001&quot;, &quot;version&quot; =&gt; &quot;6.2.3&quot;, &quot;name&quot; =&gt; &quot;node001&quot; &#125;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;node001&quot;, &quot;response&quot; =&gt; &quot;200&quot;, &quot;bytes&quot; =&gt; &quot;4877&quot;, &quot;agent&quot; =&gt; &quot;\&quot;Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20140205 Firefox/24.0 Iceweasel/24.3.0\&quot;&quot;, &quot;request&quot; =&gt; &quot;/style2.css&quot;, &quot;offset&quot; =&gt; 24898, &quot;tags&quot; =&gt; [ [0] &quot;beats_input_codec_plain_applied&quot; ], &quot;auth&quot; =&gt; &quot;-&quot;, &quot;httpversion&quot; =&gt; &quot;1.1&quot;&#125; %{COMBINEDAPACHELOG}包含的格式是： Information Field Name IP Address clientip User ID ident User Authentication auth timestamp timestamp HTTP Verb verb Request body request HTTP Version httpversion HTTP Status Code response Bytes served bytes Referrer URL referrer User agent agent 注意： 使用了grok之后，生成了我们需要的自定义字段之后，发往output的这个event依旧包含这个原始的未被拆分的message 设置output为es在配置文件中，配置输出为es: 12345output &#123; elasticsearch &#123; hosts =&gt; [ &quot;localhost:9200&quot; ] &#125;&#125; 现在整体的配置为： 123456789101112131415161718192021[elk@node004 logstash-6.2.3]$ cat config/first-pipeline.confinput &#123; beats &#123; port =&gt; &quot;5144&quot;&#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;COMBINEDAPACHELOG&#125;&quot; &#125;&#125; geoip &#123; source =&gt; &quot;clientip&quot;&#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [ &quot;192.168.100.113:9500&quot; ]&#125;&#125; 启动参数主要需要关注的参数有： -f：指定配置文件 -w：指定pipeline的工作进程数量。 -b：指定每个工作线程最大处理的event数量，默认是125。调大这个数量也就以为着更大的内存消耗，这个需要参考JVM参数进行配置。 —pipeline.unsafe_shutdown：强制logstash在还有event没有处理完成的情况下就退出。 -r, —config.reload.automatic：在配置文件修改之后，自动reload配置文件 logstash工作模型logstash事件处理管道有3个阶段，输入–&gt;过滤–&gt;输出 logstash事件管道的每一个input阶段都运行在其自己的线程中。input将event事件写入内存(默认)或者磁盘上的中央队列中，然后管道的工作线程从这个队列中取出一批event，然后通过filter过滤器过滤这一批event，将结果发送到output中。 logstash性能优化参考链接：https://www.elastic.co/guide/en/logstash/6.2/tuning-logstash.html logstash的性能优化主要涉及以下几个参数： logstash版本 启动参数-w。（–pipeline-workers） 控制filter和output模块的pipeline的线程数量，默认会是cpu的核数 logstash正则解析极其消耗计算资源，当业务要求大量的正则解析时，filter会是我们瓶颈。官方建议线程数设置大于核数，因为存在I/O等待，可以设置为n+1。 启动参数-b。（–pipeline-batch-size） batch_size，设置积累多少条日志进行向下传输。默认是125条，越大会越消耗JVM内存。 jvm内存 logstash是将输入存储在内存之中的， 主要是设置bulk批数量，工作线程数尽量保证，但是一般有一个上限，最好是物理核数下来一点，比如8核的配置6个 提取字段已经转变数据参考链接：https://www.elastic.co/guide/en/logstash/6.2/field-extraction.html 当输入是通用的日志格式时，我们可以使用grok去进行拆解 当输出是不规则的日志格式时，我们可以使用dissect去自定义我们的字段 logstash的@metadata字段参考链接：https://www.elastic.co/guide/en/logstash/6.2/event-dependent-configuration.html#metadata 每一种类型的input，logstash都会生成对应的@metadata字段，具体的信息在input部分会有记录 默认情况下，如果不进行特殊设置，我们是看不到@metadata的内容的： 添加： 1&#123; metadata =&gt; true &#125; 在调试模式下，output的设置是： 123output &#123; stdout &#123; codec =&gt; rubydebug &#123; metadata =&gt; true &#125; &#125;&#125; 在input为filebeat时，输出为： 123456 &quot;@metadata&quot; =&gt; &#123; &quot;beat&quot; =&gt; &quot;filebeat&quot;, &quot;version&quot; =&gt; &quot;6.2.3&quot;, &quot;type&quot; =&gt; &quot;doc&quot;, &quot;ip_address&quot; =&gt; &quot;192.168.100.113&quot;&#125;, input部分Common option下面这些是通用配置，在所有的input中都支持 Setting Input type Required add_field hash No codec codec No enable_metric boolean No id string No tags array No type string No beats参考配置： 1234567891011121314input &#123; beats &#123; port =&gt; 5044 &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; &quot;localhost:9200&quot; manage_template =&gt; false index =&gt; &quot;%&#123;[@metadata][beat]&#125;-%&#123;[@metadata][version]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; document_type =&gt; &quot;%&#123;[@metadata][type]&#125;&quot; &#125;&#125; kafka本质上是logstash启动一个kafka的客户端，因为存在一定的版本兼容性问题，这里需要注意 在kafka中，主要关注这些参数： auto_offset_reset 当没有最初始的偏移量或者偏移量超出范围了，这个时候，做什么操作 earliest：reset偏移量为最早的 latest：reset偏移量为最晚的，也就是最新的。 bootstrap_servers kafka集群的地址。格式是：host1:port1,host2:port2 group_id 默认值是logstash。设置消费组的名称。 session_timeout_ms 如果poll_timeout_ms时间（等待kafka推送消息的超时时间，默认是100ms）超时，过多长时间之后，这个消费者将会被标记为dead。 request_timeout_ms logstash上的kafka客户端等待响应的超时时间。 max_poll_records 单次调用的最大值 consumer_threads 默认值为1。最理想的情况是线程总数与topic的partition总数相同。如果大于partition总数，则会产生线程空闲，导致资源浪费。 topics 监听的topic列表，默认值是logstash。 filter部分grok解析非结构化的日志，将日志拆分成为结构化的可查询的字段数据。 grok中预先定义了很多的正则表达式，在使用的时候只需要调用即可。 grok 表达式的打印复制格式的完整语法是下面这样的： 1%&#123;PATTERN_NAME:capture_name:data_type&#125; PATTERN_NAME。代表匹配值的类型,例如3.44可以用NUMBER类型所匹配,127.0.0.1可以使用IP类型匹配。 capture_name。代表存储该值的一个变量名称,例如 3.44 可能是一个事件的持续时间,127.0.0.1可能是请求的client地址。所以这两个值可以用 %{NUMBER:duration} %{IP:client_ip} 来匹配。 案例1: 123456789input &#123;stdin&#123;&#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;WORD&#125; %&#123;NUMBER:request_time:float&#125; %&#123;WORD&#125;&quot; &#125; &#125;&#125;output &#123;stdout&#123;&#125;&#125; 运行如下： 1234567&#123; &quot;message&quot; =&gt; &quot;begin 123.456 end&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;@timestamp&quot; =&gt; &quot;2014-08-09T12:23:36.634Z&quot;, &quot;host&quot; =&gt; &quot;raochenlindeMacBook-Air.local&quot;, &quot;request_time&quot; =&gt; 123.456&#125; 把 request_time 变成数值类型。如果不使用这个，使用下面这种，那么将会拆分为字符串 123456789input &#123;stdin&#123;&#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;\s+(?&lt;request_time&gt;\d+(?:\.\d+)?)\s+&quot; &#125; &#125;&#125;output &#123;stdout&#123;&#125;&#125; 注意： grok match本质是一个正则匹配,默认出来的数据都是String.有些时候我们知道某个值其实是个数据类型,这时候可以直接指定数据类型. 不过match中仅支持直接转换成int ,float,语法是 %{NUMBER:response_time:int} 案例2： 12345filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;IP:client&#125; %&#123;WORD:method&#125; %&#123;URIPATHPARAM:request&#125; %&#123;NUMBER:bytes&#125; %&#123;NUMBER:duration&#125;&quot; &#125; &#125;&#125; 以下是filter结果： 12345● client: 55.3.244.1● method: GET● request: /index.html● bytes: 15824● duration: 0.043 自定义pattern引入文件方法 很多时候logstash grok没办法提供你所需要的匹配类型，这个时候我们可以使用自定义 grok里面的match的message，其实是定义在这里的各种pattern。我们可以自定义pattern。 形式如下 12345USERNAME [a-zA-Z0-9._-]+USER %&#123;USERNAME&#125;EMAILLOCALPART [a-zA-Z][a-zA-Z0-9_.+-=:]+EMAILADDRESS %&#123;EMAILLOCALPART&#125;@%&#123;HOSTNAME&#125;INT (?:[+-]?(?:[0-9]+)) 将这些pattern保存在文件中，然后在logstash的config中制定读取pattern的目录即可。 123456filter &#123; grok &#123; patterns_dir =&gt; [&quot;/usr/local/logstash/patterns&quot;] match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;USER: user&#125; %&#123;INT: age&#125; %&#123;EMAILADDRESS: email&#125;&quot;&#125; &#125;&#125; 这样，我们输出的时候可以获得到message,user,age,email这几个field。 配置文件内即时配置方法： 1语法：(?&lt;field_name&gt;the pattern here) 就是上面的这个案例： 123456789input &#123;stdin&#123;&#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;\s+(?&lt;request_time&gt;\d+(?:\.\d+)?)\s+&quot; &#125; &#125;&#125;output &#123;stdout&#123;&#125;&#125; 拆分出来，就是这个： 123?&lt;request_time&gt;\d+(?:\.\d+)?\d+(?:\.\d+)? 这个就是这个字段的匹配规则 \s：匹配任何不可见字符，包括空格、制表符、换页符等等。等价于[ \f\n\r\t\v]。+表示匹配次数为1次或者多次 (? )：这个是grok语法,request_time表示要将捕获的字符定义成的字段名 \d+：匹配一个或者多个数字 (?:.\d+)：为正则表达式， (?: pattern):非获取匹配，匹配pattern但不获取匹配结果，不进行存储供以后使用。这在使用或字符“(|)”来组合一个模式的各个部分是很有用。例如“industr(?:y|ies)”就是一个比“industry|industries”更简略的表达式。 .\d+：表示点后面跟一个或者多个 数字，(?:.\d+)?表示点后面跟一个或多个数字这种情况出现0次或者多次，如果为0次，则request_time为一个整数。所以匹配到的结果可能为123.456或者123或者123.4.5.6，这些都满足条件 注意： ()表示捕获分组，()会把每个分组里的匹配的值保存起来，使用$n(n是一个数字，表示第n个捕获组的内容)(?:)表示非捕获分组，和捕获分组唯一的区别在于，非捕获分组匹配的值不会保存起来 overwrite如果原本的字段需要重写掉，那么： 123456filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;SYSLOGBASE&#125; %&#123;DATA:message&#125;&quot; &#125; overwrite =&gt; [ &quot;message&quot; ] &#125;&#125; add_fieldremove_field#### dissectdissect主要用于切割操作。dissect不使用正则表达式去进行匹配，因此，它的处理速度是非常快的。 如果你的内容每一行都是比较规则的，那么grok是更合适的。 分隔字段的这个动作，我们叫做解刨dissection 语法如下： 1%&#123;a&#125; - %&#123;b&#125; - %&#123;c&#125; {}内的是拆分之后的字段名称 一个案例： 1234567filter &#123; dissect &#123; mapping =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;ts&#125; %&#123;+ts&#125; %&#123;+ts&#125; %&#123;src&#125; %&#123;&#125; %&#123;prog&#125;[%&#123;pid&#125;]: %&#123;msg&#125;&quot; &#125; &#125;&#125; 字段追加使用+号 mutatemutate过滤器能够允许你修改、优化你的event。包括：rename、remove、replace、modify字段 replace案例为： 123 mutate &#123; replace =&gt; &#123; &quot;type&quot; =&gt; &quot;nginx&quot;&#125;&#125; json配置案例： 123456filter &#123; json &#123; source =&gt; &quot;message&quot; remove_field =&gt; [&quot;message&quot;,&quot;@version&quot;] &#125;&#125; 将消息流中的message按照json格式进行解析 解析完毕之后再将消息流中不需要的字段进行删除 默认情况下，logstash会将解析出来的json字段放在event事件的根中（也就是第一层），如果想要将json字段放在二级的事件字段中，那么可以使用target字段。 也就相当于是给这个json串再组织一层并赋予名称。 案例如下： 123456filter &#123; json &#123; source =&gt; &quot;message&quot; target =&gt; &quot;jsoncontent&quot; &#125;&#125; 执行之后的结果： 12345678910&#123; &quot;@version&quot;: &quot;1&quot;, &quot;@timestamp&quot;: &quot;2014-11-18T08:11:33.000Z&quot;, &quot;host&quot;: &quot;web121.mweibo.tc.sinanode.com&quot;, &quot;message&quot;: &quot;&#123;\&quot;uid\&quot;:3081609001,\&quot;type\&quot;:\&quot;signal\&quot;&#125;&quot;, &quot;jsoncontent&quot;: &#123; &quot;uid&quot;: 3081609001, &quot;type&quot;: &quot;signal&quot; &#125;&#125; 去掉target之后的显示是： 12345678&#123; &quot;@version&quot;: &quot;1&quot;, &quot;@timestamp&quot;: &quot;2014-11-18T08:11:33.000Z&quot;, &quot;host&quot;: &quot;web121.mweibo.tc.sinanode.com&quot;, &quot;message&quot;: &quot;&#123;\&quot;uid\&quot;:3081609001,\&quot;type\&quot;:\&quot;signal\&quot;&#125;&quot;, &quot;uid&quot;: 3081609001, &quot;type&quot;: &quot;signal&quot;&#125; date相关链接：https://www.elastic.co/guide/en/logstash/6.2/plugins-filters-date.html#plugins-filters-date-locale date过滤器的作用是解析从事件字段中解析出时间，然后使用这个时间作为这个事件的timestamp（默认情况下，如果不进行任何设置，logstash使用接收到这个事件时的UTC时间） 时间的格式化对应关系： match字段：值的类型是列表，格式是：[ field,formats… ] 例如： 时间是：”Aug 13 2010 00:03:44” 12match =&gt; [ &quot;logdate&quot;, &quot;MMM dd yyyy HH:mm:ss&quot;, &quot;MMM d yyyy HH:mm:ss&quot;, &quot;ISO8601&quot; ] 匹配logdate字段，logdate字段的时间格式是后面中的其中一个（我们可以在这里定义多个匹配规则，ISO8601也是其中一个匹配规则） 下面这些匹配规则已经事先定好好了时间格式，帮助我们节省时间并且确保格式正确： ISO8601 - should parse any valid ISO8601 timestamp, such as 2011-04-19T03:44:01.103Z UNIX - will parse float or int value expressing unix time in seconds since epoch like 1326149001.132 as well as 1326149001 UNIX_MS - will parse int value expressing unix time in milliseconds since epoch like 1366125117000 TAI64N - will parse tai64n time values target字段 可以将匹配到的时间写入到一个指定名称字段中，如果没有设置，默认值是@timestamp 实际案例12345678910111213141516171819202122232425filter &#123; json &#123; source =&gt; &quot;message&quot; remove_field =&gt; [&quot;message&quot;,&quot;@version&quot;] &#125; date &#123; match =&gt; [&quot;startTime&quot;, &quot;yyyy-MM-dd HH:mm:ss,SSS&quot;,&quot;ISO8601&quot;] &#125; date &#123; match =&gt; [&quot;sendTime&quot;, &quot;yyyy-MM-dd HH:mm:ss,SSS&quot;,&quot;ISO8601&quot;] target =&gt; &quot;sendTime_Z&quot; &#125; date &#123; match =&gt; [&quot;endTime&quot;, &quot;yyyy-MM-dd HH:mm:ss,SSS&quot;,&quot;ISO8601&quot;] target =&gt; &quot;endTime_Z&quot; &#125; date &#123; match =&gt; [&quot;execTime&quot;, &quot;yyyy-MM-dd HH:mm:ss,SSS&quot;,&quot;ISO8601&quot;] target =&gt; &quot;execTime_Z&quot; &#125; date &#123; match =&gt; [&quot;startTime&quot;, &quot;yyyy-MM-dd HH:mm:ss,SSS&quot;,&quot;ISO8601&quot;] target =&gt; &quot;startTime_Z&quot; &#125;&#125; 时间格式化链接：https://www.elastic.co/guide/en/logstash/6.2/plugins-filters-date.html#_description_104 More details on the syntax部分 通用参数参考链接：https://www.elastic.co/guide/en/logstash/6.2/plugins-filters-grok.html#plugins-filters-grok-common-options The following configuration options are supported by all filter plugins: Setting Input type Required add_field hash No add_tag array No enable_metric boolean No id string No periodic_flush boolean No remove_field array No remove_tag array No if条件判断参考链接： https://www.elastic.co/guide/en/logstash/6.2/event-dependent-configuration.html output部分kafka只要关注的参数有： bootstrap_servers kafka集群的地址。格式是：host1:port1,host2:port2 topic_id topic的名称 compression_type 默认值为none。取值范围为：none、gzip、snappy、lz4 在logstash向后端kafka传输数据时，可以对数据进行压缩。这里定义使用压缩的算法 codec 定义输出的编码，默认是明文plain。 一个案例如下： 12345678910output &#123; kafka &#123; bootstrap_servers =&gt; &quot;172.24.48.63:19092,172.24.48.64:19092,172.24.48.65:19092&quot; topic_id =&gt; &quot;%&#123;topicname&#125;&quot; compression_type =&gt; &quot;lz4&quot; codec =&gt; plain &#123; format =&gt; &quot;%&#123;topicinfo&#125;&quot; &#125; &#125; &#125; Elasticsearch一个实际的案例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647input &#123; kafka &#123; codec =&gt; &quot;plain&quot; group_id =&gt; &quot;kafkatoes&quot; bootstrap_servers =&gt; &quot;172.24.48.76:19092,172.24.48.77:19092,172.24.48.78:19092&quot; auto_offset_reset =&gt; &quot;earliest&quot; consumer_threads =&gt; 4 topics =&gt; &quot;applog6&quot; &#125;&#125;filter &#123; dissect &#123; mapping =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;type&#125; %&#123;host&#125; %&#123;logtime&#125; %&#123;+logtime&#125; %&#123;level&#125; %&#123;content&#125;&quot; &#125; &#125; if &quot;_dissectfailure&quot; in [tags] &#123; drop &#123; &#125; &#125; if [type] !~ &quot;^[A-Za-z]&quot; &#123; drop &#123;&#125; &#125; if [logtime] !~ &quot;^[0-9]&#123;4&#125;-[0-9]&#123;2&#125;-[0-9]&#123;2&#125;&quot; &#123; drop &#123;&#125; &#125; mutate &#123; add_field =&gt; &#123; &quot;[@metadata][type]&quot; =&gt; &quot;%&#123;type&#125;&quot; &#125; remove_field =&gt; [&quot;type&quot;] &#125; date &#123; match =&gt; [&quot;logtime&quot;, &quot;yyyy-MM-dd HH:mm:ss,SSS&quot;, &quot;yyyy-MM-dd HH:mm:ss&quot;, &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;, &quot;yyyy-MM-dd HH:mm:ss:SSS&quot;, &quot;ISO8601&quot;] #timezone =&gt; &quot;+08:00&quot; remove_field =&gt; [&quot;message&quot;,&quot;logtime&quot;,&quot;@version&quot;] &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;10.10.10.86:9200&quot;,&quot;10.10.10.87:9200&quot;,&quot;10.10.10.88:9200&quot;] index =&gt; &quot;%&#123;[@metadata][type]&#125;-logstash-%&#123;+YYYY.MM.dd&#125;&quot; template_overwrite =&gt; true document_type =&gt; &quot;doc&quot; &#125;&#125; 一些参数说明： 参考链接：https://www.elastic.co/guide/en/logstash/6.2/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-action template_overwrite 布尔类型，取值为true|false。 The template_overwrite option will always overwrite the indicated template in Elasticsearch with either the one indicated by template or the included one. This option is set to false by default. If you always want to stay up to date with the template provided by Logstash, this option could be very useful to you. Likewise, if you have your own template file managed by puppet, for example, and you wanted to be able to update it regularly, this option could help there as well. Please note that if you are using your own customized version of the Logstash template (logstash), setting this to true will make Logstash to overwrite the “logstash” template (i.e. removing all customized settings) document_type document的type值，具体查看es部分，这里不过多概述 document_id 可以在写入es的时候，使用event的一个字段指定这个id，例如： 1document_id =&gt; &quot;%&#123;traceId&#125;&quot; 使用写入es中的一个document中的一个字段的值去生成id，主要目的是为了后续的搜索 action 这个参数说明在写入数据到es中的时候，是执行什么样的操作 取值范围是：index、delete、create、update 主要的区别以及使用场景是： create。创建指定id的文档，并且只能创建一次的场景下使用create。只适合用在像是用户创建之后就不能再更新的场景。其具体行为是： 必须指定document_id ，若不指定，直接返回失败 如果 document_id 在elasticsearch中不存在，创建一条新的文档，使用document_id 作为该文档的_id。 如果 document_id 在elasticsearch中存在，直接返回失败 update。对指定id的文档进行更新，并且在文档不存在的情况下，需要使用自定义的数据内容而非原始数据内容的，使用update。其具体行为是： 必须指定document_id ，若不指定，直接无法启动pipeline 有document_id 的情况下： 如果 document_id 在elasticsearch中不存在，创建一条新的文档，使用document_id 作为该文档的_id。 doc_as_upsert为true，使用event的值做为文档的值 scripted_upsert为true，使用script作为文档的值 其余情况，使用upsert作为文档的值 如果 document_id 在elasticsearch中存在，直接更新该文档。 index。这个是action的默认值。其具体行为是： 没有document_id 的情况下，会将数据索引到elasticsearch当中，并且由elasticsearch生成该文档的_id。因为没有指定document_id ，重复的数据会生成多条内容相同但_id不同的文档。 有document_id 的情况下： 如果 document_id 在elasticsearch中不存在，创建一条新的文档，使用document_id 作为该文档的_id。 如果 document_id 在elasticsearch中存在，直接更新该文档。 注意：如果指定了document_id ，会造成写入的效率降低，因为额外增加了查询该document_id 是否存在的过程。因此，在日志分析等只写入不更新的场景，就不要尝试自己去指定id。 update操作中的upsert（中文直译为更新插入）说明： &gt; update操作通过id去更新一个文档，其中有个特殊案例-如果被更新的文档事先不存在，那么就会调用upsert配置进行操作 &gt; &gt; upsert的配置有以下几种： &gt; &gt; upsert：数据类型为string，使用这个的值，而不是事件原本的内容作为文档的内容，也就是这个是最高优先级，可以在这里自定义控制文档的值 &gt; &gt; scripted_upsert：使用script作为文档的值 &gt; &gt; doc_as_upsert：使用logstash事件中的数据作为文档的内容 file官网的案例： 123456output &#123; file &#123; path =&gt; ... codec =&gt; line &#123; format =&gt; &quot;custom format: %&#123;message&#125;&quot;&#125; &#125;&#125; 一个实际的案例： 12345678910output &#123; if [logdate] =~ &quot;^[0-9]&#123;4&#125;-[0-9]&#123;2&#125;-[0-9]&#123;2&#125;&quot; and [loghour] =~ &quot;^[0-9]&#123;2&#125;&quot; &#123; file &#123; path =&gt; &quot;/nas/logs/%&#123;type&#125;/%&#123;logdate&#125;/%&#123;host&#125;/%&#123;type&#125;.log-%&#123;logdate&#125;-%&#123;loghour&#125;&quot; codec =&gt; line &#123; format =&gt; &quot;%&#123;logdate&#125; %&#123;loghour&#125;:%&#123;logms&#125; %&#123;content&#125;&quot; &#125; &#125;&#125;&#125; codec部分codec插件，改变了event中的数据展示方式/格式，codecs本质上也是一个过滤器，可以作用在input和output阶段 Codec 是 logstash 从 1.3.0 版开始新引入的概念(Codec 来自 Coder/decoder 两个单词的首字母缩写)。 在此之前，logstash 只支持纯文本形式输入，然后以过滤器处理它。但现在，我们可以在输入 期处理不同类型的数据，这全是因为有了 codec 设置。 Logstash 不只是一个input | filter | output 的数据流，而是一个 input | decode | filter | encode | output 的数据流！codec 就是用来 decode、encode 事件的。 简单说，就是在logstash读入的时候，通过codec编码解析日志为相应格式，从logstash输出的时候，通过codec解码成相应格式。 codec 的引入，使得 logstash 可以更好更方便的与其他有自定义数据格式的运维产品共存，比如 graphite、fluent、netflow、collectd，以及使用 msgpack、json、edn 等通用数据格式的其他产品等。 事实上，我们在第一个 “hello world” 用例中就已经用过 codec 了 —— rubydebug 就是一种 codec！虽然它一般只会用在 stdout 插件中，作为配置测试或者调试的工具。 plain用于event之间没有分隔的文本 在消息队列时，例如kafka，input一般都是plain format设置，指定输出的内容，例如在output是kafka时： 12345678910output &#123; kafka &#123; bootstrap_servers =&gt; &quot;172.24.48.63:19092,172.24.48.64:19092,172.24.48.65:19092&quot; topic_id =&gt; &quot;%&#123;topicname&#125;&quot; compression_type =&gt; &quot;lz4&quot; codec =&gt; plain &#123; format =&gt; &quot;%&#123;topicinfo&#125;&quot; &#125; &#125;&#125; line实际案例公网服务器日志收集公网服务器的filebeat，将日志发送到有对外IP的logstash，然后由logstash去处理数据，获取到数据之后，再将数据写入到kafka集群中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192[elk@node004 config]$ grep -v &apos;#&apos; httpdns.confinput &#123; beats &#123; port =&gt; &quot;5144&quot;&#125;&#125;filter &#123; if [source] == &quot;/home/appdeploy/deploy/logs/httpdns/httpdns.log&quot; &#123; mutate &#123; add_field =&gt; &#123; &quot;[@metadata][topic_name]&quot; =&gt; &quot;applog8&quot; &quot;[@metadata][app_name]&quot; =&gt; &quot;httpdns&quot;&#125;&#125;&#125; if [source] == &quot;/home/appdeploy/deploy/logs/httpdns/httpdns_error.log&quot; &#123; mutate &#123; add_field =&gt; &#123; &quot;[@metadata][topic_name]&quot; =&gt; &quot;errorlog&quot; &quot;[@metadata][app_name]&quot; =&gt; &quot;httpdns&quot;&#125;&#125;&#125; if [source] == &quot;/home/appdeploy/deploy/logs/monitor/httpdns_monitor.log&quot; &#123; mutate &#123; add_field =&gt; &#123; &quot;[@metadata][topic_name]&quot; =&gt; &quot;monitor&quot; &quot;[@metadata][app_name]&quot; =&gt; &quot;monitor&quot;&#125;&#125;&#125; if [source] == &quot;/var/log/secure&quot; &#123; mutate &#123; add_field =&gt; &#123; &quot;[@metadata][topic_name]&quot; =&gt; &quot;oslog&quot; &quot;[@metadata][app_name]&quot; =&gt; &quot;secure&quot;&#125;&#125;&#125; if [source] == &quot;/var/log/messages&quot; &#123; mutate &#123; add_field =&gt; &#123; &quot;[@metadata][topic_name]&quot; =&gt; &quot;oslog&quot; &quot;[@metadata][app_name]&quot; =&gt; &quot;messages&quot;&#125;&#125;&#125; if [source] == &quot;/var/log/cron&quot; &#123; mutate &#123; add_field =&gt; &#123; &quot;[@metadata][topic_name]&quot; =&gt; &quot;oslog&quot; &quot;[@metadata][app_name]&quot; =&gt; &quot;cron&quot;&#125;&#125;&#125; if [source] == &quot;/var/log/maillog&quot; &#123; mutate &#123; add_field =&gt; &#123; &quot;[@metadata][topic_name]&quot; =&gt; &quot;oslog&quot; &quot;[@metadata][app_name]&quot; =&gt; &quot;maillog&quot;&#125;&#125;&#125; if [source] == &quot;/var/log/kern.log&quot; &#123; mutate &#123; add_field =&gt; &#123; &quot;[@metadata][topic_name]&quot; =&gt; &quot;oslog&quot; &quot;[@metadata][app_name]&quot; =&gt; &quot;kern&quot;&#125;&#125;&#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#123; metadata =&gt; true &#125; &#125; file &#123; path =&gt; &quot;/home/elk/filebeat-to-file&quot; codec =&gt; line &#123; format =&gt; &quot;%&#123;[@metadata][topic_name]&#125; %&#123;[@metadata][app_name]&#125; %&#123;[beat][hostname]&#125; %&#123;message&#125;&quot;&#125;&#125; kafka &#123; bootstrap_servers =&gt; &quot;172.16.1.132:9092&quot; topic_id =&gt; &quot;%&#123;[@metadata][topic_name]&#125;&quot; compression_type =&gt; &quot;lz4&quot; codec =&gt; plain &#123; format =&gt; &quot;%&#123;[@metadata][app_name]&#125; %&#123;[beat][hostname]&#125; %&#123;message&#125;&quot; &#125;&#125;&#125; nginx错误日志收集filebeat端配置： 主配置文件 123456789101112131415161718[appdeploy@node001 filebeat6]$ grep -v &apos;#&apos; applogg.ymlfilebeat.shutdown_timeout: 10sfilebeat.config.prospectors: enabled: true path: conf/*.yml reload.enabled: true reload.period: 10soutput.kafka: hosts: [&quot;172.16.1.132:9092&quot;] topic: &apos;%&#123;[fields.log_topic]&#125;&apos; key: &apos;%&#123;[fields.app_name]&#125; %&#123;[beat.hostname]&#125;&apos; required_acks: 1 workers: 6 timeout: 120 compression: snappy max_message_bytes: 10240000 codec.format: string: &apos;%&#123;[fields.app_name]&#125; %&#123;[beat.hostname]&#125; %&#123;[message]&#125;&apos; 子配置文件 123456789101112131415161718192021[appdeploy@node001 conf]$ cat nginx.yml- type: log paths: - /data1/logs/nginx/access-*.log fields: log_topic: nginx app_name: nginx scan_frequency: 5s tail_files: true [appdeploy@node001 conf]$ cat nginx_error.yml- type: log paths: - /usr/local/tengine/logs/error.log - /data1/logs/nginx/error.log fields: log_topic: nginx_error app_name: nginx_error scan_frequency: 5s tail_files: true 启动命令： 1nohup ./filebeat -e -c applogg.yml &gt; start.log 2&gt;&amp;1 &amp; logstash端配置 落地到文件的配置 12345678910111213141516171819202122232425262728293031323334353637383940414243[elk@node004 config]$ cat nginx_errortofile.ymlinput&#123; kafka &#123; codec =&gt; &quot;plain&quot; group_id =&gt; &quot;kafkatofile&quot; bootstrap_servers =&gt; &quot;172.16.1.132:9092&quot; auto_offset_reset =&gt; &quot;latest&quot; consumer_threads =&gt; 5 topics =&gt; &quot;nginx_error&quot; &#125;&#125;filter &#123; dissect &#123; mapping =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;type&#125; %&#123;host&#125; %&#123;content&#125;&quot;&#125;&#125; mutate &#123; replace =&gt; &#123; &quot;type&quot; =&gt; &quot;nginx&quot;&#125;&#125; grok &#123; match =&gt; &#123; &quot;content&quot; =&gt; &quot;%&#123;YEAR:logyear&#125;/%&#123;MONTHNUM2:logmonth&#125;/%&#123;MONTHDAY:logday&#125; %&#123;HOUR:loghour&#125;:%&#123;MINUTE:logmin&#125;:%&#123;SECOND:logsec&#125; (?&lt;content1&gt;.+)&quot; &#125;&#125; if &quot;_dissectfailure&quot; in [tags] &#123; drop &#123; &#125; &#125; if &quot;_grokparsefailure&quot; in [tags] &#123; drop &#123; &#125; &#125;&#125;output &#123; if [type] == &quot;nginx&quot; &#123; file &#123; path =&gt; &quot;/nas/logs/%&#123;type&#125;/%&#123;logyear&#125;-%&#123;logmonth&#125;-%&#123;logday&#125;/%&#123;host&#125;/error.log-%&#123;logyear&#125;-%&#123;logmonth&#125;-%&#123;logday&#125;-%&#123;loghour&#125;&quot; codec =&gt; line &#123; format =&gt; &quot;%&#123;content&#125;&quot; &#125; &#125; &#125;&#125; 落地的文件的时候，和nginx的access放在同一个目录下，所以把type转变成为nginx 写入到es的配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344[elk@node004 config]$ cat nginx_errortoes.ymlinput &#123; kafka &#123; codec =&gt; &quot;plain&quot; group_id =&gt; &quot;kafkatoes&quot; bootstrap_servers =&gt; &quot;172.16.1.132:9092&quot; auto_offset_reset =&gt; &quot;latest&quot; consumer_threads =&gt; 5 topics =&gt; &quot;nginx_error&quot; &#125;&#125;filter &#123; dissect &#123; mapping =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;type&#125; %&#123;host&#125; %&#123;logtime&#125; %&#123;+logtime&#125; %&#123;content&#125;&quot; &#125; &#125; mutate &#123; add_field =&gt; &#123; &quot;[@metadata][type]&quot; =&gt; &quot;%&#123;type&#125;&quot; &#125; remove_field =&gt; [&quot;type&quot;,&quot;@version&quot;,&quot;message&quot;] &#125; date &#123; match =&gt; [&quot;logtime&quot;, &quot;yyyy/MM/dd HH:mm:ss&quot;] remove_field =&gt; [&quot;logtime&quot;] &#125; if &quot;_dissectfailure&quot; in [tags] &#123; drop &#123; &#125; &#125; if &quot;_dateparsefailure&quot; in [tags] &#123; drop &#123; &#125; &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#123; metadata =&gt; true &#125; &#125; elasticsearch &#123; hosts =&gt; [&quot;172.16.1.128:9500&quot;] index =&gt; &quot;%&#123;[@metadata][type]&#125;-logstash-%&#123;+YYYY.MM.dd&#125;&quot; template_overwrite =&gt; true document_type =&gt; &quot;doc&quot; &#125;&#125; 索引的名称是：nginx_error-logstash-年月日 在es中创建索引模板： 例如： 123456789101112131415161718192021222324PUT _template/nginx_error&#123; &quot;order&quot;: 0, &quot;template&quot;: &quot;nginx_error-logstash*&quot;, &quot;settings&quot;:&#123; &quot;index&quot; : &#123; &quot;refresh_interval&quot;: &quot;5s&quot;, &quot;number_of_shards&quot;: &quot;5&quot;, &quot;number_of_replicas&quot;: &quot;1&quot; &#125; &#125;, &quot;mappings&quot;:&#123; &quot;doc&quot;: &#123; &quot;properties&quot;: &#123; &quot;host&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132GET _template/nginxlog&#123; &quot;nginxlog&quot; : &#123; &quot;order&quot; : 0, &quot;index_patterns&quot; : [ &quot;nginx*-logstash-*&quot; ], &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;mapping&quot; : &#123; &quot;total_fields&quot; : &#123; &quot;limit&quot; : &quot;2000&quot; &#125; &#125;, &quot;refresh_interval&quot; : &quot;60s&quot;, &quot;number_of_shards&quot; : &quot;6&quot;, &quot;translog&quot; : &#123; &quot;sync_interval&quot; : &quot;10s&quot;, &quot;durability&quot; : &quot;async&quot; &#125;, &quot;number_of_replicas&quot; : &quot;0&quot; &#125; &#125;, &quot;mappings&quot; : &#123; &quot;doc&quot; : &#123; &quot;dynamic_templates&quot; : [ &#123; &quot;strings_as_keywords&quot; : &#123; &quot;match_mapping_type&quot; : &quot;string&quot;, &quot;mapping&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;doc_values&quot; : &quot;false&quot; &#125; &#125; &#125; ], &quot;properties&quot; : &#123; &quot;geoip&quot; : &#123; &quot;properties&quot; : &#123; &quot;city_name&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;country_name&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;latitude&quot; : &#123; &quot;type&quot; : &quot;float&quot; &#125;, &quot;location&quot; : &#123; &quot;type&quot; : &quot;geo_point&quot; &#125;, &quot;longitude&quot; : &#123; &quot;type&quot; : &quot;float&quot; &#125;, &quot;region_name&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125; &#125; &#125;, &quot;remote_user&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;request_time&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125;, &quot;status&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;tags&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;index&quot; : false &#125;, &quot;type&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;upstream&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;upstream_response_time&quot; : &#123; &quot;type&quot; : &quot;float&quot; &#125;, &quot;url_path&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;user_agent&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;verb&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;index&quot; : false &#125;, &quot;host&quot; : &#123; &quot;index&quot; : &quot;true&quot;, &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;client&quot; : &#123; &quot;index&quot; : &quot;true&quot;, &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;parameters.day&quot; : &#123; &quot;fielddata&quot; : true, &quot;type&quot; : &quot;text&quot; &#125;, &quot;parameters.shopId&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;parameters.cityId&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;parameters.userId&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;parameters.riderId&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;http_host&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;parameters.createTimeEnd&quot; : &#123; &quot;fielddata&quot; : true, &quot;type&quot; : &quot;text&quot; &#125;, &quot;parameters.createTimeStart&quot; : &#123; &quot;fielddata&quot; : true, &quot;type&quot; : &quot;text&quot; &#125; &#125; &#125; &#125;, &quot;aliases&quot; : &#123; &#125; &#125;&#125; 启动命令为： 1nohup ./bin/logstash -f config/nginxtofile.yml --config.reload.automatic &gt; start.log 2&gt;&amp;1 &amp; 也可以不输出到start.log中，直接输出到/dev/null Elasticsearch部署参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/5.1/getting-started.html#getting-started ES工作原理基本概念Near Realtime (NRT) ES是一个接近实时的搜索平台，也就是说有些许的延迟，从索引文件到能够检索出结果的延迟一般在秒内 Cluser 集群由一个或多个node节点组成，这些节点协同存储所有的条目数据，并且都支持检索的功能以提高性能。 一个集群由一个唯一的名称标识，默认为：elasticsearch。一个节点只能够属于一个集群 Node 集群中的每一个服务器/服务节点叫做node。在一个集群中，每一个node都通过一个唯一的名称去标识(默认是一个随机的UUID)。每一个node会加到一个es集群当中，默认的集群名称是elasticsearch，可以指定集群的名称以创建多个不同的es集群。 一个集群可以只有一个node节点 Index 索引是一批收集的拥有相似特征的文件集合。 索引通过不同的名称去标识(必须为小写)，这个索引名称用于后续的检索、更新、删除文件内容等操作 Type 在一个索引中，可以定义一个或多个type类型，type就相当于是索引数据的分类字段。例如你的索引内容一个博客系统，你可以分别定义用户字段、评论字段、内容字段。 在5版本中有，6版本中只能设置一个，7版本中废弃。 index相当于是一个数据库，type相当于是表。 Document document是存储信息以及创建索引的基本单元。document通常是以JSON的方式存在的。document其实也可以理解是一段内容 注意：document在索引中存在时，必须被分配或者说指定给某一个type document是一条记录，其中包含各个字段。 Shard &amp; Replicas 每一个索引可以存储大量的数据。这些数据在落地到物理磁盘上时，很有可能超出单个node的限制。为了解决这个问题，ES提供了把索引分割的功能，每一个被分割出来的部分我们叫做shard分片。 在创建index的时候，我们可以指定分片的数量。 副本是为了提供分片的高可用，并且在查询的时候可以并行执行 这一部分比较核心，我们会有单独的篇幅讲解。 集群状态ES集群有3种状态：green，yellow，red。其中green状态表明集群是健康的。yellow表示集群可用，但是有部分索引的shard有异常。red状态表示集群有索引异常。 功能讲解es与lucene的对应关系及基本结构es将一个索引的数据分为多个shard分片（也就是主分片，副本分片是完整复制） shard分片在es中，是读写和存储数据的基本单元，使用分片的目的是分割大索引，让读写可以并行操作，每个读写请求是落到具体的某一个分片上的，并且分片可以独立执行读写任务。 一个es索引可以有多个shard，每一个shard对应一个lucene索引 lucene索引又是由很多的segment分段组成，每一个分段中会包含若干个document文档，每一个分段的数据结构是倒排索引。也就是在每一个分段内部，文档的字段会被拆分出来（使用分词器或语言处理工具），形成许多的Term es的每一次refresh都会生成一个新的分段 数据写入与刷新在写操作时，会现在内存中缓存一段数据，再将这些数据写入磁盘，每次写入磁盘的这批数据成为一个分段 一般情况下，通过操作系统write接口写入到磁盘的数据会先到达系统缓存（内存），write函数返回成功时，数据未必是被刷到磁盘。 通过手动的调用flush，或者操作操作系统通过一定策略将系统缓存刷到磁盘。这种策略大幅提升了写入效率，从write返回成功开始，无论数据有没有被刷到磁盘，该数据就已经对读取可见。 es正是使用了这种特性实现了近实时搜索，每秒refresh的时候产生一个分段，新的分段先写入文件系统缓存（也就是内存中），然后再执行调用flush执行刷盘操作，一旦写成功，就可以像其他文件一样被打开和读取了。 由于系统缓存机制是缓存一段数据之后才会写磁盘，也就意味着新分段不会立即写进磁盘。这个过程中，如果出现某些意外情况，例如主机断电等，可能会存在数据丢失。一般的做法是记录事务日志，对每次es的操作进行记录。当es启动的时候，会对translog中最后一个commit之后的变更操作重新执行（最后一个commit之后的操作不清楚是成功了还是失败了，所以再执行一次，操作的成功失败已commit为准）。 段合并在es中，每秒情况一次写缓存，将这些数据写入文件，这个过程称之为refresh，每次refresh会创建一个新的lucene分段。 分段数量太多会带来很多问题，因为每个分段都会消耗fd、内存，并且每个搜索请求都要轮流检查每个段（一个shard中有很多段），查询完再对结果进行合并。所以分段越多，搜索就会越慢，因此需要通过一定的策略将一些较小的分段合并成较大的分段。 常用的方案是选择大小相似的分段进行合并，在合并过程中，标记为删除的数据不会被写入新的分段，当合并结束，旧的分段数据会被删除，标记删除的数据才会从磁盘中删除（因为删除旧分段） 注意： 分段过程中，新段的产生需要一定的磁盘空间，所以我们需要保证系统有足够的剩余空间，如果旧的分段达到磁盘空间的一半时，将无法进行段合并。之前的旧分段中的数据也就无法删除 节点启动流程整体来说，一个节点的启动过程如下： 解析配置，包括配置文件和命令行参数 检查外部环境和内部环境，例如JVM版本、操作系统内核参数等 初始化内部资源，创建内部模块，初始化探测器 启动各个子模块和keepalive线程 ES安装配置基础环境配置JAVA配置 ES的运行需要java作为基础，最低版本要求为java 8。 根据以下命令操作： 1234567891011121314[root@node001 ~]# tar -zxvf jdk1.8.0_77.tar.gz -C /usr/local/[root@node001 ~]# ln -s /usr/local/jdk1.8.0_77 /usr/local/jdk[root@node001 ~]# vim /etc/profile # 在文件末尾添加一下内容JAVA_HOME=/usr/local/jdkPATH=$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/libJRE_HOME=$JAVA_HOME/jreexport JAVA_HOME PATH CLASSPATH JRE_HOME[root@node001 ~]# source /etc/profile[root@node001 ~]# java -versionjava version &quot;1.8.0_77&quot;Java(TM) SE Runtime Environment (build 1.8.0_77-b03)Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode) 系统环境配置 打开服务器的/etc/sysctl.conf文件，增加下面内容： 123456[root@node001 ~]# vim /etc/sysctl.confvm.max_map_count=655360vm.zone_reclaim_mode=0第二个一般都是为0，所以正常只需要设置vm.max_map_count=655360 保存后，执行命令: 1[root@node001 ~]# sysctl -p 创建elk用户，创建数据目录，设置数据目录的权限 123[root@node001 ~]# useradd elk[root@node001 ~]# su - elk[elk@node001 ~]$ mkdir data 注意，一般我们会将数据目录放在其他盘中，这个时候需要注意权限问题，此时的操作类型： 12345[root@node001 ~]# mkdir -p /data1/elk[root@node001 ~]# chown -R elk:elk /data1/elk如果是多目录：# mkdir -p /data&#123;1,2&#125;/elk/&#123;logs,data&#125;# chown -R elk:elk /data&#123;1,2&#125;/elk 配置/etc/security/limits.conf，修改文件描述符以及进程数限制 1234567891011[root@node001 ~]# vim /etc/security/limits.conf* soft nofile 655360* hard nofile 655360elk soft memlock unlimitedelk hard memlock unlimited[root@node001 ~]# vim /etc/security/limits.d/90-nproc.conf* soft nproc 65536root soft nproc unlimited# * hard nproc 65536 这个可选 注意：在limits.conf文件中的nproc的设置和/etc/security/limits.d/90-nproc.conf中proc设置有关 当在limits.conf中使用*号让全局用户生效的时候，生效的nproc的值大小受后者制约 因为，Linux系统默认先读取/etc/security/limits.conf 中的信息, 如果/etc/security/limits.d/目录下还有配置文件的话, 也会依次遍历读取, 最终, /etc/security/limits.d/中的配置会覆盖/etc/security/limits.conf 中的配置. 当在limits.conf中指定用户时，生效的nproc值不受该文件制约 下载执行以下命令 12[elk@node001 ~]$ curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.1.2.tar.gz[elk@node001 ~]$ tar -xvf elasticsearch-5.1.2.tar.gz 在做测试时，我们可以不经过任何配置直接启动，执行以下命令： 12[elk@node001 ~]$ cd elasticsearch-5.1.2/bin[elk@node001 bin]$ ./elasticsearch 如果需要使用守护进程方式启动，执行以下命令： 1234[elk@node001 bin]$ ./elasticsearch -d -p pid.file[elk@node001 bin]$ cat pid.file3104注：-p可以指定将pid记录到某一个文件中 ES配置及配置文件详解Elasticsearch loads its configuration from the $ES_HOME/config/elasticsearch.yml file by default. The format of this config file is explained in Configuring Elasticsearch. Any settings that can be specified in the config file can also be specified on the command line, using the -E syntax as follows: 1./bin/elasticsearch -d -Ecluster.name=my_cluster -Enode.name=node_1 ES在启动的时候，默认会从ES目录下的config目录下，读取elasticsearch.yml文件。 但是，除了从配置文件读取之外，也可以在启动的时候，手动的指定一些配置。 注意： 建议都从elasticsearch.yml配置文件中读取，不建议命令行指定。 建议将配置文件目录、数据目录、日志目录进行分离。 ES目录结构解压出来的es目录结构如下所示 1234567891011121314[elk@node001 elasticsearch-5.1.2]$ pwd/home/elk/elasticsearch-5.1.2[elk@node001 elasticsearch-5.1.2]$ lltotal 56drwxr-xr-x. 2 elk elk 4096 Jun 28 15:56 bindrwxr-xr-x. 3 elk elk 4096 Jun 20 17:16 configdrwxrwxr-x. 3 elk elk 4096 Jun 20 17:16 datadrwxr-xr-x. 2 elk elk 4096 Jan 12 2017 lib-rw-r--r--. 1 elk elk 11358 Jan 12 2017 LICENSE.txtdrwxrwxr-x. 2 elk elk 4096 Jun 28 15:20 logsdrwxr-xr-x. 12 elk elk 4096 Jan 12 2017 modules-rw-r--r--. 1 elk elk 150 Jan 12 2017 NOTICE.txtdrwxr-xr-x. 2 elk elk 4096 Jan 12 2017 plugins-rw-r--r--. 1 elk elk 9108 Jan 12 2017 README.textile 下面是官方的定义： Type Description Default Location Setting home Elasticsearch home directory or $ES_HOME Directory created by unpacking the archive bin Binary scripts including elasticsearch to start a node and elasticsearch-plugin to install plugins $ES_HOME/bin conf Configuration files including elasticsearch.yml $ES_HOME/config path.conf data The location of the data files of each index / shard allocated on the node. Can hold multiple locations. $ES_HOME/data path.data logs Log files location. $ES_HOME/logs path.logs plugins Plugin files location. Each plugin will be contained in a subdirectory. $ES_HOME/plugins repo Shared file system repository locations. Can hold multiple locations. A file system repository can be placed in to any subdirectory of any directory specified here. Not configured path.repo script Location of script files. $ES_HOME/scripts path.scripts 如何配置ESElasticsearch ships with good defaults and requires very little configuration. Most settings can be changed on a running cluster using the Cluster Update Settings API. The configuration files should contain settings which are node-specific (such as node.name and paths), or settings which a node requires in order to be able to join a cluster, such as cluster.nameand network.host. 注意： ES的大部分配置可以使用ES提供的接口在集群运行过程中动态的修改。 配置文件中应该要包含node节点相关的配置(例如node.name和path)，如果要加入es集群的话还需要添加集群相关的配置 配置文件： ES需要关注的配置文件有一下2个： elasticsearch.yml # ES的主配置文件 log4j2.properties # ES的日志相关配置 默认这两个配置文件在$ES_HOME/config/路径下，但是，我们也可以在启动的时候手动指定配置文件的加载路径，启动命令为： 1./bin/elasticsearch -Epath.conf=/path/to/my/config/ 配置格式： ES的相关配置都是使用YAML语法格式，下面是两种配置方式： 123path: data: /var/lib/elasticsearch logs: /var/log/elasticsearch 等价于 12path.data: /var/lib/elasticsearchpath.logs: /var/log/elasticsearch 环境变量引用 如果需要在配置文件中引入系统的环境变量，可以使用下面这种方式： 12node.name: $&#123;HOSTNAME&#125;network.host: $&#123;ES_NETWORK_HOST&#125; 根据提示输入配置 ES的配置文件支持交互式的输入配置，可以配置在启动的时候手动输入对应的配置，配置方式如下： 12node: name: $&#123;prompt.text&#125; 放前台启动es的时候，会有下面这种提示信息： 1Enter value for [node.name]: 注意：这种方式在启动方式为后台守护进程启动时是无效的。 设置默认配置 一些默认配置如果不想让es自动生成，那么我们可以进行自定义，命令如下： 1./bin/elasticsearch -Edefault.node.name=My_Node 注意：默认值只是在配置文件中没有配置，并且没有在命令行手动指定时生效。 日志相关配置 Elasticsearch uses Log4j 2 for logging. Log4j 2 can be configured using the log4j2.properties file. Elasticsearch exposes a single property ${sys:es.logs} that can be referenced in the configuration file to determine the location of the log files; this will resolve to a prefix for the Elasticsearch log file at runtime. For example, if your log directory (path.logs) is /var/log/elasticsearch and your cluster is named production then ${sys:es.logs} will resolve to /var/log/elasticsearch/production. ES使用java的log4J 2日志框架。ES在内部维护了一个变量：${sys:es.logs}，该变量记录的是es的集群名称 在输出日志时，这个日志框架将会调用这个变量生成对应的文件。 例如： log4j2.properties配置文件是这个内容时 123appender.rolling.type = RollingFile appender.rolling.name = rollingappender.rolling.fileName = $&#123;sys:es.logs&#125;.log 例如es集群名称为：testes，那么生成的日志文件名称将会是：testes.log 下面拿配置文件中的一段配置进行举例说明： 12345678910appender.rolling.type = RollingFileappender.rolling.name = rollingappender.rolling.fileName = $&#123;sys:es.logs&#125;.logappender.rolling.layout.type = PatternLayoutappender.rolling.layout.pattern = [%d&#123;ISO8601&#125;][%-5p][%-25c&#123;1.&#125;] %marker%.-10000m%nappender.rolling.filePattern = $&#123;sys:es.logs&#125;-%d&#123;yyyy-MM-dd&#125;.logappender.rolling.policies.type = Policiesappender.rolling.policies.time.type = TimeBasedTriggeringPolicyappender.rolling.policies.time.interval = 1appender.rolling.policies.time.modulate = true 注意： appender.rolling.filePattern。给这个配置参数赋值时，如果添加：.gz或者.zip这种后缀，那么在进行日志轮转的时候将会自动被压缩 deprecation log In addition to regular logging, Elasticsearch allows you to enable logging of deprecated actions. For example this allows you to determine early, if you need to migrate certain functionality in the future. By default, deprecation logging is enabled at the WARN level, the level at which all deprecation log messages will be emitted. 1logger.deprecation.level = warn This will create a daily rolling deprecation log file in your log directory. Check this file regularly, especially when you intend to upgrade to a new major version. The default logging configuration has set the roll policy for the deprecation logs to roll and compress after 1 GB, and to preserve a maximum of five log files (four rolled logs, and the active log). You can disable it in the config/log4j2.properties file by setting the deprecation log level to error. 在ES中，还专门定义了这么一个日志文件：查看被废弃或者被修改的功能的调用日志 当你在调用ES时，如果涉及到一些在后续版本中要废弃或者要修改的配置项时，ES将会将该内容，以便管理员知悉并判断是否需要升级例如 _all 即将被废弃，如果你某一个索引启用了 _all，则可能会有一条 deprecation log；通过这个日志的内容，可以指导你决定是否迁移到新的版本 重要配置参数说明path.data和path.logs在默认情况下，数据目录和日志目录在es的主目录下，如果我们修改了路径，那么在升级es版本的时候，这些目录可能被删除 但是，实际情况是，我们一般都会把这两个目录自定义出来 如果数据目录只需要一个，那么可以是： 123path: logs: /var/log/elasticsearch data: /var/data/elasticsearch 但是，es的数据目录可以设置为多个，如果我们有这方面的需求，我们可以设置为： 12345path: data: - /mnt/elasticsearch_1 - /mnt/elasticsearch_2 - /mnt/elasticsearch_3 注意：一个索引的分片只会存储在某一个目录中，不会分散存储。 cluster.name一个es节点只能加入到一个es集群当中，es集群的默认名称为：elasticsearch。在实际使用时，你应该将它修改为符合用途的名称 例如： 1cluster.name: logging-prod node.name如果在配置文件中没有配置，es默认将会分配一个随机的7个字符对节点进行标识。 注意，一旦分配，在这个node后续重启时，也将使用这个字符串，不会再重新生成。 这个配置参数建议是最好进行配置，配置方式例如： 1node.name: prod-data-2 如果想把node的名称定义为系统的主机名，es也提供了这个功能，使用这种方式： 1node.name: $&#123;HOSTNAME&#125; bootstrap.memory_lockIt is vitally important to the health of your node that none of the JVM is ever swapped out to disk. One way of achieving that is set the bootstrap.memory_lock setting to true. For this setting to have effect, other system settings need to be configured first. See Enable bootstrap.memory_lock for more details about how to set up memory locking correctly. 我们一般称之为锁内存配置。这个在ES中是及其重要的一个参数，开启了该参数后，能够保证，JVM内存被锁定在内存中，其中的数据永远不会被添加到swap交换分区中。 注意：如要要正确的开启这个参数，对系统的一些配置有要求。可以点击这个链接进行查看 开启配置： 1bootstrap.memory_lock: true 在es启动之后，我们可以通过下面的接口来确认是否正确开启： 1GET _nodes?filter_path=**.mlockall If you see that mlockall is false, then it means that the mlockall request has failed. You will also see a line with more information in the logs with the words Unable to lock JVM Memory. The most probable reason, on Linux/Unix systems, is that the user running Elasticsearch doesn’t have permission to lock memory. This can be granted as follows: 如果接口的返回结果是false，那么，我们可以在日志文件中根据关键字：”Unable to lock JVM Memory”进行搜索，查看具体原因 有许多原因会导致开启失败，罗列为： Set ulimit -l unlimited as root before starting Elasticsearch, or set memlock to unlimited in/etc/security/limits.conf. 需要打开涉及用户对内存的限制，如果没有配置，需要按照下方格式配置，这一部分我们在上面的基础环境配置时有记录 12elk soft memlock unlimitedelk hard memlock unlimited Another possible reason why mlockall can fail is that the temporary directory (usually /tmp) is mounted with the noexec option. This can be solved by specifying a new temp directory using the ES_JAVA_OPTS environment variable: 如果系统在挂载临时目录(通常为/tmp)时，添加了noexec（禁止执行二进制文件）参数，也会引起这个问题，我们可以通过自定义临时目录进行解决，配置为： 12export ES_JAVA_OPTS=&quot;$ES_JAVA_OPTS -Djava.io.tmpdir=/path/to/temp/dir&quot;./bin/elasticsearch 如果想永久生效，可以把这个配置记录到jvm.options配置文件中。 注意：因为我们是源码包安装形式，所以这里我们不关注通过rpm包以及yum等管理工具安装的情况 锁内存问题分析： 大多数操作系统都会将在内存中没有使用的数据转移到swap中，如果没有配置上述的参数，那么在es运行过程中，那么可能就会出现es占用的java堆内存中的数据被转移到swap中，也就是磁盘上。 我们知道，磁盘的读写性能和内存有非常大的差距，当在执行gc的时候，可能会导致gc时间从ms级别变成min级别，这么长的gc时间是绝对不能接受的，很有可能就会导致这个node节点响应变慢，甚至从集群中脱离。 通过上面的问题分析，我们知道，以下方式也是能杜绝这个问题 安装系统时不设置swap 如有swap，关闭swap 降低使用swap的倾向 关闭swap的操作如下： 临时生效，执行命令： 1sudo swapoff -a 永久生效，也就是系统重启后还生效 1编辑/etc/fstab文件，将挂载swap的配置注释 降低使用swap的倾向： 临时生效，执行命令： 1sysctl -w vm.swappiness=1 永久生效，也就是系统重启后还生效： 1echo &quot;vm.swappiness = 1&quot; &gt;&gt; /etc/sysctl.conf network.host默认情况下，es将服务监听在本机的回环地址，我们可以使用下列参数修改监听地址 1network.host: 192.168.1.10 注意： 默认情况下，es假定你是工作在开发模式，假如你有一些配置没有配置正确，es也能正常启动，并且将这些配置记录到日志文件中(日志级别为warning) 但是，一旦你配置了网络相关的配置，例如network.host，那么es将判断你从开发者模式提升为生产模式，这个时候如果有一些配置没有配置正确，那么es将产生异常，并且启动失败。 这种一种非常重要的安全机制，能够确保你不会因为错误的配置而导致数据丢失。 下面是当作为生产模式运行时，具体要检测的依赖配置 Ideally, Elasticsearch should run alone on a server and use all of the resources available to it. In order to do so, you need to configure your operating system to allow the user running Elasticsearch to access more resources than allowed by default. The following settings must be addressed before going to production: Set JVM heap size Disable swapping Increase file descriptors Ensure sufficient virtual memory Ensure sufficient threads 网络相关的配置，除了这个常见的监听地址的配置，还有很多，我们在下面会进行汇总讲解 discovery.zen.ping.unicast.hosts当在组成一个es集群的时候，需要添加其他集群节点，这个时候需要如下的配置： 1234discovery.zen.ping.unicast.hosts: - 192.168.1.10:9300 - 192.168.1.11 - seeds.mydomain.com 注意： 如果没有指定端口，则默认为：transport.profiles.default.port和transport.tcp.port(这里指的是TCP端口) 如果一个主机名映射了多个ip，那么会把解析出来的所有ip都添加进来 discovery.zen.minimum_master_nodes为了防止数据的丢失，需要确保每一个主节点都知晓：组成一个集群最少可以有几个主节点 如果没有这个配置，当集群遭遇到网络问题导致各个集群节点之间的通信出现问题时，整个集群可能会变成2个独立的集群，这个现象我们称之为：a split brain(脑裂)，这会导致数据的丢失。 配置规则： 1(master_eligible_nodes / 2) + 1 举例说明，例如当前集群有3个节点，那么配置就应该是(3 / 2) + 1或者2(注意，我们是取整数)： 1discovery.zen.minimum_master_nodes: 2 除了在配置文件中配置，在运行过程中，我们可以通过接口临时修改 123456PUT _cluster/settings&#123; &quot;transient&quot;: &#123; &quot;discovery.zen.minimum_master_nodes&quot;: 2 &#125;&#125; a split brain详解： 假定集群有2个节点，minimum_master_nodes的值设置为1，当网络出现问题时，每个节点都会把自己提升为集群的主节点(认为另一个节点已经终止)，结果就是会产生2个集群。 当网络恢复时，节点不会重新加入集群。只有当节点重启的时候，才会重新加入集群，但是，重启节点上的数据会丢失。 也正是因为这个原因，所以，我们的集群主节点最好是3个以上 网络相关配置汇总es配置文件中，有以下网络相关配置 network.host。略。 discovery.zen.ping.unicast.hosts。略 http.port Port to bind to for incoming HTTP requests. Accepts a single value or a range. If a range is specified, the node will bind to the first available port in the range. Defaults to 9200-9300. 设置节点的http端口，提供接口处理相对应的请求，可以赋值为单个值或者一个范围，如果是一个范围，那么将会使用第一个值。 transport.tcp.port Port to bind for communication between nodes. Accepts a single value or a range. If a range is specified, the node will bind to the first available port in the range. Defaults to 9300-9400. 设置节点的TCP接口，用于集群之间的通信，和上面类似，可以赋值为单个值或者一个范围，如果是一个范围，那么将会使用第一个值。 ES的启动引导检查在ES的早期版本，一些es和系统配置不符合要求时，只是输出到日志文件中，供用户查看，但是考虑到很多用户不去看这些日志文件，因此，在后续版本中，es把这些检查放到了启动过程中，当有配置不符合要求时，就是在启动过程中显示出来(注意，只针对生产模式，开发模式只是记录日志) 下面是一些检测指标，对应的详细配置会在下面的重要的系统配置中说明 Heap size check JVM初始值和最大值必须要设置一致。 File descriptor check ES需要大量的文件描述符(Linux中一切皆文件)，是否&gt;=65535 Memory lock check 锁内存配置是否开启。这个在前面的配置参数中有说明了。 Maximum number of threads check 最大线程数限制。ES将接收到处理请求拆分成为多个步骤，每个步骤都会使用不同的线程池。 因此，ES需要创建大量的线程，在启动前的check操作能够保证es在运行过程中有足够多的资源。 系统需要保证用户至少能创建2048个线程。 这个的配置在/etc/security/limits.conf和/etc/security/limits.d/90-nproc.conf配置文件中，参数是nproc，建议配置&gt;=65536 Maximum size virtual memory check 虚拟内存限制。需要把内存限制打开 这个的配置在/etc/security/limits.conf配置文件中，参数是memlock，设置为unlimited 123[elk@node001 logs]$ cat /etc/security/limits.confelk soft memlock unlimitedelk hard memlock unlimited vm.max_map_count 还是内存相关的配置，这个是内存参数配置。 配置文件：/etc/sysctl.conf，配置参数：vm.max_map_count，默认值是262144，建议配置&gt;=655360 重要的系统配置JVM堆内存ES的JVM内存配置有以下注意事项 每个节点的Xms和Xmx设置为相等的值 更大的内存设置可能会导致gc时间变长，需要考虑一个平衡 JVM最大内存设置不要超过系统物理内存的一半，需要确保给内核文件系统缓存留有足够的物理内存 上限不要超过32GB 26G是一个非常合理设置 文件描述符当文件描述符不足时，造成的后果将会是灾难性的，可以造成数据的丢失。 这个的配置在/etc/security/limits.conf配置文件中，参数是nofile，建议配置&gt;=65536 确保为启动es服务的用户设置的值&gt;=65536 123[elk@node001 logs]$ cat /etc/security/limits.conf* soft nofile 65536* hard nofile 65536 可以不需要登录服务器，通过以下接口获取当前的配置值 1GET _nodes/stats/process?filter_path=**.max_file_descriptors 关闭swap这部分在es的配置讲解部分（bootstrap.memory_lock部分）记录了 虚拟内存和ES使用内存映射系统有关，需要我们优化内核参数 默认值为262144 12[elk@node001 logs]$ sysctl -a | grep vm.max_map_countvm.max_map_count = 262144 建议配置为&gt;=655360 临时生效，执行命令： 1sysctl -w vm.max_map_count=655360 永久生效，也就是系统重启后还生效： 1echo &quot;vm.max_map_count = 655360&quot; &gt;&gt; /etc/sysctl.conf 最大线程数其实也就是最大进程数 123[elk@node001 logs]$ cat /etc/security/limits.d/90-nproc.conf* soft nproc 65536* hard nproc 65536 注意：如果设置为所有用户生效，则需要修改90-nproc.conf配置文件，如果是指定某个用户生效，则修改/etc/security/limits.conf即可，例如： 123[elk@node001 logs]$ cat /etc/security/limits.confelk soft nproc 65536elk hard nproc 65536 ES集群部署ES集群配置在使用最简化的3节点时，master和data参数都设置为true 下面以一个节点的配置为例，每个节点上的配置基本相同，就只有监听的ip不一致。 12345678910111213141516171819202122 cluster.name: testelkhttp.port: 9500transport.tcp.port: 9600-9700path: logs: /home/elk/logs-5.3.0 data: /home/elk/data-5.3.0node.master: truenode.data: truenode.name: node001network.host: 192.168.100.113bootstrap.memory_lock: truediscovery.zen.ping.unicast.hosts: [&quot;192.168.100.113&quot;, &quot;192.168.101.172&quot;, &quot;192.168.103.133&quot;]discovery.zen.minimum_master_nodes: 2transport.tcp.compress: truehttp.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;thread_pool: search: size: 200 queue_size: 2000 bulk: queue_size: 2000 kibana配置Mappingmapping的作用是定义document，包括： 包含哪些字段 哪些字段需要存储 哪些字段能被索引 例如： 哪一个字符串字段被定义为full text fields 哪个字段包含数字、日志、地理位置 是否所有的字段都能被索引(通过_all参数) 日期格式 自定义规则 每一个索引拥有一个或者多个的mapping type，这些mapping通常用于区分不同逻辑组的document，例如：用户相关的document通常type设置为user，博客相关的document通常type设置为blog。每一个type可以定义一个mapping mapping type每一个mapping类型都会有以下的字段： 元类型字段 mapping的元类型字段，主要是作用于定义document的元字段，例如：_index, _type, _id, and _source fields. 字段或者自定义属性 每一个mapping类型都包含一组字段。例如user type包含title, name, and age fields，a blogpost type might contain title, body, user_idand created fields 注意：在一个索引中，两个不同的type中如果有一个同名的字段，那么这个字段必须使用相同的mapping 字段数据类型有以下数据类型可以供字段选择 基础类型：like text, keyword, date, long, double, boolean or ip. json类型： object or nested. 专门的类型：like geo_point, geo_shape, or completion. 动态mappingmapping type以及具体的字段在使用的时候，可以不需要提前指定配置，而是可以动态自动生成。 自动发现并且给字段添加相对应的类型我们称之为：”dynamic mapping“ 动态mapping规则可以根据你的需求进行下自定义，有以下几种： _default_ mapping 为新的mapping配置基础的mapping配置。相当于是继承关系 Dynamic field mappings 这个规则管理动态字段发现 Dynamic templates 动态添加字段 _default_ mappingdefault mapping的作用是定义一些全局的配置，在后续的多个mapping定义的时候如果没有手动的指定这个配置，那么会继承这个默认配置。 例如： 12345678910111213141516PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;_default_&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125; &#125;, &quot;user&quot;: &#123;&#125;, &quot;blogpost&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: true &#125; &#125; &#125;&#125; 设置全局的配置：将_all字段的标志位设置为false user这个type的mapping将会继承这个配置 blogpost这个type的mapping将会重写这个配置为true 注意：尽管这个配置可以在索引已经存在的情况下执行，但是执行之后只是影响后来生成的mapping。 同样的，这个功能也可以用于索引的模板中 例如： 1234567891011121314151617181920212223242526272829303132PUT _template/logging&#123; &quot;template&quot;: &quot;logs-*&quot;, &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;_default_&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125;, &quot;dynamic_templates&quot;: [ &#123; &quot;strings&quot;: &#123; &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; ] &#125; &#125;&#125;测试：PUT logs-2015.10.01/event/1&#123; &quot;message&quot;: &quot;error:16&quot; &#125; 明确mapping- Explicit mappings实际情况下，我们会比es更了解我们的数据，因此，我们可以在创建索引的时候就定义好mapping 我们可以： 在创建索引的时候，初始化定义mapping 当索引已存在时，可以新增字段 注意：不能更新现有的mapping的字段，更新通过下面的方式 更新已存在的mapping已经存在的mapping中的字段是不能直接被update的，如果修改mapping则意味着现有索引中的document都会成为无效数据，因此，如果我们想要更新mapping： 新建一个mapping 重建索引 reindex：索引数据迁移 字段共享mapping不同的mapping通常用于划分和组织字段组，但是以下的这些字段不会只作用于单个mapping局部 字段的名称相同 位于同一个索引中 在不同的mapping type中 mapping内部有相同的字段 必须拥有相同的mapping 举例说明： 如果title字段同时存在于user和blogpost这两个mapping type中，那么这个字段在两个mapping type中就必须保持一致的mapping 但是存在这些例外，直接看官方原文即可： The only exceptions to this rule are the copy_to, dynamic, enabled, ignore_above, include_in_all, and properties parameters, which may have different settings per field. 案例下面是一个案例： 123456789101112131415161718192021222324252627PUT my_index &#123; &quot;mappings&quot;: &#123; &quot;user&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125;, &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125; &#125;, &quot;blogpost&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125;, &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;body&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;user_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;created&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot; &#125; &#125; &#125; &#125;&#125; 元数据类型字段字段数据类型keyword数据类型这个数据类型适用于例如： email addresses email地址 Hostnames 主机名 Status code 状态码 zip codes 编码 tags 指定的标志 这种数据类型，主要的作用是过滤功能(例如博客文章中可以通过status为published已发布的来进行过滤)、sort排序功能、以及聚合。 keyword是唯一一种类型：他们的值可以被检索 下面是一个例子：创建索引的实时设定mapping，mapping中包含一个字段属性，其类型为keyword 123456789101112PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;tags&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125; keyword中有以下这些参数，需要重点关注一下： ignore_above 参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/5.3/ignore-above.html Text数据类型顾名思义，这个数据类型主要是作用于例如：邮件内容、产品描述信息等不规则的并且数据量较大的文本内容。 案例： 123456789101112PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;full_name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 二级字段类型-fields配置参考连接：https://www.elastic.co/guide/en/elasticsearch/reference/5.3/multi-fields.html ES APIAPI约定及惯例multiple indices大部分涉及索引的api都支持匹配多个index操作,有如下这些方式实现 多个索引：test1,test2,test3 所有索引：_all 通配符匹配：例如：test*、*test 、 te*t 、 *test* 加减： “add” (+) and “remove” (-), for example: +test*,-test3. All multi indices API support the following url query string parameters: 使用这种方式的api都有一下参数 ignore_unavailable 是否忽略不可用(不存在或者异常的)的索引，取值为true|false allow_no_indices 控制当使用通配符匹配时，如果没有结果，是否返回fail。取值为true|false 不止针对通配符情况，也支持_all以及不指定任何index的情况 这个配置同样支持aliases expand_wildcards 取值为open|closed|(open,closed) 控制通配符匹配到的索引，当设置为open时，将只会匹配到open的索引，也就是正常的索引，当设置为closed时，将只会匹配到closed的索引。 也可以设置为open+closed，以匹配所以的索引 注意：如果设置为none，这个功能将会被disable。如果设置为all，那么等价于open+closed 注意：只作用于单个索引的api（例如document api等等），没有上面这些参数 Date math support in index names后续涉及时再进行补充。 Common option通用选项。 URL-based access control后续补充 Document APIsReading and Writing documents-读写document链接：https://www.elastic.co/guide/en/elasticsearch/reference/5.1/docs-replication.html#_introduction 每一个索引在ES中都会被分割成为shard分片，并且每一个shard都会有不等的copies副本。 当有多个副本的时候，这多个副本，我们就称之为replication group-副本组，当document有新增或者删除的时候，我们需要保证这个组中的每个副本的数据和分片是同步的，防止出现数据不一致的情况 保证分片和副本之间的数据同步的进程，我们称之为：data replication model 基础写模型- Basic write modelES接收索引写请求，首先根据路由服务（根据document为处理依据），将请求转发给副本组。 副本组收到请求之后，这个请求将会被转发给副本组中的主分片(每个副本组中还会区分主从关系)，这个副本主分片转发请求给其他的副本， 主分片按照下面的流程处理： 检验请求是否符合标准，拒绝无效的请求 在本地执行这个操作(写document的操作) 转发这个请求给所有的副本，这个过程是同步的，而不是异步 一旦所有的副本都执行完毕，并且返回结构给主分片，主分片返回ack给客户端。 写失败处理暂时略 基础读模型读操作的时候，可以是非常轻量级的根据document的id进行，也可以是非常重的请求(非常复杂的聚合条件) 当一个node节点接收到一个读请求时，这个node负责转发这个请求给具有相关分片的node，并且收集响应，然后返回给客户端，我们称这样的node为坐标node(coordinating node) 这个node的基本的工作流为： 解析请求，获取这个请求涉及哪些分片，因为很多读请求会涉及到多个索引，他们也就需要读取到多个分片 从对应分片的副本组中选择一个活跃状态的副本，这个副本可以使主或者从角色，默认情况下，es会轮询调度到这些副本 发送请求到这个指定的副本 坐标node，汇总所有副本返回的结果，然后响应客户端。 读失败处理暂时略 新建document案例： 新建一个document 123456PUT twitter/tweet/1&#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125; 执行之后，es的响应返回为： 12345678910111213&#123; &quot;_shards&quot; : &#123; &quot;total&quot; : 3, &quot;failed&quot; : 0, &quot;successful&quot; : 3 &#125;, &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;tweet&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;created&quot; : true, &quot;result&quot; : created&#125; 因为，这个索引之前在创建的时候设置为3个分片，所以这里的shard为3，_shards这一个块展示的是复制过程相关的结果信息，success表示所有的副本都是成功状态的。 分片数量可以根据索引相关的接口的进行查询： 12345[elk@node003 ~]$ curl http://192.168.103.133:9500/_cat/indices?vhealth status index uuid pri rep docs.count docs.deleted store.size pri.store.sizegreen open twitter 2Gxyl8acRa222-gdYhE4ig 3 2 1 0 14.1kb 4.7kbgreen open .kibana JjZFr5NvTXSOUv6DjwyF0w 1 1 1 0 6.3kb 3.1kbgreen open logs-2015.10.01 Ubo73cz5RRO4ipU33zdCqQ 1 1 0 0 318b 159b 不手动指定id，使用自动增长的序列号(注意使用POST代替PUT) 123456POST twitter/tweet/&#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125; 返回结果为： 12345678910111213&#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;tweet&quot;, &quot;_id&quot; : &quot;AWu2sgI5Rw8iDPJNYgMY&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 3, &quot;successful&quot; : 3, &quot;failed&quot; : 0 &#125;, &quot;created&quot; : true&#125; 可以看到，document的为：AWu2sgI5Rw8iDPJNYgMY 查询document根据document的id进行查询 123456789101112131415GET twitter/tweet/0[elk@node003 ~]$ curl http://192.168.103.133:9500/twitter/tweet/1?pretty&#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;tweet&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot; &#125;&#125; 删除document基础document的id进行删除 1格式例如：curl -XDELETE &apos;http://localhost:9200/twitter/tweet/1&apos; Indices APIs-索引api创建索引例如： 123456789101112131415161718PUT twitter&#123; &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 2 &#125; &#125;&#125;或者可以省略indexPUT twitter&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 2 &#125;&#125; 创建索引并指定mapping 12345678910111213PUT test&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 1 &#125;, &quot;mappings&quot; : &#123; &quot;type1&quot; : &#123; &quot;properties&quot; : &#123; &quot;field1&quot; : &#123; &quot;type&quot; : &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 创建索引并设置alias 123456789101112PUT test&#123; &quot;aliases&quot; : &#123; &quot;alias_1&quot; : &#123;&#125;, &quot;alias_2&quot; : &#123; &quot;filter&quot; : &#123; &quot;term&quot; : &#123;&quot;user&quot; : &quot;kimchy&quot; &#125; &#125;, &quot;routing&quot; : &quot;kimchy&quot; &#125; &#125;&#125; 删除索引执行下面的接口： 1DELETE /twitter 索引可以是： 单个索引 _all 所有索引 通配符表达式，例如添加*等 因为可以批量的删除，所以存在一定的风险，当我们希望关闭这个功能的时候，设置action.destructive_requires_name的值为true。这个配置也可以在集群运行过程中被修改。 查看索引接口为： 1GET twitter 查看twitter索引，注意可以使用通配符、alias、_all等匹配多个索引 过滤索引的特殊配置 123GET twitter/_settingsGET twitter/_mappingsGET twitter/_aliases 索引是否存在1HEAD twitter 接口返回404为不存在，200则为存在。 put mapping-mapping的新增我们可以实现： 在现有的索引上新增type的mapping 创建索引时指定mapping 在现有的mapping基础上新增字段 案例如下： 123456789101112131415161718192021222324252627282930313233# 创建索引时指定mappingPUT twitter &#123; &quot;mappings&quot;: &#123; &quot;tweet&quot;: &#123; &quot;properties&quot;: &#123; &quot;message&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125;# 在现有索引上新增mapping typePUT twitter/_mapping/user &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125;&#125;# 现有mapping上新增字段PUT twitter/_mapping/tweet &#123; &quot;properties&quot;: &#123; &quot;user_name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125;&#125; get mapping语法格式为：host:port/{index}/_mapping/{type} 案例为: 1234567GET /_mapping/tweet,kimchy=GET /_all/_mapping/tweet,bookGET /_all/_mapping=GET /_mapping 获取具体字段的mapping语法： 1GET /twitter/_mapping/tweet/field/field1,field2,... 判断mapping type是否存在语法： 1HEAD twitter/_mapping/tweet 返回值为404或者200 索引别名-index aliases索引别名允许给索引重新定义一个名称，当在调用这个alias的时候，es会自动的把这个alias转换为真实的index, 一个alias可以被映射到多个index上， 添加alias: 123456789101112131415161718192021POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test1&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125;, &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test2&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125; ]&#125;或者POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;add&quot; : &#123; &quot;indices&quot; : [&quot;test1&quot;, &quot;test2&quot;], &quot;alias&quot; : &quot;alias1&quot; &#125; &#125; ]&#125;或者POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test*&quot;, &quot;alias&quot; : &quot;all_test_indices&quot; &#125; &#125; ]&#125; 获取索引alias语法： 12语法格式：/&#123;index&#125;/_alias/&#123;alias&#125;例如：GET /logs_20162801/_alias/* 在所有index中搜索alias为2016 1GET /_alias/2016 下面的方式可以检查alias是否存在： 123HEAD /_alias/2016HEAD /_alias/20*HEAD /logs_20162801/_alias/* 索引模板作用：新建的index能够继承这一套的配置 模板中包括setting、mapping等配置，以及控制哪些index可以继承这个模板配置。 例如： 新建索引模板： 1234567891011121314151617181920212223PUT _template/template_1&#123; &quot;template&quot;: &quot;te*&quot;, &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;type1&quot;: &#123; &quot;_source&quot;: &#123; &quot;enabled&quot;: false &#125;, &quot;properties&quot;: &#123; &quot;host_name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;created_at&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;EEE MMM dd HH:mm:ss Z YYYY&quot; &#125; &#125; &#125; &#125;&#125; 一个生产案例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124PUT _template/realtime_baobei_order&#123; &quot;order&quot;: 0, &quot;template&quot;: &quot;realtime_baobei_order-*&quot;, &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;max_result_window&quot;: &quot;30000&quot;, &quot;codec&quot;: &quot;best_compression&quot;, &quot;refresh_interval&quot;: &quot;-1&quot;, &quot;number_of_shards&quot;: &quot;6&quot;, &quot;number_of_replicas&quot;: &quot;0&quot; &#125; &#125;, &quot;aliases&quot;: &#123;&#125;, &quot;mappings&quot;: &#123; &quot;order&quot;:&#123; &quot;properties&quot;: &#123; &quot;cancel_reason&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;cook_tm&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot; &#125;, &quot;customer_name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;customer_tel&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;expect_send_tm&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot; &#125;, &quot;finish_tm&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot; &#125;, &quot;food_agg&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;join_expire_tm&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot; &#125;, &quot;leave_shop_tm&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot; &#125;, &quot;location_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;location_name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;order_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;package_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;pay_tm&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot; &#125;, &quot;place_tm&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot; &#125;, &quot;pt&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;remark&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;serial_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;settment_cost_1&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;settment_cost_2&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;settment_cost_4&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;settment_cost_5&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;settment_cost_6&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;settment_cost_7&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;settment_cost_998&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;settment_cost_999&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;shop_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;shop_name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;shop_tel&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;status&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;user_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125; &#125; 删除索引模板： 1DELETE /_template/template_1 查看索引模板信息： 12345GET /_template/template_1GET /_template/temp*GET /_template/template_1,template_2获取所有的模板：GET /_template 判断索引模板是否存在： 1HEAD _template/template_1 索引状态语法如下： 12345# 查看所有的索引的状态GET /_stats# 查看指定索引的状态GET /index1,index2/_stats refresh例如： 123456# 刷新一个indexPOST /twitter/_refresh# 刷新多个indexPOST /kimchy,elasticsearch/_refresh# 刷新所有的indexPOST /_refresh cat apis作用尽管json格式是一种非常好的数据格式，但是在shh终端上，不利于人类的可读，在终端界面，我们需要线性的显示。那么cat api就是干这个的，它可以把输出转换为方便人类查看的格式 参数在cat api中，支持一下参数 verbose。输出全量信息 12345678# 无参数[elk@node003 bin]$ curl http://192.168.103.133:9500/_cat/masterMb1156xHRuSYgSZ3RG1n8Q 192.168.101.172 192.168.101.172 node002# 有参数[elk@node003 bin]$ curl http://192.168.103.133:9500/_cat/master?vid host ip nodeMb1156xHRuSYgSZ3RG1n8Q 192.168.101.172 192.168.101.172 node002 help，每一个方法都可以使用help查看可以获得哪些输出信息 12345[elk@node003 bin]$ curl http://192.168.103.133:9500/_cat/master?helpid | | node idhost | h | host nameip | | ip addressnode | n | node name Headers，可以指定要输出的字段，例如： 123456789101112131415# 获取支持的字段信息[elk@node003 bin]$ curl http://192.168.103.133:9500/_cat/nodes?help# 默认的全量输出[elk@node003 bin]$ curl http://192.168.103.133:9500/_cat/nodes?vip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name192.168.103.133 12 97 0 0.39 0.37 0.26 mdi - node003192.168.100.113 12 96 0 0.16 0.29 0.22 mdi - node001192.168.101.172 14 96 1 0.40 0.37 0.25 mdi * node002# 指定字段输出[elk@node003 bin]$ curl http://192.168.103.133:9500/_cat/nodes?h=ip,port,heapPercent,name192.168.103.133 9600 12 node003192.168.100.113 9600 12 node001192.168.101.172 9600 14 node002 具体接口链接：https://www.elastic.co/guide/en/elasticsearch/reference/5.1/cat.html ES常用API 集群健康状态：GET /_cat/health?v 123[elk@node001 ~]$ curl http://127.0.0.1:9200/_cat/health?vepoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1561023164 17:32:44 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0% 集群节点：GET /_cat/nodes?v 123[elk@node001 ~]$ curl http://127.0.0.1:9200/_cat/nodes?vip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name127.0.0.1 10 96 0 0.00 0.06 0.08 mdi * Jd3JKT1 创建索引：PUT /customer?pretty 12345[elk@node001 ~]$ curl -X PUT http://127.0.0.1:9200/customer?pretty&#123; &quot;acknowledged&quot; : true, &quot;shards_acknowledged&quot; : true&#125; 查看所有的索引：GET /_cat/indices?v 123[elk@node001 ~]$ curl -X GET http://127.0.0.1:9200/_cat/indices?vhealth status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open customer cDDJVvowS-uZs0um21huWw 5 1 0 0 650b 650b 传输document到index中：PUT /customer/external/1?pretty 1234567891011121314[root@node001 ~]# curl -X PUT http://127.0.0.1:9200/customer/external/1?pretty -d &apos;&#123;&quot;name&quot;:&quot;John Doe&quot;&#125;&apos;&#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 &#125;, &quot;created&quot; : true&#125; 注意：customer是index索引，external是type，1是id。POST方式是自增id，put方式必须要加上id。 这个操作，相当于是给这个索引新增一行数据 查看document：/customer/external/1?pretty 1234567891011[root@node001 ~]# curl -X GET http://127.0.0.1:9200/customer/external/1?pretty&#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;John Doe&quot; &#125;&#125; 查看id为1的document的具体内容 删除索引：DELETE /customer?pretty 123456[root@node001 ~]# curl -X DELETE http://127.0.0.1:9200/customer?pretty&#123; &quot;acknowledged&quot; : true&#125;[root@node001 ~]# curl -X GET http://127.0.0.1:9200/_cat/indices?vhealth status index uuid pri rep docs.count docs.deleted store.size pri.store.size 注意：上面这些接口路径中的?pretty均可以省略，这个的作用只是为了显示美观，没有实际意义。 更新document：POST /customer/external/1/_update?pretty 12345678910111213141516171819202122232425[root@node001 ~]# curl -X POST http://127.0.0.1:9200/customer/external/1?pretty -d &apos;&#123;&quot;doc&quot;:&quot;John Doe&quot;&#125;&apos;&#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 2, &quot;result&quot; : &quot;updated&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 &#125;, &quot;created&quot; : false&#125;[root@node001 ~]# curl -X GET http://127.0.0.1:9200/customer/external/1?pretty&#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 2, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;doc&quot; : &quot;John Doe&quot; &#125;&#125; 注意：这里所说的更新操作，实际上在ES内部部署更新替换，而是删除之后再重新创建 删除document：DELETE /customer/external/2?pretty 123456789101112131415161718192021[root@node001 ~]# curl -X DELETE http://127.0.0.1:9200/customer/external/1?pretty&#123; &quot;found&quot; : true, &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 6, &quot;result&quot; : &quot;deleted&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 &#125;&#125;[root@node001 ~]# curl -X GET http://127.0.0.1:9200/customer/external/1?pretty&#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;found&quot; : false&#125; 查看node上有哪些shard 1curl http://172.16.1.129:9500/_cat/shards| grep node004 关闭打开索引 12关闭：curl -XPOST localhost:9200/xxx_indices/_close打开： 关闭集群的reroute-(有新节点新增之后，不要立马挪动shards) 这里是控制集群中的shard的移动，一般是在新增节点之后，如果不想马上就进行shards的挪动，那么可以暂停这个功能 12345678PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;cluster.routing.allocation.enable&quot;: &quot;none&quot; &#125;&#125;如果想取消，那么设置为null即可。 集群统计信息 1GET _cluster/stats 包括索引数量、shard的数量、主、副shard的比例等等、docs的数量、存储使用量、插件情况 cluster reroute 参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/6.3/cluster-reroute.html 主要是涉及集群的一些内部路由，主要是shards分片的移动（move）、重新分片未分配的副本分片（allocate_replica）。 1234567891011121314151617POST /_cluster/reroute&#123; &quot;commands&quot; : [ &#123; &quot;move&quot; : &#123; &quot;index&quot; : &quot;test&quot;, &quot;shard&quot; : 0, &quot;from_node&quot; : &quot;node1&quot;, &quot;to_node&quot; : &quot;node2&quot; &#125; &#125;, &#123; &quot;allocate_replica&quot; : &#123; &quot;index&quot; : &quot;test&quot;, &quot;shard&quot; : 1, &quot;node&quot; : &quot;node3&quot; &#125; &#125; ]&#125; 修改副本分片数量 123456PUT applog-prod-2016.12.18/_settings&#123; &quot;index&quot;:&#123; &quot;number_of_replicas&quot;:0 &#125;&#125; 查看每个node上的task 参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/6.3/tasks.html#_current_tasks_information 123GET _tasks GET _tasks?nodes=nodeId1,nodeId2 GET _tasks?nodes=nodeId1,nodeId2&amp;actions=cluster:* cluster allocation explain API 集群内部分配相关的解释api，可以解释未分配shard的原因等等，在诊断shard问题的时候非常有用 1 分片迁移 参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/5.3/cluster-reroute.html 1234567891011121314151617POST /_cluster/reroute&#123; &quot;commands&quot; : [ &#123; &quot;move&quot; : &#123; &quot;index&quot; : &quot;test&quot;, &quot;shard&quot; : 0, &quot;from_node&quot; : &quot;node1&quot;, &quot;to_node&quot; : &quot;node2&quot; &#125; &#125;, &#123; &quot;allocate_replica&quot; : &#123; &quot;index&quot; : &quot;test&quot;, &quot;shard&quot; : 1, &quot;node&quot; : &quot;node3&quot; &#125; &#125; ]&#125; 更新索引模板 例如更新索引名称的匹配规则：从nginx*-logstash-* 修改为nginx-logstash-* 直接使用PUT的话将会是覆盖操作 操作如下： 1 更新monitor的索引保存天数 12PUT _cluster/settings&#123;&quot;persistent&quot;: &#123;&quot;xpack.monitoring.history.duration&quot;:&quot;1d&quot;&#125;&#125; 查看集群安装的插件 1curl http://172.24.148.199:9200/_cat/plugins 查看shard分配失败原因 1curl http://172.24.48.14:9200/_cluster/allocation/explain?pretty Search apihttps://www.elastic.co/guide/en/elasticsearch/reference/5.1/_the_search_api.html ES检索相关-【这部分相当重要】 这个api允许你执行一些查询请求， 参数形式查找twitter索引下的tweet,user这2个type中user为kimchy的document 1234567891011121314151617GET /twitter/tweet,user/_search?q=user:kimchy实际执行为：[elk@node003 ~]$ curl http://192.168.103.133:9500/twitter/tweet,user/_search?q=user:kimchy&#123;&quot;took&quot;:31,&quot;timed_out&quot;:false,&quot;_shards&quot;:&#123;&quot;total&quot;:3,&quot;successful&quot;:3,&quot;failed&quot;:0&#125;,&quot;hits&quot;:&#123;&quot;total&quot;:3,&quot;max_score&quot;:0.2876821,&quot;hits&quot;:[&#123;&quot;_index&quot;:&quot;twitter&quot;,&quot;_type&quot;:&quot;tweet&quot;,&quot;_id&quot;:&quot;AWu2sYdrRw8iDPJNYgMX&quot;,&quot;_score&quot;:0.2876821,&quot;_source&quot;:&#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125;&#125;,&#123;&quot;_index&quot;:&quot;twitter&quot;,&quot;_type&quot;:&quot;tweet&quot;,&quot;_id&quot;:&quot;AWu2sgI5Rw8iDPJNYgMY&quot;,&quot;_score&quot;:0.2876821,&quot;_source&quot;:&#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125;&#125;,&#123;&quot;_index&quot;:&quot;twitter&quot;,&quot;_type&quot;:&quot;tweet&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_score&quot;:0.2876821,&quot;_source&quot;:&#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125;&#125;]&#125;&#125;[elk@node003 ~]$ 查找所有索引中type为tweet的内容： 1GET /_all/tweet/_search?q=tag:wow 查找所有索引所有type 1GET /_search?q=tag:wow 请求体形式除了上面的在url请求中携带参数去search，es还支持在get请求使用body的方式 例如： 123456GET /twitter/tweet/_search&#123; &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot; &#125; &#125;&#125; 执行后的输出和上面是一致的 123456789101112131415161718[elk@node003 ~]$ curl -XGET http://192.168.103.133:9500/twitter/tweet/_search -d &apos;&#123; &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot; &#125; &#125;&#125;&apos;&#123;&quot;took&quot;:48,&quot;timed_out&quot;:false,&quot;_shards&quot;:&#123;&quot;total&quot;:3,&quot;successful&quot;:3,&quot;failed&quot;:0&#125;,&quot;hits&quot;:&#123;&quot;total&quot;:3,&quot;max_score&quot;:0.2876821,&quot;hits&quot;:[&#123;&quot;_index&quot;:&quot;twitter&quot;,&quot;_type&quot;:&quot;tweet&quot;,&quot;_id&quot;:&quot;AWu2sYdrRw8iDPJNYgMX&quot;,&quot;_score&quot;:0.2876821,&quot;_source&quot;:&#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125;&#125;,&#123;&quot;_index&quot;:&quot;twitter&quot;,&quot;_type&quot;:&quot;tweet&quot;,&quot;_id&quot;:&quot;AWu2sgI5Rw8iDPJNYgMY&quot;,&quot;_score&quot;:0.2876821,&quot;_source&quot;:&#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125;&#125;,&#123;&quot;_index&quot;:&quot;twitter&quot;,&quot;_type&quot;:&quot;tweet&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_score&quot;:0.2876821,&quot;_source&quot;:&#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125;&#125;]&#125;&#125;[elk@node003 ~]$ count只获取搜索结果的条数 12[elk@node003 ~]$ curl http://192.168.103.133:9500/twitter/tweet/_count?q=user:kimchy&#123;&quot;count&quot;:3,&quot;_shards&quot;:&#123;&quot;total&quot;:3,&quot;successful&quot;:3,&quot;failed&quot;:0&#125;&#125;[elk@node003 ~]$ Cluster api-集群apiAnalyis-分词在安装完插件之后，如果不确定插件的名称，可以通过这个接口查看信息： 12345678910GET /_nodes# 插件部分的内容 &quot;plugins&quot;: [ &#123; &quot;name&quot;: &quot;analysis-ik&quot;, &quot;version&quot;: &quot;5.1.2&quot;, &quot;description&quot;: &quot;IK Analyzer for Elasticsearch&quot;, &quot;classname&quot;: &quot;org.elasticsearch.plugin.analysis.ik.AnalysisIkPlugin&quot; &#125; ], Ik分词器IK分词器，是一个针对中文的分词器。 IK分词器github项目地址：medcl/elasticsearch-analysis-ik IK分词器有两种分词模式：ik_max_word和ik_smart模式。 ik_max_word 会将文本做最细粒度的拆分，比如会将“中华人民共和国人民大会堂”拆分为“中华人民共和国、中华人民、中华、华人、人民共和国、人民、共和国、大会堂、大会、会堂等词语。 ik_smart 会做最粗粒度的拆分，比如会将“中华人民共和国人民大会堂”拆分为中华人民共和国、人民大会堂。 测试： 123456POST /_analyze&#123;&quot;text&quot;:&quot;中华人民共和国人民大会堂&quot;,&quot;analyzer&quot;:&quot;ik_smart&quot; &#125;POST /_analyze&#123;&quot;text&quot;:&quot;中华人民共和国人民大会堂&quot;,&quot;analyzer&quot;:&quot;ik_max_word&quot; &#125; 两种分词器使用的最佳实践是： 索引时用ik_max_word 在搜索时用ik_smart 即：索引时最大化的将文章内容分词，搜索时更精确的搜索到想要的结果。 插件管理如何看es中安装了哪些插件？ GET _cat/plugins 插件安装sql插件安装命令如下： 1./bin/elasticsearch-plugin install https://github.com/NLPchina/elasticsearch-sql/releases/download/6.5.4.0/elasticsearch-sql-6.5.4.0.zip IK分词器插件安装安装之后就是：analysis-ik Index Modules-索引模块slow logslow log分为两种 search slow log index slow log search slow log es支持记录分片级别的查询（包括query和fetch两个阶段）慢查询日志。 慢查询的阈值设置 在search中分为query和fetch两个部分 123456789index.search.slowlog.threshold.query.warn: 10sindex.search.slowlog.threshold.query.info: 5sindex.search.slowlog.threshold.query.debug: 2sindex.search.slowlog.threshold.query.trace: 500msindex.search.slowlog.threshold.fetch.warn: 1sindex.search.slowlog.threshold.fetch.info: 800msindex.search.slowlog.threshold.fetch.debug: 500msindex.search.slowlog.threshold.fetch.trace: 200ms 上面的这些配置可以在配置文件中指定，也可以通过接口去动态的设置 在默认情况下，上面这些参数没有一个是enabled的（默认值是 -1），提供的日志级别(warn, info, debug, trace)允许我们灵活的控制哪些级别是我们要打印输出的 上面这些配置并不是所有的都必须要设置。 上面是定义慢查询的阈值，除了这里，我们还需要再日志文件中进行配置 注意：日志中记录的shard级别的日志，也就是分片级别，这就意味着，这个es实例的日志可能不是一个完整的查询，而只是查询的一部分。使用这种方式的好处是可以毕竟精细的看到是哪一个节点的问题。 默认的日志配置（log4j2.properties）如下： 123456789101112131415appender.index_search_slowlog_rolling.type = RollingFileappender.index_search_slowlog_rolling.name = index_search_slowlog_rollingappender.index_search_slowlog_rolling.fileName = $&#123;sys:es.logs&#125;_index_search_slowlog.logappender.index_search_slowlog_rolling.layout.type = PatternLayoutappender.index_search_slowlog_rolling.layout.pattern = [%d&#123;ISO8601&#125;][%-5p][%-25c] %marker%.-10000m%nappender.index_search_slowlog_rolling.filePattern = $&#123;sys:es.logs&#125;_index_search_slowlog-%d&#123;yyyy-MM-dd&#125;.logappender.index_search_slowlog_rolling.policies.type = Policiesappender.index_search_slowlog_rolling.policies.time.type = TimeBasedTriggeringPolicyappender.index_search_slowlog_rolling.policies.time.interval = 1appender.index_search_slowlog_rolling.policies.time.modulate = truelogger.index_search_slowlog_rolling.name = index.search.slowloglogger.index_search_slowlog_rolling.level = tracelogger.index_search_slowlog_rolling.appenderRef.index_search_slowlog_rolling.ref = index_search_slowlog_rollinglogger.index_search_slowlog_rolling.additivity = false 这里的配置根据版本的不同可能会不同 index low log Indexing slow log的相关内容和上面的类似，文件名称也是以_index_indexing_slowlog.log结尾 阈值的配置如下： 123456index.indexing.slowlog.threshold.index.warn: 10sindex.indexing.slowlog.threshold.index.info: 5sindex.indexing.slowlog.threshold.index.debug: 2sindex.indexing.slowlog.threshold.index.trace: 500msindex.indexing.slowlog.level: infoindex.indexing.slowlog.source: 1000 这些配置也是一样，可以在配置文件中设置，也可以通过接口进行设置。 注意： 在默认情况下，es将只会把 _source 中的前1000个字符输出到日志文件中。如果有额外的需求，我们可以通过接口去进行设置。如果设置为false或者0，那么es将会忽略整个source。如果设置为true，那么将会无视size的限制。 最原始状态的_source将会被重新格式化，以便能够输出为单行的格式。如果document的格式非常重要，不能被重新格式化，那么我们可以将index.indexing.slowlog.reformat设置为false。 同样的，我们也需要在日志文件中进行设置： 123456789101112131415appender.index_indexing_slowlog_rolling.type = RollingFileappender.index_indexing_slowlog_rolling.name = index_indexing_slowlog_rollingappender.index_indexing_slowlog_rolling.fileName = $&#123;sys:es.logs&#125;_index_indexing_slowlog.logappender.index_indexing_slowlog_rolling.layout.type = PatternLayoutappender.index_indexing_slowlog_rolling.layout.pattern = [%d&#123;ISO8601&#125;][%-5p][%-25c] %marker%.10000m%nappender.index_indexing_slowlog_rolling.filePattern = $&#123;sys:es.logs&#125;_index_indexing_slowlog-%d&#123;yyyy-MM-dd&#125;.logappender.index_indexing_slowlog_rolling.policies.type = Policiesappender.index_indexing_slowlog_rolling.policies.time.type = TimeBasedTriggeringPolicyappender.index_indexing_slowlog_rolling.policies.time.interval = 1appender.index_indexing_slowlog_rolling.policies.time.modulate = truelogger.index_indexing_slowlog.name = index.indexing.slowlog.indexlogger.index_indexing_slowlog.level = tracelogger.index_indexing_slowlog.appenderRef.index_indexing_slowlog_rolling.ref = index_indexing_slowlog_rollinglogger.index_indexing_slowlog.additivity = false 这里的配置根据版本的不同可能会不同 slow log实际案例我们可以通过api去动态的调整 123456PUT /my_index/_settings&#123; &quot;index.search.slowlog.threshold.query.warn&quot; : &quot;10s&quot;, &quot;index.search.slowlog.threshold.fetch.debug&quot;: &quot;500ms&quot;, &quot;index.indexing.slowlog.threshold.index.info&quot;: &quot;5s&quot; &#125; 这里设置的是： 123查询慢于 10 秒输出一个 WARN 日志。获取慢于 500 毫秒输出一个 DEBUG 日志。索引慢于 5 秒输出一个 INFO 日志。 实际案例 123456789PUT testindex/_settings&#123; &quot;index.search.slowlog.threshold.query.info&quot; : &quot;200ms&quot;, &quot;index.search.slowlog.threshold.query.warn&quot; : &quot;500ms&quot;, &quot;index.search.slowlog.threshold.fetch.info&quot;: &quot;200ms&quot;, &quot;index.search.slowlog.threshold.fetch.warn&quot;: &quot;500ms&quot;, &quot;index.indexing.slowlog.threshold.index.info&quot;: &quot;800ms&quot;, &quot;index.indexing.slowlog.threshold.index.warn&quot;: &quot;1s&quot; &#125; 查询DSL查询语句以及过滤语句查询语句的处理行为取决于这个语句是在查询上下文中还是在过滤上下文中 两个的匹配级别不一样 Query context：关注于document和查询语句的匹配程度，也就是说目的是匹配出document Filter context：关注于document中的一些字段的过滤，例如过滤出document之后，过滤status字段为published的、timestamp字段的范围在2015到2016之间的 案例: 123456789101112131415GET /_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Search&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;Elasticsearch&quot; &#125;&#125; ], &quot;filter&quot;: [ &#123; &quot;term&quot;: &#123; &quot;status&quot;: &quot;published&quot; &#125;&#125;, &#123; &quot;range&quot;: &#123; &quot;publish_date&quot;: &#123; &quot;gte&quot;: &quot;2015-01-01&quot; &#125;&#125;&#125; ] &#125; &#125;&#125; full-text query-全文查询Term Query-术语查询term 查询可以包含指定术语的document 例如： 123456POST _search&#123; &quot;query&quot;: &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;Kimchy&quot; &#125; &#125;&#125; 在查询的时候可以给查询添加添加优先级，也就是紧急程度：boost字段 例如: 12345678910111213141516171819202122GET _search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;term&quot;: &#123; &quot;status&quot;: &#123; &quot;value&quot;: &quot;urgent&quot;, &quot;boost&quot;: 2.0 &#125; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;status&quot;: &quot;normal&quot; &#125; &#125; ] &#125; &#125;&#125; 默认语句的boost值为1.0 注意: 在es中，在进行查询的时候，会把text类型的字段进行分割，例如： “Quick Brown Fox!” 会被切割成术语：[quick, brown, fox] 因此，在进行term查询的时候，使用完整的字符串是查不到东西的 例如： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 生成数据PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;full_text&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;exact_value&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125;PUT my_index/my_type/1&#123; &quot;full_text&quot;: &quot;Quick Foxes!&quot;, &quot;exact_value&quot;: &quot;Quick Foxes!&quot; &#125;查询数据# 查询成功GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;exact_value&quot;: &quot;Quick Foxes!&quot; &#125; &#125;&#125;# 查询失败GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;full_text&quot;: &quot;Quick Foxes!&quot; &#125; &#125;&#125;# 查询成功GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;full_text&quot;: &quot;foxes&quot; &#125; &#125;&#125;# 查询成功GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;full_text&quot;: &quot;Quick Foxes!&quot; &#125; &#125;&#125; 注意：match查询方式的处理逻辑和term的方式是不一样的,match是属于全文查询级别。 批处理-Batch Processinghttps://www.elastic.co/guide/en/elasticsearch/reference/5.1/_batch_processing.html ES的批处理主要使用bulk api来实现。 Modules-ES组件node集群中的每一个节点都能感知到其他节点，并且，在收到请求之后，能够转发请求到对应的node节点上。 master node-主节点 主节点主要用于集群的控制以及元数据的处理，例如索引的新增、删除、分片的分配信息等等 创建或删除索引 跟踪哪些节点是集群的一部分 决定要分配给哪些节点哪些分片 配置为： 123node.master=truenode.data=falsenode.ingest=false 注意： 虽然有多个节点开启了这个配置，但是一个集群只有一个主节点，其他的都是master候选节点 data node-数据节点 数据节点保存数据(索引)分片，它负责数据相关操作，涉及： CRUD，以及搜索和聚合 因此，数据节点的CPU、内存、IO以及存储资源都消耗 如果数据节点的资源负载过高，则需要添加数据节点 配置为： 123node.master=falsenode.data=truenode.ingest=false client node-路由节点 可以看做是一个负载均衡器，负责： 处理路由请求 处理搜索聚合节点 分发批量索引请求 设置为： 123node.master=falsenode.data=falsenode.ingest=false 如果es集群不是规模很大的话，我们一般不设置client节点，使用master提供类似的功能即可。 线程池-Thread Pooles中，定义了许多种类的线程池，为的是更好的控制线程的内存消耗 注意： 所有的线程池都会队列的方式，这将允许在线程池满了之后，之后的请求将会排队，而不是直接被丢弃。 线程池类型分类固定的数值的（fixed），可以自行动态调整的(也就是说有上下限)（scaling） 进程数量（available_processors）一般会自动识别，上限为32个，也就是在超过32核的机器上，最多使用32个进程。有这个限制的原因是防止产生太多的线程，如果你修改系统的ulimit上限，那么可以手动调整这个上限。可以使用node的api去check具体的进程数量 固定数值的配置案例： 1234thread_pool: index: size: 30 queue_size: 1000 注意：当queue_size设置为-1的时候，就意味着关闭队列功能，当线程池满了之后，后面来的请求都会被丢弃 弹性数值的配置案例： 12345thread_pool: warmer: core: 1 max: 8 keep_alive: 2m 工作线程数量的范围是：core-max。keep_alive的作用是：当多长时间线程池没有任务后，不保持可用的工作线程 进程数量设置，如果需要手动指定，可以使用下列的配置 1processors: 2 目前有以下这些种类的线程池 generic 可动态调整类型 通用操作，例如：用于后台的node发现 index 固定类型 用于索引的新增删除操作，size的数量是进程数量(available processors)，最大的数量是1 + # of available processors.队列长度queue_size默认为200 search 固定类型。 用于统计、检索等操作，默认配置值为：int((# of available_processors * 3) / 2) + 1，队列长度默认为1000。 下面还有很多略，看文档 zen discovery-集群发现过程暂时略 ES的Merge功能参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/6.5/index-modules-merge.html 基本概念：ES中的shard分片，其实是lucene中的index索引。在lucene中，每一个index都会被分解成segments段，segments是index内部的基本存储单元 一些比较小的segment会被周期性的merge（合并）成为一个比较大的segment，为的是保持index的大小在一个稳定的范围，在合并之后将会删除这些小segment segment是lucene索引的一种存储结构，每个segment都是一部分数据的完整索引，它是lucene每次flush或merge时候形成。每次flush就是将内存中的索引写出一个独立segment的过程。所以随着数据的不断增加，会形成越来越多的segment。因为segment是不可变的，删除操作不会改变segment内部数据，只是会在另外的地方记录某些数据删除，这样可能会导致segment中存在大量无用数据。搜索时，每个segment都需要一个reader来读取里面的数据，大量的segment会严重影响搜索效率。而merge过程，会将小的segment写到一起形成一个大的segment，减少其数量。同时重写过程会抛弃那些已经删除的数据。因此segment的merge是有利于查询效率的。 总结：大量的分段留在Lucene索引里面，意味着较慢的搜索和占用更多的内存。分段合并设计用来减少分段数量，但是合并动作是非常昂贵的，尤其是在低IO环境中，可能会受到存储性能上的限制。 merge schedulingmerge scheduler（并行调度方式）控制了我们在什么时候需要去执行merge操作。 merge使用单独的线程去实现，当到达了设定的最大线程数，余下到来的merge任务将会排队等候。 merge scheduler可以按照下面的这种方式进行动态的设置 1index.merge.scheduler.max_thread_count elasticsearch的merge其实就是lucene的merge机制。merge过程是lucene有一个后台线程，它会根据merge策略来决定是否进行merge，一旦merge的条件满足，就会启动后台merge。merge策略分为两种，这也是大多数大数据框架所采用的，segment的大小和segment中doc的数量。以这两个标准为基础实现了三种merge策略：TieredMergePolicy、LogDocMergePolicy 及LogByteSizeMergePolicy。elasticsearch这一部分就是对这三种合并策略的封装，并提供了对于的配置。 forcemerge参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/6.5/indices-forcemerge.html 调用接口： 1POST /twitter/_forcemerge merge优化merge的优化可以是整个集群的调整，也可以是某些索引的调整，一般建议是索引级别。 merge操作是一个很消耗性能的过程，特别是对于磁盘的IO 因为如果有某个节点的io出现问题时，通过热线程定位，发现是merge导致的时，可以针对当前比较大的索引，修改一下merge相关的配置。 ES数据查询routing参数在将document写入到es中的时候，默认的路由算法规则是根据document的id。 但是我们可以指定某个字段为路由值，确保这些doc分布在同一个shard上。 使用自定义路由规则的作用： 我们知道，正常的一次查询(search)，请求会被发给所有shard(不考虑副本)，然后等所有shard返回，再将结果聚合，返回给调用方。如果我们事先已经知道数据可能分布在哪些shard上，那么就可以减少不必要的请求。 参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/5.1/search.html 示例： 插入数据 123456POST /twitter/tweet?routing=kimchy&#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;postDate&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125; 检索数据 123456789101112131415POST /twitter/tweet/_search?routing=kimchy&#123; &quot;query&quot;: &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;query_string&quot; : &#123; &quot;query&quot; : &quot;some query string here&quot; &#125; &#125;, &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot; &#125; &#125; &#125; &#125;&#125; searchThe query can either be provided using a simple query string as a parameter, or using arequest body. 使用search api有2种方式，一种是把查询字符串作为URI参数传入，一种是在请求体中嵌入查询语句 多索引及多type操作search api支持操作多个索引以及多个type 例如： 在所有的type中检索数据 1GET /twitter/_search?q=user:kimchy index确定，type多个 1GET /twitter/tweet,user/_search?q=user:kimchy type确定，索引多个(这里的type是tweet) 1234GET /kimchy,elasticsearch/tweet/_search?q=tag:wow或者所有的indexGET /_all/tweet/_search?q=tag:wow 在所有的index和type下搜索 1GET /_search?q=tag:wow 注意：默认情况下，es拒绝超过涉及1000个shard的search请求，因为这么大的请求，会非常消耗coordinating node的cpu、内存等资源 当然，这个1000的值也是可以调整的，在cluster setting中，使用action.search.shard_count.limit参数。 URI search格式例如： 1GET twitter/tweet/_search?q=user:kimchy 参数说明： The parameters allowed in the URI are: Name Description q The query string (maps to the query_string query, see Query String Queryfor more details). df The default field to use when no field prefix is defined within the query. analyzer The analyzer name to be used when analyzing the query string. analyze_wildcard Should wildcard and prefix queries be analyzed or not. Defaults to false. default_operator The default operator to be used, can be AND or OR. Defaults to OR. lenient If set to true will cause format based failures (like providing text to a numeric field) to be ignored. Defaults to false. explain For each hit, contain an explanation of how scoring of the hits was computed. _source Set to false to disable retrieval of the _source field. You can also retrieve part of the document by using _source_include &amp; _source_exclude (see the request body documentation for more details) stored_fields The selective stored fields of the document to return for each hit, comma delimited. Not specifying any value will cause no fields to return. sort Sorting to perform. Can either be in the form of fieldName, orfieldName:asc/fieldName:desc. The fieldName can either be an actual field within the document, or the special _score name to indicate sorting based on scores. There can be several sort parameters (order is important). track_scores When sorting, set to true in order to still track scores and return them as part of each hit. timeout A search timeout, bounding the search request to be executed within the specified time value and bail with the hits accumulated up to that point when expired. Defaults to no timeout. terminate_after The maximum number of documents to collect for each shard, upon reaching which the query execution will terminate early. If set, the response will have a boolean field terminated_early to indicate whether the query execution has actually terminated_early. Defaults to no terminate_after. from The starting from index of the hits to return. Defaults to 0. size The number of hits to return. Defaults to 10. search_type The type of the search operation to perform. Can bedfs_query_then_fetch or query_then_fetch. Defaults to query_then_fetch. See Search Type for more details on the different types of search that can be performed. Request body search同样的，我们也可以使用在请求体中嵌入查询语句的方式来查询数据。 格式例如： 123456GET /twitter/tweet/_search&#123; &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot; &#125; &#125;&#125; 支持下面这些参数 timeout A search timeout, bounding the search request to be executed within the specified time value and bail with the hits accumulated up to that point when expired. Defaults to no timeout. See Time units. from To retrieve hits from a certain offset. Defaults to 0. size The number of hits to return. Defaults to 10. If you do not care about getting some hits back but only about the number of matches and/or aggregations, setting the value to 0 will help performance. search_type The type of the search operation to perform. Can be dfs_query_then_fetchor query_then_fetch. Defaults to query_then_fetch. See Search Type for more. request_cache Set to true or false to enable or disable the caching of search results for requests where size is 0, ie aggregations and suggestions (no top hits returned). See Shard request cache. terminate_after The maximum number of documents to collect for each shard, upon reaching which the query execution will terminate early. If set, the response will have a boolean field terminated_early to indicate whether the query execution has actually terminated_early. Defaults to no terminate_after. 注意： Both HTTP GET and HTTP POST can be used to execute search with body. Since not all clients support GET with body, POST is allowed as well. 也就是说，es接受HTTP的GET请求和POST请求携带这些参数。但是有些客户端不支持在使用GET时携带参数，那么可以使用POST 当我们仅仅需要知道是否有doc匹配，而不用关注具体的数据和数量时，为了加快检索速度，我们可以使用这些参数 1GET /_search?q=message:elasticsearch&amp;size=0&amp;terminate_after=1 From/size我们可以结合使用from和size参数，来控制输出结果分页显示。 from控制开始显示的偏移量，size控制一次输出/读取的记录条数 from的默认值为0，size的默认值为10 from和size参数可以设置到URI中，也可以设置在request body中。 1234567GET /_search&#123; &quot;from&quot; : 0, &quot;size&quot; : 10, &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot; &#125; &#125;&#125; 注意： from + size can not be more than the index.max_result_window index setting which defaults to 10,000. See the Scroll or Search After API for more efficient ways to do deep scrolling. sortsort允许我们从doc中获取我们指定的一些字段数据，并且还可以根据这些字段进行排序输出 例如： 12345678910111213GET /nginx_count/_search&#123; &quot;sort&quot; : [ &#123; &quot;timestamp&quot; : &#123;&quot;order&quot; : &quot;asc&quot;&#125;&#125;, &quot;city&quot;, &#123; &quot;isp&quot; : &quot;desc&quot; &#125;, &#123; &quot;province&quot; : &quot;desc&quot; &#125;, &quot;_score&quot; ], &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;count&quot; : 23 &#125; &#125;&#125; 执行后的输出是： 12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;took&quot;: 15, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 268, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;nginx_count&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;299&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;city&quot;: &quot;赣州市&quot;, &quot;count&quot;: 23, &quot;isp&quot;: &quot;移动&quot;, &quot;province&quot;: &quot;江西省&quot;, &quot;timestamp&quot;: 1567146209783, &quot;domain&quot;: &quot;ops.nidianwo.com&quot;, &quot;aliyun&quot;: 700, &quot;shanghai&quot;: 900, &quot;binjiang&quot;: 100 &#125;, &quot;sort&quot;: [ 1567146209783, &quot;赣州市&quot;, &quot;移动&quot;, &quot;江西省&quot;, 1 ] &#125;, ......下面省略 说明： 在这里，获取timestamp、city、isp、province这4个信息，并且对匹配到的doc，再根据timestamp进行正向排序（也就是从小到大） _source filtering我们还可以对要输出的内容进行过滤 例如： 不输出所有的doc内容 1234567GET /_search&#123; &quot;_source&quot;: false, &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot; &#125; &#125;&#125; 或者只输出匹配的 12345678910111213141516GET /_search&#123; &quot;_source&quot;: &quot;obj.*&quot;, &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot; &#125; &#125;&#125;或者GET /_search&#123; &quot;_source&quot;: [ &quot;obj1.*&quot;, &quot;obj2.*&quot; ], &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot; &#125; &#125;&#125; 还可以编写过滤条件 12345678910GET /_search&#123; &quot;_source&quot;: &#123; &quot;includes&quot;: [ &quot;obj1.*&quot;, &quot;obj2.*&quot; ], &quot;excludes&quot;: [ &quot;*.description&quot; ] &#125;, &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot; &#125; &#125;&#125; search template链接：https://www.elastic.co/guide/en/elasticsearch/reference/5.1/search-template.html DSL-查询语句ES提供了一个完整的基于json的查询DSL。 DSL主要分为两类： 子句查询。查询条件是某个具体字段的具体的值。有match、term、range等 复合查询子句 _score 使用ES时，对于查询出的文档无疑会有文档相似度之别。而理想的排序是和查询条件相关性越高排序越靠前，而这个排序的依据就是_score。本文就是详解_score有关的信息，希望能对排序评分的理解有所帮助。 query and filter contextDSL在两种不用的使用场景下，又具有不同的特性 query context 在这种模式下，es思考的问题是：“doc和这个查询子句的匹配程度如何？“ 注意：是匹配程度 这个功能，通常用来获取数据 filter context 在这种模式下，es思考的问题是：”这个doc是否匹配这个查询子句” 注意：这个的答案是非常简单的：yes or no。不涉及其他的计算任务 这个功能，通常用来过滤数据 案例分析： 123456789101112131415GET /_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Search&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;Elasticsearch&quot; &#125;&#125; ], &quot;filter&quot;: [ &#123; &quot;term&quot;: &#123; &quot;status&quot;: &quot;published&quot; &#125;&#125;, &#123; &quot;range&quot;: &#123; &quot;publish_date&quot;: &#123; &quot;gte&quot;: &quot;2015-01-01&quot; &#125;&#125;&#125; ] &#125; &#125;&#125; 说明： The query parameter indicates query context. The bool and two match clauses are used in query context, which means that they are used to score how well each document matches. The filter parameter indicates filter context. The term and range clauses are used in filter context. They will filter out documents which do not match, but they will not affect the score for matching documents. match all query匹配所有的doc 123456GET /_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 在这个基础上，我们可以添加_score参数（默认是1.0） 123456GET /_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123; &quot;boost&quot; : 1.2 &#125; &#125;&#125; 不匹配所有的doc 123456GET /_search&#123; &quot;query&quot;: &#123; &quot;match_none&quot;: &#123;&#125; &#125;&#125; full text queries-text数据类型的全文内容检索用于字段类型是text的信息，like the body of an email. 针对这种内容，我们还需要指定字段的分析器analyzer。 match query参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/5.1/query-dsl-match-query.html#query-dsl-match-query-boolean match queries accept text/numerics/dates, analyzes them, and constructs a query. 例如： 12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot; : &#123; &quot;message&quot; : &quot;this is a test&quot; &#125; &#125;&#125; 说明： The match query is of type boolean. It means that the text provided is analyzed and the analysis process constructs a boolean query from the provided text. The operator flag can be set to or or and to control the boolean clauses (defaults to or). The minimum number of optional should clauses to match can be set using the minimum_should_match parameter. The analyzer can be set to control which analyzer will perform the analysis process on the text. It defaults to the field explicit mapping definition, or the default search analyzer. The lenient parameter can be set to true to ignore exceptions caused by data-type mismatches, such as trying to query a numeric field with a text query string. Defaults to false. 深入案例： 123456789101112GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot; : &#123; &quot;message&quot; : &#123; &quot;query&quot; : &quot;to be or not to be&quot;, &quot;operator&quot; : &quot;and&quot;, &quot;zero_terms_query&quot;: &quot;all&quot; &#125; &#125; &#125;&#125; multi match queryThe multi_match query builds on the match query to allow multi-field queries: 这种方式支持在多个字段中进行查询。 例如： 123456789GET /_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot; : &#123; &quot;query&quot;: &quot;this is a test&quot;, &quot;fields&quot;: [ &quot;subject&quot;, &quot;message&quot; ] &#125; &#125;&#125; Fields can be specified with wildcards, eg: 123456789GET /_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot; : &#123; &quot;query&quot;: &quot;Will Smith&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;*_name&quot; ] &#125; &#125;&#125; term level queries-术语级别查询不同于上面的全文内容检测，这个是查询非常具体的值。 因此，这种查询，通常适用于结构化的数据，例如numbers, dates, and enums，而不是没有结构的text文本类型。 term query例如： 123456POST _search&#123; &quot;query&quot;: &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;Kimchy&quot; &#125; &#125;&#125; 也可以再语句中加入boost参数，指定_score 12345678910111213141516171819202122GET _search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;term&quot;: &#123; &quot;status&quot;: &#123; &quot;value&quot;: &quot;urgent&quot;, &quot;boost&quot;: 2.0 &#125; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;status&quot;: &quot;normal&quot; &#125; &#125; ] &#125; &#125;&#125; terms query可以定义多个术语： 12345678910GET /_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot; : &#123; &quot;filter&quot; : &#123; &quot;terms&quot; : &#123; &quot;user&quot; : [&quot;kimchy&quot;, &quot;elasticsearch&quot;]&#125; &#125; &#125; &#125;&#125; range query例如： 123456789101112GET _search&#123; &quot;query&quot;: &#123; &quot;range&quot; : &#123; &quot;age&quot; : &#123; &quot;gte&quot; : 10, &quot;lte&quot; : 20, &quot;boost&quot; : 2.0 &#125; &#125; &#125;&#125; The range query accepts the following parameters: gte Greater-than or equal to gt Greater-than lte Less-than or equal to lt Less-than boost Sets the boost value of the query, defaults to 1.0 date类型相关的 123456789101112GET _search&#123; &quot;query&quot;: &#123; &quot;range&quot; : &#123; &quot;born&quot; : &#123; &quot;gte&quot;: &quot;01/01/2012&quot;, &quot;lte&quot;: &quot;2013&quot;, &quot;format&quot;: &quot;dd/MM/yyyy||yyyy&quot; &#125; &#125; &#125;&#125; exists query123456GET /_search&#123; &quot;query&quot;: &#123; &quot;exists&quot; : &#123; &quot;field&quot; : &quot;user&quot; &#125; &#125;&#125; 除此之外，还有非常多的类型，包括 Prefix Query Wildcard Query Regexp Query Fuzzy Query Type Query Ids Query 复合查询bool query主要是一些情况的判断，选项有： Occur Description must The clause (query) must appear in matching documents and will contribute to the score. filter The clause (query) must appear in matching documents. However unlike must the score of the query will be ignored. Filter clauses are executed in filter context, meaning that scoring is ignored and clauses are considered for caching. should The clause (query) should appear in the matching document. In a boolean query with no must or filter clauses, one or more should clauses must match a document. The minimum number of should clauses to match can be set using theminimum_should_match parameter. must_not The clause (query) must not appear in the matching documents. Clauses are executed in filter context meaning that scoring is ignored and clauses are considered for caching. Because scoring is ignored, a score of 0 for all documents is returned. 下面是一个案例 123456789101112131415161718192021222324POST _search&#123; &quot;query&quot;: &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot; &#125; &#125;, &quot;filter&quot;: &#123; &quot;term&quot; : &#123; &quot;tag&quot; : &quot;tech&quot; &#125; &#125;, &quot;must_not&quot; : &#123; &quot;range&quot; : &#123; &quot;age&quot; : &#123; &quot;gte&quot; : 10, &quot;lte&quot; : 20 &#125; &#125; &#125;, &quot;should&quot; : [ &#123; &quot;term&quot; : &#123; &quot;tag&quot; : &quot;wow&quot; &#125; &#125;, &#123; &quot;term&quot; : &#123; &quot;tag&quot; : &quot;elasticsearch&quot; &#125; &#125; ], &quot;minimum_should_match&quot; : 1, &quot;boost&quot; : 1.0 &#125; &#125;&#125; kibana查询数据上面说的都是直接从es中检索数据，但是对于大多数用户来说，更多的还是通过kibana去做 ES监控zabbix上的配置： key： 集群相关接口：GET /_cat/health?v、/_cat/allocation?v、GET /_cat/master?v 集群颜色状态（非绿色报警）：es_status[9002,cluster,status] 集群分片健康度（非100%报警）：es_status[9002,cluster,active_shards_percent] 集群节点总数（有变化报警）：es_status[9002,cluster,total_node] 集群data节点总数（有变化报警）：es_status[9002,cluster,total_datanode] 集群未分配的分片数量（大于0报警）：es_status[9002,cluster,unassigned_shards] 集群的磁盘存储空间（打到80%报警）：es_status[9002,cluster,disk_used_percent] 集群master节点（有变化报警）：es_status[9002,cluster,masternode] 集群的写入速度 集群的读写速度 线程池的监控，总数以及每个线程池的监控 gc的情况 节点相关： 相关api： es_status[9002,node,xx] JVM内存占用超过90 负载 cpu使用率 日志中有报错就报警 索引相关： 索引总数： es_status[9002,index,] 主机相关监控，流量，cpu什么的，怎么能按照一个集群的维度-添加到grafana中 可以添加一下监控，针对每一个es集群： 集群的磁盘使用情况 1GET /_cat/allocation?v 集群的健康性检查，例如红色和黄色、分片的状态等等 1GET /_cat/health?v 集群的不健康可能是多种原因导致的，这里不区分具体的原因，如果有问题，则报警，收到报警之后再去查看具体是什么原因。 具体的指标有： 监控数据节点的负载情况 11 node的可用性监控 11 node的性能监控 11 集群负载情况 11 集群的索引监控 1dd 监控指标主要是索引的请求速率、请求处理延迟、索引大小等、索引异常等 ES的jmx监控(因为是java进程) ES调优refresh时间间隔：refresh_interval replica数目设置：在bulk大量数据到ES集群的可以把副本数设置为0，在数据导入完成之后再设置为1或者你集群的适合的数目。 ES常见问题慢查慢写日志设置及收集设置 热线程参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/6.5/cluster-nodes-hot-threads.html 当有一些节点的cpu、负载、io等有异常时，我们可以通过热线程来定位到具体的产生原因。 使用格式： 1234[elk@es13.dwb.dgt ~]$ curl http://10.10.10.83:9200/_nodes/es13-data/hot_threads或者添加上时间，指定抓取多久时间内占用资源的热线程GET /_nodes/hot_threads&amp;interval=30s 执行后的输出： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394[elk@es13.dwb.dgt ~]$ curl http://10.10.10.83:9200/_nodes/es13-data/hot_threads::: &#123;es13-data&#125;&#123;76c2DSkdRRqTyHUCoK1pyQ&#125;&#123;egVogFoJRzuFvJxUX1VnWQ&#125;&#123;10.10.10.83&#125;&#123;10.10.10.83:9300&#125;&#123;ml.machine_memory=270056525824, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true&#125; Hot threads at 2019-07-23T07:21:24.850, interval=500ms, busiestThreads=3, ignoreIdleThreads=true: 54.7% (273.5ms out of 500ms) cpu usage by thread &apos;elasticsearch[es13-data][[app-publishing-logstash-2019.07.23][1]: Lucene Merge Thread #983]&apos; 2/10 snapshots sharing following 24 elements sun.misc.Unsafe.park(Native Method) java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215) java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078) org.apache.lucene.index.MergePolicy$OneMergeProgress.pauseNanos(MergePolicy.java:156) org.apache.lucene.index.MergeRateLimiter.maybePause(MergeRateLimiter.java:148) org.apache.lucene.index.MergeRateLimiter.pause(MergeRateLimiter.java:93) org.apache.lucene.store.RateLimitedIndexOutput.checkRate(RateLimitedIndexOutput.java:78) org.apache.lucene.store.RateLimitedIndexOutput.writeBytes(RateLimitedIndexOutput.java:72) org.apache.lucene.store.DataOutput.writeBytes(DataOutput.java:52) org.apache.lucene.store.RAMOutputStream.writeTo(RAMOutputStream.java:90) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter$TermsWriter.writeBlock(BlockTreeTermsWriter.java:820) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter$TermsWriter.writeBlocks(BlockTreeTermsWriter.java:624) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter$TermsWriter.pushTerm(BlockTreeTermsWriter.java:905) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter$TermsWriter.write(BlockTreeTermsWriter.java:869) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter.write(BlockTreeTermsWriter.java:343) org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:105) org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsWriter.merge(PerFieldPostingsFormat.java:164) org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:231) org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:116) org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4446) org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:4068) org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:625) org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:99) org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:662) 5/10 snapshots sharing following 11 elements org.apache.lucene.index.FilterLeafReader$FilterTermsEnum.next(FilterLeafReader.java:189) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter.write(BlockTreeTermsWriter.java:335) org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:105) org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsWriter.merge(PerFieldPostingsFormat.java:164) org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:231) org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:116) org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4446) org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:4068) org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:625) org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:99) org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:662) 2/10 snapshots sharing following 13 elements org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter$TermsWriter.writeBlocks(BlockTreeTermsWriter.java:633) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter$TermsWriter.pushTerm(BlockTreeTermsWriter.java:905) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter$TermsWriter.write(BlockTreeTermsWriter.java:869) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter.write(BlockTreeTermsWriter.java:343) org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:105) org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsWriter.merge(PerFieldPostingsFormat.java:164) org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:231) org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:116) org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4446) org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:4068) org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:625) org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:99) org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:662) unique snapshot sun.nio.ch.FileDispatcherImpl.write0(Native Method) sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60) sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) sun.nio.ch.IOUtil.write(IOUtil.java:65) sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211) java.nio.channels.Channels.writeFullyImpl(Channels.java:78) java.nio.channels.Channels.writeFully(Channels.java:101) java.nio.channels.Channels.access$000(Channels.java:61) java.nio.channels.Channels$1.write(Channels.java:174) org.apache.lucene.store.FSDirectory$FSIndexOutput$1.write(FSDirectory.java:417) java.util.zip.CheckedOutputStream.write(CheckedOutputStream.java:73) java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) java.io.BufferedOutputStream.write(BufferedOutputStream.java:126) org.apache.lucene.store.OutputStreamIndexOutput.writeBytes(OutputStreamIndexOutput.java:53) org.elasticsearch.common.lucene.store.FilterIndexOutput.writeBytes(FilterIndexOutput.java:59) org.apache.lucene.store.RateLimitedIndexOutput.writeBytes(RateLimitedIndexOutput.java:73) org.apache.lucene.store.DataOutput.writeBytes(DataOutput.java:52) org.apache.lucene.store.RAMOutputStream.writeTo(RAMOutputStream.java:90) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter$TermsWriter.writeBlock(BlockTreeTermsWriter.java:820) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter$TermsWriter.writeBlocks(BlockTreeTermsWriter.java:624) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter$TermsWriter.pushTerm(BlockTreeTermsWriter.java:905) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter$TermsWriter.write(BlockTreeTermsWriter.java:869) org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter.write(BlockTreeTermsWriter.java:343) org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:105) org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsWriter.merge(PerFieldPostingsFormat.java:164) org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:231) org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:116) org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4446) org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:4068) org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:625) org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:99) org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:662) 5.2% (26ms out of 500ms) cpu usage by thread &apos;elasticsearch[es13-data][[transport_server_worker.default]][T#16]&apos; 4/10 snapshots sharing following 2 elements io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) java.lang.Thread.run(Thread.java:745) 输出分析： 节点多磁盘时分配不均匀问题：一个数据量比较大的索引分片分配到某一台机器上的时候，整体的资源是够的，但是某一块盘的使用率是比较高的 如下图所示 123456[ops@es061.ecs.east1-g ~]$ df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 40G 5.9G 32G 16% /tmpfs 16G 12K 16G 1% /dev/shm/dev/vdb1 197G 59G 129G 32% /data1/dev/vdc1 197G 135G 53G 73% /data2 可以看到，两块盘的使用率将近相差了40%。 我们知道，es分配数据，主要是根据索引的分片级别到es的节点上，如果这个分配不均匀，我们可以通过挪动shard进行均衡。 但是，如果是节点下面的多快磁盘，这个的分配策略又是什么呢？ 多硬盘时的分配策略 磁盘的IO不均匀是因为可能挂在了裸盘，并且配置了多个data.path的原因。 一个shard只能分布在其中一块磁盘上，如果某个索引比较热，写入量大，对应的shard所在磁盘就比其他繁忙很多。 多块磁盘推荐做raid，每个磁盘的IO基本上是均匀分布的。 官方原文： 123456The path.data settings can be set to multiple paths, in which case all paths will be used to store data (although the files belonging to a single shard will all be stored on the same data path):path: data: - /mnt/elasticsearch_1 - /mnt/elasticsearch_2 - /mnt/elasticsearch_3 也就是说，我们可以设置多个存储路径来存储数据，es会自动去协调分配，但是一个shard的文件只会被存储到一个path中。 综上 导致这个问题出现的根本原因是： 这个node上分配了一个大索引的一个shard。 node级别只会分配这个shard存储到其中一个路径下，这里是/data2下 分配路径的策略是：优先找磁盘剩余空间大的盘 队列溢出使用这个命令查看队列的情况： 1GET /_cat/thread_pool?v 当看到rejected列中有数据的时候，就表示队列被打满了。 出现问题的原因可能有很多，例如： 往es写数据的程序写入的数据量太大 es集群需要扩容等等 ES扩容及缩容扩容扩容比较简单，在配置文件中指定现有es节点的地址，启动之后就会自动添加进当前的集群，注意cluster name需要一致。 在新节点添加进来之后，shard会自动挪动，如果集群压力较大，数据量较多，在挪动的时候可能会影响集群的读写，那么，这个时候我可以自定义的选择合适的时间去操作，配置为： 123456PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;cluster.routing.allocation.enable&quot;: &quot;none&quot; &#125;&#125; 避免集群shard移动，待到合适的时间，设置： 1PUT _cluster/settings &#123; "persistent": &#123; "cluster.routing.allocation.enable": null &#125; &#125; 恢复shard移动 具体可以参考：https://www.elastic.co/guide/en/elasticsearch/reference/6.2/rolling-upgrades.html 注意： 设置为none之后，如果创建了新的索引，那么该索引的分片将不会被分配，会被置为：UNASSIGNED。也就意味着无法往这个新索引中写入数据 在设置之前的索引，数据写入不受影响。 缩容使用exclude： 123456PUT _cluster/settings&#123; &quot;transient&quot; : &#123; &quot;cluster.routing.allocation.exclude._ip&quot; : &quot;10.0.0.1&quot; &#125;&#125; 把要下线的机器exclude，然后数据会自动迁移到其他节点并且自动rebalance，集群不需要重启。 迁移完后可以重新划分节点角色。 12345678#通过IP，排除集群中的某个节点：节点IP：10.100.0.11curl -XPUT http://&lt;domain&gt;:&lt;port&gt;/_cluster/settings?pretty -d &apos;&#123;&quot;transient&quot;:&#123;&quot;cluster.routing.allocation.exclude._ip&quot;:&quot;10.100.0.11&quot;&#125;&#125;&apos;#通过IP，排除集群中的多个节点：节点IP：10.10.0.11,10.100.0.12curl -XPUT http://&lt;domain&gt;:&lt;port&gt;/_cluster/settings?pretty -d &apos;&#123;&quot;transient&quot;:&#123;&quot;cluster.routing.allocation.exclude._ip&quot;:&quot;10.100.0.11,10.100.0.12&quot;&#125;&#125;&apos;#取消节点排除的限制curl -XPUT http://&lt;domain&gt;:&lt;port&gt;/_cluster/settings?pretty -d &apos;&#123;&quot;transient&quot;:&#123;&quot;cluster.routing.allocation.exclude._ip&quot;: null&#125;&#125;&apos; 注意：当迁移过程中，其他节点的负载较高时，可以先取消，就是上面的设置为null 所以，整个的下线操作是： exclude掉要下线的节点 确认shard，待该节点上没有shard之后，停止该节点 将exclude设置为null 问题：如何查看和控制shard的移动速度，防止数据量过大时出现问题。 相关参数除了上面提到的哪些，还有一些集群级别的参数可以动态定义 迁移相关： 1cluster.routing.allocation.cluster_concurrent_rebalance 参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/5.3/shards-allocation.html 主要是modules中的集群部分内容 索引相关： 1indices.recovery.max_bytes_per_sec 参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/5.3/recovery.html ES故障处理集群节点宕机导致shard unassigned使用这个去获取未分配的分片详情 1curl -XGET http://192.168.1.188:9200/_cat/shards|grep UNASSIGNED 未分配的是副本分片，那么修改副本数量： 123456PUT applog-prod-2016.12.18/_settings&#123; &quot;index&quot;:&#123; &quot;number_of_replicas&quot;:0 &#125;&#125; 如果是主分片丢失了，那么没有办法了，要承受数据丢失，我们使用reindex，来尽可能的保证数据的完整 整体的流程如下： 创建一个新的索引，mapping与要reindex的源index保持一致 执行reindex 删除原有的index 创建新的index，名称使用删除的名称 再次执行reindex，将数据恢复进去 删除掉临时的index即可 接口如下： 123456789POST _reindex&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125; 不想再次创建索引的话也可以使用alias 123456POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;twitter&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125; ]&#125; 这种形式的话，那么临时的索引就能删除掉 kibana部署安装部署参考链接：https://www.elastic.co/guide/en/kibana/6.5/install.html ES数据如何展示在kibana上如何将es的索引数据展示在kibana上？ manager–&gt;index patterns 输入index的匹配规则，注意索引需要有一个时间字段，以作为排序 负载均衡参考链接：https://www.elastic.co/guide/en/kibana/5.3/production.html#load-balancing 使用es中的转发节点去实现，es的这个转发节点其实就是负载均衡的作用，它会去协调各个节点。 ES的监控指标如何展示在kibana上kibana监控kibana搜索参考链接：https://www.elastic.co/guide/en/kibana/6.5/kuery-query.html#_new_simplified_syntax lucence query syntaxkibana上的查询方式是基于lucence的， 官方的内容： Kibana’s query language has historically been based on the Lucene query syntax. The following are some tips that can help get you started. To perform a free text search, simply enter a text string. For example, if you’re searching web server logs, you could enter safari to search all fields for the term safari. To search for a value in a specific field, prefix the value with the name of the field. For example, you could enter status:200 to find all of the entries that contain the value 200 in the status field. To search for a range of values, you can use the bracketed range syntax, [START_VALUE TO END_VALUE]. For example, to find entries that have 4xx status codes, you could enter status:[400 TO 499]. To specify more complex search criteria, you can use the Boolean operators AND, OR, and NOT. For example, to find entries that have 4xx status codes and have an extension of php or html, you could enter status:[400 TO 499] AND (extension:php OR extension:html). For more detailed information about the Lucene query syntax, see the Query String Query docs. kibana apm展示xpack-monitor监控默认情况下，在kibana界面是不展示各es节点的负载、资源使用率、索引信息等指标的，我们可以添加上这个功能，这在排查问题时还是挺有效果的。 除了能监控es，xpack体系还能监控filebeat、logstash等组件 注意：在6.3版本之后，xpack已经默认内嵌到elk组件当中了。 参考链接：https://www.elastic.co/cn/support/matrix#matrix_compatibility 在线方式安装es端配置： 1bin/elasticsearch-plugin install x-pack 如果es设置了关闭自动创建index，那么还需要添加下面这行配置： 1action.auto_create_index: .security,.monitoring*,.watches,.triggered_watches,.watcher-history* kibana端配置： 1bin/kibana-plugin install x-pack 离线方式安装参考链接：https://www.elastic.co/guide/en/x-pack/5.1/installing-xpack.html#xpack-installing-offline 如果服务器不能直连公网等，那么可以把相关的包文件下载下来上次到服务器上。 首先下载文件，并传输到服务器上。 然后执行以下命令 es： 1bin/elasticsearch-plugin install file:///path/to/file/x-pack-5.1.2.zip kibana： 1bin/kibana-plugin install file:///path/to/file/x-pack-5.1.2.zip xpack相关配置参数有以下相关参数： Setting Description xpack.security.enabled Set to false to disable X-Pack security. Configure in both elasticsearch.yml and kibana.yml. xpack.monitoring.enabled Set to false to disable X-Pack monitoring. Configure in both elasticsearch.yml and kibana.yml. xpack.graph.enabled Set to false to disable X-Pack graph. Configure in both elasticsearch.yml and kibana.yml. xpack.watcher.enabled Set to false to disable Watcher. Configure in elasticsearch.ymlonly. xpack.reporting.enabled Set to false to disable X-Pack reporting. Configure in kibana.yml only. 默认情况下，所有的参数功能都是打开的，可以根据实际情况去选择开启 我们一般的实际配置是： es节点 123456[elk@es032.ecs.east1-e config]$ grep xpack elasticsearch.ymlxpack.security.audit.enabled: falsexpack.monitoring.enabled: truexpack.security.enabled: falsexpack.graph.enabled: falsexpack.watcher.enabled: false kibana 123456[elk@redis2.dwb.dgt config]$ grep xpack kibana.ymlxpack.monitoring.elasticsearch.url: &quot;http://172.24.48.34:9200&quot;xpack.monitoring.enabled: truexpack.security.enabled: falsexpack.graph.enabled: falsexpack.reporting.enabled: false license更新1、去下面这个链接提交 基础 license 申请 https://license.elastic.co/registration/ 2、ELK 会给你填写的邮箱中发一封邮件。 去中间这个链接地址下载 license 文件 3、如果你和我一样使用的 ELK 6.3 版本，那么你需要使用以下以命令更新 License。注：License 更新命令经常变化，如果出现报错请注意报错信息。 12# @license.json是你刚才下载的 license 文件上传到服务器上。curl -XPOST &apos;http://192.168.1.1:9200/_xpack/license/start_basic?acknowledge=true&apos; -H &quot;Content-Type:application/json&quot; -d @license.json 5版本的命令是： 1curl -XPUT -u elastic &apos;http://172.16.1.128:9500/_xpack/license?acknowledge=true&apos; -H &quot;Content-Type: application/json&quot; -d @xiaohua-wang-9f31d302-4f35-46ad-b67d-2278833f3d58-v5.json 默认密码是changeme 注意需要上传这个json文件到服务器上 参考链接：https://www.elastic.co/guide/en/x-pack/5.6/installing-license.html 补充-yaml语法YAML语言的设计参考了JSON，XML和SDL等语言。也是一种数据交互的格式语言，YAML 强调以数据为中心,简洁易读,编写简单。 YAML主要的作用是作为程序的配置文件。 特点yaml语言有以下特点： 大小写敏感 通过缩进表示层级关系 禁止使用tab缩进，只能使用空格键 （个人感觉这条最重要） 缩进的空格数目不重要，只要相同层级左对齐即可 使用#表示注释 支持的数据结构支持以下数据结构： 对象：键值对的集合，又称为映射（mapping）/ 哈希（hashes） / 字典（dictionary） 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list） 纯量（scalars）：单个的、不可再分的值 数据结构的书写形式对象Map（属性和值）（键值对）的形式： key:(空格)v ：表示一堆键值对，空格不可省略。 123car: color: red brand: BMW 一行写法 1car:&#123;color: red，brand: BMW&#125; 相当于JSON格式： 1&#123;&quot;color&quot;:&quot;red&quot;,&quot;brand&quot;:&quot;BMW&quot;&#125; 数组一组连词线开头的行，构成一个数组。 1234brand: - audi - bmw - ferrari 一行写法 1brand: [audi,bmw,ferrari] 相当于JSON 1[&quot;auri&quot;,&quot;bmw&quot;,&quot;ferrari&quot;] 纯量纯量是最基本的、不可再分的值。以下数据类型都属于 JavaScript 的纯量。 字符串 布尔值 整数 浮点数 Null 时间 日期 补充-kafka安装部署kafka参考文档： https://kafka.apache.org/11/documentation.html#quickstart 启动启动zk 123bin/zookeeper-server-start.sh config/zookeeper.properties可以是：nohup bin/zookeeper-server-start.sh config/zookeeper.properties &gt; zk.log 2&gt;&amp;1 &amp; 启动kafka 123bin/kafka-server-start.sh config/server.properties可以是nohup bin/kafka-server-start.sh config/server.properties &gt; kafka.log 2&gt;&amp;1 &amp; kafka-manager参考链接： https://github.com/yahoo/kafka-manager/tree/1.3.3.16 https://www.cnblogs.com/frankdeng/p/9584870.html 注意：.sbt/repositories文件中，每行后面不能有空格，每行都是以换行结尾的。如果存在空格，那么使用delete键删除。 启动1[admin@node21 kafka-manager-1.3.3.18]$ nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080 &amp; 补充-logrotate参考链接： https://linux.cn/article-4126-1.html https://linux.cn/article-8227-1.html 注意： 在一个配置文件中，如果指定了多个日志文件，那么当第一个文件不需要进行轮转的时候，下面的文件就算需要进行轮转，也都会跳过，也就是说：当有多个日志文件时，下一个文件的操作，完全依赖上一个文件是否已经存在，当已经存在的时候，下面就不会再进行操作。 例如： 12345678910[root@node001 logrotate.d]# cat /etc/logrotate.d/syslog/var/log/cron/var/log/maillog&#123; sharedscripts create 0664 root root postrotate /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true endscript&#125; 当执行 1logrotate /etc/logrotate.conf -f 强制生成新文件的时候，当cron的轮询文件已经存在，但是maillog不存在的时候，这个时候，将会不操作，也就是，不会对maillog进行日志轮转。 补充-JVM知识补充-lucene知识]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>ELK</category>
        <category>ELK部署</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据科普]]></title>
    <url>%2F2019%2F06%2F18%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A7%91%E6%99%AE%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A7%91%E6%99%AE%2F</url>
    <content type="text"><![CDATA[参考文献： https://wiki.mbalib.com/wiki/%E5%A4%A7%E6%95%B0%E6%8D%AE 基础知识大数据大数据定义：大数据是指无法在一定时间内用常规软件工具对其内容进行抓取、管理(存储等)和处理(分析等)的数据集合。 大数据意义：有人把数据比喻为蕴藏能量的煤矿。煤炭按照性质有焦煤、无烟煤、肥煤、贫煤等分类，而露天煤矿、深山煤矿的挖掘成本又不一样。与此类似，大数据并不在“大”，而在于“有用”。价值含量、挖掘成本比数量更为重要。对于很多行业而言，如何利用这些大规模数据是赢得竞争的关键 大数据特征：一般来说，大数据具有以下几个特征 数据体量巨大。 数据类型复杂多样。现在的数据类型不仅是文本形式，更多的是图片、视频、音频、地理位置信息等多类型的数据，个性化数据占绝对多数。 价值密度低。以视频为例，一小时的视频，在不间断的监控过程中，可能有用的数据仅仅只有一两秒。 大数据技术大数据技术是指从各种各样类型的数据中，快速获得有价值信息的能力。 常见问题及误区数据不等于信息经常有人把数据和信息当作同义词来用。其实不然，数据指的是一个原始的数据点（无论是通过数字，文字，图片还是视频等等），信息则直接与内容挂钩，需要有资讯性（informative）。数据越多，不一定就能代表信息越多，更不能代表信息就会成比例增多。有两个简单的例子： 备份。很多人如今已经会定期的对自己的硬盘进行备份。这个没什么好多解释的，每次备份都会创造出一组新的数据，但信息并没有增多。 多个社交网站上的信息。我们当中的很多人在多个社交网站上活跃，随着我们上的社交网站越多，我们获得的数据就会成比例的增多，我们获得的信息虽然也会增多，但却不会成比例的增多。不单单因为我们会互相转发好友的微博（或者其他社交网站上的内容），更因为很多内容会十分类似，有些微博虽然具体文字不同，但表达的内容十分相似。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据科普</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcpdump]]></title>
    <url>%2F2019%2F06%2F04%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FLinux%E5%91%BD%E4%BB%A4%2Ftcpdump%2F</url>
    <content type="text"><![CDATA[基础知识命令语法tcpdump命令的使用格式如下： 1tcpdump [-adeflnNOpqStvx][-c&lt;数据包数目&gt;][-dd][-ddd][-F&lt;表达文件&gt;][-i&lt;网络界面&gt;][-r&lt;数据包文件&gt;][-s&lt;数据包大小&gt;][-tt][-T&lt;数据包类型&gt;][-vv][-w&lt;数据包文件&gt;][输出数据栏位] 参数说明： -a 尝试将网络和广播地址转换成名称。 -c&lt;数据包数目&gt; 收到指定的数据包数目后，就停止进行倾倒操作。 -d 把编译过的数据包编码转换成可阅读的格式，并倾倒到标准输出。 -dd 把编译过的数据包编码转换成C语言的格式，并倾倒到标准输出。 -ddd 把编译过的数据包编码转换成十进制数字的格式，并倾倒到标准输出。 -e 在每列倾倒资料上显示连接层级的文件头。 -f 用数字显示网际网络地址。 -F&lt;表达文件&gt; 指定内含表达方式的文件。 -i&lt;网络界面&gt; 使用指定的网络截面送出数据包。 -l 使用标准输出列的缓冲区。 -n 不把主机的网络地址转换成名字。 -N 不列出域名。 -O 不将数据包编码最佳化。 -p 不让网络界面进入混杂模式。 -q 快速输出，仅列出少数的传输协议信息。 -r&lt;数据包文件&gt; 从指定的文件读取数据包数据。 -s&lt;数据包大小&gt; 设置每个数据包的大小。 -S 用绝对而非相对数值列出TCP关联数。 -t 在每列倾倒资料上不显示时间戳记。 -tt 在每列倾倒资料上显示未经格式化的时间戳记。 -T&lt;数据包类型&gt; 强制将表达方式所指定的数据包转译成设置的数据包类型。 -v 详细显示指令执行过程。 -vv 更详细显示指令执行过程。 -x 用十六进制字码列出数据包资料。 -w&lt;数据包文件&gt; 把数据包数据写入指定的文件。 实际案例有以下案例 案例1：指定对端ip地址的流量 1tcpdump -i eth0 -nnn -s0 host 172.24.0.20 -w /root/dhub-api.pcap]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>Linux命令</category>
      </categories>
      <tags>
        <tag>tcpdump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper从入门到实践]]></title>
    <url>%2F2019%2F04%2F08%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E8%BF%90%E7%BB%B4%E6%9E%B6%E6%9E%84%2F%E5%88%86%E5%B8%83%E5%BC%8F%2Fzookeeper%2Fzookeeper%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[zookeeper基础知识学习链接：http://zookeeper.apache.org/ zk概述zookeeper是一个分布式的、开源的分布式应用程序协调服务。它公开了一组简单的原语，分布式应用程序可以在此基础上实现更高级别的同步、配置维护、组和命名服务。它的设计易于编程，并使用了一个熟悉的目录树结构的文件系统。它在java中运行，并且有java和c的绑定。 众所周知，协调服务很难做好。它们特别容易出错，例如竞争条件和死锁。zookeeper背后的动机是从零开始减轻分布式应用程序实现协调服务的责任。 zk设计目标-理念ZooKeeper is simple. ZooKeeper allows distributed processes to coordinate with each other through a shared hierarchal namespace which is organized similarly to a standard file system. The name space consists of data registers - called znodes, in ZooKeeper parlance - and these are similar to files and directories. Unlike a typical file system, which is designed for storage, ZooKeeper data is kept in-memory, which means ZooKeeper can acheive high throughput and low latency numbers. zk允许分布式程序之间通过一个共享的命名空间进行协调-这个命名空间被组织成类似linux文件系统的方式。 这个空间由多个数据寄存器（data registry）组成，这个在zk中叫做znode，他们类似文件和目录。 但与典型的文件系统不同，zk的数据保存在内存中，这意味着zk可以获得高吞吐量和低延迟。 ZooKeeper is replicated. Like the distributed processes it coordinates, ZooKeeper itself is intended to be replicated over a sets of hosts called an ensemble. The servers that make up the ZooKeeper service must all know about each other. They maintain an in-memory image of state, along with a transaction logs and snapshots in a persistent store. As long as a majority of the servers are available, the ZooKeeper service will be available. Clients connect to a single ZooKeeper server. The client maintains a TCP connection through which it sends requests, gets responses, gets watch events, and sends heart beats. If the TCP connection to the server breaks, the client will connect to a different server. 就像分布式程序一样，zk本身也趋向于在一组主机上进行复制，这一组主机就叫做ensemble-剧团 组成zk服务（zk service）的服务器们必须相互之间能够感知，它们在内存中维护状态的映像，同时持久化事务日志以及内容快照。 客户端连接到单个ZooKeeper服务器。客户端维护一个TCP连接，通过它发送请求、获取响应、获取监视事件和发送心跳。如果到服务器的TCP连接中断，客户端将连接到另一个服务器。 ZooKeeper is ordered. ZooKeeper stamps each update with a number that reflects the order of all ZooKeeper transactions. Subsequent operations can use the order to implement higher-level abstractions, such as synchronization primitives. zk使用数字标记每一个事务更新。所以，后续的更新可以使用这个数字来实现更高级别的抽象操作，例如同步原语等 ZooKeeper is fast. It is especially fast in “read-dominant” workloads. ZooKeeper applications run on thousands of machines, and it performs best where reads are more common than writes, at ratios of around 10:1. zk在读多写少的工作模式中非常快。 zk应用程序运行在数千台机器上，当读写比例为10：1时，它的性能最好。 数据模型及分层命名空间原文如下： The name space provided by ZooKeeper is much like that of a standard file system. A name is a sequence of path elements separated by a slash (/). Every node in ZooKeeper’s name space is identified by a path. zk的命名空间非常类似标准的文件系统，以/为开始，在命名空间中的每一个节点都以路径为标识。 模型如下： ZooKeeper’s Hierarchical Namespace 重要概念基础概念 ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持znode中存储的数据量较小的进一步原因）。 ZooKeeper有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。 ZooKeeper 底层其实只提供了两个功能：①管理（存储、读取）用户程序提交的数据；②为用户程序提交数据节点监听服务。 sessionSession 指的是 ZooKeeper 服务器与客户端会话。在 ZooKeeper 中，一个客户端连接是指客户端和服务器之间的一个 TCP 长连接。客户端启动的时候，首先会与服务器建立一个 TCP 连接，从第一次连接建立开始，客户端会话的生命周期也开始了。通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向Zookeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watch事件通知。 Session的sessionTimeout值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。 在为客户端创建会话之前，服务端首先会为每个客户端都分配一个sessionID。由于 sessionID 是 Zookeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 sessionID 的，因此，无论是哪台服务器为客户端分配的 sessionID，都务必保证全局唯一。 小总结： 客户端配置中有一个sessiontimeout参数，用于控制和服务器之间的连接的超时时间，当超过这个时间检测这个tcp连接还是失败的话，那么就会断开这个会话 znode在谈到分布式的时候，我们通常说的“节点”是指组成集群的每一台机器。然而，在Zookeeper中，“节点”分为两类，第一类同样是指构成集群的机器，我们称之为机器节点；第二类则是指数据模型中的数据单元，我们称之为数据节点一一ZNode。 Zookeeper将所有数据存储在内存中，数据模型是一棵树（Znode Tree)，由斜杠（/）的进行分割的路径，就是一个Znode，例如/foo/path1。每个上都会保存自己的数据内容，同时还会保存一系列属性信息。 在Zookeeper中，node可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。另外，ZooKeeper还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL.一旦节点被标记上这个属性，那么在这个节点被创建的时候，Zookeeper会自动在其节点名后面追加上一个整型数字，这个整型数字是一个由父节点维护的自增数字。 版本在前面我们已经提到，Zookeeper 的每个 ZNode 上都会存储数据，对应于每个ZNode，Zookeeper 都会为其维护一个叫作 Stat 的数据结构，Stat中记录了这个 ZNode 的三个数据版本，分别是version（当前ZNode的版本）、cversion（当前ZNode子节点的版本）和 cversion（当前ZNode的ACL版本）。 watcher-监听原文： ZooKeeper supports the concept of watches. Clients can set a watch on a znodes. A watch will be triggered and removed when the znode changes. When a watch is triggered the client receives a packet saying that the znode has changed. And if the connection between the client and one of the Zoo Keeper servers is broken, the client will receive a local notification. These can be used to [tbd]. Watcher（事件监听器），是Zookeeper中的一个很重要的特性。Zookeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是Zookeeper实现分布式协调服务的重要特性。 ### AClZookeeper采用ACL（AccessControlLists）策略来进行权限控制，类似于 UNIX 文件系统的权限控制。Zookeeper 定义了如下5种权限。 create：创建子节点权限 read：获取节点数据和子节点列表的权限 write：更新节点数据的权限 delete：删除子节点权限 admin：设置节点acl权限 其中尤其需要注意的是，CREATE和DELETE这两种权限都是针对子节点的权限控制。 zk的每一个znode都可以配置acl，控制谁可以访问 leader和follower所有的写请求都会被转发到leader节点上。follower节点同步主节点的数据。 zk内部保证了数据的原子性，也就是一个写入操作，在整个集群内部保证数据一致之后才认为成功 节点数量参考链接：https://zookeeper.apache.org/doc/current/zookeeperAdmin.html 的Cross Machine Requirements部分 当你想允许F台主机/实例进程异常终止时，集群总数就需要是2*F+1 APIzk提供了下面这些操作接口 create : creates a node at a location in the tree delete : deletes a node exists : tests if a node exists at a location get data : reads the data from a node set data : writes data to a node get children : retrieves a list of children of a node sync : waits for data to be propagated For a more in-depth discussion on these, and how they can be used to implement higher level operations, please refer to [tbd] zk实现如下图所示： 组件讲解 复制数据库 The replicated database is an in-memory database containing the entire data tree. Updates are logged to disk for recoverability, and writes are serialized to disk before they are applied to the in-memory database. 复制数据库是一个在内存中的数据库，它包含整个数据目录树。更新操作被记录到磁盘中以便于恢复操作，写操作在被应用到内存之前会被序列化到磁盘上。 读写请求流程 Every ZooKeeper server services clients. Clients connect to exactly one server to submit irequests. Read requests are serviced from the local replica of each server database. Requests that change the state of the service, write requests, are processed by an agreement protocol. As part of the agreement protocol all write requests from clients are forwarded to a single server, called the leader. The rest of the ZooKeeper servers, called followers, receive message proposals from the leader and agree upon message delivery. The messaging layer takes care of replacing leaders on failures and syncing followers with leaders. 客户端会精确的连接到一个zk server上，读请求会被提供服务-从本地的复制数据库中。如果是状态变更或者是写请求，将会被一致性协议处理 作为一致性协议的一部分，来自客户端的所有写请求，都会被转发到leader节点进行，其余的zk servers，叫做followers，这些节点接收leader的协商消息并进行集群内转发。这些协商信息层控制leader节点的故障替换，然后重新进行关系建立 ZooKeeper uses a custom atomic messaging protocol. Since the messaging layer is atomic, ZooKeeper can guarantee that the local replicas never diverge. When the leader receives a write request, it calculates what the state of the system is when the write is to be applied and transforms this into a transaction that captures this new state. 安装部署 目标服务器新建 zookeeper 用户 拷贝线上在用的zookeeper的文件夹（路径：/home/zookeeper/2181/zookeeper），到目标服务器 新建data文件夹（路径：/home/zookeeper/2181/data） 新建myid文件（路径：/home/zookeeper/2181/data/myid），第一个节点内容为1，之后类推 standalone模式执行下面步骤，运行在单实例模式： 解压缩： 123456# tar -zxvf zookeeper-3.4.6.tar.gz# cp -rp zookeeper-3.4.6 zookeeper# mkdir 2181# mv zookeeper 2181# cd 2181# mkdir data logs 修改配置文件 12345678[zookeeper@node001 zookeeper]$ cd /home/zookeeper/2181/zookeeper/conf/[zookeeper@node001 conf]$ cp zoo_sample.cfg zoo.cfg[zookeeper@node001 conf]$ grep -v &apos;#&apos; zoo.cfgtickTime=2000initLimit=10syncLimit=5dataDir=/home/zookeeper/2181/dataclientPort=2181 设置java堆内存 1234567891011[zookeeper@node001 bin]$ vim zkServer.sh搜索nohup将原内容： nohup &quot;$JAVA&quot; &quot;-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;&quot; &quot;-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;&quot; \ -cp &quot;$CLASSPATH&quot; $JVMFLAGS $ZOOMAIN &quot;$ZOOCFG&quot; &gt; &quot;$_ZOO_DAEMON_OUT&quot; 2&gt;&amp;1 &lt; /dev/null &amp;修改为： nohup &quot;$JAVA&quot; &quot;-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;&quot; &quot;-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;&quot; \ -cp &quot;$CLASSPATH&quot; $JVMFLAGS -Xmx2500M -Xms2500M $ZOOMAIN &quot;$ZOOCFG&quot; &gt; &quot;$_ZOO_DAEMON_OUT&quot; 2&gt;&amp;1 &lt; /dev/null &amp; 内存大小自定义设置，可以是3G或者是2500M这种 启动zk 123[zookeeper@node001 zookeeper]$ pwd/home/zookeeper/2181/zookeeper[zookeeper@node001 zookeeper]$ ./bin/zkServer.sh start 集群模式上面的配置要变成下面这种： 12345678tickTime=2000dataDir=/var/lib/zookeeper/clientPort=2181initLimit=5syncLimit=2server.1=host1:2888:3888server.2=host2:2888:3888server.3=host3:2888:3888 注意：server id的取值范围是1-255 创建myid文件 myid文件的所在路径是dataDir参数指定的目录 zk配置文件详解参考链接：https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_configuration 1234567891011121314151617181920212223242526272829303132333435[zookeeper@common001-dev.novalocal conf]$ cat zoo.cfg# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/home/zookeeper/2181/data# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clientsmaxClientCnxns=0## Be sure to read the maintenance section of the# administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDirautopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge featureautopurge.purgeInterval=1server.1=192.168.11.29:2888:3888server.2=192.168.11.32:2888:3888server.3=192.168.11.20:2888:3888 基础配置 tickTime Client-Server通信心跳时间，以毫秒为单位。 它用来控制心跳和session超时，默认情况下最小的会话超时时间为两倍的 tickTime。 dataDir是存放内存数据库快照的位置； dataLogDir 是事务日志目录； clientPort是client连接的端口。 initLimit： 集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。 如果超时这个时间，节点还没有连接上leader，那么就会放弃加入集群。 参数设定了允许所有跟随者与领导者进行连接并同步的时间，如果在设定的时间段内，半数以上的跟随者未能完成同步，领导者便会宣布放弃领导地位，进行另一次的领导选举。如果zk集群环境数量确实很大，同步数据的时间会变长，因此这种情况下可以适当调大该参数。默认为10 syncLimit： 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数（tickTime的数量）。 此配置表示， leader 与 follower 之间发送消息，请求 和 应答 时间长度。如果 follower 在设置的时间内不能与leader 进行通信，那么此 follower 将被丢弃。 参数设定了允许一个跟随者与一个领导者进行同步的时间，如果在设定的时间段内，跟随者未完成同步，它将会被集群丢弃。所有关联到这个跟随者的客户端将连接到另外一个跟随着。 autopurge.snapRetainCount这个参数指定了需要保留的文件数目，默认保留3个； autopurge.purgeInterval这个参数指定了清理频率，单位是小时，需要填写一个1或者更大的数据，默认0表示不开启自动清理功能。 server段server.X代表组成整个服务的机器，当服务启动时，会在数据目录下查找这个文件myid,这个文件中存有服务器的号码。 配置格式： 1server.A=B：C：D A 是一个数字，表示这个是第几号服务器； B 是这个服务器的 ip 地址； C 表示的是这个服务器与集群中的 Leader 服务器交换信息的端口； D 表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方式，由于 B 都是一样，所以不同的 Zookeeper 实例通信端口号不能一样，所以要给它们分配不同的端口号。 除了修改 zoo.cfg 配置文件，集群模式下还要配置一个文件 myid，这个文件在 dataDir 目录下，这个文件里面就有一个数据就是 A 的值，Zookeeper 启动时会读取这个文件，拿到里面的数据与 zoo.cfg 里面的配置信息比较从而判断到底是那个 server。 myid配置在dataDir所定义的目录下新建myid文件，本例中在/home/zookeeper/2181/data下新建myid文件，填入各主机之ID。如192.168.11.29主机的myid文件内容为1。 maxClientCnxnsmaxClientCnxns默认值60，这个连接数不是针对某个ip的，请注意这个限制的使用范围 指的是单台客户端机器与单台zookeeper服务器之间的连接数限制，不是针对指定客户端IP，也不是zookeeper集群的连接数限制 zookeeper日志管理日志分类zookeeper服务器会产生三类日志： 事务日志 快照日志 log4j日志。 在zookeeper默认配置文件zoo.cfg（可以修改文件名）中有一个配置项dataDir，该配置项用于配置zookeeper快照日志和事务日志的存储地址。 在官方提供的默认参考配置文件zoo_sample.cfg中，只有dataDir配置项。 其实在实际应用中，还可以为事务日志专门配置存储地址，配置项名称为dataLogDir，在zoo_sample.cfg中并未体现出来。在没有dataLogDir配置项的时候，zookeeper默认将事务日志文件和快照日志文件都存储在dataDir对应的目录下。 建议将事务日志（dataLogDir）与快照日志（dataLog）单独配置，因为当zookeeper集群进行频繁的数据读写操作是，会产生大量的事务日志信息，将两类日志分开存储会提高系统性能，而且，可以允许将两类日志存在在不同的存储介质上，减少磁盘压力。 log4j-运行日志log4j用于记录zookeeper集群服务器运行日志，该日志的配置地址在conf/目录下的log4j.properties文件中，该文件中有一个配置项为“zookeeper.log.dir=.”，表示log4j日志文件在与执行程序（zkServer.sh）在同一目录下。当执行zkServer.sh 时，在该文件夹下会产生zookeeper.out日志文件。下面主要介绍事务日志与快照日志。 事务日志概念事务日志指zookeeper系统在正常运行过程中，针对所有的更新操作，在返回客户端“更新成功”的响应前，zookeeper会保证已经将本次更新操作的事务日志已经写到磁盘上，只有这样，整个更新操作才会生效。 根据上文所述，可以通过zoo.cfg文件中的dataLogDir配置项找到事物日志存储地点：dataDir=/home/kafka/data/zookeeper在datalog/目录下存在一个文件夹version-2，该文件夹中保存着事物日志文件:log.504e25800日志文件的命名规则为log.，文件大小为64MB，表示写入该日志的第一个事务的ID，十六进制表示。 事务日志可视化zookeeper的事务日志为二进制文件，不能通过vim等工具直接访问。其实可以通过zookeeper自带的jar包读取事务日志文件。首先将libs中的slf4j-api-1.6.1.jar文件和zookeeper根目录下的zookeeper-3.4.8.jar文件复制到临时文件夹tmplibs中，然后执行如下命令,将日志内容输出至a.txt文件中： 1# java -classpath .:slf4j-api-1.6.1.jar:zookeeper-3.4.8.jar org.apache.zookeeper.server.LogFormatter /home/zookeeper/2181/data/version-2/log.2f019aaf7f &gt; a.txt 日志分析第一行：ZooKeeper Transactional Log File with dbid 0 txnlog format version 2 上面的代码分析中有说到每个日志文件都有一个这就是那里所说的日志头，这里magic没有输出，只输出了dbid还有version； 第二行：15-8-12 下午03时59分53秒 session 0x14f20ea71c10000 cxid 0x0 zxid 0x1 createSession 4000这也就是具体的事务日志内容了，这里是说xxx时间有一个sessionid为0x14f20ea71c10000、cxid为0x0、zxid 为0x1、类型为createSession、超时时间为4000毫秒 第三行：15-8-12 下午03时59分54秒 session 0x14f20ea71c10000 cxid 0x1 zxid 0x2 create ‘/solinx0000000000,#736f6c696e78,v{s{31,s{‘world,’anyone}}},F,1sessionID 为0x14f20ea71c10000，cxid：0x01、zxid：0x02、创建了一个节点路径为：/solinx0000000000、节点内容 为：#736f6c696e78(经过ASCII，实际内容为solinx)、acl为world:anyone任何人都可以管理该节点、节点不是 ephemeral节点的、父节点子版本：1 第四行：15-8-12 下午04时15分56秒 session 0x14f20ea71c10000 cxid 0x0 zxid 0x3 closeSession null这里是说xxx时间有一个sessionid为0x14f20ea71c10000、cxid为0x0、zxid为0x3、类型为 closeSession 快照日志zookeeper的数据在内存中是以树形结构进行存储的，而快照就是每隔一段时间就会把整个DataTree的数据序列化后存储在磁盘中，这就是zookeeper的快照文件。 zookeeper快照日志的存储路径同样可以在zoo.cfg中查看，如上文截图所示。访问dataDir路径可以看到version-2文件夹: 1dataDir=/home/zookeeper/2181/data zookeeper快照文件的命名规则为snapshot.，其中表示zookeeper触发快照的那个瞬间，提交的最后一个事务的ID。 与上面说的事务日志文件一样，Zookeeper也为快照文件提供了可视化的工具：org.apache.zookeeper.server包中的SnapshotFormatter类，接下来就使用该工具输出该事务日志文件，并解释该数据； SnapshotFormatter工具使用命令如下： 1# java -classpath .:slf4j-api-1.6.1.jar:zookeeper-3.4.8.jar org.apache.zookeeper.server.SnapshotFormatter /home/zookeeper/2181/data/version-2/snapshot.2f019b7bc0 |less 输出如下： 123456789101112131415161718192021222324252627ZNode Details (count=105078):----/ cZxid = 0x00000000000000 ctime = Thu Jan 01 08:00:00 CST 1970 mZxid = 0x00000000000000 mtime = Thu Jan 01 08:00:00 CST 1970 pZxid = 0x00002c05e92d48 cversion = 71 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x00000000000000 dataLength = 0----/com cZxid = 0x0000010003a93a ctime = Mon Mar 06 00:26:24 CST 2017 mZxid = 0x0000010003a93a mtime = Mon Mar 06 00:26:24 CST 2017 pZxid = 0x0000010003a93b cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x00000000000000 dataLength = 0 ...... 快照分析快照文件就很容易看得懂了，这就是Zookeeper整个节点数据的输出；第一行：ZNode Details (count=105078):ZNode节点数总共有105078个/cZxid = 0x00000000000000ctime = Thu Jan 01 08:00:00 CST 1970mZxid = 0x00000000000000mtime = Thu Jan 01 08:00:00 CST 1970pZxid = 0x00002c05e92d48cversion = 71dataVersion = 0aclVersion = 0ephemeralOwner = 0x00000000000000dataLength = 0 这么一段数据是说： 根节点/：cZxid：创建节点时的ZXIDctime：创建节点的时间mZxid：节点最新一次更新发生时的zxidmtime：最近一次节点更新的时间pZxid：父节点的zxidcversion：子节点更新次数dataVersion：节点数据更新次数aclVersion：节点acl更新次数ephemeralOwner：如果节点为ephemeral节点则该值为sessionid，否则为0dataLength：该节点数据的长度快照文件的末尾：Session Details (sid, timeout, ephemeralCount): 0x14f211584840000, 4000, 0 0x14f211399480001, 4000, 0 这里是说当前抓取快照文件的时间Zookeeper中Session的详情，有两个session超时时间都是4000毫秒ephemeral节点为0； 日志清理在zookeeper 3.4.0以后，zookeeper提供了自动清理snapshot和事务日志功能 通过配置zoo.cfg下的autopurge.snapRetainCount和autopurge.purgeInterval这两个参数实现日志文件的定时清理。 autopurge.snapRetainCount这个参数指定了需要保留的文件数目，默认保留3个； autopurge.purgeInterval这个参数指定了清理频率，单位是小时，需要填写一个1或者更大的数据，默认0表示不开启自动清理功能。 日志管理配置修改注意，如果Zookeeper集群只有3个实例，那么日志修改务必先修改follower 节点的配置，再修改leader 节点的配置，否则可能会导致问题。 配置集群运行日志并设置切割操作文件：log4j.properties 1234zookeeper.root.logger=INFO, ROLLINGFILElog4j.appender.ROLLINGFILE.MaxFileSize=100MB # 每个日志文件的最大size为100Mlog4j.appender.ROLLINGFILE.MaxBackupIndex=50 # 保留5个G的日志 操作文件：bin/zkEnv.sh文件 将 12345678if [ &quot;x$&#123;ZOO_LOG_DIR&#125;&quot; = &quot;x&quot; ]then ZOO_LOG_DIR=&quot;.&quot;fiif [ &quot;x$&#123;ZOO_LOG4J_PROP&#125;&quot; = &quot;x&quot; ]then ZOO_LOG4J_PROP=&quot;INFO,CONSOLE&quot;fi 修改成： 12345678if [ &quot;x$&#123;ZOO_LOG_DIR&#125;&quot; = &quot;x&quot; ]then ZOO_LOG_DIR=&quot;/home/zookeeper/2181/logs&quot;fiif [ &quot;x$&#123;ZOO_LOG4J_PROP&#125;&quot; = &quot;x&quot; ]then ZOO_LOG4J_PROP=&quot;INFO,ROLLINGFILE&quot;fi 注意：log4j.properties中的zookeeper.root.logger的值需要和zkEnv.sh文件的配置ZOO_LOG4J_PROP保持一致。 去除zookeeper.out 文件 操作文件：bin/zkServer.sh 注释如下内容： 1# _ZOO_DAEMON_OUT=&quot;$ZOO_LOG_DIR/zookeeper.out&quot; 将如下内容： 12nohup &quot;$JAVA&quot; &quot;-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;&quot; &quot;-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;&quot; \ -cp &quot;$CLASSPATH&quot; $JVMFLAGS $ZOOMAIN &quot;$ZOOCFG&quot; &gt; &quot;$_ZOO_DAEMON_OUT&quot; 2&gt;&amp;1 &lt; /dev/null &amp; 修改为： 12nohup &quot;$JAVA&quot; &quot;-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;&quot; &quot;-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;&quot; \ -cp &quot;$CLASSPATH&quot; $JVMFLAGS $ZOOMAIN &quot;$ZOOCFG&quot; &gt;&amp;1 &lt; /dev/null &amp; 然后重启zk即可 事务日志操作文件：zoo.cfg 在zoo.cfg文件的中添加如下这行 1dataLogDir=/home/zookeeper/2181/data/event 这部分是可选操作，默认不配置的话就会使用快照日志的配置。一般不进行额外配置 快照日志操作文件：zoo.cfg 快照日志不需要额外的处理，默认的配置就是针对快照日志，也就是： 123dataDir=/home/zookeeper/2181/dataautopurge.snapRetainCount=10 # 需要保留的文件数目，默认设置为3个；autopurge.purgeInterval=24 # 清理频率，默认单位是小时 zk运维测试版本使用3.4.6 zkcli使用使用方式： 1# ./bin/zkCli.sh -server 127.0.0.1:2181 支持的命令： 1234567891011121314151617181920212223[zk: 127.0.0.1:2181(CONNECTED) 5] helpZooKeeper -server host:port cmd args stat path [watch] set path data [version] ls path [watch] delquota [-n|-b] path ls2 path [watch] setAcl path acl setquota -n|-b val path history redo cmdno printwatches on|off delete path [version] sync path listquota path rmr path get path [watch] create [-s] [-e] path data acl addauth scheme auth quit getAcl path close connect host:port 常用命令快照数据文件清理数据备份及恢复集群节点停机及恢复内存设置zk集群节点奇数标准官方原文： For the ZooKeeper service to be active, there must be a majority of non-failing machines that can communicate with each other. To create a deployment that can tolerate the failure of F machines, you should count on deploying 2xF+1 machines. Thus, a deployment that consists of three machines can handle one failure, and a deployment of five machines can handle two failures. Note that a deployment of six machines can only handle two failures since three machines is not a majority. For this reason, ZooKeeper deployments are usually made up of an odd number of machines. 也就是说： 为了保证整体zk服务的可用。必须保证zk集群中的大多数节点是可用的，可以互相通信的。 如果需要创建一个能够容忍F节点故障的集群，那么集群总数需要是2*F+1。 因此，3节点的集群，能够容忍1个节点故障，5节点的集群能够容忍2个节点 如果是6个节点的集群，那么也只能容忍2个节点故障，因为如果是3个节点的话，不满足大于大多数的要求。因为这个原因，所以我们的zk集群设计都是按照奇数来进行。 zk监控官方链接：https://zookeeper.apache.org/doc/r3.4.6/zookeeperAdmin.html#sc_monitoring The ZooKeeper service can be monitored in one of two primary ways; 1) the command port through the use of 4 letter words and 2) JMX. See the appropriate section for your environment/requirements. zk集群服务的监控，可以通过两种方式来实现。 1种是4个字母的命令，第2种是JMX方式。具体选择哪一种需要看具体的环境和需求来定 zk性能优化有以下这些操作可以执行 To get low latencies on updates it is important to have a dedicated transaction log directory. By default transaction logs are put in the same directory as the data snapshots and myid file. The dataLogDir parameters indicates a different directory to use for the transaction logs. 为了降低更新请求的处理延迟，有一个非常重要的配置-将事务日志存储到一个指定的目录。 默认情况下，事务日志会和数据快照以及myid文件存放在一个目录中。 使用dataLogDir参数来执行事务日志的存储目录。 不要使zk使用到swap 4字命令参考链接：https://zookeeper.apache.org/doc/r3.4.6/zookeeperAdmin.html#sc_zkCommands zk提供了命令行方式去获取zk集群的状态，这个可以拿来做监控等。 使用语法： 12345678910111213141516[root@common001-dev.novalocal ~]# echo mntr | nc 127.0.0.1 2181zk_version 3.4.8--1, built on 02/06/2016 03:18 GMTzk_avg_latency 0zk_max_latency 2812zk_min_latency 0zk_packets_received 42630860zk_packets_sent 42878757zk_num_alive_connections 509zk_outstanding_requests 0zk_server_state followerzk_znode_count 108438zk_watch_count 84655zk_ephemerals_count 9526zk_approximate_data_size 17629011zk_open_file_descriptor_count 538zk_max_file_descriptor_count 65535 下面是支持的命令 conf New in 3.3.0: Print details about serving configuration. 输出配置信息 cons New in 3.3.0: List full connection/session details for all clients connected to this server. Includes information on numbers of packets received/sent, session id, operation latencies, last operation performed, etc… 列出详细的客户端连接信息，包括ip、session id等信息。 crst New in 3.3.0: Reset connection/session statistics for all connections. 重置所有的连接统计信息，一般用不到。 dump Lists the outstanding sessions and ephemeral nodes. This only works on the leader. 输出当前集群的所有会话信息，主要是会话id的形式，还有创建的临时节点信息。【排查问题时涉及】 envi Print details about serving environment 输出环境变量信息 ruok Tests if server is running in a non-error state. The server will respond with imok if it is running. Otherwise it will not respond at all. A response of “imok” does not necessarily indicate that the server has joined the quorum, just that the server process is active and bound to the specified client port. Use “stat” for details on state wrt quorum and client connection information. 检测zk节点的存活状态【监控时需要添加】 srst Reset server statistics. 重置统计信息 srvr New in 3.3.0: Lists full details for the server. 输出zk服务的一些信息，例如zk版本、角色、znode总数、延迟、队列数量、连接数、接收包的总数等 stat Lists brief details for the server and connected clients. 除了上面的信息之外，还是会显示客户端连接信息（只显示ip:port，接发包） wchs New in 3.3.0: Lists brief information on watches for the server. 获取watch的zonde总数 例如： 12540 connections watching 5362 pathsTotal watches:17544 wchc New in 3.3.0: Lists detailed information on watches for the server, by session. This outputs a list of sessions(connections) with associated watches (paths). Note, depending on the number of watches this operation may be expensive (ie impact server performance), use it carefully. 输出watch的详细信息，分类字段是session ID。获取它watch的path 因为数据量可能比较大，所以在执行的时候，可能会影响性能 wchp New in 3.3.0: Lists detailed information on watches for the server, by path. This outputs a list of paths (znodes) with associated sessions. Note, depending on the number of watches this operation may be expensive (ie impact server performance), use it carefully. 和上面类似，输出watch的详细信息，只不过分类字段是path，获取某个path，有哪些session在watch它。 因为数据量可能比较大，所以在执行的时候，也可能会影响性能 mntr New in 3.4.0: Outputs a list of variables that could be used for monitoring the health of the cluster. 输出集群的汇总统计信息 事务日志可视化案例： 将libs中的slf4j-api-1.7.5.jar文件和zookeeper根目录下的zookeeper.jar文件复制到临时文件夹tmplibs中，然后执行如下命令： 12# cd tmplibs# java -classpath .:slf4j-api-1.7.5.jar:zookeeper.jar org.apache.zookeeper.server.LogFormatter /data/hadoop/var/lib/zookeeper/version-2/log.7600180b03 jvm调优 在zk的jvm相关默认参数满足需求的时候，就可以设置一下： 修改zk目录下的bin子目录下的zkServer.sh文件，搜索nohup关键词，定位到需要修改的行。 1234nohup &quot;$JAVA&quot; &quot;-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;&quot; &quot;-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;&quot; \-cp &quot;$CLASSPATH&quot; $JVMFLAGS -verbose:gc -Xloggc:/home/zookeeper/2181/logs/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps \-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/zookeeper/2181/logs/java.hprof -XX:+PrintAdaptiveSizePolicy \-server -Xms3G -Xmx3G $ZOOMAIN &quot;$ZOOCFG&quot; &gt; &quot;$_ZOO_DAEMON_OUT&quot; 2&gt;&amp;1 &lt; /dev/null &amp; 如代码框中的内容显示，我们添加打印gc日志、打印gc的AdaptiveSizePolicy自动优化调整日志，jvm大小等配置。 zk的事务日志和快照以及数据恢复数据恢复过程数据初始化工作其实就是从磁盘中加载数据的过程，主要包括了从快照文件中加载快照数据的根据事务日志进行数据订正两个过程。1.初始化FileTxnSnapLogFileTxnSnapLog是ZooKeeper事务日志和快照数据访问层，用于衔接上层业务与底层数据存储。底层数据包括了事务日志和快照两部分，因此FileTxnSnapLog内部氛围FileTxnLog和FileSnap的初始化，分别代表事务日志管理器和快照数据管理器的初始化。2.初始化ZKDatabase接下来就开始构建内存数据库ZKDatabase了。在初始化过程中，首先会构建一个初始化的DataTree，同时将步骤1中初始化的FileTxnSnapLog交给ZKDatabase，以便于内存数据库能够对事务日志和快照数据进行访问。DataTree是ZooKeeper内存模型的核心模型，简而言之就是一棵树，保存了ZooKeeper上的所有节点信息，在每个ZooKeeper服务器内部都是单例。在ZKDatabase初始化的时候，DataTree也会进行相应的初始化工作——创建一些ZooKeeper的默认节点，包括/、/zookeeper、/zookeeper/quota三个节点的创建。除了ZooKeeper的数据节点，在ZKDatabase的初始化阶段还会创建一个用于保存所有客户端会话超时时间的记录器：sessionsWithTimeouts——会话超时时间记录器。3.创建PlayBackListener监听器PlayBackListener监听器主要用来接收事务应用过程中的回调。在后面读者会看到，在ZooKeeper数据恢复后期，会有一个事务订正过程，在这个过程中会回调PlayBackListener监听器来进行对应的数据订正。4.处理快照文件完成内存数据库的初始化之后，ZooKeeper就开始从磁盘中恢复数据了。在上文中我们已经提到，每一个快照数据文件中都保存了ZooKeeper服务器近似全量的数据，因此首先从这些快照文件开始加载。5.获取最新的100个快照文件ZooKeeper服务器运行一段时间之后，磁盘上会保留许多快照文件。另外由于每次数据快照过程中，ZooKeeper都会将全量数据Dump到磁盘快照文件中，因此往往更新时间最晚的那个文件包含了最新的全量数据。那么是否我们只需要这个罪行的快照文件就可以了呢？在ZooKeeper的实现中，会获取最新的之多100个快照文件。6.解析快照文件获取到这之多100个文件之后，ZooKeeper会“逐个”进行解析每个快照文件都是内存数据序列化到磁盘的二进制文件，因此在这里需要对其进行反序列化，生成DataTree对象和sessionsWithTimeouts集合。同时在这个过程中，还会进行文件的checkSum校验以确认快照文件的正确性。在“逐个”解析的过程中，如果正确性校验通过的话，呢么通常只会解析最新的那个快照文件。换句话说，只有当最新的快照文件不可用的时候，才会逐个进行解析，知道将这100个文件全部解析完成。如果将步骤4中获取的所有快照文件都解析完成后还是无法完成恢复一个完整的DataTree和sessionWithTimeouts，则认为无法从磁盘中加载数据，服务器启动失败。7.获取罪行的ZXID完成6之后，就已经基于开招文件构建了一个完整的DataTree实例和sessionsWithTimeouts集合了。此时根据这个快照文件的文件名就可以解析出一个最新的ZXID：zxid_for_snap，它代表了ZooKeeper开始进行数据快照的时刻。8.处理事务日志在经过前面7处理后，此时ZooKeeper服务器内存中已经有了一份近似全量的数据了，开始就要通过事务日志来更新增量数据了。9.获取所有zxid_for_snap之后提交的事务到这里，我们已经获取到了快照数据的最新ZXID。ZooKeeper中数据的快照机制决定了快照文件中并非包含了所有的事务操作。蛋是未被包含在快照中的那部分事务操作是可以我替你故宫 数据订正来实现的。因此这里我们只需要从事务日志中获取所有ZXID比步骤7中得到的zxid_for_snap大的事务操作。10.事务应用获取到所有ZXID大于zxid_for_snap的事务后，将其逐个应用到之前基于快照数据文件恢复出来的DataTree和sessionsWithTimeouts中去。在事务应用的过程中，还有一个细节需要我们注意，每当有一个事务被应用到内存数据库中，ZooKeeper同时会回调PlayBackListener监听器，将这一事务操作记录转换成Proposal，保存到ZKDatabase.committedLog中，以便Follower进行快速同步。11.获取最新ZXID待所有事务都被完整地应用到内存数据库中，基本上就完成了数据的初始化过程，此时再次获取一个ZXID，用来标识上次服务器正常运行时提交的最大事务ID。12.校验epochepoch是ZooKeeper中一个非常特别的变量，其字面意思是“时代”，在ZooKeeper中，epoch标识了当前Leader周期。每次选举产生一个新的Leader服务器之后，就会生成一个新的cpoch。在运行期间集群中机器互相通信的过程中，都会带上这个epoch一确保彼此在同一个Leader周期内。在完成数据加载后，ZooKeeper会从步骤11中确定的ZXID中解析出事务处理的Leader周期：epochOfZxid。同时会从磁盘的currentEpoch和acceptedEpoch文件中对去出上次记录的最新的epoch值，进行校验。以上就是ZooKeeper服务器启动时期的数据初始化的全过程。 实际进行数据恢复的时候，操作如下： 找到要恢复的快照和事务文件 传输到目标端 要恢复数据的目标端，停止zk进程，删除数据目录下的文件，复制刚才的两个文件到数据目录下。 启动zk服务 注意：如果是3台及以上的zk集群的话，则需要全部停止进程，删除数据，恢复其中的一台，然后等数据恢复完成后，再启动其余的两台服务让zk自己同步数据过去 zk缩容扩容zk_avg_latency指标分析参考链接：https://mp.weixin.qq.com/s/j8p-953azVVHwpnebuJvbQ Zookeeper服务器累计所有请求的延迟时间（totalLatency），累计总共请求次数（count），通过totalLatency/count获取avgLatency指标。至于avgLatency指标持续维持为0ms，由于totalLatency &lt;count导致。也就是多次请求Latency的延迟为0ms。 zk案例-dubbo讲解]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维架构</category>
        <category>分布式</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PostgreSQL安装部署及使用]]></title>
    <url>%2F2019%2F03%2F27%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FPostgreSQL%2FPostgreSQL%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[基础知识安装部署下载页面：https://www.postgresql.org/download/ 本文以安装version 11为例 centos6安装repo源 1yum -y install https://download.postgresql.org/pub/repos/yum/11/redhat/rhel-6-x86_64/pgdg-centos11-11-2.noarch.rpm 安装client 1yum -y install postgresql11 安装server 1yum -y install postgresql11-server 初始化数据库以及启动 123service postgresql-11 initdbchkconfig postgresql-11 onservice postgresql-11 start centos7安装repo源 1yum -y install https://download.postgresql.org/pub/repos/yum/11/redhat/rhel-7-x86_64/pgdg-centos11-11-2.noarch.rpm 安装client 1yum -y install postgresql11 安装server 1yum -y install postgresql11-server 初始化数据库以及启动 123/usr/pgsql-11/bin/postgresql-11-setup initdbsystemctl enable postgresql-11systemctl start postgresql-11 在启动之后，默认监控的端口是127.0.0.1:5432 使用在初次安装后，默认生成一个名为postgres的数据库和一个名为postgres的数据库用户。这里需要注意的是，同时还生成了一个名为postgres的Linux系统用户。有两种方式为PostgreSQL添加用户和添加数据库 设置登录密码在启动之后，默认是没有密码的，因此我们首先要创建密码： 12345[root@node010-dev data]# su - postgres-bash-4.2$ psqlpsql (11.2)输入 &quot;help&quot; 来获取帮助信息.postgres=# \password postgres 注意，这里虽然设置了，但是不允许直接登录，需要进行一下设置 配置监听ip编辑/var/lib/pgsql/11/data/postgresql.conf 文件 将#listen_addresses = &#39;localhost&#39;修改为listen_addresses=&#39;*&#39; （当然，此处‘*’也可以改为任何你想开放的服务器IP） 配置允许登录默认情况下PostgreSQL不支持密码登录，在登录的时候会有如下报错： 12[root@node010-dev data]# psql -U postgrespsql: 致命错误: 对用户&quot;postgres&quot;的对等认证失败 如需支持需要修改配置文件 编辑该文件，将未注释的peer都替换成为md5 1# vim /var/lib/pgsql/11/data/pg_hba.conf 重启服务之后即可正常登录数据库 配置允许远程登录在进行了上面的配置之后，是可以进行本地端的登录的，但是在远端使用连接命令，例如： 1# psql -U devuser -d registry -h 192.168.1.196 -p 5432 会产生报错： 1psql: 致命错误: 没有用于主机 &quot;192.168.1.219&quot;, 用户 &quot;devuser&quot;, 数据库 &quot;registry&quot;, SSL 关闭 的 pg_hba.conf 记录 解决：在server端的pg_hba.conf文件末尾添加以下内容： 1234# TYPE DATABASE USER CIDR-ADDRESS METHODhost all all 0.0.0.0/0 md5全网段换成指定的网段也可以 然后重启服务即可。 数据库使用数据库使用首先需要psql -U postgres进入数据后再执行 创建用户创建用户并设置密码： 1234CREATE USER dbuser WITH PASSWORD &apos;password&apos;;例如：postgres=# create user devuser with password &apos;Devuser123&apos;;postgres=# create user prouser with password &apos;Prouser123&apos;; 注意：如果只设置了数据库用户，那么在系统的shell登录的时候需要使用-U指定登录用户，如果还在系统中useradd了同名用户，那么可以切换到这个同名用户然后执行psql即可。 创建数据库及授权创建数据库并指定用户 1234CREATE DATABASE exampledb OWNER dbuser;例如：postgres=# create database registry owner devuser;postgres=# create database registry owner prouser; 进行用户授权，将指定数据库的所有权限都赋予dbuser，否则dbuser只能登录控制台，没有任何数据库操作权限。 1234GRANT ALL PRIVILEGES ON DATABASE exampledb to dbuser;例如：postgres=# grant all privileges on database registry to devuser;postgres=# grant all privileges on database registry to prouser; 删除数据库12DROP DATABASE chado删除名为 chado 的数据库 登录数据库添加了新用户和新数据库后，以新用户的身份登陆数据库。 1psql -U dbuser -d exampledb -h 127.0.0.1 -p 5432 注意： psql命令存在简写形式，如果当前的Linux系统用户存在于postgreSQL中，则可以省略用户名，不需要使用-U参数，只需要其他参数。 PostgreSQL内部还存在与当前系统用户同名的数据库，则连数据库名都可以省略。 数据导入导出数据导出完整的语法格式： 1pg_dump --host hostname --port port --username username -t tablename -d dbname &gt;/home/jihite/table.sql 一般情况下，我们的命令会是： 123pg_dump -U devuser registry &gt; registry.sql pg_dump -U prouser registry &gt; registry.sql 数据导入完整的语法格式： 1psql -h host_ip -p 5432 -d dbname -U postgres -W postgres -f 2.sql 一般情况下，我们的命令会是： 1234psql exampledb &lt; exampledb.sqlpsql -d registry -f registry.sql devuserpsql -d registry -f registry.sql prouser create user prouser with password ‘Prouser123’; postgreSQL命令pg常用命令： \h：查看SQL命令的详细解释，例如 \h select ?：查看psql命令列表 \l：列出所有数据库 \c [database_name]：连接其他数据库 \d 或者\dt：列出数据库的所有表 \du：列出所有数据库用户 \conninfo：列出连接 查看表的所有数据：select * from table_name; 查看数据库数据量（占用存储空间）情况： 1select pg_size_pretty(pg_database_size(&apos;registry&apos;)); 查看表的数据量大小(注意，首先需要\c切换数据库下再执行)： 123select pg_size_pretty(pg_relation_size(&apos;test&apos;));orselect pg_size_pretty(pg_table_size(&apos;test&apos;)); 12345678910111213141516171819202122232425262728registry=# \dt List of relations Schema | Name | Type | Owner--------+-------------------------------+-------+--------- public | access | table | prouser public | access_log | table | prouser public | admin_job | table | prouser public | alembic_version | table | prouser public | clair_vuln_timestamp | table | prouser public | harbor_label | table | prouser public | harbor_resource_label | table | prouser public | harbor_user | table | prouser public | img_scan_job | table | prouser public | img_scan_overview | table | prouser public | job_log | table | prouser public | project | table | prouser public | project_member | table | prouser public | project_metadata | table | prouser public | properties | table | prouser public | replication_immediate_trigger | table | prouser public | replication_job | table | prouser public | replication_policy | table | prouser public | replication_target | table | prouser public | repository | table | prouser public | role | table | prouser public | schema_migrations | table | prouser public | user_group | table | prouser(23 rows) pg免密操作参考链接：http://www.oradbca.com/529.html?imrgvm=a1ndo2 在远程备份或者登录时总要手工输入密码，导致效率很低。 postgresql可以通过密码文件来实现”无钥验证“。 在用户的根目录下，需要创建一个.pgpass文件，并将权限设置为0600，就可以实现了。 文件的格式如下： 1hostname:port:database:username:password 演示： 1.psql登入 12345678910[postgres@oradbca ~]$ cd[postgres@oradbca ~]$ touch .pgpass[postgres@oradbca ~]$ vi .pgpass 192.168.1.11:5432:postgres:postgres:postgres[postgres@oradbca ~]$ chmod 600 .pgpass [postgres@oradbca ~]$ psql -h 192.168.1.11 -p 5432 -d postgres -U postgrespsql (9.3.4)Type &quot;help&quot; for help.postgres=# 2.pg_dump备份 1pg_dump -d postgres -h 192.168.1.11 -p 5532 -U postgres &gt;back.dmp 3.pg_basebackup同步 在使用pg_basebackup时需要在.pgpass中增加一行 replication类型的 192.168.1.11:5432:replication:repuser:repuser 1234567[postgres@oradbca ~]$ pg_basebackup -D /u01/pgdata -Fp -Xs -v -P -h 192.168.1.11 -p 5432 -U repusertransaction log start point: 0/1A000028 on timeline 1pg_basebackup: starting background WAL receiver37929/37929 kB (100%), 2/2 tablespaces transaction log end point: 0/1A0000F0pg_basebackup: waiting for background process to finish streaming ...pg_basebackup: base backup completed]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>数据库</category>
        <category>PostgreSQL</category>
      </categories>
      <tags>
        <tag>PostgreSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NFS网络存储]]></title>
    <url>%2F2019%2F03%2F25%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E5%8F%8A%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%2F%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%2FNFS%2FNFS%E7%BD%91%E7%BB%9C%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[基础知识NFS（Network File System）即网络文件系统，是FreeBSD支持的文件系统中的一种，它允许网络中的计算机之间通过TCP/IP网络共享资源。在NFS的应用中，本地NFS的客户端应用可以透明地读写位于远端NFS服务器上的文件，就像访问本地文件一样。 NFS与Samba服务类似，但一般Samba服务常用于办公局域网共享，而NFS常用于互联网中小型网站集群架构后端的数据共享。 NFS客户端将NFS服务端设置好的共享目录挂载到本地某个挂载点，对于客户端来说，共享的资源就相当于在本地的目录下。 系统架构NFS在传输数据时使用的端口是随机选择的，依赖RPC服务来与外部通信，要想正常使用NFS,就必须保证RPC正常。 RPCRPC（Remote Procedure Call Protocol）远程过程调用协议。它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。 在NFS服务端和NFS客户端之间，RPC服务扮演一个中介角色，NFS客户端通过RPC服务得知NFS服务端使用的端口，从而双方可以进行数据通信。 流程当NFS服务端启动服务时会随机取用若干端口，并主动向RPC服务注册取用相关端口及功能信息，这样，RPC服务就知道NFS每个端口对应的的NFS功能了，然后RPC服务使用固定的111端口来监听NFS客户端提交的请求，并将正确的NFS端口信息回复给请求的NFS客户端。这样，NFS客户就可以与NFS服务端进行数据传输了。 部署服务端部署安装nfs 与 rpc 相关软件包1# yum -y install nfs-utils rpcbind 注意：在centos7下其实只需要安装nfs-utils即可，因为rpcbind 属于它的依赖，yum会自动安装上。 根据官网说明 Chapter 8. Network File System (NFS) - Red Hat Customer Portal，CentOS 7.4 以后，支持 NFS v4.2 不需要 rpcbind 了，但是如果客户端只支持 NFC v3 则需要 rpcbind 这个服务。 配置配置说明NFS默认的配置文件是 ：/etc/exports 配置格式为： 123NFS共享目录绝对路径 NFS客户端1地址范围（参数）NFS客户端2地址范围（参数）.....注意：客户端IP范围配置中，* 代表所有，即没有限制。 常用参数： rw read-write 读写 ro read-only 只读 sync 请求或写入数据时，数据同步写入到NFS server的硬盘后才返回。数据安全，但性能降低了 async 优先将数据保存到内存，硬盘有空档时再写入硬盘，效率更高，但可能造成数据丢失。 root_squash 当NFS 客户端使用root 用户访问时，映射为NFS 服务端的匿名用户。NFS为了安全考虑，默认会将root账户降权为普通匿名账户。所以，如果不进行配置的话，这个将会是默认值 no_root_squash 当NFS 客户端使用root 用户访问时，映射为NFS 服务端的root 用户 all_squash 不论NFS 客户端使用任何帐户，均映射为NFS 服务端的匿名用户 配置案例修改配置文件 12# vim /etc/exports/sharedir 192.168.0.0/16(rw,sync,root_squash) 创建共享目录以及测试文件： 123mkdir -p /sharedirtouch /sharedir/Welcom.fileecho &quot;Welcome to onlylink.top&quot; &gt;/sharedir/Welcom.file 给共享目录添加权限： 1chown -R nfsnobody.nfsnobody /sharedir/ 把NFS共享目录赋予 NFS默认用户nfsnobody用户和用户组权限，如不设置，会导致NFS客户端无法在挂载好的共享目录中写入数据 启动启动 rpc服务并设置成开机自启动： 12# /etc/init.d/rpcbind start# chkconfig rpcbind on 启动 nfs服务并设置成开机自启动： 12# /etc/init.d/nfs start# chkconfig nfs on 在centos 7下的操作： 12345678910111213141516# systemctl enable rpcbind# systemctl enable nfs# systemctl start rpcbind# systemctl start nfs如果开启了防火墙，还需要进行设置# firewall-cmd --zone=public --permanent --add-service=rpc-bindsuccess# firewall-cmd --zone=public --permanent --add-service=mountdsuccess# firewall-cmd --zone=public --permanent --add-service=nfssuccess# firewall-cmd --reloadsuccess 客户端部署安装nfs 与 rpc 相关软件包1# yum -y install nfs-utils rpcbind 注意：在centos7下其实只需要安装nfs-utils即可，因为rpcbind 属于它的依赖，yum会自动安装上。 配置启动设置 rpcbind 服务的开机启动 1# systemctl enable rpcbind 启动 NFS 服务 1# systemctl start rpcbind 注意： 客户端不需要打开防火墙，因为客户端时发出请求方，网络能连接到服务端即可。 客户端也不需要开启 NFS 服务，因为不共享目录。 客户端挂载服务端客户端连接服务端其实就是正常的挂载操作，只不过对象从本地变成了远端 先查服务端的共享目录 123# showmount -e 192.168.0.101Export list for 192.168.0.101:/data 192.168.0.0/24 在客户端创建目录 1# mkdir /data 挂载 1# mount -t nfs 192.168.0.101:/data /data 当然，可以写到fstab中，设置为开机自启动。 1234# cat /etc/fstab在文件末尾添加一下内容192.168.0.101:/data /data nfs defaults 0 0 常用命令 showmount -e 服务器的IP地址 #查看服务端的的共享配置 mount -t nfs IP地址:/目录 /目录 # 挂载 例如：mount 192.168.4.5:/common /common exportfs -a：表示全部挂载或者全部卸载 -r：表示重新挂载 -u：表示卸载某一个目录 -v：表示显示共享目录 其他配置案例案例1共享/common目录，192.168.0.0网络的主机均可以只读访问 123#vim /etc/exports/common 192.168.0.0/24(ro) 案例2192.168.0.1可以读写的方式访问/abc,192.168.0.2可以只读的方式访问/abc 1/abc 192.168.0.1(rw) 192.168.0.2(ro) 案例3任何人均可以只读的形式访问/dvd 1/dvd *(ro) 案例4-root权限管理客户端使用root登录系统，访问服务器的NFS，则会以root身份访问NFS共享，如果客户端系统使用tom登录，访问服务器的NFS，则会以tom身份访问NFS共享。 实现客户端可读写的方式： 修改目录本身的权限（exports已经设置好了rw） 12chmod 777 目录名称 或者其他值，但是需要读写 仅让root可以写，则需要修改exports,让NFS不对root进行降权 1234#vim /etc/exports/abc *(rw,no_root_squash)#service nfs restart 客户端使用root登录系统后，cd到NFS共享目录，则可以获得root权限 案例5-触发挂载在客户端实现触发挂载NFS服务器共享的/usr/src目录到本地/data/nfsdir 安装软件包 12# rpm -q autofs# yum -y install autofs 修改主配置文件 12345# grep mnt /etc/auto.master /data /etc/auto.data# cat /etc/auto.datanfsdir2 -fstype=nfs,rw 192.168.10.10:/usr/src 启动服务 12# service autofs restart# chkconfig autofs on]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识及网络服务</category>
        <category>网络服务</category>
        <category>NFS</category>
      </categories>
      <tags>
        <tag>NFS网络存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos6的iptables及Centos7的firewalld配置]]></title>
    <url>%2F2019%2F03%2F15%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E8%BF%90%E7%BB%B4%E5%AE%89%E5%85%A8%2F%E9%98%B2%E7%81%AB%E5%A2%99%2FCentos7%E7%9A%84firewalld%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Centos6的iptables配置基础知识案例-SNAT配置假如在NAT机器上，想要将192.168.1.0/24网段的数据包的源地址修改为公网的ip58.20.51.66，通过网卡eth1送出，可以： 1iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -o eth1 -j SNAT --to-source 58.20.51.66 Centos7的firewalld配置参考文献： CentOS 上的 FirewallD 简明指南 基础知识FirewallD 是 iptables 的前端控制器，用于实现持久的网络流量规则。它提供命令行和图形界面，在大多数 Linux 发行版的仓库中都有。与直接控制 iptables 相比，使用 FirewallD 有两个主要区别： FirewallD 使用区域和服务而不是链式规则。 它动态管理规则集，允许更新规则而不破坏现有会话和连接。 注意：FirewallD 是 iptables 的一个封装，可以让你更容易地管理 iptables 规则 - 它并不是 iptables 的替代品。虽然 iptables 命令仍可用于 FirewallD，但建议使用 FirewallD 时仅使用 FirewallD 命令。 启停查看相关命令1、 启动服务，并在系统引导时启动该服务： 12sudo systemctl start firewalldsudo systemctl enable firewalld 要停止并禁用： 12sudo systemctl stop firewalldsudo systemctl disable firewalld 2、 检查防火墙状态。输出应该是 running 或者 not running。 1sudo firewall-cmd --state 3、 要查看 FirewallD 守护进程的状态： 1sudo systemctl status firewalld 示例输出 1firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled) Active: active (running) since Wed 2015-09-02 18:03:22 UTC; 1min 12s ago Main PID: 11954 (firewalld) CGroup: /system.slice/firewalld.service └─11954 /usr/bin/python -Es /usr/sbin/firewalld --nofork --nopid 4、 重新加载 FirewallD 配置： 1sudo firewall-cmd --reload 配置firewalldFirewallD 使用 XML 进行配置。除非是非常特殊的配置，你不必处理它们，而应该使用 firewall-cmd。 配置文件位于两个目录中： /usr/lib/FirewallD 下保存默认配置，如默认区域和公用服务。 避免修改它们，因为每次 firewall 软件包更新时都会覆盖这些文件。 /etc/firewalld 下保存系统配置文件。 这些文件将覆盖默认配置。 配置集FirewallD 使用两个配置集：“运行时”和“持久”。 在系统重新启动或重新启动 FirewallD 时，不会保留运行时的配置更改，而对持久配置集的更改不会应用于正在运行的系统。 小总结：2种配置方式，当前的实时生效和永久生效，实时是直接写入到内存中，永久是写入到配置文件当中 默认情况下，firewall-cmd 命令适用于运行时配置，但使用 --permanent 标志将保存到持久配置中。要添加和激活持久性规则，你可以使用两种方法之一。 1、 将规则同时添加到持久规则集和运行时规则集中。  1sudo firewall-cmd --zone=public --add-service=http --permanentsudo firewall-cmd --zone=public --add-service=http 2、 将规则添加到持久规则集中并重新加载 FirewallD。  1sudo firewall-cmd --zone=public --add-service=http --permanentsudo firewall-cmd --reload 特别注意：reload 命令会删除所有运行时配置并应用永久配置。因为 firewalld 动态管理规则集，所以它不会破坏现有的连接和会话。 防火墙的区域“区域”是针对给定位置或场景（例如家庭、公共、受信任等）可能具有的各种信任级别的预构建规则集。不同的区域允许不同的网络服务和入站流量类型，而拒绝其他任何流量。 首次启用 FirewallD 后，public 将是默认区域。 区域也可以用于不同的网络接口。例如，要分离内部网络和互联网的接口，你可以在 internal 区域上允许 DHCP，但在external 区域仅允许 HTTP 和 SSH。未明确设置为特定区域的任何接口将添加到默认区域。 要找到默认区域：  1sudo firewall-cmd --get-default-zone 要修改默认区域： 1sudo firewall-cmd --set-default-zone=internal 要查看你网络接口使用的区域： 1sudo firewall-cmd --get-active-zones 示例输出： 1public interfaces: eth0 要得到特定区域的所有配置： 1sudo firewall-cmd --zone=public --list-all 示例输出： 1public (default, active) interfaces: ens160 sources: services: dhcpv6-client http ssh ports: 12345/tcp masquerade: no forward-ports: icmp-blocks: rich rules: 要得到所有区域的配置：  1sudo firewall-cmd --list-all-zones 示例输出： 1block interfaces: sources: services: ports: masquerade: no forward-ports: icmp-blocks: rich rules: ...work interfaces: sources: services: dhcpv6-client ipp-client ssh ports: masquerade: no forward-ports: icmp-blocks: rich rules: 与服务一起使用FirewallD 可以根据特定网络服务的预定义规则来允许相关流量。你可以创建自己的自定义系统规则，并将它们添加到任何区域。 默认支持的服务的配置文件位于 /usr/lib /firewalld/services，用户创建的服务文件在 /etc/firewalld/services 中。 要查看默认的可用服务： 1sudo firewall-cmd --get-services 比如，要启用或禁用 HTTP 服务：  12sudo firewall-cmd --zone=public --add-service=http --permanentsudo firewall-cmd --zone=public --remove-service=http --permanent 与端口一起使用比如：允许或者禁用 12345 端口的 TCP 流量。 12sudo firewall-cmd --zone=public --add-port=12345/tcp --permanentsudo firewall-cmd --zone=public --remove-port=12345/tcp --permanent 端口转发下面是在同一台服务器上将 80 端口的流量转发到 12345 端口。 1sudo firewall-cmd --zone=&quot;public&quot; --add-forward-port=port=80:proto=tcp:toport=12345 要将端口转发到另外一台服务器上： 1、 在需要的区域中激活 masquerade。 1sudo firewall-cmd --zone=public --add-masquerade 2、 添加转发规则。例子中是将本地的 80 端口的流量转发到 IP 地址为 ：123.456.78.9 的远程服务器上的 8080 端口。 1sudo firewall-cmd --zone=&quot;public&quot; --add-forward-port=port=80:proto=tcp:toport=8080:toaddr=123.456.78.9 要删除规则，用 --remove 替换 --add。比如： 1sudo firewall-cmd --zone=public --remove-masquerade 用 FirewallD 构建规则集例如，以下是如何使用 FirewallD 为你的服务器配置基本规则（如果您正在运行 web 服务器）。 将 eth0 的默认区域设置为 dmz。 在所提供的默认区域中，dmz（非军事区）是最适合于这个程序的，因为它只允许 SSH 和 ICMP。 1sudo firewall-cmd --set-default-zone=dmzsudo firewall-cmd --zone=dmz --add-interface=eth0 2、 把 HTTP 和 HTTPS 添加永久的服务规则到 dmz 区域中： 1sudo firewall-cmd --zone=dmz --add-service=http --permanentsudo firewall-cmd --zone=dmz --add-service=https --permanent  3、 重新加载 FirewallD 让规则立即生效： 1sudo firewall-cmd --reload  如果你运行 firewall-cmd --zone=dmz --list-all， 会有下面的输出： 1dmz (default) interfaces: eth0 sources: services: http https ssh ports: masquerade: no forward-ports: icmp-blocks: rich rules:  这告诉我们，dmz 区域是我们的默认区域，它被用于 eth0 接口中所有网络的源地址和端口。 允许传入 HTTP（端口 80）、HTTPS（端口 443）和 SSH（端口 22）的流量，并且由于没有 IP 版本控制的限制，这些适用于 IPv4 和 IPv6。 不允许IP 伪装以及端口转发。 我们没有 ICMP 块，所以 ICMP 流量是完全允许的。没有丰富Rich规则，允许所有出站流量。 高级配置服务和端口适用于基本配置，但对于高级情景可能会限制较多。 Rich规则和Direct接口允许你为任何端口、协议、地址和操作向任何区域 添加完全自定义的防火墙规则。 rich规则rich规则的语法有很多，但都完整地记录在 firewalld.richlanguage(5) 的手册页中（或在终端中 man firewalld.richlanguage）。 使用 --add-rich-rule、--list-rich-rules 、 --remove-rich-rule 和 firewall-cmd 命令来管理它们。 这里有一些常见的例子： 允许来自主机 192.168.0.14 的所有 IPv4 流量。 1sudo firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=192.168.0.14 accept&apos; 拒绝来自主机 192.168.1.10 到 22 端口的 IPv4 的 TCP 流量。 1sudo firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.1.10&quot; port port=22 protocol=tcp reject&apos; 允许来自主机 10.1.0.3 到 80 端口的 IPv4 的 TCP 流量，并将流量转发到 6532 端口上。  1sudo firewall-cmd --zone=public --add-rich-rule &apos;rule family=ipv4 source address=10.1.0.3 forward-port port=80 protocol=tcp to-port=6532&apos; 将主机 172.31.4.2 上 80 端口的 IPv4 流量转发到 8080 端口（需要在区域上激活 masquerade）。 1sudo firewall-cmd --zone=public --add-rich-rule &apos;rule family=ipv4 forward-port port=80 protocol=tcp to-port=8080 to-addr=172.31.4.2&apos; 列出你目前的丰富规则： 1sudo firewall-cmd --list-rich-rules 实例案例IDC机房机器，有以下需求： ssh只允许内网网段访问，拒绝所有其他网段 开放指定的端口（8080、9000）给所有网段 注意，因为使用的是public这个zone，所有在配置都配置完成之后，需要将默认的ssh服务remove掉 具体的命令如下： 添加内网网段 123456789101112131415即时生效# firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=172.24.0.0/16 accept&apos;#firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=10.11.0.0/16 accept&apos;#firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=10.11.10.0/24 accept&apos;永久生效# firewall-cmd --permanent --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=172.24.0.0/16 accept&apos;#firewall-cmd --permanent --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=10.11.0.0/16 accept&apos;#firewall-cmd --permanent --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=10.10.10.0/24 accept&apos; 开放指定端口 123456789101112131415161718192021222324252627282930313233343536即时生效# firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=60.191.68.43 port port=8080 protocol=tcp accept&apos;# firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=60.191.68.43 port port=9000 protocol=tcp accept&apos;# firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=101.68.94.0/29 port port=8080 protocol=tcp accept&apos;# firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=101.68.94.0/29 port port=9000 protocol=tcp accept&apos;# firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=183.129.221.128/29 port port=8080 protocol=tcp accept&apos;# firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=183.129.221.128/29 port port=9000 protocol=tcp accept&apos;# firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=122.224.251.144/29 port port=8080 protocol=tcp accept&apos;# firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=122.224.251.144/29 port port=9000 protocol=tcp accept&apos;永久生效# firewall-cmd --permanent --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=60.191.68.43 port port=8080 protocol=tcp accept&apos;# firewall-cmd --permanent --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=60.191.68.43 port port=9000 protocol=tcp accept&apos;# firewall-cmd --permanent --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=101.68.94.0/29 port port=8080 protocol=tcp accept&apos;# firewall-cmd --permanent --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=101.68.94.0/29 port port=9000 protocol=tcp accept&apos;# firewall-cmd --permanent --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=183.129.221.128/29 port port=8080 protocol=tcp accept&apos;# firewall-cmd --permanent --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=183.129.221.128/29 port port=9000 protocol=tcp accept&apos;# firewall-cmd --permanent --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=122.224.251.144/29 port port=8080 protocol=tcp accept&apos;# firewall-cmd --permanent --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=122.224.251.144/29 port port=9000 protocol=tcp accept&apos; 删除ssh服务 123# firewall-cmd --zone=public --remove-service=ssh # firewall-cmd --zone=public --remove-service=ssh --permanent 123# firewall-cmd --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=101.68.94.0/29 port port=443 protocol=tcp accept&apos;# firewall-cmd --permanent --zone=public --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=101.68.94.0/29 port port=443 protocol=tcp accept&apos;]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维安全</category>
        <category>防火墙</category>
      </categories>
      <tags>
        <tag>firewalld</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos升级openssh]]></title>
    <url>%2F2019%2F02%2F19%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FLinux%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%2Fcentos%E5%8D%87%E7%BA%A7openssh%2F</url>
    <content type="text"><![CDATA[主要对象分为2个版本，一个是centos6.8，一个是centos7.3和7.4. 升级原因7.4以下openssh版本存在严重漏洞： 1.OpenSSH 远程权限提升漏洞(CVE-2016-10010)2.OpenSSH J-PAKE授权问题漏洞(CVE-2010-4478)3.Openssh MaxAuthTries限制绕过漏洞(CVE-2015-5600)OpenSSL&gt;=1.0.1可以不用升级OpenSSL 当前版本6.8 123456[root@app018-dev.novalocal ~]# cat /etc/redhat-releaseCentOS release 6.8 (Final)[root@app018-dev.novalocal ~]# rpm -qa | grep opensshopenssh-clients-5.3p1-118.1.el6_8.x86_64openssh-5.3p1-118.1.el6_8.x86_64openssh-server-5.3p1-118.1.el6_8.x86_64 7 1de 可以看到，当前的版本是： centos6.8：5.3p1 centos7：7.4p1 目标版本要求的目标版本是7.5及以上 在当前时间节点（2019年02月19日10:09:56），最高版本为：OpenSSH 7.9/7.9p1 (2018-10-19) 在这里，我们选择升级到最新版本 整个的升级操作是server和client都要升级 实际操作依赖关系1# yum -y install gcc pam-devel zlib-devel openssl openssl-devel wget telnet-server* telnet 安装及配置telnet安装 1# yum -y install telnet-server* telnet 配置 12345678# vi /etc/xinetd.d/telnet 将其中disable字段的yes改为no以启用telnet服务 # mv /etc/securetty /etc/securetty.old #允许root用户通过telnet登录 # service xinetd start #启动telnet服务 # chkconfig xinetd on #使telnet服务开机启动，避免升级过程中服务器意外重启后无法远程登录系统 在centos7下的配置 123456# mv /etc/securetty /etc/securetty.old # systemctl start telnet.socket# systemctl enable telnet.socket 测试 测试telnet能否正常登入系统 1[root@common001-dev.novalocal ~]# telnet 192.168.11.27 安装ssh下载软件包 1# wget https://cdn.openbsd.org/pub/OpenBSD/OpenSSH/portable/openssh-7.9p1.tar.gz 解压安装 12345# tar zxvf openssh-7.4p1.tar.gz# cd openssh-7.4p1# ./configure# make# make install 我们使用默认的安装，不加指定路径，在安装完毕之后sshd将会安装到/usr/local/sbin/下。而ssh、ssh-keygen等都会安装到/usr/local/bin目录下。 而操作系统的PATH路径是优先选择/usr/local/的，所以普通命令都可以使用到最新的，但是server端的sshd我们还需要做额外的配置 修改sshd启动脚本 将sshd启动脚本中的sshd命令路径修改为指定版本的路径： centos6下就一个启动脚本文件的内容需要替换 /etc/init.d/sshd # vim /etc/init.d/sshd 1234567将KEYGEN=/usr/bin/ssh-keygenSSHD=/usr/sbin/sshd修改成为：KEYGEN=/usr/local/bin/ssh-keygenSSHD=/usr/local/sbin/sshd centos7下有一下几个文件的内容需要替换 /usr/lib/systemd/system/sshd-keygen.service 12345将ExecStart=/usr/sbin/sshd-keygen替换为：ExecStart=/usr/local/bin/sshd-keygen /usr/lib/systemd/system/sshd.service 12345将ExecStart=/usr/sbin/sshd $OPTIONS替换为：ExecStart=/usr/local/sbin/sshd $OPTIONS /usr/lib/systemd/system/sshd@.service 12345将ExecStart=-/usr/sbin/sshd -i $OPTIONS替换为：ExecStart=-/usr/local/sbin/sshd -i $OPTIONS 重启服务注意事项防火墙问题有防火墙的要添加一条23端口的记录]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>Linux系统管理</category>
      </categories>
      <tags>
        <tag>centos升级openssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis集群]]></title>
    <url>%2F2019%2F02%2F16%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FRedis%2FRedis%E9%9B%86%E7%BE%A4%2Fredis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[基础知识数据分布常见的数据分布方式有：哈希分区和顺序分区。Redis集群使用的是哈希分析，因此这里主要关注哈希分区。 Hash（哈希）也叫散列，指的是一类特征，也就是把任意长度的输入（又叫做预映射pre-image）通过散列算法变换成固定长度的输出，该输出就是散列值或者叫hash值。 因此只要是符合这个特征的方式（因为会存在很多不同的算法实现）都可以叫做哈希方式。 常见的哈希分区主要有以下几种（Redis集群使用的是第三种）： 节点取余。使用特定的值，例如Redis的key名称或者指定id，然后根据节点数量计算出哈希值（hash(key)%N）。但是这种方式在节点数量变化时，映射关系会变化，会涉及到数据牵引。 一致性哈希。可以参考这篇文章：https://www.jianshu.com/p/e8fb89bb3a61 虚拟槽分区（优化后的一致性哈希）。在一个哈希环之后，定义出16364个节点，然后将这些物理的节点动态的分配给集群节点，让每一个节点负责一定数量的slot。在数据写入的时候，hash（key）–&gt;[0,16383]，实际的计算公式为：slot=CRC16(key)&amp;16383。 一个slot实际上一个物理的存储节点，由于这些slot是不会发生变化的（在默认的一致性哈希中数量会发生变化，由此会对数据产生影响），所有也就保证了集群数据的可靠性和平衡性。 Redis集群使用虚拟槽分区的特点： 解耦数据和节点之间的关系，简化了扩容和缩容的难度。因此实际存储的slot数量是不变的，变化的只是分配关系。 每一个集群节点本身去维护节点和slot的映射关系，不需要客户端或者代理服务去维护这个映射关系。 支持集群节点、slot、key等之间的映射关系查询，通过获取这些信息，可以用于数据路由、在线伸缩等场景。 Redis集群中的每一个节点会分配若干个slot。 作为分布式解决方案，需要为每一个集群节点定义一个固定的id，也就是上面说的哈希值，外层哈希计算之后然后进行分布式的存储。 集群功能的限制Redis集群相对于单实例模式，在功能上会有一些限制 不支持链式复制。也就是从节点只能复制主节点，不能复制从节点。 key的批量操作，只支持在同一个slot中的key。 key的事务操作，只支持在同一个slot中，当多个key分布在不同槽时无法使用事务功能。 只支持db0 集群创建集群的创建过程也比较简单，只需要3个步骤： 准备节点 节点握手，组成集群 为每个节点分配slot槽 创建主从关系 集群节点创建在配置文件中开启：cluster-enabled yes即可让节点运行在集群模式之下。 建议为所有的集群节点规划统一的配置。 12345678910[root@common007-dev.novalocal conf]# cat redis-cluster-6379.conf # 以下只显示部分核心的配置daemonize yesdir &quot;/home/cachecloud/data&quot;port 6379protected-mode nobind 0.0.0.0cluster-enabled yescluster-node-timeout 15000cluster-config-file &quot;nodes-6379.conf&quot; 关于集群配置文件集群模式的Redis除了原有的配置文件之外，又加了一份集群配置文件。当集群内节点信息发生变化，例如添加节点、节点下线、故障转移等时，节点会自动保存这些信息到集群配置文件。 需要注意的是： Redis实例会自动维护集群配置文件，不要手动修改。防止节点重启时产生集群信息错乱。 集群配置文件会存储在指定的dir路径下。也就是说和aof、rdb等数据文件再同一个路径下。 集群节点握手节点握手是指一批运行在集群模式下的节点通过gossip协议彼此通信，感知对方的过程。 使用命令： 1127.0.0.1:6379&gt; cluster meet 127.0.0.1 6380 注意：我们只需要在其中一个节点执行cluster meet命令即可，握手状态会通过消息在集群内进行传播，最终所有的节点都会发现其他节点并发起握手流程。最后我们执行cluster nodes命令去查看验证效果即可。 所有集群节点都握手建立连接之后，此时集群处于下线状态。 为集群节点分配slotRedis集群把所有数据都映射到16384个slot当中，每个key都会映射到一个固定的slot当中，只有当节点分配了slot，才能响应和这些slot关联的key命令。 因为在交互模式下，不能批量的输入slot的范围，因此使用这种非交互的方式进行添加slot 12[cachecloud@common007-dev.novalocal logs]$ redis-cli -p 7000 CLUSTER ADDSLOTS &#123;0..16383&#125;OK 集群的从节点可以看到，和sentinel等方式不一样，节点的主从关系我们没有在配置文件当中使用slaveof进行指定，在当前状态下，每一个集群的节点都是master。 一般情况下我们会对半，一半为master，一半为slave，因此在上面分配slot时，如果节点总数为6，那么将16384个slot进行三等分，分配之后，还剩下3个节点时为空，这个时候，我们执行以下命令。 1127.0.0.1:6380&gt; cluster replicate 集群node的id redis-trib.rb方式创建使用这种方式首先需要解决ruby的相关问题，默认使用yum安装的版本太低，无法正确安装Redis ruby的官方网站为：http://www.rvm.io/ 整个的操作过程为： 第1步：解决ruby问题 12345678910111213141516171819202122232425# gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB下载rvm# curl -sSL https://get.rvm.io | bash -s stable查找配置文件# find / -name rvm.sh配置文件生效# source /etc/profile.d/rvm.sh 下载rvm依赖# rvm requirements 查看rvm库ruby版本# rvm list known安装ruby指定版本# rvm install ruby-2.4.1使用ruby版本默认# rvm use 2.4.1 default安装Redis# gem install redis 可能还需要：yum install -y rubygems 第2步：启动集群节点 第3步：创建Redis集群 接下来我们创建Redis集群 12使用 --replicas 1 创建 每个master带一个 slave 指令# ./redis-trib.rb create --replicas 1 192.168.11.84:7000 192.168.11.84:7001 192.168.11.84:7002 192.168.11.11:7000 192.168.11.11:7001 192.168.11.11:7002 如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[cachecloud@common007-dev.novalocal src]$ ./redis-trib.rb create --replicas 1 192.168.11.84:7000 192.168.11.84:7001 192.168.11.84:7002 192.168.11.11:7000 192.168.11.11:7001 192.168.11.11:7002&gt;&gt;&gt; Creating cluster&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:192.168.11.84:7000192.168.11.11:7000192.168.11.84:7001Adding replica 192.168.11.11:7001 to 192.168.11.84:7000Adding replica 192.168.11.84:7002 to 192.168.11.11:7000Adding replica 192.168.11.11:7002 to 192.168.11.84:7001M: 2fcb0a92c0b055e9e0c0ec7a279a1c33e400b92c 192.168.11.84:7000 slots:0-5460 (5461 slots) masterM: 94d79d6303e6de4745ec4f1dd61d51d61f73919e 192.168.11.84:7001 slots:10923-16383 (5461 slots) masterS: 1c6db7f5c2998db943cb26bf9716d13a367034b9 192.168.11.84:7002 replicates 1073c42cefa1e192ff219e554c843cbdc1eabd80M: 1073c42cefa1e192ff219e554c843cbdc1eabd80 192.168.11.11:7000 slots:5461-10922 (5462 slots) masterS: 8c911465ef9118912ab848f6a9e5790027b0d4fa 192.168.11.11:7001 replicates 2fcb0a92c0b055e9e0c0ec7a279a1c33e400b92cS: 39f885d55462cd7fc4e975317da9db19abcc1835 192.168.11.11:7002 replicates 94d79d6303e6de4745ec4f1dd61d51d61f73919eCan I set the above configuration? (type &apos;yes&apos; to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join....&gt;&gt;&gt; Performing Cluster Check (using node 192.168.11.84:7000)M: 2fcb0a92c0b055e9e0c0ec7a279a1c33e400b92c 192.168.11.84:7000 slots:0-5460 (5461 slots) master 1 additional replica(s)S: 8c911465ef9118912ab848f6a9e5790027b0d4fa 192.168.11.11:7001 slots: (0 slots) slave replicates 2fcb0a92c0b055e9e0c0ec7a279a1c33e400b92cM: 94d79d6303e6de4745ec4f1dd61d51d61f73919e 192.168.11.84:7001 slots:10923-16383 (5461 slots) master 1 additional replica(s)M: 1073c42cefa1e192ff219e554c843cbdc1eabd80 192.168.11.11:7000 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 1c6db7f5c2998db943cb26bf9716d13a367034b9 192.168.11.84:7002 slots: (0 slots) slave replicates 1073c42cefa1e192ff219e554c843cbdc1eabd80S: 39f885d55462cd7fc4e975317da9db19abcc1835 192.168.11.11:7002 slots: (0 slots) slave replicates 94d79d6303e6de4745ec4f1dd61d51d61f73919e[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 192.168.11.84:7000:2048192.168.11.11:7001:2048192.168.11.11:7000:2048192.168.11.84:7002:2048192.168.11.84:7001:2048192.168.11.11:7002:2048 12345678./redis-trib.rb create --replicas 1 10.11.4.1:6526 10.11.4.2:6526 10.11.4.3:6526 10.11.4.4:6526 10.11.4.5:6526 10.11.4.6:6526 ./redis-trib.rb create --replicas 1 10.11.4.1:6527 10.11.4.2:6527 10.11.4.3:6527 10.11.4.4:6527 10.11.4.5:6527 10.11.4.6:6527 ./redis-trib.rb create --replicas 1 10.11.4.1:6528 10.11.4.2:6528 10.11.4.3:6528 10.11.4.4:6528 10.11.4.5:6528 10.11.4.6:6528 ./redis-trib.rb create --replicas 1 10.11.4.1:6529 10.11.4.2:6529 10.11.4.3:6529 10.11.4.4:6529 10.11.4.5:6529 10.11.4.6:6529 10.11.4.1:6526:8192 10.11.4.4:6526:8192 10.11.4.2:6526:8192 10.11.4.5:6526:8192 10.11.4.3:6526:8192 10.11.4.6:6526:8192 10.11.4.1:6529:819210.11.4.4:6529:819210.11.4.2:6529:819210.11.4.5:6529:819210.11.4.3:6529:819210.11.4.6:6529:8192 集群运维集群伸缩Redis集群可以实现对节点的灵活上下线控制，其中的原理可以抽象为槽和对应数据在不同节点之间的灵活移动。 也就是：集群伸缩=slot槽和数据在节点之间的移动 集群扩容整个集群的扩容操作可以分为以下步骤： 启动新节点 加入现有集群 迁移slot和数据 新集群节点创建和之前的配置一样，在配置文件中设置cluster-enabled为yes，然后启动即可。 加入现有集群其实也就是一个meet操作 在随便一个集群节点当中执行 1127.0.0.1:6379&gt; cluster meet ip port 即可将该节点加入现有集群当中，注意，这里应该是加入了2个节点（另一个后续会作为从节点） 1. 152 multi-message-gw-service rider-station-base-service 221 red-packet-unit-service 105 rider-elastic-probe 153 rich-unit-service 220 order-rule-unit-service candidate-select-service dispatch-alg-config dispatch-bywaydegree-provider dispatch-config-dispatch-mode dispatch-consistent-service dispatch-fairy-provider dispatch-pathplan-rider dispatch-pathplan-service dispatch-revise-params dispatch-timer-task grab-order-service lbs-provider lbs-rider-position-service lbs-rider-space-service order-diagnose-service package-order-service parameter-service recommend-config-provider rider-gateway-current-limit rider-invert-select system-dispatch /usr/bin/nohup /usr/local/jdk/bin/java -Dproject.name=${name} -Dlogging.file.path=”/home/${start_user}/deploy/logs/${name}” -Dmonitor.file.path=”/home/${start_user}/deploy/logs/monitor” -Daction.file.path=”/home/${start_user}/deploy/logs/action/“ -Dbigdata.file.path=”/home/${start_user}/deploy/logs/bigdata/“ -Ddisconf.env=${disconf_env} -Ddisconf.enable.remote.conf=true -Ddisconf.conf_server_host=${disconf_url} &lt;#if zone?? &amp;&amp; zone != “”&gt; -Dspring.cloud.config.label=${region}.${zone} -Dspring.cloud.config.index=${region}.${zone} -Dzone=${zone} &lt;/#if&gt; &lt;#if region?? &amp;&amp; region != “”&gt; -Dregion=${region} &lt;/#if&gt; &lt;#if service_chain??&gt; -javaagent:/home/${start_user}/deploy/tools/service-chain-1.0-RELEASE.jar -Xbootclasspath/a:/home/${start_user}/deploy/tools/transmittable-thread-local-2.5.1.jar -javaagent:/home/${start_user}/deploy/tools/transmittable-thread-local-2.5.1.jar -Dservice-chain=${service_chain} &lt;/#if&gt; -Dlogging.console.level=off -server &lt;#if config_prefix == “production”&gt; -Xmx4g -Xms4g &lt;#else&gt; -Xms${xms}m -Xmx${xmx}m &lt;/#if&gt; -XX:NewRatio=1 -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=512m -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+CMSScavengeBeforeRemark -XX:+ParallelRefProcEnabled -XX:+CMSParallelInitialMarkEnabled -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+ExplicitGCInvokesConcurrent -XX:+AlwaysPreTouch -server -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=”/home/${start_user}/deploy/logs/${name}/“ -XX:-UseBiasedLocking -XX:AutoBoxCacheMax=20000 -XX:+UseCondCardMark -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintHeapAtGC -XX:+PrintJNIGCStalls -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintPromotionFailure -XX:+PrintCommandLineFlags -XX:-OmitStackTraceInFastThrow -Xloggc:/dev/shm/${name}-gc.log -XX:ErrorFile=/home/${start_user}/deploy/logs/${name}/hserr%p.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=20M -Djava.security.egd=file:/dev/./urandom -Dlog4j.shutdownHookEnabled=false -jar ${name}.jar –spring.profiles.active=${config_prefix} &gt; &lt;#if config_prefix == “production” || config_prefix == “dwd-pre”&gt; /dev/null &lt;#else&gt; /home/${start_user}/deploy/logs/${name}/${name}-start.log &lt;/#if&gt; 2&gt;&amp;1 &amp; -server -Xmx4g -Xms4g -XX:NewRatio=1 -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=512m -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+CMSScavengeBeforeRemark -XX:+ParallelRefProcEnabled -XX:+CMSParallelInitialMarkEnabled -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+ExplicitGCInvokesConcurrent -XX:+AlwaysPreTouch -server -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=”/home/appdeploy/deploy/logs/scan-order-service/“ -XX:-UseBiasedLocking -XX:AutoBoxCacheMax=20000 -XX:+UseCondCardMark -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintHeapAtGC -XX:+PrintJNIGCStalls -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintPromotionFailure -XX:+PrintCommandLineFlags -XX:-OmitStackTraceInFastThrow -Xloggc:/dev/shm/scan-order-service-gc.log -XX:ErrorFile=/home/appdeploy/deploy/logs/scan-order-service/hserr%p.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=20M -Djava.security.egd=file:/dev/./urandom -Dlog4j.shutdownHookEnabled=false -jar scan-order-service.jar –spring.profiles.active=production &gt; /dev/null 2&gt;&amp;1 &amp; 上海的多活环境，jvm内存1.5G。没问题的话，下面这个模板就作为你们所有应用部署上海机房的默认启动脚本了，有问题跟我说下。 #!/bin/bashmkdir -p /home/${start_user}/deploy/logs/${name}cd /home/${start_user}/deploy/apps/${name}/usr/bin/nohup /usr/local/jdk/bin/java -Dproject.name=${name} -verbose:gc -Xloggc:/home/${start_user}/deploy/logs/${name}/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/${start_user}/deploy/logs/${name}/java.hprof -XX:ErrorFile=/home/${start_user}/deploy/logs/${name}/java_error.log -Dlogging.file.path=”/home/${start_user}/deploy/logs/${name}” &lt;#if zone?? &amp;&amp; zone != “”&gt; -Dspring.cloud.config.label=${region}.${zone} -Dspring.application.index=${region}.${zone} -Dlogging.console.level=off -Dzone=${zone} &lt;/#if&gt; &lt;#if region?? &amp;&amp; region != “”&gt; -Dregion=${region} &lt;/#if&gt; &lt;#if service_chain??&gt; -javaagent:/home/${start_user}/deploy/tools/service-chain-1.0-RELEASE.jar -Xbootclasspath/a:/home/${start_user}/deploy/tools/transmittable-thread-local-2.5.1.jar -javaagent:/home/${start_user}/deploy/tools/transmittable-thread-local-2.5.1.jar -Dservice-chain=${service_chain} &lt;/#if&gt; -Dmonitor.file.path=/home/${start_user}/deploy/logs/monitor -Daction.file.path=/home/${start_user}/deploy/logs/action/ -Dbigdata.file.path=/home/${start_user}/deploy/logs/bigdata/ -Ddisconf.env=${disconf_env} -Ddisconf.enable.remote.conf=true -Ddisconf.conf_server_host=${disconf_url} -server -Xms${xms}m -Xmx${xmx}m -XX:MaxNewSize=${max_new_size}m -XX:ThreadStackSize=${thread_stack_size} -jar ${name}.jar –spring.profiles.active=${config_prefix} &gt;/home/${start_user}/deploy/logs/${name}/${name}-start.log 2&gt;&amp;1 &amp; #!/bin/bashmkdir -p /home/${start_user}/deploy/logs/${name}cd /home/${start_user}/deploy/apps/${name}/usr/bin/nohup /usr/local/jdk/bin/java -Dproject.name=${name} -Dlogging.file.path=”/home/${start_user}/deploy/logs/${name}” &lt;#if zone?? &amp;&amp; zone != “”&gt; -Dspring.cloud.config.label=${region}.${zone} -Dspring.application.index=${region}.${zone} -Dspring.application.name=fortune-unit-service-primary -Dlogging.console.level=off -Dzone=${zone} &lt;/#if&gt; &lt;#if region?? &amp;&amp; region != “”&gt; -Dregion=${region} &lt;/#if&gt; &lt;#if service_chain??&gt; -javaagent:/home/${start_user}/deploy/tools/service-chain-1.0-RELEASE.jar -Xbootclasspath/a:/home/${start_user}/deploy/tools/transmittable-thread-local-2.5.1.jar -javaagent:/home/${start_user}/deploy/tools/transmittable-thread-local-2.5.1.jar -Dservice-chain=${service_chain} &lt;/#if&gt; -server -Xms3000m -Xmx3000m -XX:MaxNewSize=${max_new_size}m -XX:ThreadStackSize=${thread_stack_size} -jar ${name}.jar –spring.profiles.active=${config_prefix} &gt; &lt;#if config_prefix == “production” || config_prefix == “dwd-pre” || config_prefix == “production-sh”&gt; /dev/null &lt;#else&gt; /home/${start_user}/deploy/logs/${name}/${name}-start.log &lt;/#if&gt; 2&gt;&amp;1 &amp; grant all on zentaopro.* to ‘dev’@’192.168.%’ identified by ‘zentao123’;]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>数据库</category>
        <category>Redis</category>
        <category>Redis集群</category>
      </categories>
      <tags>
        <tag>redis集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[语言学基础]]></title>
    <url>%2F2019%2F01%2F26%2F%E4%B8%AA%E4%BA%BA%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%2F%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%8F%8A%E8%B7%AF%E7%BA%BF%2F%E8%AF%AD%E8%A8%80%E5%AD%A6%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"></content>
      <categories>
        <category>个人知识体系</category>
        <category>英语学习</category>
        <category>英语学习方法及路线</category>
      </categories>
      <tags>
        <tag>语言学基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rsync用法]]></title>
    <url>%2F2019%2F01%2F24%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E5%8F%8A%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%2F%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%2Frsync%2Frsync%2F</url>
    <content type="text"><![CDATA[基础知识rsync作为一个数据同步的工具，可通过LAN/WAN快速同步多台主机间的文件。 主要解决的核心问题是：使两端的数据保持同步 可以分为2个部分： 全量传输（也可以理解为全量备份） 差异化传输（只传输有变化的部分内容） 语法及6种命令格式抽象后的语法表达式为： 1rsync [option] src dest 在实际使用中，源和目的地的组合有一下几种方式： 123456789Usage: rsync [OPTION]... SRC [SRC]... DEST or rsync [OPTION]... SRC [SRC]... [USER@]HOST:DEST or rsync [OPTION]... SRC [SRC]... [USER@]HOST::DEST or rsync [OPTION]... SRC [SRC]... rsync://[USER@]HOST[:PORT]/DEST or rsync [OPTION]... [USER@]HOST:SRC [DEST] or rsync [OPTION]... [USER@]HOST::SRC [DEST] or rsync [OPTION]... rsync://[USER@]HOST[:PORT]/SRC [DEST]The &apos;:&apos; usages connect via remote shell, while &apos;::&apos; &amp; &apos;rsync://&apos; usages connectto an rsync daemon, and require SRC or DEST to start with a module name. :表示连接的是远端的shell。而::或者rsync://表示连接的是远端的rsync守护进程。 以上这几种命令格式对应的是不同的工作模式和场景，我们把它提取精简出来就是： 本地文件之间的传输。src dest 将本地同步到远端。src user@host:dest 将本地文件同步到远端的默认配置rsync服务端。src user@host::dest 将本地文件同步到远端的指定配置的rsync服务端。src rsync://user@host:port/dest 将远端机器上的数据同步传输到本地。user@host:src local_dest 将远端rsync服务器(默认配置)上的数据同步传输到本地。user@host::src local_dest 将远端rsync服务器(指定配置)上的数据同步传输到本地。rsync://user@host:por/src local_dest 一共7种方式。 6种命令格式对总结目的或者说要解决的问题rsync作为一个数据同步的工具，可通过LAN/WAN快速同步多台主机间的文件。 主要解决的核心问题是：使两端的数据保持同步 可以分为2个部分： 全量传输（也可以理解为全量备份） 差异化传输（只传输有变化的部分内容） 核心观点和内容目的和核心内容是怎么连接的整体的实现逻辑启发点和疑问点抽象化应用场景Redis数据备份这个我已经把核心的sentinel这些实现好了，cpu、内存、流量、sentinel日志这些可能会出现的报警情况都已经思考确认优化过了。 刚才11点的时候做了一次测试，没有问题，待会我把时间点换成凌晨。 cluster部分的数据备份和恢复稍微有点不一样，我先把上海机房Redis都创建之后再来做下这个。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识及网络服务</category>
        <category>网络服务</category>
        <category>rsync</category>
      </categories>
      <tags>
        <tag>rsync</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django-定时任务]]></title>
    <url>%2F2019%2F01%2F13%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%2FPython%2Fdjango%2Fdjango-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[在编写django项目的时候，很多时候会有定时任务的需求。 安装配置步骤1：安装django-crontab库1pip install django-crontab 再在settings.py中添加app: 1234INSTALLED_APPS = ( ... &apos;django_crontab&apos;, ) 步骤2：创建定时任务在app内新建py文件，文件名称随意。 例如我们在名为bind_ops的app下新建了一个bind_crontab.py文件。 文件内容： 1234567from lib.logger import loggerimport datetimetime = datetime.datetime.now()time = str(time)def crontab_test(): print (111) logger.info(&quot;bind crontab&quot; +time ) 因为要看定时任务的效果，所以采用了直接输出和记录到文件的形式 经过测试发现：当定时任务在执行时，如果你只是输出一些语句，那么你将看不到任何内容，所以请不要怀疑这个定时任务没有执行。 然后在 settings.py中增加： 1234567# 最简单配置CRONJOBS = [ # 表示每天2：01执行 (&apos;01 2 * * *&apos;, &apos;bind_ops.bind_crontab.crontab_test&apos;)]# 参数及字段说明： 第一个参数（表示时间），前5个字段分别表示： 分钟：0-59 小时：1-23 日期：1-31 月份：1-12 星期：0-6（0表示周日） 一些特殊符号： *： 表示任何时刻 ,： 表示分割 -：表示一个段，如在第二段里： 1-5，就表示1到5点 /n : 表示每个n的单位执行一次，如第二段里，*/1, 就表示每隔1个小时执行一次命令。也可以写成1-23/1. 第二个参数（表示路径）： 格式：app名称.文件名.函数名 如果想生成日志，那就再加一个字符串类型的参数：’&gt;&gt; path/name.log’， path路径，name文件名。’&gt;&gt;’表示追加写入，’&gt;’表示覆盖写入。 提示： 如果你有多个定时任务，以逗号隔开，都放入CORNJOBS的列表中即可。 路径必须写绝对路径，写相对路径是不识别的。 步骤3：启动任务以上都完成后，需要执行以下命令将任务添加并生效 1python manage.py crontab add 显示当前的定时任务 1python manage.py crontab show 删除所有定时任务 1python manage.py crontab remove 重启django服务执行（可能不需要，因为并没有用，也正常使用了。） 1python manage.py corntab -e 实际案例dname项目-bind定时获取主机名需求说明需求：bind服务需要提供解析主机名的功能 拆分： 从数据源中获取主机名的对应信息并写入zone数据文件-这里的数据源是cmdb 定期检测删除的主机 定期更新有ip变化的主机记录 代码实现]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>编程开发</category>
        <category>Python</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>django-定时任务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sed命令]]></title>
    <url>%2F2019%2F01%2F07%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FLinux%E5%91%BD%E4%BB%A4%2Fsed%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[基础知识sed全称stream editor，流编辑器，用程序的方式来编辑文本，它是文本处理中常用的工具，能够完美的配合正则表达式使用。 sed处理文件的具体过程如下： 首先sed把当前正在处理的行保存在一个临时缓存区中（也称为模式空间），然后处理临时缓冲区中的行，完成后把该行发送到屏幕上。 sed每处理完一行就将其从临时缓冲区删除，然后将下一行读入，进行处理和显示。 处理完输入文件的最后一行后，sed便结束运行。 sed把每一行都存在临时缓冲区中，对这个副本进行编辑，所以不会修改原文件。 sed语法sed的操作过程包括： 定位。定址用于决定对哪些行进行编辑。地址的形式可以是数字、正则表达式、或二者的结合。如果没有指定地址，sed将处理输入文件的所有行。 地址是一个数字，则表示行号；是“$”符号，则表示最后一行。例如： 1`sed -n &apos;3p&apos; datafile只打印第三行` 只显示指定行范围的文件内容，例如： # 只查看文件的第100行到第200行 1sed -n &apos;100,200p&apos; mysql_slow_query.log 地址是逗号分隔的，那么需要处理的地址是这两行之间的范围（包括这两行在内）。范围可以用数字、正则表达式、或二者的组合表示。例如： 1`sed &apos;2,5d&apos; datafile#删除第二到第五行sed &apos;/My/,/You/d&apos; datafile#删除包含&quot;My&quot;的行到包含&quot;You&quot;的行之间的行sed &apos;/My/,10d&apos; datafile#删除包含&quot;My&quot;的行到第十行的内容` sed实际案例sed -n ‘/;[ ]{1,5}serial/s/[0-9]{1,10}/&amp;+1/;s/;[ ]{1,5}serial[ ][0-9]{1,10}/; serial date+1/;s/[0-9]{1,10}$/no+1/p’ dianwoba.com.zone sed -n ‘/;[ ]{1,5}serial/s/[0-9]{1,10}$/no+1/p’ dianwoba.com.zone sed -n ‘/;[ ]{1,5}serial/s#[0-9]{1,10}#&amp;+1#;/;[ ]{1,5}serial/s#;[ ]{1,5}serial[ ][0-9]{1,10}#; serial date+1#;/;[ ]{1,5}serial/s#[0-9]{1,10}$#no+1#p’ dianwoba.com.zone sed -n ‘/;[ ]{1,5}serial/s/[0-9]{1,10}/&amp;+1/;/;[ ]{1,5}serial/s/;[ ]{1,5}serial[ ][0-9]{1,10}/; serial date+1/;/;[ ]{1,5}serial/s/[0-9]{1,10}$/no+1/p’ dianwoba.com.zone sed -n ‘3s/[0-9]{1,10}/&amp;+1/;3s/;[ ]{1,5}serial[ ][0-9]{1,10}/; serial date+1/;3s/[0-9]{1,10}$/no+1/p’ dianwoba.com.zone sed -n ‘3s/[0-9]{1,10}/&amp;+1/;3s/;[ ]{1,5}serial[ ][0-9]{1,10}/; serial date/;3s/$/no+1/p’ dianwoba.com.zone 高级知识]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>Linux命令</category>
      </categories>
      <tags>
        <tag>sed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[思维模型]]></title>
    <url>%2F2019%2F01%2F04%2F%E4%B8%AA%E4%BA%BA%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F%E8%AE%A4%E7%9F%A5%E5%8D%87%E7%BA%A7%2F%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%2F%E6%80%9D%E7%BB%B4%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[基础知识什么是思维模型查理芒格曾说：每个人都需要掌握至少100个思维模型，才能解决生活中的80%、90%的问题。 那么，思维模型究竟是什么？以及思维模型究竟怎么样在生活中帮助我们解决问题？ 查理芒格本身说：思维模式是重要学科的重要原理，通常是那些不证自明的基础规律 我的个人理解：思维模型是高级思维方式的底层框架抽象，是一个系统框架 学习思维模型，关键不在于具体的技巧和答案，而在于思考的过程。 思维模型的分层思维模型其实是分层次的 从上到下适用范围越来越基础，也就是说适用范围越来越广 最上层：解决具体问题的思维模型。比如FFC、SWOT法则、冰山模型等等 中间层：某一学科的原理或者规律。比如精益创业、边际效应、价值投资理念等等，解决一个学科中问题的思维模型 最下层：最底层的抽象的跨学科的原理和规律。比如哲学上的递弱代偿、物理学的机械论、物理学的熵增定律，生物学的进化论等等 思维模型越往上就越有用，工具属性就越强，解决某个具体场景的问题 思维模型越往下就越无用，距离具体问题越远，解释事情的属性更强 思维模型的作用思维模型主要有2个作用： 帮助我们提高做决策的质量和正确率 生活是一个又一个决定的延续，每个选择做的好与不好对于我们的影响是巨大的。可是我们很少仔细地思考，我们究竟是怎么做决策的，做决策的过程是什么样的。具体看补充知识 提升创新力 我们的创新力往往来自于那些无用的思维模型。 熊彼特：创新是生产要素的重组，关键在于我们要懂看世界的方式，我们只有掌握了更多理解理解世界的方式，更多解释世界问题的方式，才能够打破我们过去经验世界里各要素的关系，用新的视角来将各个要素进行重新组合，进而产生创新….. 如何发现和学习思维模型？查理芒格在《穷查理宝典》中只写了十几个思维模型，那么还有那么多的思维模型我们去哪里学习呢？ 其实在我们的日常生活中，我们可以发现很多很好的思维模型，只不过我们没有方法、没有意识去发现这些思维模型。 那么，我们如何在生活中发现思维模型呢？ 方法1：启发、问题、思路、模型在生活当中，经常会遇到一些让你觉得有启发的事情。大部分人往往不会去深究背后到底在解决什么问题？ 所以，不是碰到问题，再去发现思维模型，而是碰到让你有启发的事情的时候，就可以去思考思维模型。 分析： 解决了什么问题 思路是什么，为什么有效，有哪些基本的要素，本质是什么 把思路抽象提炼出模型，思考这个思路所调用的更底层的思维模型 要想实现这个，需要我们做相应的改变： 心态的转变。需要有好奇心，主动的去问，为什么这个现象的背后有这么一个答案，为什么这个答案起作用 重点的转变。过去在学习时，时间都主要花在：记笔记、看书、记答案。可是真正重要的地方是将时间花在思考的过程。把时间花在硬问题，打通思维的过程。 方法的转变。顺着答案不断的和我们过去已有的经验结合。重要的不是答案，重要的是知识之间的联系。 如何运用思维模型?我们学习了思维模型之后，在我们日常生活中应该怎么去应用呢？ 运用思维模型，关键点在于：多层次、多视角 多层次：把一个问题，使用不同层次的思维模型进行分析。这里的层次可以理解为学科，不同的学科解释问题的层次 多视角：在同一个层次当中，用不同的角度来分析问题 欧内斯特-内格尔：各学科就是一系列解释的层次。 补充知识我们是如何做决策的？我们在外部刺激和做出反应之间，并不是简单的刺激行为模式，而是一个ABC模式。 A: Activatiing event 诱发刺激 B: beliefs 信念反应，决策依据 C: consequences 行动结果 对于同一个刺激A，由于背后的信念B不同，我们会做出不同的行为决策C。我把它理解是一种流式模式，信息源输入一个系统框架（决策依据）中，然后进行运算，得出一个输出结果。 每个人由于过往的经历、经验、家庭等等因素的不同，会有不同的信念B，而信念往往是很难改变的，因为这个形成的过程往往是自己不自知的，并且容易成为潜意识。 我们经常说的认知升级，其实本质就是面对同一个问题，你有不同的处理方式，也就是你的中间信念反应，决策依据发生了改变。 所以，我们成长进步的过程实际上是用更好的决策依据来替换自己之前因为有限知识或经验得出的那些不太可靠的决策依据。 这个新的，好的决策依据就是我们所说的思维模型。也就是说，我们把这个决策的模型进行了修改，修改之后的实现顺序是：刺激–&gt;思维模型–&gt;行动结果 总结：我们要把中间的这个黑箱（决策依据）变成我们可以调控的白箱 多元思维模型对于一个系统而言，结构远远大于单个要素本身，系统的性质是取决于要素的结构，而不是要素本身。 就行C原子，它可以排列成钻石，也可以是石墨，这不是取决于碳元素本身的原因，而是它相互之间关系形成的系统。 同样的，就算你把查理芒格的思维模型全都学会了，但是你不知道在应用之间它们的关系，结构，权重，先后关系，哪些互相有冲突，你也成为不了查理芒格。最终的结果是你知识知道了很多的思维模型的一个someone。 如何更高效的学习思维模型学习思维模型，重要的不是那些思维模型本身，而不是那些知识点。而是理解我们运用思维模型的思路，它解决问题的过程，所以我们在记笔记的时候可以换一种方法。 我们记笔记的目的是：描绘出这个思维模型的一个全局框架，而不是零散的看起来非常用来但是不能建立连接的知识点，当问题进入这个框架，出去的时候就是满意的解决。 七星笔记法目的：事先人为的描述整体全局框架的关键点，在过程中，填充着7个关键点，填充完了之后，这个框架也就描绘出来了。也就是说，用你的学习的思维模型去学习其他思维模型 在学习做记笔记的时候，7个星星，也就是7个问题 上课或者学习的时候就是回答这7个问题，当这7个问题回答完了，那么笔记也记完了 这门课的目的是什么？也就是：为什么要学习它，它解决什么问题？ 这门课的核心观点和内容是什么？ 目的和这些核心观点是怎么连接的？ 老师或作者讲述的逻辑是什么？ 我在听课学习的过程中，有哪些疑问（说的对吗）和启发（为什么能这么思考）的地方？ 上面的这些疑问和启发背后的思维模型是什么？ 有哪些事情可以让我直接就可以开始找应用场景并采取行动？ 一些说明： 在学习之前，可以通过上网查资料等方式先了解下这门课，这个领域的核心框架是什么，之后在学习的时候去进行比较。 老师或者作者的逻辑和我自己理解的有什么差别，重要的不是知识点，重要的是为什么这么组织推演思考过程，为什么这么思考？ 做自己理解的思维导图就像上面我说的，要使用自己的框架去记忆这些知识点，如果只是单纯的这些知识点，那么将会是孤立的，没有办法建立连接。 收集启发点和疑问点你在碎片时间进行学习的时候，当碰到启发点和疑问点的时候，简单记录一下，当你有整块时间的时候再去深究它。 通过预想使用的场景去记忆这一点在学习英语等学科中尤为有效，你在记忆单词的时候，最好是在记忆句子的时候顺便记忆单词，如果只是单纯的记忆单词，效果根据不好。 并且在学习一些语法知识点的时候，最好也是通过一个句子来记忆。 同样的，在学习其他学科知识的时候，通过结果去记忆这些知识点。 思维模型FFC在于他们进行沟通交流的时候，可以使用FFC模型 F：feeling。描述感觉 F：fact。描述事实 C: compare。进行比较 常规赞美：你好漂亮、今天很漂亮。 优化：你的打扮让我眼前一亮，你今天的绿色衣服和围巾和项链特别搭配，简直是一股生机盎然的气息，你是今天嘉宾里面最亮眼的。]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>认知升级</category>
        <category>学习方法</category>
      </categories>
      <tags>
        <tag>思维模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识之网理论]]></title>
    <url>%2F2019%2F01%2F03%2F%E4%B8%AA%E4%BA%BA%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F%E8%AE%A4%E7%9F%A5%E5%8D%87%E7%BA%A7%2F%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%2F%E7%9F%A5%E8%AF%86%E4%B9%8B%E7%BD%91%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[概述我把一个人的知识看成是一张知识之网（可以类比为蜘蛛网），那么中心节点就是我这个人，由我这个中心发起的每一条边（实际上射线，在这里不严谨，这么用主要是便于直观理解），就是所有基础学科的知识 这里放上一张蜘蛛网的图 这个时候这张网只有边，过于分散，是无法捕捉到猎物的，因此需要使用一定的规则将所有边组合起来，对于蜘蛛网来说，我们直观的感受是一个个的同心圆。 同样的，对于我来说，我怎么把这些分散的知识有机的结合起来，形成为一个整体？我的方法就是使用思维模型，通过跨界思维，将所学的知识结合成为一个整体。 也就是说，我只把知识分为2类：基础学科的知识和思维模型。 这里有几个问题我需要说明下： 思维模型是什么？ 跨界思维是什么意思？ 为什么要构建这张知识之网，有了这张网之后又什么作用？ 第一个问题：思维模型是什么？思维模型可以理解为：思维方式的抽象。 生活是由一个个的选择构成的，那么是什么在背后指导我们做选择？我们决定是否买一个东西时需要考虑各种因素（价钱，必要性，时机等等），我们在思考这些因素的时候就是调用我们过往认知产生的思维模型，只不过我们不自知而已。我们在处理问题时，在做考试题目时，解题的思路和步骤更是思维方式的直接体现。 而思维模型是思维方式的抽象，很多时候我们拥有某一种思维的方式，但是自己却不清楚是怎么运作的，所以这个时候需要站在更高的维度，把它抽象出来，让它能为更多人学习使用。 第二个问题：跨界思维是什么意思？关于学习知识，我们需要拉大尺度，在大时间线上，在宏观上看待这件事。 在人类历史中，一开始根本就没有学科，早期的知识主要是为了生存（如果要分类的话可以理解为生存学科），后来解决了生存危机，有了火，有了农耕畜牧之后，慢慢的知识的总量得到的极大增长，人类开始探索自然，这一个阶段的学科其实就一个-自然科学，然后随着文明的进一步发展，才慢慢有了今天我们所看到的样子 说了这么多，其实就是想说：人类目前的所有科目其实都是后天分类出来的。这一点可能很多人往往会忽略。 每一个学科之间其实是相互影响的，一个学科的重要基础理论可能也同样适用于另一个学科，能够提高你在学习这个学科时的效率 所以，将所有的学科看成为一个整体，不要局限于某一个具体的学科，才是真正的有效学习。 第三个问题：知识之网的作用？当我们把思维+具体的知识编织成为一张网，这个时候我们可以把自己看做是等到猎物的蜘蛛，当有问题（蜘蛛的猎物）来的时候，只要落在这张网上面，我们就能找到方法消灭（解决） 如果有问题解决不了，这个时候我们就知道，我们需要更新我们的知识系统，加固或者扩大我们的知识之网。]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>认知升级</category>
        <category>学习方法</category>
      </categories>
      <tags>
        <tag>学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何输入和输出]]></title>
    <url>%2F2019%2F01%2F02%2F%E4%B8%AA%E4%BA%BA%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F%E8%AE%A4%E7%9F%A5%E5%8D%87%E7%BA%A7%2F%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%2F%E5%A6%82%E4%BD%95%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA%2F</url>
    <content type="text"><![CDATA[基础概念就我个人而言，整个学习过程分为以下3个步骤（注意是有顺序的）： 输入。信息来源 内化。最核心部分，将所学的知识、思维模型及专业技能存储能够被直接调用的资源池中，内化成直觉 输出。将思维模型或者专业的技能变成直觉的直接输出 虽然内化是最核心的部分，但是学习是一个完整的系统，输入和输出环境也影响着内化环节。 我针对我的情况画了一张学习整体结构图，仅供参考： 下面我们来介绍这三个方面 输入输入也就是信息（知识）的摄入，在日常生活中，我的信息输入源主要有： 书籍 视频 音频 网上获取的知识 生活中看到的事情得到的启发 自己深度思考得出的知识（也就是说知识的来源可以是本身） 交际活动获取的知识 这一部分包含的范围就比较广了，可以是与他人交流时获取的，也可以是参加线下或线上会议、论坛等获取的知识 我把通过这些输入源学到的知识主要分为以下2类： 思维模型。一个思维方式的系统框架。为下面所说的专业领域知识所服务，主要作用是不断优化改善分析和处理问题的逻辑，关于这部分的内容我有专门的文章《思维模型》。 专业领域的知识。比如计算机科学、英语、数学、历史、心理学、生命科学、经济学、哲学等等学科的知识 按道理来说，学到的知识应该可以分为很多个类别，为什么我只分了2类？ 这就涉及到个人的知识体系结构问题了，这一点在我的另一篇文章《知识之网理论》有概述。 因此最佳阅读顺序是：《思维模型》–&gt;《知识之网理论》–&gt;《如何输入和输出》。当然不按照这个顺序来也可以 内化-关于内化的一些前提知识这一章节是全文的重点和核心 整个内化的过程比较复杂和抽象，因此会花很长的篇幅来进行解释和记录。 内化是将所学到知识融会贯通的过程，这个过程根据个人的差异（主要是所采取的方法技巧（包括时间管理、一些提升效率的工具的使用等）的差异、思维方式的差异等），整个内化的时间和效果会有非常大的差异性 下面我要介绍的这些有关学习的基础知识，都是我个人实践过的或者正在实践的，在了解了这些基础知识之后，我们再来说说如何才能达到最佳的内化效果。 下面我们说一说这些有关内化的基础知识 大脑结构-神经元连接桑代克在《教育心理学》中说：学习就是联结，人之所以善于学习，主要是因为能够形成大量的联结。学习会使人成为异常复杂而精致的联结系统 一学习之所以有助于另一学习，是因为两种学习具有相同因素的原因 真正的高手会把模型与模型之间也建立连接，其实这一部分我主要在《知识之网理论》中说明，各个知识或者思维模型可以联结成为一个网站结构，多个网状结构可以形成知识的互联网，这一点其实也和当前宇宙的结构类似。 艾宾浩斯遗忘曲线遗忘曲线的背后实质原因其实还是因为神经元之间没有建立连接 烧开水理论为什么自学比培训好？主要的一个原因是神经元无法建立连接以及遗忘曲线的原因。 这一个部分其实在《知识之网理论》中能够找到答案，在培训班中，大量的知识输入大脑，在现有知识没有建立神经元连接的时候，学习新的知识，不断的恶性循环，就像是在沙子上盖楼，一层水泥还没有干，就开始网上买铺第二层水泥。 认知升级在问题来的时候，原本是走这个神经元通路，经过反复的训练，让大脑走另一个神经元网络通路，新的网络通路越来越强壮，旧的就会慢慢消失。 所谓的认知升级本质上其实是：建立新的神经网络链路，替换原有的旧的神经网络。修新路，绕旧路。 内化的5个阶段我们在学习新知识的时候，一般来说都会经历这几个阶段 事前想不到 事后诸葛亮 事中有启发。在中某些事的时候，还是用以前的思维的时候，突然会提醒自己，我应该用更好的方式。 事前会想起 自然而然。内化成为我们的直觉。 总结在这里我们对上面的这些知识进行一个总结 内化-如何达到最佳内化效果在这个部分，记录各种学习的方法和基本准则，帮助我们进行有效的内化，提升效率。 就像我在开头说的，虽然内化是最核心的部分，但是学习是一个完整的系统，输入和输出环境也影响着内化环节。 所以在这个部分，众多方法我会先讲输入和输出是怎么提升内化的 通用方法学习知识的顺序： 网上了解，了解粗略的框架 看视频，视频偏实际的应用，先实践，了解具体是怎么运作的 看书籍，通过书籍去深入 输入提升内化在这里我们主要以“读书”这个输入方式进行讲解。 我们在读书的时候，书和书是不一样的，书的类型不同，难度不同，因为所使用的方法也会有所不同。 下面简单介绍3个方法帮助大家提高阅读时吸收知识的效率。 熔断不读书法在利用碎片时间进行读书的时候，就可以使用这个方法，比如只有十几二十分钟的时候。 股市中的熔断机制：当股市下跌到某一个指标的时候，就强制不允许买卖交易了，暂停，熔断。 同样的，当你读书的时候，遇到有疑问或者有启发的地方，立马合上书，然后思考，这里为什么让我有启发，启发点在哪里，我可以把它用在生活中的哪些场景中，这个背后有什么基本原理吗。这个时候其实也就是我在《思维模型》中所说的6星学习法中的后3点。 使用这种方法，哪怕你只有20分钟的时间，你也可以在书中找到对你有帮助的启发点，我们读书并不需要全部看完，寻找到启发点远比看到毫无收益要好的多。 总结：每一个单点的知识，都可以让我们真正地去理解思考，通过很短的时间来大幅度提升效率。对于大多数的畅销书而言，使用这个方法去阅读就足够了。 这种方法的弊端：当你专注于某一个点，你就很容易看不到整个系统。而我们知道，对于一个系统而言，结构远大于单点的要素。 如果你遇到想深入阅读，理解作者思路和逻辑，看清整个系统的时候，这个时候就不能再使用这个方法。 关键词读书法一本书，无论它的组织思想是什么，它一定是用一些节点串联起来的，而这些节点往往是概念或者关键词的形式。 你把这些关键词提炼出来，然后寻找它们之间的关系，再结合这本书的主题和目的，梳理出它们之间的关系网络，就找到了作者的思路。 因为，我们核心要做的就是3件事： 作者在这本书中想要解决什么问题？ 关于这个问题，他提出了哪些核心观点（观点通常会以关键词的方式展示出来） 目的和关键词之间的关系是什么？（也就是：要解决的问题，是怎么和观点连接起来的，它们的联系） 如何获取核心观点或者说关键词，需要我们把整本书读完再去总结吗？ 我们其实可以用这个方法去提炼：封序目尾（封面，序言，目录，结尾） 换一个视角来看，其实阅读不仅仅是我们去读书，也就是说，不仅仅是我们一个人的事，还有作者和出版社，阅读，其实是3个元素构成的一个系统。就像人际沟通，其实也就是双方与信息三者构成的系统。 封面：出版社会想尽办法把这本书的重点、主要价值提炼出来。主要是为了更好的销售。 序言：一般情况在封面之后会有说明，提炼出在这本书中，作者最核心的观点。 目录：关键词列表，作者在写目录的时候，通常会那些关键的概念提炼出来 结尾： 任何一个作者，通常都会在开篇或者结尾的时候，把自己的书的特点、最关键的思想提炼出来。 所以，我们要找到一本书的关键词，通常情况下，根据不需要读完一本书，通过这4个步骤，通常就能获取。 经典书籍-里应外合读书法里：书里写的文字，阅读书籍的内容 外：书籍里没有写的内容部分 我们知道，我们写书籍的目的是，通过书籍这种语言文字符号去表达我们头脑的思想。 斯蒂芬-平克曾说：写作的难点在于要把网状的思考用树状的结构体现在线性展开的语句里。所以，写作实际上是作者思维层层压缩维度的一个过程。 因此，黑格尔会说：每当我用语言和符号去表达我的思想，我的思想就发生质变和肤浅化。 所以说，语言符号这种方式本身会带来一些限制问题，因此我们要精读那些最经典的书籍的时候，就需要注意到这种缺陷，因此，在阅读经典书籍的时候，使用里应外合的方法。 因此，在这种情况下，我们要知人论世，就是要研究作者的成长背景，他过去的经历对他有什么影响，哪些人的思想对他有很大的影响。他所处的时代，主流思想是什么。文化背景是什么，这些都是我们额外需要去做的功夫。 我们要把这本书放在时代、人物、文化等大的背景下理解，才能真正理解经典思想。 输出提升内化-输出倒逼输入以教为学把教别人当做一个学习的过程，你不懂哪个思维模型，你不会用哪一个，你就去教别人哪一个。 主要有3个点再驱动你： 压力。压力转化为动力。 发现自己的知识阻塞。在准备的过程中，你会发现一些原来自己以为懂但是实际上是不懂的地方。 通过别人的提问检验反馈。 具象抽象再具象我们所学习到的思维模型，一般情况下来说是已经具象化过的。 这个时候，我们需要把它进行抽象，抽象出一个系统框架，再填充上我们自己的内容，再重新具象化。 举例来说，我们在学习查理芒格的“复利”思维模型的时候，如果从表面上学习的话，很难被我们在日常生活中所运用，因为它是已经被查理芒格具象化的，我们所看到的并不是抽象层面的。 这个时候，我们把他说的这个思维模型进行抽象，可以看到“复利”的本质是： 做A会导致B，而B反过来又会强化A。所以我们在生活中看到的，凡是符合做A产生B，B又会强化A的都是“复利”这个思维模型的具象表现。 当我们知道了这个抽象的思维模型之后，我们可以看到，生活中的很多事情都可以用复利来进行解释。 例如，亚马逊的增长飞轮模式其实就是一个方法的复利的具象应用，是一个更大的综合系统下来思考。 所以，明白了复利的核心之后，我们就知道，我们需要使用长线思维去做一些事情，而不能短视。 刻意练习先把结论放出来：刻意练习=核心算法*大量练习。最终的结果是把思维模型和专业技能这些转换成为直觉。 把思维模型和专业技能转化成为直觉是需要大量的训练的。但是应该怎样去训练呢？不断的重复，遵循一万小时定律吗？ 但是我们每天要上班，各种事情，哪有那么多的时间去训练呢？ 就算我有时间，我应该怎么去训练这些思维模型和知识呢？ 《助推》：如果我们想要持续的产生一个行为，那么外部的环境影响是非常重要的。 也就是说：不同的环境会影响我们某个行为产生的频率。而不断的练习和反省训练思维模型和专业技能也需要这样一个助推的环节。你需要设计一个场景，能够不断的触发我们反省思考 一个非常重要的工具：《反思日记》。提高我们成长进步斜率的杠杆点 核心算法1-对标练习在日常生活中，经常碰到的一个问题是，学习了一个思维模型或者一些我们觉得非常有用的知识点之后，找不到使用的场景。也就是说，我们学的这些知识和我们是脱节的。 核心问题：不是学习的这个思维模型不起作用，而是我们看不到它如何起作用。 对标：把思维模型当做思考问题的标准。寻找可以用思维模型的场景。 核心算法2-举一反三学习了一个思维模型之后，至少在3个以上的不同场景中练习应用。 你必须找到不同的场景去运用同一个思维模型，如果找不到，说明还是不理解这个事情。 七星笔记法目的：事先人为的描述整体全局框架的关键点，在过程中，填充着7个关键点，填充完了之后，这个框架也就描绘出来了。也就是说，用你的学习的思维模型去学习其他思维模型 在学习做记笔记的时候，7个星星，也就是7个问题。上课或者学习的时候就是回答这7个问题，当这7个问题回答完了，那么笔记也记完了 这门课的目的是什么？也就是：为什么要学习它，它解决什么问题？ 这门课的核心观点和内容是什么？ 目的和这些核心观点是怎么连接的？ 老师或作者讲述的逻辑是什么？ 我在听课学习的过程中，有哪些疑问（说的对吗）和启发（为什么能这么思考）的地方？ 上面的这些疑问和启发背后的思维模型是什么？ 有哪些事情可以让我直接就可以开始找应用场景并采取行动？ 一些说明： 在学习之前，可以通过上网查资料等方式先了解下这门课，这个领域的核心框架是什么，之后在学习的时候去进行比较。 老师或者作者的逻辑和我自己理解的有什么差别，重要的不是知识点，重要的是为什么这么组织推演思考过程，为什么这么思考？ 逻辑思维star1:学习逻辑的目的通过学习逻辑思维的相关知识，提高我们在今后学习的时候，提取知识和梳理脉络的效率。最终目的是为了更好的构建思维模型。 star2:逻辑思维的核心观点和内容什么是逻辑-知识和思考方法知识：是问题的答案，比如人有一张嘴、2个耳朵；达尔文的进化论。 思考的方法：知识背后的。 举例来说：我们都学了达尔文的演化论，我们是要学习达尔文的演化论究竟说了什么东西吗？演化论里面，它的知识点有哪些吗？我们学习演化论其实是要琢磨这么一个事情：全人类都面对同一个大自然，但是有的人找出的是神造论，那么，为什么达尔文就偏偏能够解读出演化论这件事情，他是怎么找出这个视角、怎么从这个视角推论出道理、怎么证明这个道理。视角–&gt;道理–&gt;证明。这个系列就是达尔文的思考方法。 逻辑就是：从找到角度进而论述道理，进而证明道理，这整条的脉络，运算机制就是逻辑。 总结一下，逻辑思维是分析思考事物的方法，帮我们找出知识的脉络。 为什么要学逻辑？逻辑是每一个人先天就有的，我们常常会误认为不学习逻辑就没有逻辑。 我们身处其中却又不知自的逻辑通常有2个： 逻辑1：归纳法。同一件事情，它的因果链条在不同的情况中，高频率的重复出现，我们自然而然的就会把它归纳起来，认为这就是一个道理。此蘑菇有毒。这个道理的整个过程是不需要进行推理的，只需要记忆就可以。 逻辑2：演绎推理法。例如祈祷这个事情，前提：有病是因为我冒犯了神灵。过程：向神灵沟通，就能够让他宽恕我的罪过。因此结论就是：求神能治病。这个就是演绎法（演绎推理法）的雏形。 补充：演绎推理是严格的逻辑推理，一般表现为大前提、小前提、结论的三段论模式：即从两个反映客观世界对象的联系和关系的判断中得出新的判断的推理形式。如：“自然界一切物质都是可分的，基本粒子是自然界的物质，因此，基本粒子是可分的。”演绎推理的基本要求是：一是大、小前提的判断必须是真实的；二是推理过程必须符合正确的逻辑形式和规则。演绎推理的正确与否首先取决于大前提的正确与否，如果大前提错了，结论自然不会正确。 那么，既然逻辑是我们每一个人先天就有的，为什么我们还要学习逻辑呢？ 2个原因： 直观答案阻碍我们启动思考。只要我们对某一个疑问有了一个答案，那我们很自然的就会让我们的大脑停止思考。例如：学音乐的孩子比较乖。这个说法是不是有点因果倒置了？当我们开始进行推敲的时候，我们往往就能发现潜藏在背后的真正答案。 世界越来越复杂。很难直观的发现，我们现在的一个行为最终会造成什么样的后果。商场中，所有的决策，永远不会只有一个面的效果，所以我们需要让我们的视角更开阔，让判断更精密。 头脑中拥有非常多的逻辑知识，不代表你的逻辑能力强。逻辑其实是一个能力，而不是知识 所以，学逻辑应该说是：锻炼我们先天既有的逻辑能力，让它变得更精准 如何启动触发逻辑也就是如何运用 人的大脑是分为两个部分的： 理性思考。也就是逻辑思维。 感性直觉。人在碰到问题的时候，最直观的答案。 人的大脑总是：先感性直觉，再决定要不要启动理性思考。理性思考往往滞后于感性直觉。 思考工具：为什么、凭什么？前提是方向的确定性。 4个判断： 事实判断 功利判断 价值判断 审美判断。主要是感受 我们在进行思考的时候，往往会有多个判断同时浮现，这个时候方向的确定就显得非常重要。 提高逻辑的效率足够的信息-更多信息很多人认为，一个有逻辑的人，他找出的答案一定是正确的。这是不一定的 案例：例如四方形的面积等于长*宽。如果这个时候只是知道了长，那么是不能就算出面积的。 也就是说：逻辑思维要能给我们反馈一个准确的答案，还有一个重要的前提就是：我们注入的信息是足够丰富的。 也就是说表象要足够丰富，我们才能够进行抽象 封装-更多观念掌握更多封装的观念。一个就久经验证的道理，这个道理可以直接为我们所用，不用我们再去麻烦的推演的一套观念。 例如：勾股定理。供需关系。 找出更多元的答案当我们习惯用一个方法来解决问题之后，就会不由自主的在下一次碰到同类问题的时候会重复沿用。 但是每一个问题或多或少都是不一样的。人不可能在同一条河中出现两次。 工具：类比思维。这个事情在其他领域有没有类似性质可以参考。把其他领域的解法引用进来帮助我们解决问题。 总结信息—&gt;逻辑—&gt;本质—&gt;逻辑—&gt;方法。 star3:观点内容和目的是如何连接的？目的-内容： 了解逻辑思维。第一部分的知识和思考方法以及第二部分的为什么要学。说明了什么是逻辑思维，逻辑思维是分析思考事物的方法；以及学习逻辑思维对我们有什么影响，默认内置的2个逻辑思维不够不能满足实际的需求，因为直观答案阻碍我们并且当今世界越来越复杂。 掌握逻辑思维。首先确定思考的方向，然后使用工具：为什么？凭什么？ 更高效的使用逻辑思维。信息、可以直接调用的封装框架，多元角度。 star4:讲述的逻辑是什么样的？讲述的顺序： 简介学习的目的 介绍核心观点和内容(是什么、为什么、怎么做、如何提高) star5:有哪些启发点和疑问点启发点：主要是在为什么要学习逻辑这一个部分。介绍了我们已经被内置的2个逻辑，也就是归纳法和演绎推理法。但是这2个先天的逻辑在没有经过训练强化的情况下不能满足我们的使用需求，所以我们要做的事情其实上是强化锻炼我们的先天逻辑，让它变得更精准。而不是所谓的创建逻辑思维，它本身就是存在的。 star6:这些启发点和疑问点背后的本质和思维模型是什么？我的理解：其实逻辑还是算是思维模型体系中的一个部分，并不是独立存在的。逻辑其实是我们做决策的一个依据，也就是处理一个事情的脉络，也就是抽象后的单个或多个思维模型的组合。 而逻辑能力是发现和从构建思维模型的能力。 star7:有哪些我可以直接运用的应用场景？思维模型的抽象有一个前提，就是需要输入大量的具象的信息，有了大量的具象信息之后我们才能进行抽象，依靠唯心主义，凭空创造是不可取的。 总结：这个逻辑思维的内容部分，我学习完了之后其实是没有多少收获的，说的都是我已经知道的内容，但是在没学之前又生怕错过了什么。如果对你有启发，那就最好。 输出当经历了输入和内化之后，就只剩下输出了。 在很多人看到，输出很简单，通过文章或者讲解把自己的要表达的这些内容说抛出来就完成了。 但是，行百里者半九十！良好的输出和输入及内化同等重要！ 但是，行百里者半九十！良好的输出和输入及内化同等重要！ 但是，行百里者半九十！良好的输出和输入及内化同等重要！ 重要的事情说三遍！ 一般人在进行输出的时候，往往是采取：是什么？、为什么？、怎么做？这么3个问题来展开。这种方式有其一定的科学道理，但并不是任何场景都试用的，这些还是在表象层面，我们要做的，是真正的发掘更本质的东西。 因此，我个人思考得出的观点是在输出的时候，有没有想过一下这些问题： 说的对吗？你是否正确的把你要表达的内容输出出来了？是否达到了本次输出的目的？ 听/看完了吗？信息接受者有没有真的听或看完你的内容？为什么不能坚持？是输出的方式有问题还是输出结构还是什么问题？ 听/看懂了吗？信息接受者有没有理解你表达出来的内容。 是科学的吗？ 你的输出方式是不是合理科学的？有时间的控制还是任意发挥? 在阐述一些其他人没有接触过的新知识的时候，有没有更好的方法帮助他人吸收？ 输出介质（文章、图片、视频等）的组织结构是否清晰易读，内容是否重点突出（最终的目的是为了更好的理解），是否可以学习下写作技巧等科学的方法进行提升 下面我们进行拆分讲解。]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>认知升级</category>
        <category>学习方法</category>
      </categories>
      <tags>
        <tag>学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何自学计算机科学]]></title>
    <url>%2F2018%2F12%2F31%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FIT%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F%E5%A6%82%E4%BD%95%E8%87%AA%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%2F%E5%A6%82%E4%BD%95%E8%87%AA%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[引言]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>IT基础知识</category>
        <category>如何自学计算机科学</category>
      </categories>
      <tags>
        <tag>计算机科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IT技术学习网站及学习资料汇总]]></title>
    <url>%2F2018%2F12%2F31%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FIT%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F%E5%A6%82%E4%BD%95%E8%87%AA%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%2FIT%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99%E5%8F%8A%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[说明本文有2个目的： 记录同级目录下的另一篇文章：《如何自学计算机科学》中所提及的资料，便于检索查找。 记录在学习及工作中所涉及到的相关网站或者书籍资料等。同样的，也是为了便于检索查找。 文章内容的一些说明： 我会尽量说明资料的类型以及对应的作者。这一点也是非常重要的，IT领域经过这么多年的快速发展，资料重名已经是不可避免的了，所以有必要提及作者。 如果一些书籍存在多个译本，那么我会尽量说明哪一个译本的质量最高（因为是以我个人为主，所以仅供参考）。毕竟如果不能直接看英语的原版，那么对知识的吸收理解以及内化程度和翻译者的水平成正相关。高质量的译本和低质量的相差真的是非常之大 如果一些书我看过并且有做记录笔记的话，将会在我的另一个目录《个人知识体系》的读书笔记下 这篇文章的内容其实可以做成一个图形的导航界面，只是当前时间没有这个能力，后续我会实现这个的。 技术社区类网站纯IT技术类： csdn 51cto ChinaUnix技术论坛 博客园 开源中国 Linux运维部落 运维派 SegmentFault StackOverflow 其他的像知乎什么的，上面的也有非常多的IT方面的知识可以学习 工具类网站学习网站汇总网站汇总如下： 实验楼 还是比较有特色，提供的不是视频，而是配置好的虚拟机，注重实践，通过虚拟的实验环境，可以边看文档边动手操作 菜鸟教程 它以文档教程和查询参考手册为主。看文档理解稍微费劲点，但相比视频教程，可以节省很多时间。 慕课网 课程主要针对目前很热门的职业和内容，整体质量也不错 计蒜课 不是单纯的为求职快速上手而培训，偏向于真正培养IT技术人才 极客学院 我要自学网 比较久的一个网站，学习一些基础类的还可以 网易云课堂 腾讯课堂 网易公开课 W3School W3School 是因特网上最大的 WEB 开发者资源，主要针对web开发，重点在前端 i 春秋 主要是安全方面 可汗学院 麻省理工开放式课程计划(MIT OpenCourseWare | Free Online Course Materials) edX | Free online courses from the world’s best universities edX是麻省理工和哈佛大学于2012年4月联手创建的大规模开放在线课堂平台。它免费给大众提供大学教育水平的在线课堂。两所大学在这个非盈利性计划各资助三千万美元。2012年秋天，edX在MITx启动。 学习资料汇总科普计算机科学历史及基础知识科普 书籍《浪潮之巅》。作者吴军。这本真的是入门必读经典。 网络知识科普数据库技术nosql 书籍《Redis运维与开发》。搜狐视频团队出品，其还开源了cachecloud云平台。不错的一本好书。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>IT基础知识</category>
        <category>如何自学计算机科学</category>
      </categories>
      <tags>
        <tag>计算机科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[harbor镜像仓库实战]]></title>
    <url>%2F2018%2F12%2F18%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E8%99%9A%E6%8B%9F%E5%8C%96%2FDocker%2Bk8s%2Fharbor%2F</url>
    <content type="text"><![CDATA[参考文献 github主页 Harbor基础知识Architecture-体系结构 As depicted in the above diagram, Harbor comprises 6 components: Proxy: Components of Harbor, such as registry, UI and token services, are all behind a reversed proxy. The proxy forwards requests from browsers and Docker clients to various backend services. harbor使用代理结构，外部的客户端（浏览器或者docker client）访问调用是通过代理层去实现的 Registry: Responsible for storing Docker images and processing Docker push/pull commands. As Harbor needs to enforce access control to images, the Registry will direct clients to a token service to obtain a valid token for each pull or push request. 注册部分：响应操作docker镜像的请求 harbor为了确保安全性，客户端在调用的时候，需要取得valid token才可以进行操作 Core services: Harbor’s core functions, which mainly provides the following services: UI: a graphical user interface to help users manage images on the Registry Webhook: Webhook is a mechanism configured in the Registry so that image status changes in the Registry can be populated to the Webhook endpoint of Harbor. Harbor uses webhook to update logs, initiate replications, and some other functions. Token service: Responsible for issuing a token for every docker push/pull command according to a user’s role of a project. If there is no token in a request sent from a Docker client, the Registry will redirect the request to the token service. Database: Database stores the meta data of projects, users, roles, replication policies and images. 提供一个图形的用户入口，方便用户在注册钩子系统（registry webhook）中管理镜像。 Job services: used for image replication, local images can be replicated(synchronized) to other Harbor instances. Log collector: Responsible for collecting logs of other modules in a single place. 安装部署参考文献 官方安装手册 环境要求Harbor is deployed as several Docker containers, and, therefore, can be deployed on any Linux distribution that supports Docker. The target host requires Python, Docker, and Docker Compose to be installed. harbor是使用docker的方式部署的，整个harbor包含几个容器，因此能够部署在任何只要支持docker的Linux发行版本上。 目标主机只需要包含：python、docker、docker compose这3个东西即可 Hardware Resource Capacity Description CPU minimal 2 CPU 4 CPU is prefered Mem minimal 4GB 8GB is prefered Disk minimal 40GB 160GB is prefered Software Software Version Description Python version 2.7 or higher Note that you may have to install Python on Linux distributions (Gentoo, Arch) that do not come with a Python interpreter installed by default Docker engine version 1.10 or higher For installation instructions, please refer to: https://docs.docker.com/engine/installation/ Docker Compose version 1.6.0 or higher For installation instructions, please refer to: https://docs.docker.com/compose/install/ Openssl latest is prefered Generate certificate and keys for Harbor Network ports Port Protocol Description 443 HTTPS Harbor UI and API will accept requests on this port for https protocol 4443 HTTPS Connections to the Docker Content Trust service for Harbor, only needed when Notary is enabled 80 HTTP Harbor UI and API will accept requests on this port for http protocol 官方的安装步骤（The installation steps boil down to the following）： Download the installer; Configure harbor.cfg; Run install.sh to install and start Harbor; 下面我们开始实际操作 下载安装下载并解压所有软件包的下载地址 1# tar -xvf harbor-online-installer-&lt;version&gt;.tgz 当前时间节点我使用的是harbor.v1.4.0.tar.gz 安装docker-ce12345# sudo yum install -y yum-utils device-mapper-persistent-data lvm2# sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo# sudo yum -y install docker-ce 启动docker12# systemctl start docker# systemctl enable docker 安装Docker Compose12345# sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose# sudo chmod +x /usr/local/bin/docker-compose# docker-compose --version 配置配置概述Configuration parameters are located in the file harbor.cfg There are two categories of parameters in harbor.cfg, required parameters and optional parameters. required parameters: These parameters are required to be set in the configuration file. They will take effect if a user updates them in harbor.cfg and run the install.sh script to reinstall Harbor. optional parameters: These parameters are optional for updating, i.e. user can leave them as default and update them on Web UI after Harbor is started. If they are set in harbor.cfg, they only take effect in the first launch of Harbor. Subsequent update to these parameters in harbor.cfg will be ignored. Note: If you choose to set these parameters via the UI, be sure to do so right after Harbor is started. In particular, you must set the desired auth_mode before registering or creating any new users in Harbor. When there are users in the system (besides the default admin user), auth_mode cannot be changed. 配置文件中有2种配置内容，一种是必须配置的参数，一种是可选参数 必须参数：这是是在配置文件当中必须要存在的参数，当用户修改之后并且重新安装harbor之后生效 可选参数：这些参数是可以动态更新的，你能够在harbor启动之后，在web界面中进行更新。但是如果你把这些参数直接配置在harbor.cfg配置文件中（默认的配置不算人为配置了），它们只能在你第一次启动harbor的时候加载生效，之后再更新参数将不会生效。这一点需要尤为注意 注意1：你使用动态的可选参数方式的时候，首先要确认harbor已经启动 注意2：有一个特别需要注意的参数，当有用户（除了默认的管理员用户）已经存在于harbor中的时候，auth_mode这个参数是不能被修改的。所以你需要在有新的用户注册进harbor之前，设置好认证模式（auth_mode） 配置文件参数详解Required parameters-必需参数 hostname: The target host’s hostname, which is used to access the UI and the registry service. It should be the IP address or the fully qualified domain name (FQDN) of your target machine, e.g., 192.168.1.10 or reg.yourdomain.com. Do NOT use localhost or 127.0.0.1 for the hostname - the registry service needs to be accessible by external clients! hostname一般配置为域名,也就是整个harbor的入口 ui_url_protocol: (http or https. Default is http) The protocol used to access the UI and the token/notification service. If Notary is enabled, this parameter has to be https. By default, this is http. To set up the https protocol, refer to Configuring Harbor with HTTPS Access. 走HTT还是HTTPS db_password: The root password for the MySQL database used for db_auth. Change this password for any production use! max_job_workers: (default value is 10) The maximum number of replication workers in job service. For each image replication job, a worker synchronizes all tags of a repository to the remote destination. Increasing this number allows more concurrent replication jobs in the system. However, since each worker consumes a certain amount of network/CPU/IO resources, please carefully pick the value of this attribute based on the hardware resource of the host. 复制任务的进程数量，默认10个，这些进程，将这些镜像同步到远端的存储中 每个进程都需要消耗系统的资源，因此合理设置数量 customize_crt: (on or off. Default is on) When this attribute is on, the prepare script creates private key and root certificate for the generation/verification of the registry’s token. Set this attribute to off when the key and root certificate are supplied by external sources. Refer to Customize Key and Certificate of Harbor Token Service for more info. ssl_cert: The path of SSL certificate, it’s applied only when the protocol is set to https ssl_cert_key: The path of SSL key, it’s applied only when the protocol is set to https secretkey_path: The path of key for encrypt or decrypt the password of a remote registry in a replication policy. log_rotate_count: Log files are rotated log_rotate_count times before being removed. If count is 0, old versions are removed rather than rotated. log_rotate_size: Log files are rotated only if they grow bigger than log_rotate_size bytes. If size is followed by k, the size is assumed to be in kilobytes. If the M is used, the size is in megabytes, and if G is used, the size is in gigabytes. So size 100, size 100k, size 100M and size 100G are all valid. optional parameters-动态可选参数harbor启动修改完毕之后的配置如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@app028-dev harbor]# less harbor.cfg | egrep -v &apos;^$|^#&apos;hostname = harbor.wxh.comui_url_protocol = httpsmax_job_workers = 3customize_crt = onssl_cert = /data/cert/server.cerssl_cert_key = /data/cert/server.keysecretkey_path = /dataadmiral_url = NAlog_rotate_count = 50log_rotate_size = 200Memail_identity =email_server = smtp.mydomain.comemail_server_port = 25email_username = sample_admin@mydomain.comemail_password = abcemail_from = admin &lt;sample_admin@mydomain.com&gt;email_ssl = falseemail_insecure = falseharbor_admin_password = dHarbor12345auth_mode = db_authldap_url = ldaps://ldap.mydomain.comldap_basedn = ou=people,dc=mydomain,dc=comldap_uid = uidldap_scope = 2ldap_timeout = 5ldap_verify_cert = trueself_registration = ontoken_expiration = 30project_creation_restriction = everyonedb_host = mysqldb_password = root123db_port = 3306db_user = rootredis_url =clair_db_host = postgresclair_db_password = passwordclair_db_port = 5432clair_db_username = postgresclair_db = postgresuaa_endpoint = uaa.mydomain.orguaa_clientid = iduaa_clientsecret = secretuaa_verify_cert = trueuaa_ca_cert = /path/to/ca.pemregistry_storage_provider_name = filesystemregistry_storage_provider_config = 自己做测试时，将url类型设置成为http，并且将域名设置成为了：harbor.wxh.com 修改完配置之后： 1$ sudo ./install.sh 整个的安装过程如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119[root@localhost harbor]# ./install.sh[Step 0]: checking installation environment ...Note: docker version: 18.06.1Note: docker-compose version: 1.22.0[Step 1]: loading Harbor images ...651f69aef02c: Loading layer [==================================================&gt;] 135.8MB/135.8MB40a1aad64343: Loading layer [==================================================&gt;] 23.24MB/23.24MB3fe2713e4072: Loading layer [==================================================&gt;] 12.16MB/12.16MBba3a1eb0e375: Loading layer [==================================================&gt;] 17.3MB/17.3MB447427ec5e1a: Loading layer [==================================================&gt;] 15.87kB/15.87kB4ccb4026663c: Loading layer [==================================================&gt;] 3.072kB/3.072kB16faa95946a1: Loading layer [==================================================&gt;] 29.46MB/29.46MBLoaded image: vmware/notary-server-photon:v0.5.1-v1.4.0fa7ba9fd42c9: Loading layer [==================================================&gt;] 10.95MB/10.95MB4e400f9ae23e: Loading layer [==================================================&gt;] 17.3MB/17.3MB2802fb27c88b: Loading layer [==================================================&gt;] 15.87kB/15.87kBe6367a4e1e1e: Loading layer [==================================================&gt;] 3.072kB/3.072kB8ece8dfcdd98: Loading layer [==================================================&gt;] 28.24MB/28.24MBLoaded image: vmware/notary-signer-photon:v0.5.1-v1.4.0a7dd1a8afcaf: Loading layer [==================================================&gt;] 396.7MB/396.7MB05adebbe496f: Loading layer [==================================================&gt;] 9.216kB/9.216kB86eb534949fa: Loading layer [==================================================&gt;] 9.216kB/9.216kBd7f127c69380: Loading layer [==================================================&gt;] 7.68kB/7.68kB5ac1c4dc5ee9: Loading layer [==================================================&gt;] 1.536kB/1.536kBd0bec56b5b1a: Loading layer [==================================================&gt;] 9.728kB/9.728kB4bbe83860556: Loading layer [==================================================&gt;] 2.56kB/2.56kBe526f9e6769f: Loading layer [==================================================&gt;] 3.072kB/3.072kBLoaded image: vmware/harbor-db:v1.4.01cff102bbda2: Loading layer [==================================================&gt;] 154.1MB/154.1MB04c9f3e07de1: Loading layer [==================================================&gt;] 10.75MB/10.75MB7b6c7bf54f5c: Loading layer [==================================================&gt;] 2.048kB/2.048kB42f8acdb7fe3: Loading layer [==================================================&gt;] 48.13kB/48.13kB5b6299d0a1df: Loading layer [==================================================&gt;] 10.8MB/10.8MBLoaded image: vmware/clair-photon:v2.0.1-v1.4.06534131f457c: Loading layer [==================================================&gt;] 94.76MB/94.76MB73f582101e4b: Loading layer [==================================================&gt;] 6.656kB/6.656kB86d847823c48: Loading layer [==================================================&gt;] 6.656kB/6.656kBLoaded image: vmware/postgresql-photon:v1.4.05cd250d5a352: Loading layer [==================================================&gt;] 23.24MB/23.24MBad3fd52b54f3: Loading layer [==================================================&gt;] 14.99MB/14.99MB13b1e24cc368: Loading layer [==================================================&gt;] 14.99MB/14.99MBLoaded image: vmware/harbor-adminserver:v1.4.0c26c69706710: Loading layer [==================================================&gt;] 23.24MB/23.24MB223f6fe02cc8: Loading layer [==================================================&gt;] 23.45MB/23.45MB1fc843c8698a: Loading layer [==================================================&gt;] 7.168kB/7.168kBe09293610ee7: Loading layer [==================================================&gt;] 10.39MB/10.39MBd59f9780b1d8: Loading layer [==================================================&gt;] 23.44MB/23.44MBLoaded image: vmware/harbor-ui:v1.4.0dd4753242e59: Loading layer [==================================================&gt;] 73.07MB/73.07MB95aed61ca251: Loading layer [==================================================&gt;] 3.584kB/3.584kB1864f9818562: Loading layer [==================================================&gt;] 3.072kB/3.072kBda2a19f80b81: Loading layer [==================================================&gt;] 4.096kB/4.096kB058531639e75: Loading layer [==================================================&gt;] 3.584kB/3.584kBa84e69fb619b: Loading layer [==================================================&gt;] 10.24kB/10.24kBLoaded image: vmware/harbor-log:v1.4.0b1056051f246: Loading layer [==================================================&gt;] 23.24MB/23.24MB07678065e08b: Loading layer [==================================================&gt;] 19.19MB/19.19MBa2d9bdb8f5fb: Loading layer [==================================================&gt;] 19.19MB/19.19MBLoaded image: vmware/harbor-jobservice:v1.4.07f58ce57cd5e: Loading layer [==================================================&gt;] 4.805MB/4.805MBLoaded image: vmware/nginx-photon:v1.4.04c8965978b77: Loading layer [==================================================&gt;] 23.24MB/23.24MB1466c942edde: Loading layer [==================================================&gt;] 2.048kB/2.048kBac5c17331735: Loading layer [==================================================&gt;] 2.048kB/2.048kB86824c7c466a: Loading layer [==================================================&gt;] 2.048kB/2.048kBfd3bd0e70d67: Loading layer [==================================================&gt;] 22.8MB/22.8MBb02195d77636: Loading layer [==================================================&gt;] 22.8MB/22.8MBLoaded image: vmware/registry-photon:v2.6.2-v1.4.0Loaded image: vmware/photon:1.0Loaded image: vmware/mariadb-photon:v1.4.0454c81edbd3b: Loading layer [==================================================&gt;] 135.2MB/135.2MBe99db1275091: Loading layer [==================================================&gt;] 395.4MB/395.4MB051e4ee23882: Loading layer [==================================================&gt;] 9.216kB/9.216kB6cca4437b6f6: Loading layer [==================================================&gt;] 9.216kB/9.216kB1d48fc08c8bc: Loading layer [==================================================&gt;] 7.68kB/7.68kB0419724fd942: Loading layer [==================================================&gt;] 1.536kB/1.536kB526b2156bd7a: Loading layer [==================================================&gt;] 637.8MB/637.8MB9ebf6900ecbd: Loading layer [==================================================&gt;] 78.34kB/78.34kBLoaded image: vmware/harbor-db-migrator:1.4[Step 2]: preparing environment ...Generated and saved secret to file: /data/secretkeyGenerated configuration file: ./common/config/nginx/nginx.confGenerated configuration file: ./common/config/adminserver/envGenerated configuration file: ./common/config/ui/envGenerated configuration file: ./common/config/registry/config.ymlGenerated configuration file: ./common/config/db/envGenerated configuration file: ./common/config/jobservice/envGenerated configuration file: ./common/config/log/logrotate.confGenerated configuration file: ./common/config/jobservice/app.confGenerated configuration file: ./common/config/ui/app.confGenerated certificate, key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crtThe configuration files are ready, please use docker-compose to start the service.[Step 3]: checking existing instance of Harbor ...[Step 4]: starting Harbor ...Creating network &quot;harbor_harbor&quot; with the default driverCreating harbor-log ... doneCreating harbor-db ... doneCreating registry ... doneCreating harbor-adminserver ... doneCreating harbor-ui ... doneCreating nginx ... doneCreating harbor-jobservice ... done✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at http://harbor.wxh.com.For more details, please visit https://github.com/vmware/harbor .[root@localhost harbor]# 镜像操作上传镜像push的格式为： 1# docker push reg.yourdomain.com/myproject/myrepo:mytag 注意首先需要登录： docker login domain_name 步骤1：登录 1docker login domain_name 步骤2：将要上传的镜像打上标志 123# docker tag hello-world harbor.wxh.com/apps/hello-world# docker push harbor.wxh.com/apps/hello-world 这种上传的话，默认是打上latest的标志 打上指定的标签使用： 12docker tag hello-world harbor.wxh.com/apps/hello-world:v1docker push harbor.wxh.com/apps/hello-world:v1 harbor镜像删除在web页面上删除镜像实际上只是执行的软删除，因为镜像存在很强的文件系统依赖关系 Harbor的UI界面上先删除镜像，但这个操作并没有删除磁盘上存放的镜像文件，只是镜像文件manifest的映射关系，还需要通过GC来删除。 CAUTION: If both tag A and tag B refer to the same image, after deleting tag A, B will also get deleted. if you enabled content trust, you need to use notary command line tool to delete the tag’s signature before you delete an image. 注意，如果标签A和B都指向都一个镜像（比如hello-world的2个镜像），那么删除一个之后，另外一个也会消失 步骤1：删除镜像tag 在web页面删除镜像或者使用api接口进行删除 步骤2：停止Harbor 1docker-compose stop 步骤3：执行gc回收空间 通过带有–dry-run选项，可以查看到将要删除的镜像文件： 1docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.2-photon garbage-collect --dry-run /etc/registry/config.yml1 不带–dry-run选项，直接执行删除： 1docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.2-photon garbage-collect /etc/registry/config.yml1 步骤4：启动Harbor 1docker-compose start 镜像清理策略需求： 暂时不做删除 repo 的处理【这部分手动处理】 保留 60 天内创建的所有 tag ，在 60 天之前创建的 tag ，额外保留 10 个； 标签数只有 1的镜像，不清理 保留最后一次更新的tag，有些image比较稳定，有可能超过60天都没有修改，但是却一直在用 针对一些特殊的（比如每天5个tag的镜像，那么60天就有300个），这个单独特殊处理 最终： 针对 tag ：保留 60 天内创建的所有 tag ，在 60 天之前创建的 tag ，额外保留 10 个； 针对 repo ：暂时不做删除 repo 的处理（不太好确定 repo 是否还在使用，理论上讲每个 repo 下至少应该有一个 tag 是被需要的；若打算删除，则建议 repo 负责人自行进行删除操作）； 私有仓库暂时不做处理； 具体实现： harbor 主要概念的关系：1 个 project -&gt; 每个 project 下具有 N 种不同的 repos &gt; 每个 repo 下具有 M 个 tags project 有创建时间，但这个对我们的处理策略来说没有用处； repo 有创建时间和 pull 时间，该 pull 时间对应 repo 下任意一个 tag ，最新一次，被拉取的那个时间 tag 有创建时间，但没有针对 tag 的 pull 时间（harbor 中定义的数据结构中不支持）； 因此 保留 60 天内的 tag”，这个根据 tag 的创建时间 “60 天之外的看 pull 的数量，关注 60 天之外是不是被 pull 过”，由于 pull 数量是针对 repo 整体的，无法对应到具体的 tag ，即在 API 层面无法方便的知道哪些 tag 最近被 pull 过（当然如果一定要做，就只能沟通分析 log 来搞，性价比不高），所以，只能根据 tag 创建时间的先后，“武断”的认为，后创建的 tag 应该是用户最想保留的； “如果没有被 pull 过，则只保留最新 5 个 tag”，根据上一条的说明，某个 tag 是否被 pull 过是无法知道的，但目前可以做到根据 tag 的创建时间进行保留（满足保留最新 N 个需求）； Harborclient-harbor客户端命令参考文献： 主页说明 实际案例实战案例harbor镜像复制参考文献 官网文档 镜像复制概述镜像复制有一些基本的概念需要知晓： 该功能是面向项目的，系统管理员设置之后，匹配了过滤规则的项目，在触发了事先定义好的触发条件之后，这些项目就会被复制到远程的另一个仓库中。 如果在远程镜像仓库中，该项目不存在，那么就会自动创建这个项目 如果在远程仓库中，这个项目已经存在，并且配置的用户对这个项目没有写的权限，那么这个操作将会失败 因为网络的原因，在复制传输的过程中，可能会出现一些延迟。如果复制job是因为网络原因而导致失败的，那么这个任务将会在几分钟之后再次尝试，一直尝试，知道网络恢复正常。 注意： 因为api等原因，不同版本的镜像复制可能会失败，所以尽量使用同一个版本。 用户信息不会被复制 创建复制规则注意，在创建endpoint的时候，直接test connection是会报错：“harbor Failed to ping endpoint” 这是因为网络问题导致，在内网访问的时候，还需要额外的添加hosts文件，详见注意事项 注意：在创建完毕之后，默认不会执行同步，需要手动点击一下replication 删除replication规则删除规则的时候有几个注意事项： Only rules which have no pending/running/retrying jobs can be deleted. 也就是说，只有当改规则下面没有正在运行或者等待运行或者正在重传的jobs时，才可以删除 注意事项在创建endpoint的时候，如果事先没有在容器内存配置对端的地址，那么会报连接错误 官方的issues：https://github.com/goharbor/harbor/issues/2221 harbor迁移节点完全迁移存储迁移常见问题证书问题证书生成： 1openssl req -sha256 -x509 -days 365 -nodes -newkey rsa:4096 -keyout harbor.wxh.com.key -out harbor.wxh.com.crt 注意，一些name的字段要配置成为配置文件中配置的域名，例如：harbor.wxh.com 生成之后，将证书存放到指定位置，然后修改配置文件指向这些证书文件 123less harbor.cfg | egrep -v &quot;^$|^#&quot;ssl_cert = /data/cert/harbor.wxh.com.crtssl_cert_key = /data/cert/harbor.wxh.com.key 然后需要对docker进行一些配置 1mkdir -p /etc/docker/certs.d/harbor.wxh.com 然后将上面的2个证书文件复制到这个目录之下，并将/data/cert/harbor.wxh.com.crt重命名为/data/cert/harbor.wxh.com.cert 12345# pwd;ls/etc/docker/certs.d/harbor.wxh.comharbor.wxh.com.crt harbor.wxh.com.key# mv harbor.wxh.com.crt harbor.wxh.com.cert 文件创建为目录问题/data1/harbor/data/secretkey 在harbor安装之后以及运行过程中，secretkey为文件，而不是目录。 在一些异常情况可能会出现这个文件变成目录这种问题，当出现这种问题的时候，将该目录清空，然后重新安装即可 harbor升级-支持在线gcharbor高可用架构规划整体架构如下： 首先单节点高可用，将有状态的东西分离出去，包括： Redis db数据库 文件存储 要注意session等问题,使用外部的Redis来实现共享session 在最前端使用LB做负载均衡 使用同一个db需要考虑的问题： 复制策略是保存在数据库中的，2个harbor都能获取到去发起复制，会不会存在问题？ 也就是说，要不要每个harbor都使用同一个db LB的检测机制，能否有比端口更好的方式 然后异地多活的每一个harbor集群通过分布式的方式组合起来，使用consul来记录信息 实现共享存储部署在开发环境使用nfs进行测试，在阿里云环境使用nas存储，在IDC机房使用分布式存储。 存储的部署实施略，下面说一下在harbor中的配置 db数据库创建在harbor高版本后，只支持pg，相关的部署这里就不记录了，可以看我的postgresql相关文章 创建数据库名称为：registry 数据库配置 Redis创建templates文件配置external mysql的配置在common/templates/adminserver/env中，数据库配置修改为： 123456DATABASE_TYPE=mysqlMYSQL_HOST=$db_hostMYSQL_PORT=$db_portMYSQL_USR=$db_userMYSQL_PWD=$db_passwordMYSQL_DATABASE=rds_unit_harbor 因为使用了外部的数据库，因为dbname不是默认的，其他的使用harbor.cfg中的变量传入即可。 还有一个关键配置，那就是将RESET由false改为true。只有改为true，adminserver启动时，才能读取更新后的配置： 1RESET=true Redis的连接配置在common/templates/ui/env中(在1.7版本中为core)，我们需要新增一行： 1_REDIS_URL=redis_ip:6379,100,password,0 docker-compose.yml文件配置docker-compose.yml是docker-compose工具标准配置文件，用于配置docker-compose即将启动的容器服务。针对该配置文件，我们主要做三点修改： 修改volumes路径，将存储修改为我们事先创建的共享存储由/data/xxx 改为：/data/harbor_storage/ 具体配置为： 123/data/registry:/storage:z修改为：/data/harbor_storage:/storage:z 由于使用外部数据库，因此需要删除数据库 service以及其他 service对数据库 service的依赖 (depends_on) 修改对proxy外服务端口 ports: 8070:80 harbor.cfg文件配置在该文件中， 修改一下内容： hostname的域名配置，因为https在前端使用nginx去实现。因此也不需要证书相关文件和配置 数据库连接配置 redis连接配置 修改的内容如下： 123456789101112# vim harbor.cfghostname = dhub-test.dwbops.comdb_host = rm-bp188fb70hknd9gi2o.mysql.rds.aliyuncs.comdb_password = Devuser123db_port = 3306db_user = devuserredis_host = 192.168.1.196redis_port = 6379redis_password = H5DUg@_redis 注意，所有实例节点的配置都需要保持一致 login失败问题在多实例节点时，在登录是可能会出现登录不上的情况 node1: node1节点上调用了token服务 1Mar 28 11:31:27 172.18.0.1 proxy[1116]: 192.168.1.155 - &quot;GET /service/token?account=admin&amp;client_id=docker&amp;offline_token=true&amp;service=harbor-registry HTTP/1.1&quot; 200 893 &quot;-&quot; &quot;docker/18.06.1-ce go/go1.10.3 git-commit/e68fc7a kernel/4.15.0-46-generic os/linux arch/amd64 UpstreamClient(Docker-Client/18.06.1-ce \x5C(linux\x5C))&quot; 0.029 0.029 . node2: 但是在node2节点上 12Mar 28 11:23:33 172.19.0.1 proxy[1335]: 192.168.1.155 - &quot;GET /v2/ HTTP/1.1&quot; 401 87 &quot;-&quot; &quot;docker/18.06.1-ce go/go1.10.3 git-commit/e68fc7a kernel/4.15.0-46-generic os/linux arch/amd64 UpstreamClient(Docker-Client/18.06.1-ce \x5C(linux\x5C))&quot; 0.007 0.007 .Mar 28 11:23:33 172.19.0.1 proxy[1335]: 192.168.1.155 - &quot;GET /v2/ HTTP/1.1&quot; 401 87 &quot;-&quot; &quot;docker/18.06.1-ce go/go1.10.3 git-commit/e68fc7a kernel/4.15.0-46-generic os/linux arch/amd64 UpstreamClient(Docker-Client/18.06.1-ce \x5C(linux\x5C))&quot; 0.004 0.004 . 要知道这个问题的原因，还要回溯到harbor的实现过程 wiki上对docker login流程给了简明扼要的解释。大致的流程是: docker向registry发起请求，由于registry是基于token auth的，因此registry回复应答，告诉docker client去哪个URL去获取token； docker client根据应答中的URL向token service(ui)发起请求，通过user和passwd获取token；如果user和passwd在db中通过了验证，那么token service将用自己的私钥(harbor/common/config/ui/private_key.pem)生成一个token，返回给docker client端； docker client获得token后再向registry发起login请求，registry用自己的证书(harbor/common/config/registry/root.crt)对token进行校验。通过则返回成功，否则返回失败。 从这个原理，我们可以知道问题就出在docker client多次向Harbor发起请求这个环节：对于每次请求，nginx可能会转发给不同的节点处理，因此不同请求可能落到不同的node上。这样当docker client拿着node1上token service分配的token去到node2的registry上鉴权时，就会出现鉴权失败的情况。 因此，解决这个问题的方法是：统一私钥和证书 token service的私钥(harbor/common/config/ui/private_key.pem)和registry的证书(harbor/common/config/registry/root.crt)都是在prepare时生成的，两个节点都独立prepare过，因此两个node上的private_key.pem和root.crt是不同的，这就是问题根源。 解决这个问题很简单，就是统一私钥和证书。比如：将node1上的private_key.pem和root.crt复制到node2上，并重新创建node2上的container： 1234567// node2上将node1上的harbor/common/config/ui/private_key.pem复制到node2上的harbor/common/config/ui/private_key.pem；将node1上的harbor/common/config/registry/root.crt复制到harbor/common/config/registry/root.crt；$ docker-compose down -v$ docker-compose up -d 更换了private_key.pem和root.crt的node2上的Harbor启动后，再进行login测试，就会100%成功了！ 12# docker login -u admin -p passwd http://hub.my-domain.com:36666Login Succeeded 注意： 不是修改模板中的文件 修改之后不能再次执行prepare，否则会刷掉原来的文件重新生成 使用tar包的形式进行传输解压，保留文件权限 login成功，但是push失败问题在harbor的template的nginx配置文件中，把以下内容进行注释 12[root@node009-dev nginx]# cat nginx.http.conf#proxy_set_header X-Forwarded-Proto $$scheme; 并且在nginx上使用ip_hash; 不然的话会经常出现：”blob upload unknown”的错误 这是因为由于nginx反向代理时没有配置ip_hash，默认会使用轮询算法，从而导致docker客户机的push请求分别转向Node1，Node2两个地址而引发 “blob upload unknown” 报错的 在线gc失败问题在harbor1.7中，register进行了拆分，我们的存储配置需要在registry和registryctl中都进行配置 ./salt-ops.py shidc 3.2.3 harbor redis002.dwd.gds-sh redis005.dwd.gds-sh 6530 qs7edUiOySmqzDe 8 push镜像504问题https://github.com/goharbor/harbor/issues/3446 在templates的nginx配置文件中添加以下内容： 12proxy_send_timeout 900s;proxy_read_timeout 900s; 高可用架构下的运维处理harbor日志harbor的日志如下： 12345678[root@app005.dwd.gds-sh harbor]# ll /var/log/harbor/-rw-r--r--. 1 10000 10000 2700715 Apr 11 10:39 adminserver.log-rw-r--r--. 1 10000 10000 148541326 Apr 11 10:39 core.log-rw-r--r--. 1 10000 10000 818812 Apr 11 10:38 jobservice.log-rw-r--r--. 1 10000 10000 2062849 Apr 11 10:39 portal.log-rw-r--r--. 1 10000 10000 105704008 Apr 11 10:39 proxy.log-rw-r--r--. 1 10000 10000 982835 Apr 11 10:38 registryctl.log-rw-r--r--. 1 10000 10000 193488 Apr 11 10:38 registry.log 详细说明： adminserver.log # 全局管控，复制各个接口的检测，排查问题时不怎么需要关注 portal.log # http检测，排查问题时不需要怎么关注 core.log # 一些核心功能，排查问题时一般不关注 registryctl.log # 仓库控制功能相关，删除以及健康检测，一般也不怎么关注 proxy.log # 整个harbor的全局入口，和web ui相关，可以查看访问harbor的请求日志详情 jobservice.log # 镜像复制相关，以及数据库相关 registry.log # 仓库相关，有关仓库的详细日志都在这里面]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>虚拟化</category>
        <category>Docker+k8s</category>
      </categories>
      <tags>
        <tag>harbor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[运维思想]]></title>
    <url>%2F2018%2F12%2F04%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E8%BF%90%E7%BB%B4%E6%80%9D%E6%83%B3%2F%E8%BF%90%E7%BB%B4%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[产品思维指导工作产品 站在远处、高处分析这件事对不对 所负责的工作产出是一个产品，自己本身也是一个产品 如何确定目标：主谓宾方法论，未来十年，什么是不变的 目标 不关注合理性，只讨论必要性 目标–&gt;路径–&gt;资源（目标管理本质其实是资源管理） 运营 把目标分解成没人每天每件事 成本效率（每天的工作） 流程 规范 框架 用户体验解决问题的思想中医思维解决问题知识来源：喜马拉雅 好了，我们开始正文。 在中医当中，看病主要包含这4块：望闻问切 望]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维思想</category>
      </categories>
      <tags>
        <tag>运维思想</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bind部署]]></title>
    <url>%2F2018%2F11%2F20%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E5%8F%8A%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%2F%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%2FDNS%E6%9C%8D%E5%8A%A1%E5%AE%9E%E7%8E%B0-bind%2Fbind%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[参考文献 bind官方网站 bind9官方PDF文档 鸟哥的bind文章 BIND配置文件详解（一） 《TCP/IP详解》DNS部分 google了无数网页 DNS及bind基本概念记录在bind中的一些基本概念 DNS服务器：向客户端提供域名解析服务的服务器· DNS服务器的类型； 主DNS服务器：维护所负责解析的域内解析库服务器；解析库由管理维护；读写操作均可进行； 从DNS服务器：从主DNS服务器或从其他的从DNS服务器那里区域传递(类似“复制”)一份解析库，只能进行读操作 缓存DNS服务器：负责代理客户机的递归查询工作，并且采用迭代查询的服务器 转发器：如果目标域名在本DNS服务器辖区内，直接转发 BIND (Berkeley Internet Name Domain) 是Domain Name System (DNS) 协议的一个实现，提供了DNS主要功能的开放实现 资源记录 resource records (RRs) | 表示 domain 域，树状结构上的每一个节点叫做domain Each node of the tree, called a domain, is given a label. Domain name 存储在dns分布式数据库中的每一个具体的数据被称之为域名 The data stored in the DNS is identified by domain names that are organized as a tree according to organizational or administrative boundaries zone zone是dns树状结构中某个节点的代表 a zone is a point of delegation in the DNS tree zone被用于管理界限的划分，一个域名空间被划分为各个区域，也就是zone zone本身就是一个节点（domain），一个zone包含了那些相邻并且是往下的节点 It contains all domain names from a certain point downward in thedomain tree except those which are delegated to other zones 这个zone(节点)被它的上级zone标记为NS记录，而它的上级zone将会被根标记为NS记录 A delegation point is marked by one or more NS records in the parent zone, which should be matched by equivalent NS records at the root of the delegated zone 例如，有一个 domain 叫 example.com ，它可以包含test1.aaa.example.com 和test2.bbb.example.com 这些名字，但是它的 zone 文件中却只有 2 个 zone 的记录aaa.example.com 和 bbb.example.com。 权威DNS服务器（Authoritative name server） 权威服务器用于响应dns客户端的请求，上面有完整的zone数据，下面要说的主和从服务器都是属于权威服务器 在响应数据库报中，我们可以看到authoritative answer标志位的出现 设置权威服务器可以帮助我们排查问题，例如使用dig的时候可以输出重要信息 Responses from authoritative servers have the “authoritative answer” (AA) bit set in the responsepackets. This makes them easy to identify when debugging DNS configurations usingtools like dig The Primary Master 主域名服务器 在权威服务器中，起主要重要的我们称之为主域名服务器 数据文件一般是zone file或者master file This file is called the zone file or master file. Slave Servers 从域名服务器 权威服务器的一种 slave提供必要的冗余服务，所有的slave服务器都应该记录在这个域名的ns记录中 一般来说，slave上的数据一般是通过zone transfer进程从master上同步的，但是它也能从其他slave节点上同步数据 slave节点会周期性的发送请求到master节点，检测和同步数据（sending a query for the zone’s SOA record and checking whether the SERIAL field has been updated） bind-chroot bind-chroot是bind的一个功能，使bind可以在一个chroot的模式下运行。也就是说，bind运行时的/(根)目录，并不是系统真正的/(根)目录，只是系统中的一个子目录而已。这样做的目的是为了提高安全性。因为在chroot的模式下，bind可以访问的范围仅限于这个子目录的范围里，无法进一步提升，进入到系统的其他目录中。 dns服务器工作流程 设置forward转发。如果设置了转发，则将请求转发到forward服务器 没有设置forward转发 1). 本地cache 2). 本地zone配置 2). 若本地cache和zone都没有数据，则前往root(.)进行查询 域名空间和体系结构DNS 的分布式数据库是以域名为索引的，每个域名实际上就是一棵很大的逆 向树中路径，这棵逆向树称为域名空间(domain name space)。如图 2.1 所示 树的最大深度不得超过 127 层，树中每个节点都有一个可以长达 63 个字符。 BIND 包含了一个叫 named 的后台进程，和 resolver 库。BIND 服务器程序在后台运行， 通过众所周知的端口提供服务。UDP(User Datagram Protocol)和 TCP(Transmission Control Protocol)在 DNS 中的标准端口通常是 53，在/etc/services 中设置。 结构图如下： 域和域名NS(nameserver)就是一个存储着域名资源信息的程序，它会响应来自叫 resolver 的程 序的请求，resolver 类似于一个客户端程序。NS 的基本功能就是通过回答查询请求来提供 网络信息。 利用 nameserver，整个网络可以划分成一个域的分层结构。整个的域名空间可以根据组 织划分或管理分类，组织成一个树状结构。树上的每个节点叫做 domain，就是一个标示。 节点中每个 domain都是有名字的，它的名字就是从 root 开始，到当前节点的所有 domain 标志的集合。从书写的结 构上看，就是从右到左依次用”.”来区分开。这样域名的标志才能够唯一。例如:ourhost.example.com 总结： 树中的每个节点叫做：domain域 树中每个节点的名称叫做域名 dns解析诊断工具目前主流的主要是这3个工具： dig host nslookup digdig有两种执行模式 针对一条请求的简单交互式模式 执行多查询的批模式 语法： 123dig [@服务器]域 [查询-类型] [查询-类 [+查询-选项] [-dig-选项] [%注释]通常采用以下形式:dig @服务器 域 查询类型 查询类 例如：(dig @server domain query-type query-class) hostnslookupbind管理命令 named-checkconf。检查 named.conf 文件的句法 12例如：/home/named/bind/sbin/named-checkconf /home/named/bind/chroot/etc/named.conf named-checkzone。程序检查 host 文件的句法和相容性。 1234例如：[named@shidc-1 chroot]$ /home/named/bind/sbin/named-checkzone nidianwo.com var/named/zone/shidc/nidianwo.com.zonezone nidianwo.com/IN: loaded serial 2OK rndc(Remote Name Daemon Control) 详见bind总结中的 具体部署部署方式在做测试或者只是提供基本功能的时候，我们可以使用软件源，只需用yum(redhat、centos系列)等方式安装即可。 如果需要更深层次的使用配置，我们建议使用源码包的安装方式 创建用户12groupadd nameduseradd -g named named 依赖关系依赖关系需要使用管理员用户安装 具体安装哪些需要根据实际的服务器情况决定，例如gcc、openssl等都是需要的，这里不再赘述。 下载bind为了便于后续的维护及配置，这里采用源码包的安装方式 123[named@host1 ~]$ pwd/home/named[named@host1 ~]$ wget https://www.isc.org/downloads/file/bind-9-12-3/?version=tar-gz -O bind-9.12.3.tar.gz 这里我们选择9.12.3版本（当前时间节点为：2018年11月20日） 安装在编译的时候可以添加以下功能参数 --enable-threads | 在多cpu环境下，开启多线程支持，提高性能 --with-tuning=large | 在内存充裕（12G以上）的情况下，开启以提高性能，否则不建议开启 --prefix=&lt;PREFIX&gt; | 自定义安装路径，源码安装时必须指定。 --with-openssl=&lt;PREFIX&gt; | 如果使用自定义的openssl，需要进行指定。使用系统自带的就不需要额外指定 1234tar -zxvf bind-9.12.3.tar.gz./configure --prefix=/home/named/bindmakemake install 安装完成之后在目录下会提供一些调试工具 123[named@host1 bin]$ pwd;ls/home/named/bind/bin arpaname bind9-config delv dig host isc-config.sh mdig named-rrchecker nslookup nsupdate 然后我们把必要的系统管理命令的目录路径添加到系统路径： 12345[named@host1 ~]$ vim .bash_profile # 在文件末尾添加一下2行内容PATH=/home/named/bind/sbin/:$PATHexport PATHalias rndc=&apos;rndc -c /home/named/bind/chroot/etc/rndc.conf&apos;[named@host1 ~]$ source .bash_profile 配置通用配置1. 创建chroot目录123[named@host1 bind]$ mkdir -p /home/named/bind/chroot/&#123;etc,var,log&#125;[named@host1 bind]$ mkdir -p /home/named/bind/chroot/var/&#123;run,named&#125;[named@host1 bind]$ mkdir -p /home/named/bind/chroot/var/named/&#123;zone,data&#125; 2. 生成配置文件1234[named@host1 etc]$ pwd/home/named/bind/chroot/etc[named@host1 etc]$ /home/named/bind/sbin/rndc-confgen &gt; rndc.conf[named@host1 etc]$ sed -n &apos;15,23s/#\ //p&apos; rndc.conf &gt; named.conf 两个配置文件简要说明： named.conf 这个是我们的主配置文件，关于域名的配置都记录在这里 rndc.conf 这个是rndc的安全配置文件。 rndc 是 BIND 9 之后提供一个管理工具，包括重加载zone数据而不需要重新启动整个 DNS 、检查 DNS 的状态与统计数据等等 因为 rndc 可以很深入的管理DNS 服务器，所以要进行一些管控。控管的方式是通过rndc 的设置创建一个密钥 (rndc key)，并将这个密钥相关信息写入 named.conf 配置文件当中。启动 DNS 后，你的 DNS 就能够藉由 rndc 这个命令管理！ 3. 下载name.root文件（13个根服务器的配置）1[named@host1 etc]$ wget ftp://ftp.rs.internic.net/domain/named.root -P /home/named/bind/chroot/var/named/ 4. 创建localhost.zone文件该文件一般情况下不使用，因为服务器上的本地hosts文件中都会添加这个信息 在实际部署时，不会使用。这里也进行说明，如有特殊需求可以使用。 12345678910111213[named@host1 named]$ pwd/home/named/bind/chroot/var/named[named@host1 named]$ cat localhost.zone$TTL 86400@ IN SOA @ root ( 1 ; serial (d. adams) 3H ; refresh 15M ; retry 1W ; expiry 1D ) ; minimum IN NS @ IN A 127.0.0.1 IN AAAA ::1 5. 创建文件localhost.rev该文件一般情况下不使用，因为服务器上的本地hosts文件中都会添加这个信息 在实际部署时，不会使用。这里也进行说明，如有特殊需求可以使用。 123456789101112[named@host1 named]$ pwd/home/named/bind/chroot/var/named[named@host1 named]$ cat localhost.rev$TTL 86400@ IN SOA localhost. root.localhost. ( 1 ; Serial 3H ; Refresh 15M ; Retry 1W ; Expire 1D ) ; Minimum IN NS localhost.1 IN PTR localhost. 6. 修改named.conf配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116[named@host1 etc]$ pwd/home/named/bind/chroot/etc[named@host1 etc]$ cat named.confkey &quot;rndc-key&quot; &#123; algorithm hmac-sha256; secret &quot;vqOF9VUn75lvtpCYvYffOVNT8LkLK0z78UCPVkX1ofk=&quot;;&#125;;controls &#123; inet 127.0.0.1 port 953 allow &#123; 127.0.0.1; &#125; keys &#123; &quot;rndc-key&quot;; &#125;;&#125;;options&#123; listen-on port 53&#123; 192.168.103.99; &#125;; listen-on-v6 port 53&#123; fe80::20c:29ff:fefe:d1f4; &#125;; version &quot;bind 9.12.3&quot;; directory &quot;/home/named/bind/chroot/var/named&quot;; pid-file &quot;/home/named/bind/chroot/var/run/named.pid&quot;; session-keyfile &quot;/home/named/bind/chroot/var/run/session.key&quot;; dump-file &quot;/home/named/bind/chroot/var/named/data/cache_dump.db&quot;; statistics-file &quot;/home/named/bind/chroot/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/home/named/bind/chroot/var/named/data/named_mem_stats.txt&quot;; recursion yes; allow-query&#123; any; &#125;; allow-query-cache&#123; any; &#125;; allow-transfer&#123; 192.168.101.172; &#125;; notify yes; also-notify&#123; 192.168.101.172; &#125;;&#125;;logging &#123; channel default_debug &#123; file &quot;/home/named/bind/chroot/log/named.run&quot; versions 10 size 128m; severity dynamic; print-category yes; print-severity yes; print-time yes; &#125;; channel queries_info &#123; file &quot;/home/named/bind/chroot/log/query.log&quot; versions 10 size 128m; severity info; print-category yes; print-severity yes; print-time yes; &#125;; category queries &#123; queries_info; default_debug; &#125;; channel notify_info &#123; file &quot;/home/named/bind/chroot/log/notify.log&quot; versions 10 size 128m; severity info; print-category yes; print-severity yes; print-time yes; &#125;; category notify &#123; notify_info; default_debug; &#125;; channel xfer_in_log &#123; file &quot;/home/named/bind/chroot/log/xfer_in.log&quot; versions 10 size 128m; severity info; print-category yes; print-severity yes; print-time yes; &#125;; channel xfer_out_log &#123; file &quot;/home/named/bind/chroot/log/xfer_out.log&quot; versions 10 size 128m; severity info; print-category yes; print-severity yes; print-time yes; &#125;; category xfer-in &#123; xfer_in_log; &#125;; category xfer-out &#123; xfer_out_log; &#125;;&#125;;zone &quot;.&quot; in&#123; type hint; file &quot;named.root&quot;;&#125;;zone &quot;localhost&quot; in&#123; type master; file &quot;localhost.zone&quot;;&#125;;zone &quot;0.0.127.in-addr.arpa&quot; in&#123; type master; file &quot;localhost.rev&quot;; allow-update &#123; none; &#125;;&#125;;zone &quot;test.com&quot; IN &#123; type master; file &quot;zone/test.com.zone&quot;;&#125;; 注意：localhost和0.0.127.in-addr.arpa这2个zone，我们在实际部署的时候不会添加，当有特殊需求时使用 上面是为了测试把zone信息写了出来，在实际部署中，主配置文件中zone相关部分替换成下面这种形式： 1234567include &quot;/home/named/bind/chroot/etc/acls/shidc&quot;;include &quot;/home/named/bind/chroot/etc/acls/hzidc&quot;;include &quot;/home/named/bind/chroot/etc/acls/aliyun-east1&quot;;include &quot;/home/named/bind/chroot/etc/views/shidc&quot;;include &quot;/home/named/bind/chroot/etc/views/hzidc&quot;;include &quot;/home/named/bind/chroot/etc/views/aliyun-east1&quot;; 有关acl以及view的部分，请查看本文的bind总结部分的named.conf配置文件详解 7. 创建zone相关文件123456789101112131415[named@host1 zone]$ pwd/home/named/bind/chroot/var/named/zone[named@host1 zone]$ cat test.com.zone$TTL 86400@ IN SOA test.com. admin.test.com. ( 57 ; serial (d. adams) 3H ; refresh 15M ; retry 1W ; expiry 1D ) ; minimum IN NS dns.test.com. IN MX 5 maildns IN A 192.168.11.91mail IN A 192.168.11.102www IN A 192.168.11.8 8. 检查配置文件1[named@host1 etc]$ /home/named/bind/sbin/named-checkconf /home/named/bind/chroot/etc/named.conf 9. 检查正向和反向zone配置文件1234567[named@host1 etc]$ /home/named/bind/sbin/named-checkzone localhost /home/named/bind/chroot/var/named/localhost.zonezone localhost/IN: loaded serial 42OK[named@host1 etc]$ /home/named/bind/sbin/named-checkzone 127.0.0.1 /home/named/bind/chroot/var/named/localhost.revzone 127.0.0.1/IN: loaded serial 1997022700OK 10. 检查自定义zone配置文件123[named@host1 etc]$ /home/named/bind/sbin/named-checkzone 127.0.0.1 /home/named/bind/chroot/var/named/zone/test.com.zonezone 127.0.0.1/IN: loaded serial 57OK 主从/主辅同步配置bind的主从，也称之为主辅，其实都是一样的。 master/主配置修改named.conf配置文件在option配置段中增加如下配置： 1234567allow-transfer&#123; 192.168.11.89;&#125;; notify yes;also-notify&#123; 192.168.11.89;&#125;; 上面是全局配置，如果是配置到具体的zone，只需要将以上3个参数配置zone里面即可（从服务器无需变化），如下： 1234567891011zone &quot;test.com&quot; IN &#123; type master; file &quot;zone/test.com.zone&quot;; allow-transfer&#123; 192.168.11.89; &#125;; notify yes; also-notify&#123; 192.168.11.89; &#125;;&#125;; 增加参数为： allow-transfer notify also-notify 有关这些参数的含义，请查看本文的bind总结部分的named.conf配置文件详解 注意：bind的主辅同步可以针对具体的每一个zone，也就是说每个zone都可以配置自己的从服务器。当域名的数据量很庞大时，为提高解析效率，实现读写分离，使用一台主服务器写，并将域名均分到多台从服务器上，以提高读的效率。 slave/辅配置修改named.conf配置文件在zone配置段中增加如下配置： 123[named@host1 etc]$ pwd/home/named/bind/chroot/etc[named@host1 etc]$ vim named.conf 1234567zone &quot;test.com&quot; IN &#123; type slave; file &quot;zone/test.com.zone&quot;; masters&#123; 192.168.11.91; &#125;;&#125;; 主要增加参数： masters 配置主服务器 type slave指定为从服务器 注意：slave端的zone配置文件可以事先不存在，但是对应的存储目录一定要存在 子域授权配置除了 Master/Slave 这种需要多个DNS 服务器共同提供服务之外，DNS 之间如果有上层、下属的关系时，该如何设置？ 也就是说，假设我的管理范围很大，我只想要负责上层的 DNS ，下层希望直接交给各单位的负责人来负责，要怎么设置呢？ 所以，bind可以将各个 subdomain (子域) 的管理权交给指定的的主机管理员去管理，如此一来， 域名设置会比较灵活，而且上层 DNS 服务器管理员也不用太麻烦！ 子域授权在我们当前的这种架构中不会部署，但是也提及一下 子域授权的配置相当简单，分为两个步骤 步骤1. master端配置我们只需要在master端的zone配置文件（注意是zone配置文件而不是named.conf）添加对应的NS记录和A记录即可 12sub.test.com.com. IN NS dns.sub.test.com.com.dns.sub.test.com.com. IN A 192.168.100.200 在这里，我们把sub.test.com.com.这个子域的解析交给dns.sub.test.com.com.这个域名对应的主机 因此，在这之后，例如www.sub.test.com.com.、aaa.sub.test.com.com.等域名都将由200这个主机提供解析服务 步骤2. 下层dns服务器上层 DNS 的设置非常简单！只要修改 zone file 即可 下层的dns服务器配置没有什么特殊的，按照正常的配置，只不过named.conf和zone配置文件中域名要设置成“sub.test.com.com” 1234zone &quot;sub.test.com.com&quot; IN &#123; type master; file &quot;zone/sub.test.com.com.zone&quot;;&#125;; view-视图解析配置根据客户端的ip地址，返回不同的zone解析记录，因此，我们就需要对同一个zone准备几个不同的配置 我们根据这个原则，进行如下的测试 当来源是192.168.11.0/24网段时，返回www.test.com的解析记录是192.168.11.80 当来源是非上述之外的所有网段时，返回www.test.com的解析记录是8.8.8.8 对不同的来源创建不同的zone文件 下面我们开始正式的配置 在named.conf配置文件中添加一下内容 view的配置分为2个步骤，一个是设置客户端的来源，这部分通过acl列表设置；另一个是编辑view块 acl列表12acl internal &#123;192.168.11.0/24;&#125;; acl external &#123;!192.168.11.0/24;any;&#125;; !表示反向选择的意思，也就是取反 第二行其实也可以写成如下的形式 1acl external &#123;!&quot;internal&quot;;any;&#125;; 当ip地址和网段过多时，可以采取导入外部文件的形式 文件内容如下： 123456789# cat CHINANET.aclacl &quot;CHINANET&quot; &#123;1.0.1.0/24;1.0.2.0/23;1.0.8.0/21;1.0.32.0/19;1.1.0.0/24;1.1.2.0/23;&#125; named.conf配置方式如下 导入 1include &quot;/home/named/bind/etc/CHINANET.acl&quot;; 调用 12view &quot;view_CHINANET&quot; &#123;match-clients &#123;CHINANET; &#125;; view区块配置1234567891011121314151617181920212223242526272829303132333435363738394041view &quot;internal&quot; &#123; match-clients &#123;&quot;internal&quot;;&#125;; zone &quot;.&quot; in&#123; type hint; file &quot;named.root&quot;; &#125;; zone &quot;localhost&quot; in&#123; type master; file &quot;localhost.zone&quot;; &#125;; zone &quot;0.0.127.in-addr.arpa&quot; in&#123; type master; file &quot;localhost.rev&quot;; allow-update &#123; none; &#125;; &#125;; zone &quot;test.com&quot; IN &#123; type master; file &quot;zone/test.com.zone.int&quot;; &#125;;&#125;;view &quot;external&quot; &#123; match-clients &#123;&quot;external&quot;;&#125;; zone &quot;.&quot; in&#123; type hint; file &quot;named.root&quot;; &#125;; zone &quot;localhost&quot; in&#123; type master; file &quot;localhost.zone&quot;; &#125;; zone &quot;0.0.127.in-addr.arpa&quot; in&#123; type master; file &quot;localhost.rev&quot;; allow-update &#123; none; &#125;; &#125;; zone &quot;test.com&quot; IN &#123; type master; file &quot;zone/test.com.zone.ext&quot;; &#125;;&#125;; 测试192.168.11.0/24网段主机 123456789101112131415161718192021[root@wxh-func-test-3 ~]# dig @192.168.11.91 -p 53 www.test.com; &lt;&lt;&gt;&gt; DiG 9.8.2rc1-RedHat-9.8.2-0.68.rc1.el6_10.1 &lt;&lt;&gt;&gt; @192.168.11.91 -p 53 www.test.com; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 43365;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0;; QUESTION SECTION:;www.test.com. IN A;; ANSWER SECTION:www.test.com. 86400 IN A 192.168.11.80;; Query time: 1 msec;; SERVER: 192.168.11.91#53(192.168.11.91);; WHEN: Tue Nov 27 14:14:26 2018;; MSG SIZE rcvd: 46[root@wxh-func-test-3 ~]# 其他网段主机 123456789101112131415161718192021➜ ~ dig @192.168.11.91 -p 53 www.test.com; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @192.168.11.91 -p 53 www.test.com; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 25915;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;www.test.com. IN A;; ANSWER SECTION:www.test.com. 86400 IN A 8.8.8.8;; Query time: 10 msec;; SERVER: 192.168.11.91#53(192.168.11.91);; WHEN: Tue Nov 27 14:13:07 CST 2018;; MSG SIZE rcvd: 57 dns服务器端的访问日志 123[root@Zabbix server log]# tailf query.log27-Nov-2018 14:13:12.955 queries: info: client @0x7fcf840be8e0 192.168.11.64#47324 (www.test.com): view internal: query: www.test.com IN A + (192.168.11.91)27-Nov-2018 14:13:18.323 queries: info: client @0x7fcf840be8e0 192.168.101.35#50891 (www.test.com): view extenal: query: www.test.com IN A +E(0) (192.168.11.91) 启动命令行启动通过查看源码包中提供的named.8，我们在启动的时候添加以下参数： -L logfile Log to the file logfile by default instead of the system log. -u user Setuid to user after completing privileged operations, such as creating sockets that listen on privileged ports. 启动命令： 1[root@Zabbix server chroot]# /home/named/bind/sbin/named -gc /home/named/bind/chroot/etc/named.conf 注意： 此处是在前台启动并且启动了调试模式，有问题会打印出出错信息。当调试正常后启动需要去掉g这个参数。 启动脚本内容如下： 1/home/named/bind/sbin/named -c /home/named/bind/chroot/etc/named.conf 启动脚本因为我们监听的是小于1024的知名端口，named用户默认没有权限，因此我们的启动脚本为系统用户执行，在启动命令中使用-u参数指定named用户 在后续使用过程中，有关bind服务都是管理员用户去执行（改操作很少会涉及，因为后续配置重载等操作使用rndc即完成） 启动脚本内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495[named@host1 etc]$ cat /etc/init.d/named#!/bin/sh# chkconfig: - 86 14# Source function library.. /etc/rc.d/init.d/functionsuser=&quot;named&quot;exec=&quot;/home/named/bind/sbin/named&quot;prog=&quot;named&quot;config=&quot;/home/named/bind/chroot/etc/named.conf&quot;#[ -e /usr/local/named/etc/sysconfig/$prog ] &amp;&amp; . /usr/local/named/etc/sysconfig/$proglockfile=/var/lock/subsys/namedstart() &#123; #[ -x $exec ] || exit 5 [ -e $config ] || exit 6 echo -n $&quot;Starting $prog: &quot; daemon $exec -c $config -u $user retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval&#125;stop() &#123; echo -n $&quot;Stopping $prog: &quot; killproc $prog retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval&#125;restart() &#123; stop start&#125;reload() &#123; echo -n $&quot;Reloading $prog: &quot; killproc $prog -1 retval=$? echo return $retval&#125;force_reload() &#123; restart&#125;rh_status() &#123; status $prog&#125;rh_status_q() &#123; rh_status &amp;&gt;/dev/null&#125;case &quot;$1&quot; in start) rh_status_q &amp;&amp; exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 restart ;; *) echo $&quot;Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload&#125;&quot; exit 2esacexit $? 功能测试我们使用dig命令进行测试 1# dig @192.168.11.91 -p 53 www.test.com 监控bind的监控从几个方面 端口号（53和953） 解析功能是否正常 主从同步是否正常 在这里使用zabbix去实现监控，那么这个监控模板的内容就如下所示： bind总结bind中有以下管理命令 named-checkconf。检查 named.conf 文件的句法 12例如：/home/named/bind/sbin/named-checkconf /home/named/bind/chroot/etc/named.conf named-checkzone。程序检查 host 文件的句法和相容性。 1234例如：[named@shidc-1 chroot]$ /home/named/bind/sbin/named-checkzone nidianwo.com var/named/zone/shidc/nidianwo.com.zonezone nidianwo.com/IN: loaded serial 2OK rndc(Remote Name Daemon Control)。rndc 允许系统管理员控制名称管理器的运行。这一部分下面详细说明 rndc命令rndc命令作为我们管理bind的一大利器，我们有必要对它进行掌握，这一部分就记录下常用的操作 我们首先看一下它的help输出： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118[named@host1 ~]$ rndc -hUsage: rndc [-b address] [-c config] [-s server] [-p port] [-k key-file ] [-y key] [-r] [-V] [-4 | -6] commandcommand is one of the following: addzone zone [class [view]] &#123; zone-options &#125; Add zone to given view. Requires allow-new-zones option. delzone [-clean] zone [class [view]] Removes zone from given view. dnstap -reopen Close, truncate and re-open the DNSTAP output file. dnstap -roll count Close, rename and re-open the DNSTAP output file(s). dumpdb [-all|-cache|-zones|-adb|-bad|-fail] [view ...] Dump cache(s) to the dump file (named_dump.db). flush Flushes all of the server&apos;s caches. flush [view] Flushes the server&apos;s cache for a view. flushname name [view] Flush the given name from the server&apos;s cache(s) flushtree name [view] Flush all names under the given name from the server&apos;s cache(s) freeze Suspend updates to all dynamic zones. freeze zone [class [view]] Suspend updates to a dynamic zone. halt Stop the server without saving pending updates. halt -p Stop the server without saving pending updates reporting process id. loadkeys zone [class [view]] Update keys without signing immediately. managed-keys refresh [class [view]] Check trust anchor for RFC 5011 key changes managed-keys status [class [view]] Display RFC 5011 managed keys information managed-keys sync [class [view]] Write RFC 5011 managed keys to disk modzone zone [class [view]] &#123; zone-options &#125; Modify a zone&apos;s configuration. Requires allow-new-zones option. notify zone [class [view]] Resend NOTIFY messages for the zone. notrace Set debugging level to 0. nta -dump List all negative trust anchors. nta [-lifetime duration] [-force] domain [view] Set a negative trust anchor, disabling DNSSEC validation for the given domain. Using -lifetime specifies the duration of the NTA, up to one week. Using -force prevents the NTA from expiring before its full lifetime, even if the domain can validate sooner. nta -remove domain [view] Remove a negative trust anchor, re-enabling validation for the given domain. querylog [ on | off ] Enable / disable query logging. reconfig Reload configuration file and new zones only. recursing Dump the queries that are currently recursing (named.recursing) refresh zone [class [view]] Schedule immediate maintenance for a zone. reload Reload configuration file and zones. reload zone [class [view]] Reload a single zone. retransfer zone [class [view]] Retransfer a single zone without checking serial number. scan Scan available network interfaces for changes. secroots [view ...] Write security roots to the secroots file. serve-stale [ yes | no | reset | status ] [class [view]] Control whether stale answers are returned showzone zone [class [view]] Print a zone&apos;s configuration. sign zone [class [view]] Update zone keys, and sign as needed. signing -clear all zone [class [view]] Remove the private records for all keys that have finished signing the given zone. signing -clear &lt;keyid&gt;/&lt;algorithm&gt; zone [class [view]] Remove the private record that indicating the given key has finished signing the given zone. signing -list zone [class [view]] List the private records showing the state of DNSSEC signing in the given zone. signing -nsec3param hash flags iterations salt zone [class [view]] Add NSEC3 chain to zone if already signed. Prime zone with NSEC3 chain if not yet signed. signing -nsec3param none zone [class [view]] Remove NSEC3 chains from zone. signing -serial &lt;value&gt; zone [class [view]] Set the zones&apos;s serial to &lt;value&gt;. stats Write server statistics to the statistics file. status Display status of the server. stop Save pending updates to master files and stop the server. stop -p Save pending updates to master files and stop the server reporting process id. sync [-clean] Dump changes to all dynamic zones to disk, and optionally remove their journal files. sync [-clean] zone [class [view]] Dump a single zone&apos;s changes to disk, and optionally remove its journal file. tcp-timeouts Display the tcp-*-timeout option values tcp-timeouts initial idle keepalive advertised Update the tcp-*-timeout option values thaw Enable updates to all dynamic zones and reload them. thaw zone [class [view]] Enable updates to a frozen dynamic zone and reload it. trace Increment debugging level by one. trace level Change the debugging level. tsig-delete keyname [view] Delete a TKEY-negotiated TSIG key. tsig-list List all currently active TSIG keys, including both statically configured and TKEY-negotiated keys. validation [ yes | no | status ] [view] Enable / disable DNSSEC validation. zonestatus zone [class [view]] Display the current status of a zone.Version: 9.12.3 现在开始讲解 概述rndc（Remote Name Domain Controllerr）是一个远程管理bind的工具，通过这个工具可以在本地或者远程了解当前服务器的运行状况，也可以对服务器进行关闭、重载、刷新缓存、增加删除zone等操作。 使用rndc可以在不停止DNS服务器工作的情况进行数据的更新，使修改后的配置文件生效。 在实际情况下，DNS服务器是非常繁忙的，任何短时间的停顿都会给用户的使用带来影响。因此，使用rndc工具可以使DNS服务器更好地为用户提供服务。在使用rndc管理bind前需要使用rndc生成一对密钥文件，一半保存于rndc的配置文件中，另一半保存于bind主配置文件中。rndc的配置文件默认路径为/etc/rndc.conf，在CentOS或者RHEL中，rndc的密钥保存在/etc/rndc.key文件中。rndc默认监听在953号端口（TCP），其实在bind9中rndc默认就是可以使用，不需要配置密钥文件。 rndc与DNS服务器实行连接时，需要通过数字证书进行认证，而不是传统的用户名/密码方式。在当前版本的rndc 和 named中，唯一支持的认证算法是HMAC-MD5，在连接的两端使用共享密钥。它为命令请求和名字服务器的响应提供 TSIG类型的认证。所有经由通道发送的命令都必须被一个服务器所知道的 key_id 签名。为了生成双方都认可的密钥，可以使用rndc-confgen命令产生密钥和相应的配置，再把这些配置分别放入named.conf和rndc的配置文件rndc.conf中。 常用命令rndc常用命令： status # Display status of the server 显示bind的相关信息 reload # Reload configuration file and zones. 重新加载所有配置文件和zone数据文件 reload zone [class [view]] #Reload a single zone reconfig # Reload configuration file and new zones only。重新加载配置文件以及zone文件(只涉及新的zone文件) flush [view] # 刷新服务器的所有高速缓存 stats # 将服务器统计信息写入统计文件中（统计文件的路径定义在namd.conf配置文件当中） dumpdb # 将cache高速缓存转储到转储文件 (文件的路径定义在named.conf配置文件当中) zonestatus zone [class [view]] # Display the current status of a zone 显示指定的zone的状态信息 notify zone [class [view]] # Resend NOTIFY messages for the zone 针对指定的zone发送通知消息 实际案例 status 显示bind运行状态 123456789101112131415161718[named@host1 etc]$ rndc statusversion: BIND 9.12.3 &lt;id:6c8e92c&gt; (bind 9.12.3)running on host1: Linux x86_64 2.6.32-642.el6.x86_64 #1 SMP Tue May 10 17:27:01 UTC 2016boot time: Fri, 14 Dec 2018 07:40:25 GMTlast configured: Mon, 17 Dec 2018 05:52:49 GMTconfiguration file: /home/named/bind/chroot/etc/named.confCPUs found: 4worker threads: 4UDP listeners per interface: 3number of zones: 336 (297 automatic)debug level: 0xfers running: 0xfers deferred: 0soa queries in progress: 0query logging is ONrecursive clients: 0/900/1000tcp clients: 0/150server is up and running reload单个zone的数据内容 12345[named@host1 etc]$ rndc reload dwd.gds-sh IN shidczone reload up-to-date注意：reload zone [class [view]]对应的实际执行格式是zone IN view这个执行之后就有通知从域复制的作用 单纯通知某个zone的信息 123456[named@host1 common]$ rndc notify dwd.gds-sh IN shidczone notify queued注意：经过实际的测试，使用该命令不能实现通知从节点复制域的功能这里只是手动修改了zone数据文件的searial，然后通知出去，但是由于没有reload，所以这个时候主节点的serial是没有变化的（需要执行reload加载到主程序内存中才算）所以这个只是单纯的通知，因为使用的serial还是加载到内存中的，并不是修改了文件后的，因为从节点并不会有动作 案例日志如下： 12345678910111213命令：rndc notify nidianwo.com IN hzidc21-Nov-2019 08:23:14.762 notify: info: client @0x7fa930038a10 10.11.0.128#46139/key hzidc: view hzidc: received notify for zone &apos;nidianwo.com&apos;: TSIG &apos;hzidc&apos;21-Nov-2019 08:23:14.762 general: info: zone nidianwo.com/IN/hzidc: notify from 10.11.0.128#46139: zone is up to date命令：rndc reload nidianwo.com IN hzidc21-Nov-2019 08:26:22.228 notify: info: client @0x7fa930038a10 10.11.0.128#40377/key hzidc: view hzidc: received notify for zone &apos;nidianwo.com&apos;: TSIG &apos;hzidc&apos;21-Nov-2019 08:26:22.228 general: info: zone nidianwo.com/IN/hzidc: notify from 10.11.0.128#40377: serial 521-Nov-2019 08:26:22.233 general: info: zone nidianwo.com/IN/hzidc: Transfer started.21-Nov-2019 08:26:22.236 general: info: zone nidianwo.com/IN/hzidc: transferred serial 5: TSIG &apos;hzidc&apos;21-Nov-2019 08:26:22.236 notify: info: zone nidianwo.com/IN/hzidc: sending notifies (serial 5) 从节点收到这个notify，检查序列号，发现和当前的一样，所以就会以：“zone is up to date”结束。 刷新某个view的缓存数据 1[named@host1 common]$ rndc flush shidc named.conf配置文件详解配置文件框架 常规配置配置 1234567891011121314151617181920include &quot;/path/file&quot;; # 加载外部文件key &quot;rndc-key&quot; &#123; # rndc相关配置&#125;;controls &#123; # rndc相关配置&#125;;options&#123; # 全局配置&#125;logging&#123; # 日志配置&#125;zone &quot;zone name&quot; IN &#123; # 区域（域名）配置&#125; view（视图）模式配置文件 1234567891011121314151617181920212223242526272829303132333435acl acl_name1 &#123;192.168.11.0/24;&#125;; # acl列表，用于后面调用，这部分通常置于最上方acl acl_name2 &#123;!192.168.11.0/24;any;&#125;; include &quot;/path/file&quot;; # 加载外部文件key &quot;rndc-key&quot; &#123; # rndc相关配置&#125;;controls &#123; # rndc相关配置&#125;;options&#123; # 全局配置&#125;logging&#123; # 日志配置&#125;view &quot;view_name1&quot; &#123; match-clients &#123;&quot;acl_name1&quot;;&#125;; # 定义该view匹配的网段 zone &quot;zone name&quot; IN &#123; # 区域（域名）配置 &#125;&#125;view &quot;view_name2&quot; &#123; match-clients &#123;&quot;acl_name2&quot;;&#125;; # 定义该view匹配的网络 zone &quot;zone name&quot; IN &#123; # 区域（域名）配置 &#125;&#125; # 注意：使用view之后，所有的zone都需要包含在view块中 我们拿下面这个配置文件进行案例说明 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109[named@host1 etc]$ cat named.confkey &quot;rndc-key&quot; &#123; algorithm hmac-sha256; secret &quot;vqOF9VUn75lvtpCYvYffOVNT8LkLK0z78UCPVkX1ofk=&quot;;&#125;;controls &#123; inet 127.0.0.1 port 953 allow &#123; 127.0.0.1; &#125; keys &#123; &quot;rndc-key&quot;; &#125;;&#125;;options&#123; listen-on port 53&#123; 192.168.103.99; &#125;; listen-on-v6 port 53&#123; fe80::20c:29ff:fefe:d1f4; &#125;; version &quot;bind 9.12.3&quot;; directory &quot;/home/named/bind/chroot/var/named&quot;; pid-file &quot;/home/named/bind/chroot/var/run/named.pid&quot;; session-keyfile &quot;/home/named/bind/chroot/var/run/session.key&quot;; dump-file &quot;/home/named/bind/chroot/var/named/data/cache_dump.db&quot;; statistics-file &quot;/home/named/bind/chroot/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/home/named/bind/chroot/var/named/data/named_mem_stats.txt&quot;; recursion yes; allow-query&#123; any; &#125;; allow-query-cache&#123; any; &#125;; allow-transfer&#123; 192.168.11.89; &#125;; notify yes; also-notify&#123; 192.168.11.89; &#125;; /* Path to ISC DLV key */ bindkeys-file &quot;/home/named/bind/chroot/etc/named.iscdlv.key&quot;; managed-keys-directory &quot;/home/named/bind/chroot/var/named/dynamic&quot;;&#125;;logging &#123; channel default_debug &#123; file &quot;/home/named/bind/chroot/log/named.run&quot; versions 10 size 128m; severity dynamic; print-category yes; print-severity yes; print-time yes; &#125;; channel queries_info &#123; file &quot;/home/named/bind/chroot/log/query.log&quot; versions 10 size 128m; severity info; print-category yes; print-severity yes; print-time yes; &#125;; category queries &#123; queries_info; default_debug; &#125;; channel notify_info &#123; file &quot;/home/named/bind/chroot/log/notify.log&quot; versions 10 size 128m; severity info; print-category yes; print-severity yes; print-time yes; &#125;; category notify &#123; notify_info; default_debug; &#125;; channel xfer_in_log &#123; file &quot;/home/named/bind/chroot/log/xfer_in.log&quot; versions 10 size 128m; severity info; print-category yes; print-severity yes; print-time yes; &#125;; channel xfer_out_log &#123; file &quot;/home/named/bind/chroot/log/xfer_out.log&quot; versions 10 size 128m; severity info; print-category yes; print-severity yes; print-time yes; &#125;; category xfer-in &#123; xfer_in_log; &#125;; category xfer-out &#123; xfer_out_log; &#125;;&#125;;include &quot;/home/named/bind/chroot/etc/acls/shidc&quot;;include &quot;/home/named/bind/chroot/etc/acls/hzidc&quot;;include &quot;/home/named/bind/chroot/etc/acls/aliyun-east1&quot;;include &quot;/home/named/bind/chroot/etc/views/shidc&quot;;include &quot;/home/named/bind/chroot/etc/views/hzidc&quot;;include &quot;/home/named/bind/chroot/etc/views/aliyun-east1&quot;; 某一段acl的配置 12345[named@host1 etc]$ cat /home/named/bind/chroot/etc/acls/shidcacl shidc &#123; 10.11.0.0/16; # 省略剩余网段，格式一致&#125;; 某一段view的配置 12345678910111213[named@host1 etc]$ cat /home/named/bind/chroot/etc/views/shidcview &quot;shidc&quot; &#123; match-clients &#123;&quot;shidc&quot;;&#125;; zone &quot;.&quot; in&#123; type hint; file &quot;named.root&quot;; &#125;; zone &quot;dianwoda.com&quot; IN &#123; type master; file &quot;zone/shidc/dianwoda.com.zone&quot;; &#125;; # 这里省略很多zone配置段&#125;; 注意：配置文件中每一行必须以;结尾，{} 里面两侧必须有空格 acl访问控制列表只有定义后才能使用，通常acl要定义在named.conf的最上方 除了我们自己定义的acl，在bind中有4个内置的acl any：任何主机 none：无一主机 local：本机 localnet：本机所在的网络 include include “/path/file”; 导入相关文件，为了保持主配置文件的简介，我们通常会把大段的配置文件使用include的方式进行加载，acl、view、zone等都可以使用include方式进行加载 key+controls定义rndc管理命令的相关配置 我们在文章的最开始配置章节就看到，使用了rndc-confgen命令来生成rndc文件。生成之后，需要把其中的密码信息等完整的复制进主配置文件当中 algorithm hmac-sha256; 密钥算法 secret “vqOF9VUn75lvtpCYvYffOVNT8LkLK0z78UCPVkX1ofk=”; 密钥内容 inet 127.0.0.1 port 953 rndc服务监听在本机的953端口 allow { 127.0.0.1; } keys { “rndc-key”; }; 只允许本机访问，使用的认证密码信息为前面定义的：rndc-key 有关rndc，请看下面的有关rndc章节 options这一部分主要定义全局的核心信息 listen-on port 53 {}; IPv4网络：指定提供dns服务的端口以及ip地址（很多服务器上可能会有多块网卡） listen-on-v6 port 53 {}; IPv6网络：指定提供dns服务的端口以及ip地址（很多服务器上可能会有多块网卡） version “version”; bind版本信息 directory “file_path”; zone区域文件存储目录 pid-file “file_path”; pid文件所在路径 session-keyfile “file_path”; dns安全方面的配置 dump-file “file_path”; 当执行rndc dumpdb命令时，服务器存放数据库文件的路径名。 statistics-file 当使用rndc stats命令的时候，服务器会将统计信息追加到的文件路径名。如果没有指定，默认为named.stats在服务器程序的当前目录中。 memstatistics-file “file_path”; 当服务退出的时候，会把内存使用情况统计写入到这个文件中 forwarders {}; 当设置为转发服务器时需要设置，在{}内设置上游服务器ip recursion yes; 开启递归功能，当接收到请求的域名不是自身所负责解析的域名（也就是没有对应的zone配置），这个时候，将自己变成dns客户端，向根服务器发送请求，获取解析记录之后再返回客户端。 如果是 yes，并且一个 DNS 询问要求递归，那么服务器将会做所有能够回答查询请求 的工作。如果 recursion 是 off 的，并且服务器不知道答案，它将会返回一个推荐(referral) 响应。默认值是 yes。注意把 recursion 设为 no;不会阻止用户从服务器的缓存中得到数 据，它仅仅阻止新数据作为查询的结果被缓存。服务器的内部操作还是可以影响本地 的缓存内容，如 NOTIFY 地址查询。 allow-recursion {}; 允许递归的白名单，配合上面的recursion yes使用 allow-query {}; 允许谁可以查询，也就是定义接收请求的来源，当设置为any之后表示可以接收所有人的域名请求 allow-query-cache {}; 允许谁可以查询域名记录的缓存数据，和上面一样，当设置为any之后表示可以接收所有人的域名请求 allow-transfer {}; 设定哪台主机允许和本地服务器进行域传输。allow-transfer也可以设置在zone语句中，这样全局options中的allow-transfer选项在这里就不起作用了。如果没有设定，默认值是允许和所有主机进行域传输 一般我们定义成从节点的网段 allow-update {}; 允许动态更新数据文件的主机白名单，一般设置为none notify yes; 如果是yes（默认），当一个授权的服务器修改了一个域后，DNS NOTIFY信息被发送出去。此信息将会发给列在域NS记录上的服务器（除了由SOA MNAME标示的主域名服务器）和任何列在also-notify选项中的服务器。 如果是explicit，则notify将只发给列在also-notify中的服务器。 如果是no，就不会发出任何报文。 注意：notify选项也可能设定在zone语句中，这样它就替代了options中的notify 语句。 also-notify {}; 定义一个用于全局的域名服务器IP地址列表。无论何时，当一个新的域文件被调入系统，域名服务器都会向这些地址，还有这些域中的NS记录发送NOTIFY信息。这有助于更新的域文件尽快在相关的域名服务器上收敛同步。 also-notify列表也可以配置在一个zone语句中，那么全局options中的also-notify语句就会在这里失效。 当一个zone-notify语句被设定为no，系统就不会向在全局中also-notify列表中的IP地址发送NOTIFY消息。缺省状态为空表(没有全局通知列表)。 logginglogging语句为域名服务器设定了一个多样性的logging选项。它的channel短语对应于输出方式、格式选项和分类级别，它的名称可以与category短语一起定义多样的日志信息。 只用一个logging语句就可以用来定义多个channel和category。 在BIND9中，logging的配置只有在整个配置文件被读取后才被执行。而在BIND8中，logging部分被读取后就开始执行了。当服务器启动时，所有在配置文件中关于语法错误的logging信息都转到缺省通道（channel）中，或者使用”-g”选项，指定转成标准错误。 channel所有日志会输出到一个或多个channel中；你可以定义所有你想要的通道。每个通道的定义必须包括一个目的字句，用来确定所选的相关通道的信息，将会被输出到一个文件，或者到一个特殊的syslog工具，或者到一个标准错误流，或者被忽略。它也可以随意的限制通道能接受的信息级别（默认值info），定义是否包含一个由named产生的时间标记，或者是否包含分类的名称、级别等（默认是不包含任何内容）。 目的子句为null时，会使所有发送给通道的信息被丢弃；那样的话，其他通道选项就没有意义了。 目的子句为file 时，会使通道的内容输出到一个磁盘文件。它可以包含这个文件的大小和该文件可以保存多少个版本。 如果使用versions日志文件选项，named就会自动保留多个版本的日志文件。例如，如果选择保存文件lamers.log的三个老版本，那么在它被打开的时候lamers.log.1被更名为lamers.log.2，lamers.log.0 被更名为lamers.log.1 ，lamers.log 被更名为lamers.log.0。也可以设置version unlimited，这样就没有备份版本的限制了。 如果对日志文件设置了size选项，那么仅当此文件超过了设定的大小时，系统就会进行更名。默认情况下不储存备份文件；所有存在的日志文件被简单进行追加。文件的size 选项用来限制日志的增长。如果文件超过了限制，又没有versions选项，则named 就会停止写入文件。如果保留了备份版本，则备份文件如上所述进行滚动命名，然后开始创建一个新的文件。如果没有versions选项，也没有其它的机制来删除或减小日志文件，则系统就不会有数据继续写入日志中。默认状态是不限制文件的大小的。 syslog 目的子句是把通道指向系统日志。它的参数是一个syslog的前缀，如syslog帮助中所述。syslog是怎样处理带有这些前缀的信息，可以参考syslog.conf 的帮助信息。 severity子句象syslog中的”priorites”一样工作，唯一区别的是用户可以直接写入一个文件，而不是使用syslog写入一个文件。不到严重级的信息将不会被通道选择；高严重级的信息将会被接受。 如果用户正在使用syslog，那么syslog.conf 的优先级也会决定什么会最终通过。例如，将channel facility和severity定义成daemon和debug，就不会只记录通过syslog.conf的daemon.warning信息，后者会使severity是info和notice的信息被丢弃。如果情况相反，named就会只记录warning或更高级别的信息，而syslogd则会记录来自于通道的所有信息。 stderr目的子句将通道输出到服务器的标准错误流。它用于服务器在前台运行的情况下，例如，当处于debug模式的时候，服务器能提供丰富的调试信息。如果服务器的全局debug级别（globe debug level）大于0，debug 模式将被激活。全局debug级别可以通过在启动named时设置“-d”参数加一个正数，或运行rndc trace来设置。如果要关闭debug模式，则将全局debug 级别设置成0，或运行rndc notrace。服务器中所有的debug信息有一个debug级别，高调试级给出更详细的输出。 如果使用了print-time参数，则日期和时间也将会记录下来。print-time也可以针对syslog的通道进行设置，但因为syslog也打印日期和时间，所以一般来讲，这没有什么意义。如果设置了print-category 参数，则信息的分类也会记录下来。如果设置了print-severity参数，则信息的严重级别也会记录下来。print-xxx 选项可以进行多重组合，单输出格式都是这个顺序：时间、分类、严重级别。 default_debug 通道有特殊的性质：只有当服务器的debug级别非0的时候，它才产生输出。一般来说，它会在服务器的工作目录中写入named.run文件。因为安全原因，当在命令行选项中使用了“-u”参数后，只有当named使用了新的UID后，named.run文件才会产生，以root身份启动和运行的named所产生的debug信息将会被丢弃。如果用户需要得到这些输出，则必须使用“-g”参数运行服务器，并重新将标准错误定向到一个文件中去。 一旦定义好一个通道，它就不能被重新定义。这样就不能修改内置的通道，但是可以通过把分类指向你已经定义的通道，来修改默认的日志记录。 category这里存在许多分类，用户可根据需要定义想看到或不想看到的日志。如果你不将某个分类指定到某些通道的话，那么在这个分类的日志信息就会被发送到default分类通道中。如果用户没有设定缺省的分类，下列”default”则会被系统使用： 1category &quot;default&quot; &#123; &quot;default_syslog&quot;; &quot;default_debug&quot;; &#125;; 作为一个例子，假定你要在文件中记录安全事件，但您也要保留缺省的日志文件。最好按照下面配置： 123456789101112channel &quot;my_security_channel&quot; &#123;file &quot;my_security_file&quot;;severity info;&#125;;category &quot;security&quot; &#123;&quot;my_security_channel&quot;;&quot;default_debug&quot;;&#125;;# 为了丢弃一个分类中的所有信息，可以设定null 通道：category &quot;xfer-out&quot; &#123; &quot;null&quot;; &#125;;category &quot;notify&quot; &#123; &quot;null&quot;; &#125;; 下面是简单写这里用到的分类和相关的简明描述，更多内容可以查看官网提供的bind9管理员手册 default 没有配置的分类会使用default的分类日志配置 notify notify协议相关日志 queries dns解析请求日志 xfer-in 主从域传输相关日志（入方向） xfer-out 主从域传输相关日志（出方向） viewview试图段的配置相对来说比较简单，主要分为2个部分，通过上面的案例我们就可以很直观看出来： 使用match-clients匹配来源ip 将要解析的zone都放置在view区块内，匹配来源之后，将使用对应的zone文件数据去响应请求 zonezone可以单独存在或者包含在view当中，不管哪种方式配置格式都是一样的。 type master; type字段说明了当前服务器所承担的任务职责，type可为： hint。根角色 master 主角色。可以响应该域名的解析请求。数据源为本地的zone文件 slave 从角色。可以响应该域名的解析请求。不使用自身的数据，数据源为从master端同步的zone数据 forward 转发角色。不直接响应域名解析请求，接受到请求之后，将请求直接转发给配置的上游服务器 zone配置文件详解123456789101112$TTL 86400@ IN SOA test.com. admin.test.com. ( 2018121401 ; serial (d. adams) 3H ; refresh 15M ; retry 1W ; expiry 1D ) ; minimum IN NS dns.test.com. IN MX 5 maildns IN A 192.168.11.49mail IN A 192.168.11.100www IN A 192.168.11.80 $TTL 86400资源记录的缓存超时时间，为了简化 RR 记录的设置，因此我们可以将 TTL 挪到最前面统一设定，在这里相当于是一个默认值，针对所有记录生效 单位是s，例如这里是86400秒，也就是一天 @第一列的这个符号代表 zone 对应域名，例如写在 test.com.zone 中，@ 代表 test.com.（注意不是根据名称决定的，而是根据named.conf中的配置） 在下面的配置中，第一列可以不写，那么将会继承这个配置 SOA如果你有多个 DNS 服务器管理同一个领域名时，那么最好使用 master/slave 的方式来进行管理。 既然要这样管理， 那就得要宣告被管理的 zone file 是如何进行传输的，此时就得要 SOA (Start Of Authority) 的标志了。 SOA 主要是与zone有关，SOA 后面共会接七个参数，这七个参数的意义依序是： 负责解析的域名 管理员的 email：发生问题可以联络这个管理员。要注意的是， 由于 @ 有特殊意义的，因此这里就将 admin@test.com 改写成 admin.test.com 序号 (Serial)：这个序号代表的是这个数据库档案的新旧，序号越大代表越新。 当 slave 要判断是否主动下载新的数据库时，就以序号是否比 slave 上的还要新来判断，若是则下载，若不是则不下载。 所以当你修订了数据库内容时，记得要将这个数值放大才行！ 为了方便用户记忆，通常序号都会使用日期格式『YYYYMMDDNU』来记忆，例如 2010080369 序号代表 2010/08/03 当天的第 69 次更新的感觉。不过，序号不可大于 2 的 32 次方，亦即必须小于 4294967296 一共10位数才行。 也就是说一天最多能更新99次，当到达99次之后，就不得不使用明天的日期，所以当场景的变更次数很频繁的时候，需要思考使用其他方式来定义序列号 更新频率 (Refresh)：那么什么时候 slave 会去向 master 要求数据更新的判断？ 就是这个数值定义的。 这里设置为每 10800 秒（3小时）进行一次 slave 向 master 要求数据更新。每次 slave 去更新时， 如果发现序号没有比较自身的大，那就不会下载数据库档案。 失败重新尝试时间 (Retry)：如果因为某些因素，导致 slave 无法正常访问 master， 那么在多久的时间内，slave 会尝试重新联机到 master。在这里设置为900 秒（15分钟）会重新尝试一次。意思是说，每 3小时slave 会主动向 master 联机，但如果该次联机没有成功，那接下来尝试联机的时间会变成 15分钟。若后来有成功，则又会恢复到3小时再一次联机。 失效时间 (Expire)：如果一直尝试失败，持续到到达这个时间， 那么 slave 将不再继续尝试联机，并且尝试删除这份下载的 zone file 信息。这里设置为 604800 秒（1周）。意思是说，当联机失败，每 15分钟进行一次尝试，直到到达1周后，slave 将不再更新，只能等待系统管理员的处理。 Minimum部分，这个部分定义了DNS对否定回答(NXDOMAIN即访问的记录在权威DNS上不存在)的缓存时间。 除了 Serial 不可以超过 2^32 次方之外，还有一些其他的限制： Refresh &gt;= Retry *2 Refresh + Retry &lt; Expire Expire &gt;= Rrtry * 10 Expire &gt;= 7Days 一般来说，如果 DNS RR 记录变更情况频繁，那么上述的相关数值可以设置的小一些，如果 DNS RR 是很稳定的， 为了节省带宽，则可以将 Refresh 设置的较大一些。 注意事项 再次强调，一个正向解析的RR数据库中，至少应该要有 $TTL, SOA, NS 如果是写完全的域名，RR记录中在最后一定要带上”.”。不写完整的，则系统会进行自动补全 在zone配置文件中，master和slave都需要添加NS记录，并对对应主机还需要添加A记录 有关rndc在服务启动之后，我们可以看到服务器的监听端口情况： 123456[root@Zabbix server etc]# netstat -unptl | grep namedtcp 0 0 192.168.11.91:53 0.0.0.0:* LISTEN 14861/namedtcp 0 0 127.0.0.1:953 0.0.0.0:* LISTEN 14861/namedtcp 0 0 :::53 :::* LISTEN 14861/namedudp 0 0 192.168.11.91:53 0.0.0.0:* 14861/namedudp 0 0 :::53 :::* 14861/named 那么为什么会开启953端口呢？ 其实这就是所谓的 rndc 了。rndc 是 BIND version 9 以后所提供的功能，他可以让你很轻松的管理你自己的 DNS 服务器。包括检查已经存在 DNS 当中的资料、更新某个 zone 而不需要重新启动整个 DNS ， 以及检查 DNS 的状态与统计数据。 不过，因为 rndc 可以很深入的管理你的 DNS 服务器，所以当然要进行一些管控。控管的方式是通过rndc 的设置创建一个密钥 (rndc key)，并将这个密钥相关信息写入 named.conf 配置文件当中。重新启动 DNS 后，你的 DNS 就能够藉由 rndc 这个命令管理！ 事实上，新版的 distributions 通常已经帮你主动的建立好 rndc key了。 关于rndc的更多内容可以看鸟哥的文章：http://cn.linux.vbird.org/linux_server/0350dns.php 启动用户在测试的时候，我们可以使用root用户启动，实际在生产环境中运行的时候，我们需要使用named这个普通用户去启动服务，这里一定要注意 递归查询一般客户机和服务器之间属递归查询，当客户机向DNS服务器发出请求后,若DNS服务器本身不能解析,DNS服务器则会向另外的DNS服务器发出查询请求，得到结果后转交给客户机。 当客户端的请求域名不在本地named.conf中配置的zone区块中，并且缓存中也没有的话，那么就会开启递归查询，为这个客户端去请求这个域名对应的ip。 如果请求的域名在zone配置中，但是zone的数据文件中没有这条记录，那么解析失败，server将不会再为这条记录去进行递归查询 因此： 如果是完全作为一个真正的权威服务器，那么不建议开启递归查询功能（默认为开启） 如果是既要作为权威服务器又要作为第一级的dns服务器，提供递归功能，那么可以开启 相关问题主从数据同步 Master和Slave 的数据库，都会有一个代表该数据库新旧的序列号，这个序号数值的大小，会影响是否要更新的动作， 至于更新的方式主要有两种： Master 主动告知：例如在 Master 在修改了数据库内容，并且加大数据库序号后， 重新启动 DNS 服务或者reload文件之后， master 会主动告知在配置文件中定义好的notify列表来更新数据库，此时就能够达成数据同步； 由 Slave 主动提出要求：Slave 会定时的向 Master发起请求，查看数据库的序号， 当发现Master的序号比 Slave 自己的序号还要大 (代表比较新)，那么 Slave 就会开始更新。如果序号不变， 那么就判断数据库没有更动，因此不会进行同步更新。 由上面的说明来看，其实设计数据库的序列号最重要的目的就是让 master/slave 数据的同步化。那我们也知道slave 会向 master 提出数据库更新的需求，问题是，多久提出一次更新，如果该次更新时由于网络问题，所以没有查询到 master 的序号 (亦即更新失败)，那隔多久会重新更新一次？这个可以查看上面的 SOA 的标志。 Name Servers in Multiple Roles导致不能实现域名穿透DNS服务器的类型； 主DNS服务器：维护所负责解析的域内解析库服务器；解析库由管理维护；读写操作均可进行； 从DNS服务器：从主DNS服务器或从其他的从DNS服务器那里区域传递(类似“复制”)一份解析库，只能进行读操作 缓存DNS服务器：负责代理客户机的递归查询工作，并且采用迭代查询的服务器 转发器：如果目标域名在本DNS服务器辖区内，直接转发 希望实现： 当自主dns的配置中没有请求的域名时，本地dns服务器进行迭代查询，查找dnspod或者其他上的相关记录，然后将对应信息返回给dns客户端 例如：本机可以解析test.com这个域，但是不包含www这条A记录，当有请求来的时候，希望实现dns服务器可以作为客户端，从internet上找寻对应记录信息。 存在问题： The BIND name server 能同时拥有多种角色，可以是作为权威服务器（主或者从服务器）专门负责解析，缓存服务器（递归服务器，也就是本地dns服务器）负责处理dns客户端的请求 但是，权威服务器和本地服务器的功能是冲突的，因为建议将这2个分开部署 原因是处于安全性和可靠性，权威服务器不能实现递归功能，也就是说本地没有记录的话，不会再作为客户端帮助末端去请求 只要当本地dns配置中存在某个域名的时候，就算配置数据库中不包含dns客户端请求的记录，也不会进行迭代查询，因此，最终会导致解析失败 问题解决： 因为本身的协议限制，无法实现，所以在实例应用到公司时，需要维护全量的域名资源记录。 注意事项 修改zone数据文件在每次存盘时要注意增加Serial值，主要用来让辅助服务器同步主服务器的区域数据文件。 使用绝对域名时千万别忘了后面要带”.”。 主配置文件named.conf的”;”不能少。 通常 DNS 查询的时候，是以 udp 协议来查询的， 但是万一没有办法查询到完整的信息时，就会再次的以 tcp 来重新查询的！所以启动 DNS 的 daemon (就是 named ) 时，会同时启动 tcp 及 udp 的 port 53 ,所以如果涉及到一些防火墙配置的时候，记得需要同时放行 tcp, udp port 53 bind的RPZ配置-构建混合dnsRPZ：Response Policy Zone 大部分人使用RPZ主要是提供下列这些功能 支持持本地未维护主机记录的走公网查询（默认解析失败）等 override某些域名，让其走公网查询。因为rpz的优先级比对应zone更高 把部分做NDS反射放大攻击的域名封掉 屏蔽部分暴力,黄色,诈骗网站 把内部域名屏蔽掉,直接返回NXDOMAIN 在这里，我们主要使用rpz实现第一种功能 默认配置下，如果我们有某个zone的配置，那么这个域名相关的请求，都会通过指定的数据文件进行解析，如果数据文件中没有这个主机记录，那么将会解析失败。 而在现实环境中，我们更多的是希望，如果一个主机记录没有被记录在这个zone数据文件当中，这个权威服务器就会开启递归模式，自身去公网解析之后，再把结果返回给客户端。 而在bind9，rpz就是实现这个功能的一个折中方案。 为什么说它是折中方案呢，主要是它的功能是： 支持对zone数据文件中没有的主机记录进行公网递归查询 只不过递归查询的域名需要我们手动指定。只会对配置文件中有的域名进行递归查询。 rpz数据文件和zone数据文件中都没有的域名将会解析失败。 配置rpz的配置主要分为3个部分 步骤1：主配置文件中配置 在named.conf的option配置模块中添加： 1response-policy &#123; zone &quot;rpz.zone&quot; policy given; &#125;; 注意：zone的名称（在这里是rpz.zone）不能设置为其他有效zone。否则会导致有效zone无法正常解析，也会导致rpz失效 步骤2：在zone或者view中配置 当使用view的时候，所有的zone都需要在所有view中添加 在view目录下的所有view配置文件中添加内容 12345[named@node001 etc]$ vim views/shidc 添加以下内容 zone &quot;rpz.zone&quot; IN &#123; type master; file &quot;zone/shidc/rpz.zone&quot;; &#125;; 步骤3：编辑zone数据文件 12345678910111213[named@node001 common]$ cat rpz.zone$TTL 86400@ IN SOA localhost. root.localhost. ( 1 ; serial 3H ; refresh 15M ; retry 1W ; expiry 1D ) ; minimum IN NS localhost.www.xxx.com IN A 1.1.1.1www.xxx.com IN A 1.1.1.1www3.xxx.com A 1.1.1.2mail.xxx.com CNAME mail.mxhichina.com. rpz的zone有几个注意事项： 主机记录需要是完整的域名，但是注意最后没有“.” 记录类型之前不需要写IN CNAME等对应的内容如果是域名的话，则这个域名需要添加“.” NS记录需要写localhost，末尾需要添加“.” bind的rate-limit-防止dns放大攻击放大攻击（也称为杠杆攻击，英文名字DNS Amplification Attack），属于ddos的一种，利用回复包比请求包大的特点（放大流量），伪造请求包的源IP地址，将应答包引向被攻击的目标主机，从而导致目标主机不能正常提供服务。 rate-limit真实的是在限制查询速率，当查询者速度过快时，便视为ddos攻击者而不回应查询。 在bind9中，我们可以设置rrl模块进行相关的限制 rrl的配置可以参考： 12345678910111213rate-limit &#123; responses-per-second 20; nodata-per-second 10; nxdomains-per-second 10; errors-per-second 10; //all-per-second 60; ipv4-prefix-length 32; max-table-size 10000; slip 2; //log-only yes ; qps-scale 50000; window 5;&#125;; 其中ipv4-prefix-length设置掩码为32位，就是对每个IP都独立限速， responses-per-second是对每个客户端响应速度上限。qps-scale是一个系数，比如设置qps-scale 250; responses-per-second 20，当访问的qps是1000的时候，对单个ip的限速就变成了250/1000*20=5。 配置参数在71页 详情讲解在官方文档第126页 官方支持的参数是（前面是参数，后面是数据类型）： 1234567891011121314rate-limit &#123;all-per-second integer;errors-per-second integer;exempt-clients &#123; address_match_element; ... &#125;; ipv4-prefix-length integer;ipv6-prefix-length integer;log-only boolean;max-table-size integer;min-table-size integer;nodata-per-second integer; nxdomains-per-second integer;qps-scale integer;referrals-per-second integer; responses-per-second integer;slip integer;window integer;&#125;; 配置参数讲解： all-per-second。定义全局的限速值，至少应该为其他值的4倍。 errors-per-second：结果是除NXDOMAIN之外其他error代码（例如SERVFAIL and FOR- MERR）的响应限速。默认使用responses-per-second值。 exempt-clients { address_match_element; … }; ipv4-prefix-length：v4掩码位数，默认为24。如果为32位，就是对每个IP都独立限速 ipv6-prefix-length：v6掩码位数，默认为56。如果为128位，就是对每个IP都独立限速 log-only：值为布尔型。可以设置为真，进入调试模式进行调试 max-table-size integer; min-table-size integer; nodata-per-second：有效域名但是数据为空响应的响应限速。默认使用responses-per-second值。 nxdomains-per-second。结果是NXDOMAIN的响应的限速。默认使用responses-per-second值。 qps-scale：定义接收请求数的上限，如果接收的请求超过这个值，那么相应的，responses-per-second这些值也会受到影响。如果这个值是500，响应限速是20。那么当接收到的请求数为1000的时候，响应限速会自动变成：500/1000 * 20 = 10。自动的变成1/2 referrals-per-second integer; responses-per-second：有效域名并且是非空数据响应的响应限速。 slip：默认值为2，取值范围为0-10。切割的尺度，几等分。将响应进行截断，比如发往某个ip有10个响应，如果使用默认值2，就会是只发送5个，drop掉5个。如果是5，那么只发送2个，drop掉剩余的8个。当slip=0 ，表示不切割，所有的响应会被drop。当slip=1时发送所有的响应，只不过是1个1个发送。当值为2-10时，只发送一个等分的数据 window ：单位为秒。设置跟踪的窗口时间，默认是15秒。window定义一个限速周期 bind chroot设置bind监控bind的监控指标： number_of_zones_automatic number_of_zones_custom tcp_clients recursive_clients status：up/down configuration file cpu占用 内存占用 运维管理添加zone以rpz为例 操作步骤如下： 主节点上添加对应的配置 从节点named.conf配置文件中添加配置-【仅rpz需要，一般模式不需要】 从节点view中添加配置，master指向主节点 重启从节点 注意从节点上的named.conf中需要添加rpz的配置参数,因为他是特殊的zone，如果是一般的zone则不需要配置 删除zone匹配上面的添加zone，操作顺序如下 从节点上删除掉相关的配置，然后reload 然后主节点上去掉相关的配置，reload 添加view操作步骤如下： 主节点上定义这个view的秘钥，添加入配置文件中-这里操作的是named.conf配置文件 添加这个view的acl， named.conf中添加秘钥，include进acl配置文件和view配置文件 添加view的配置，注意view中的zone配置 主节点执行reload 从节点在named.conf配置文件中添加配置-这个view的秘钥 ps：view的秘钥可以使用这个命令生成：rndc-confgen &gt; view_name.key 删除view从节点操作： 从节点named.conf配置文件中去掉秘钥、acl和view的配置 从节点reload 配置文件中去掉相关的配置就已经算是删除了view，bind进程不会加载这部分的数据，如果需要物理的删除，可以选择把相关的文件进行删除。 去掉这个view之后，原有的请求过来之后，因为没有匹配到acl列表，所以将会被refused拒绝掉 主节点操作： named.conf配置文件中去掉相关的配置项，主要是秘钥、acl、view。 执行reload 案例-多机房异地多活dns建设背景在实现多机房异地多活时，将服务分散到多个机房之后，一个核心的问题就是自建DNS，实现根据不同来源的ip段，返回相对应zone中的nginx服务端ip地址，只有这样才能 需求概述需要实现的功能如下： DNS服务端采取主从结构，阿里云端为master，IDC机房端为slave，数据保持一致 阿里云VPC内网IP地址作为dns客户端访问时，返回VPC内机房的nginx服务器IP地址 IDC机房内网IP地址作为dns客户端访问时，返回IDC机房的nginx服务器IP地址 杭州的公网IP地址访问SLB的公网地址，上海的公网IP地址访问IDC机房的IP地址 总结：dns客户端请求到的永远是最距离最近服务器 bind部署需要明确的几个问题 主从关系 暂定：阿里云为主，其他为从 服务器配置 阿里云端需要重新购置机器吗？是的话可用区及配置？ IDC机房端的机器规划，挑一台common机器 匹配ip转发规则 根据阿里云网段、机房网段等不同网段进行匹配转发 dns客户端请求域名之后，分别返回阿里云nginx服务的地址，机房的nginx服务器对应的vip 因此，阿里云端的请求的域名就有2条记录，机房端就一条记录 服务器端dns服务器配置 服务端预计配置 123# cat /etc/resolv.confnameserver 机房1dns服务器ip nameserver 机房2dns服务器ip 所有的dns客户端，在请求dns服务的时候 ，可能会连接到距离较远的服务器，但是获取记录之后将不会有影响。这里的第一次开销就保持这样吗？还是说这个也需要做优化，暂时没想到太好的方法 域名记录同步实现方式【这个目前是主要矛盾】 数据源为dnspod 如何同步dnspod上的记录到自建dns（我的想法：每隔3/5分钟去调用dnspod的api获取结果，根据对应的公网ip，去判断是阿里云还是机房的记录，然后追加进对应的zone数据文件，并且增大复制偏移量的值） 那么这个时候就存在一个问题：当一个域名只存在一个地方的时候，其他地域的主机将无法访问，因为没有记录到对应的zone数据文件中。所以需要每个地域都配置并且注册到dnspod上 主服务器有数据更新之后，触发通知的实现方式 我这边看下是否有更好的方式，没有的话暂时使用reload的方式 移动端配置？ 移动端需要考虑吗？ 总结： 阿里云走内网的slb 并不是所有的域名都需要配置成为多机房，只有一些特定的域名需要配置成为多记录的方式 当只有一条记录的时候，我们所有的配置文件中都配置成为这一个记录 当有3条记录的时候，我们将数据分发到3个配置文件当中 当只有一条记录的时候，每个数据文件都记录这个内容，保证解析成功 架构规划这里是拓扑图讲解 主从结构 Bind 视图和zone功能，实现根据来源ip进行判断 现在的所有域名导入 部署实施目录分层并不是所有的域名都需要根据地域不同返回不同的ip 我们根据不不同的地域创建了不同的zone目录，目录下有相同名称的zone数据文件 所以这个时候我们就可以进行匹配，当匹配到一个域名只有一个解析ip的时候，我们就把这条记录写入到3个不同的目录下的相同域名zone文件中 当匹配到有3个解析ip的时候，我们就对这个信息做处理，把对应的ip写入到对应的目录下的zone文件当中。 zone配置common123[named@host1 common]$ pwd;ls/home/named/bind/chroot/var/named/zone/commondwb.dgt.zone dwd.gds-sh.zone ecs.east1-b.zone ecs.east1-e.zone ecs.east1-g.zone zone文件配置模板： 123456789101112131415161718[named@host1 common]$ cat dwd.gds-sh.zone$TTL 86400@ IN SOA dwd.gds-sh. admin.dwd.gds-sh. ( 1 ; serial 3H ; refresh 15M ; retry 1W ; expiry 1D ) ; minimum IN NS dns.dwd.gds-sh.dns IN A 10.11.4.11dns IN A 10.11.4.12dns IN A 10.10.10.72dns IN A 10.10.10.73dns IN A 172.24.139.193dns IN A 172.24.139.194lvs001 IN A 10.11.0.11lvs002 IN A 11.11.0.12 说明： 为了便于后期分析及拍错等操作，我们在每个zone配置文件中都添加上6条ns记录 数据写入因为dnspod上的数据都是公网地址 所以要进行匹配，匹配公网之后，替换为对应的内网负载均衡的ip地址 优化进阶最终实现目标是编写一个平台去管理，在当前时间节点下，先只能做到通过web框架去编写api实现下面这些操作 主节点zone数据信息变更写入在master节点上启动django服务，提供api，当数据源有变化时，对端发送POST请求，参数为： 收到请求之后进行处理 注意：因为这个很重要，所以事先定义用户名和密码，生成token，然后才运行更新数据 写完之后，序列号更新为当天的时间，例如：2007041501 如果发现当天已经存在，那么在当前的序列号上+1 修改和写入一样，在master节点上启动django服务，提供api，当数据源有变化时，对端发送POST请求 删除和写入一样，在master节点上启动django服务，提供api，当数据源有变化时，对端发送delete请求 针对主机名的配置，我们不使用提供接口的方式，对主机名的配置，我们编写程序作为客户端，定时的去扫描数据源（阿里云或者jumpserver端） 主从数据同步主从数据同步主要分为2种 一种是主节点数据变化之后，主动的触发通知机制，发送通知消息给所以slave节点。 另一种是从节点定期去连接主节点，获取序列号信息，与当前的值进行比较，如果获取的更大则发起数据同步 结合实际情况，我们选择第一种方式 那么，当数据变更之后，如何不适用rndc reload的方式 序列号增长问题因为序列号最多为10位数，所以使用传统的日期表示法 dns服务稳定性问题注意事项主从节点两边的acl列表需要保持一致，这部分数据是不会同步的 会同步的只是zone数据 因此在启动之前，一定要确保主配置文件、acl列表、view配置等都保持一致 因为主的多个view会共用一些zone（主机名），所以这些zone，view配置中的路径可以一致 但是从上，每一个view之间不能存在这种通用文件。 slave的ip地址不止应该在named.conf主配置文件当中定义 在acl列表中也应该写明，不然会被refused 在master上：所有的acl列表需要包含slave节点ip 在salve节点上，所有的acl列表需要包含master节点的ip 不然： 主从同步开始时，slave向master发送soa时会被拒绝 主修改之后发送notify，slave将会拒绝这个消息，因此来源ip不在acl中 同时存在3个view，在主上修改了view3或view2，但是slave上收到的信息始终是view1的notify 在这里，只有shidc这个view能收到信息 主节点 acl配置阿里云acl： vpc的网段 【必须有】 阿里云2台dns的ip 杭州acl： 杭州机房网段 【必须有】 杭州机房2台dns服务器的ip 允许这2台主机发送soa请求并给予响应 上海机房acl 上海机房网段 【必须有】 上海机房2台dns服务器 从节点acl配置 vpc的网段 【必须有】 上海主节点的ip地址。为了主节点发送notify发送消息时不拒绝 杭州acl： 杭州机房网段 【必须有】 上海主节点的ip地址。为了主节点发送notify发送消息时不拒绝 上海机房acl 上海机房网段 【必须有】 上海主节点的ip地址。为了主节点发送notify发送消息时不拒绝 因为从节点需要做备份，所以从节点上需要维护3个view，而不只是所在地域的这一个网段 测试：有变化的只是杭州idc和阿里云 测试客户端 hzidc: common008-dev 192.168.11.88 Aliyun: common010-dev 192.168.11.76 shIDC：common007-dev 192.168.11.84 notify配置注意，notify的配置需要写在每一个zone里面，不能写在主配置文件当中 master发送的notify信息中不能携带view信息，slave接收的时候，根据本身的view的配置进行匹配 如果一个网段存在于多个view当中，那么只有第一个生效，也就是说，如果主节点更新的是view3的数据，如果view1收到之后，那么将会更新view1中的zone信息 又因为master的ip必须存在于acl中（需要允许master发送的notify，不设置的话主节点发送的消息会被拒绝）， 但是因为acl是从上到下匹配的 因此slave只能有一个view，也就是说除了主上维护的是全量的数据，其他从节点维护的只是一个view的数据 也就是说，如果存在同一个ip，那么不管配置几个view，同时生效的只会有一个view 从节点直接显示文本常见问题多view主从同步问题多个view的主备同步主要是是主备之间每个view都使用共享key进行消息的签名。master的配置和之前的稍微有点小的改动 大家经常使用bind的时候是划分不同的view的，因为每个view的zone需要单独修改，所以人肉修改是比较麻烦的。这个时候可以使用nsupdate进行批量的操作。只要注意每个view使用正确的记录就行。 每个view指定一个对应key进行能更新 最终的实现目的：主从上的数据完全一致，也就是 参考文献： bind多个view的主备同步 数据目录路径问题当有例如主机名等数据的时候，数据目录还是不能复用，需要在每个view中单独分开。 例如，我现在有3个view，主机记录这个数据在3个view中对应的数据其实是一样的，不需要考虑来源ip的事情，这个时候我想在zone目录下定义一个common目录，存在这些zone数据文件 但是实际情况是，bind不支持这种操作，如果这么设置了，那么，在启动的时候就会报错： 123[named@shidc-2 chroot]$ /home/named/bind/sbin/named-checkconf /home/named/bind/chroot/etc/named.conf/home/named/bind/chroot/etc/views/hzidc:47: writeable file &apos;zone/common/ecs.east1-g.zone&apos;: already in use: /home/named/bind/chroot/etc/views/shidc:48...... acl配置注意事项 从节点上 从节点上需要在acl中去掉主的ip。 是因为当主节点的ip在acl的网段中的时候，当匹配到acl时，就只会限定在这个view当中，数据同步会有问题。 会导致：收到主节点的notify消息会acl匹配到的view，然后会发送这个view的query去请求数据。 案例日志： 12345678910111220-Nov-2019 15:18:44.380 notify: info: client @0x7fa9300a1220 10.11.0.128#51049/key shidc: view shidc: received notify for zone &apos;dwd.gds-sh&apos;: TSIG &apos;shidc&apos;20-Nov-2019 15:18:44.381 general: info: zone dwd.gds-sh/IN/shidc: notify from 10.11.0.128#51049: serial 123820-Nov-2019 15:18:44.382 general: info: zone dwd.gds-sh/IN/shidc: Transfer started.20-Nov-2019 15:18:44.383 general: info: zone dwd.gds-sh/IN/shidc: transferred serial 1238: TSIG &apos;shidc&apos;20-Nov-2019 15:18:44.384 notify: info: zone dwd.gds-sh/IN/shidc: sending notifies (serial 1238)20-Nov-2019 15:18:45.383 notify: info: client @0x7fa9300a1220 10.11.0.128#33586/key hzidc: view shidc: received notify for zone &apos;dwd.gds-sh&apos;: TSIG &apos;hzidc&apos;20-Nov-2019 15:18:45.383 general: info: zone dwd.gds-sh/IN/shidc: notify from 10.11.0.128#33586: zone is up to date20-Nov-2019 15:18:45.884 notify: info: client @0x7fa9300a1220 10.11.0.128#33586/key aliyun-east1: view aliyun-east1: received notify for zone &apos;dwd.gds-sh&apos;: TSIG &apos;aliyun-east1&apos;20-Nov-2019 15:18:45.885 general: info: zone dwd.gds-sh/IN/aliyun-east1: notify from 10.11.0.128#33586: serial 123820-Nov-2019 15:18:45.889 general: info: zone dwd.gds-sh/IN/aliyun-east1: Transfer started.20-Nov-2019 15:18:45.894 general: info: zone dwd.gds-sh/IN/aliyun-east1: transferred serial 1238: TSIG &apos;aliyun-east1&apos;20-Nov-2019 15:18:45.895 notify: info: zone dwd.gds-sh/IN/aliyun-east1: sending notifies (serial 1238) 从节点上应该收到hzidc这个key的notify，实际上对应的view是hzidc，但是acl会把这个view转变成为shidc，所以从节点就变成去请求shidc这个view的数据，但是这个view的数据上面已经请求过并且更新了，所以会输出： 120-Nov-2019 15:18:45.383 general: info: zone dwd.gds-sh/IN/shidc: notify from 10.11.0.128#33586: zone is up to date 表示这个view的数据已经是最新状态。 而实际的hzidc这个view的数据却是没有被更新的。 主节点上 主节点上也需要在acl中去掉从的ip。因为当从节点的ip在范围内的话，发送的query请求就会被限定在某一个view当中，那么返回的数据也只是这一个view的数据。例如，上海机房的从节点访问过来，想要获取3个view中的xxx.com的数据，但是主节点匹配到这个ip是属于上海机房，那么只会返回上海机房这个view的数据。 最终会导致，从节点上3个view上的数据都是上海机房这一个view的数据。 案例日志： 123420-Nov-2019 16:05:53.804 queries: info: client @0x7f193003ec50 10.11.0.129#37280/key hzidc (dianwoda.cn): view shidc: query: dianwoda.cn IN SOA -SE(0) (10.11.0.128)20-Nov-2019 16:05:53.807 queries: info: client @0x7f19300afc90 10.11.0.129#50229/key hzidc (dianwoda.cn): view shidc: query: dianwoda.cn IN IXFR -ST (10.11.0.128)20-Nov-2019 16:05:54.309 queries: info: client @0x7f1930038a10 10.11.0.129#55843/key aliyun-east1 (dianwoda.cn): view shidc: query: dianwoda.cn IN SOA -SE(0) (10.11.0.128)20-Nov-2019 16:05:54.316 queries: info: client @0x7f192007ca10 10.11.0.129#33036/key aliyun-east1 (dianwoda.cn): view shidc: query: dianwoda.cn IN IXFR -ST (10.11.0.128) 可以看到，我是hzidc和aliyun-east1的key，但是返回的数据是：“view shidc”中的数据。 总结：主从之间的数据同步，其实也是和客户端的请求逻辑是一样的，除了key之外，第一层会匹配acl 传输限速问题在从节点第一次启动和主节点同步的数据，会看到这样的日志 123456721-Nov-2019 08:44:35.576 xfer-in: info: zone fenghuoxiaoyuan.com/IN/hzidc: zone transfer deferred due to quota21-Nov-2019 08:44:35.576 xfer-in: info: zone dwd.gds-sh/IN/hzidc: zone transfer deferred due to quota21-Nov-2019 08:44:35.576 xfer-in: info: zone dianwoda.cn/IN/hzidc: zone transfer deferred due to quota21-Nov-2019 08:44:35.581 xfer-in: info: zone ecs.east1-g/IN/hzidc: zone transfer deferred due to quota21-Nov-2019 08:44:35.581 xfer-in: info: zone dianwoba.com/IN/hzidc: zone transfer deferred due to quota21-Nov-2019 08:44:35.582 xfer-in: info: zone ecs.east1-e/IN/hzidc: zone transfer deferred due to quota21-Nov-2019 08:44:35.583 xfer-in: info: zone dwb.dgt/IN/hzidc: zone transfer deferred due to quota 解决方式： 设置这几个参数： transfers-in。从节点上。设置一次zone传输允许进入的zone数量最大值，默认值是10。设置为30 transfers-out。主节点上。设置为30 serial-query-rate transfers-per-ns。默认值为2。对入向zone传输的并发进行设置。设置为10 如果按照默认的配置，那么会是：从主节点上同步10个zone的数据，同时只能请求2个zone的数据，分5次把这10个zone的数据请求完毕。然后才会开始进行下一批10个zone数据的请求， 常见问题主节点服务器异常考虑最极端的情况，主节点服务器奔溃，需要重装系统，这个时候需要重装bind，然后恢复数据]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识及网络服务</category>
        <category>网络服务</category>
        <category>DNS服务实现-bind</category>
      </categories>
      <tags>
        <tag>bind</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git基本操作及gitlab安装部署]]></title>
    <url>%2F2018%2F10%2F18%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%2FGit%2Fgit%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[基础知识实际案例一个项目的完整过程一个项目的完整过程，主要包括以下几个步骤： git上创建一个空项目 git拉取到本地进行开发 开发完成之后上传到git仓库中 server端从git拉取代码进行发布 有额外的更新操作，则需要以下过程 本地修改调试 git上传到仓库 server端拉取最新的代码，然后重新发布 Git global setup12git config --global user.name &quot;汪小华&quot;git config --global user.email &quot;wangxiaohua@dianwoda.com&quot; Create a new repository1234567git clone git@192.168.1.66:wangxiaohua/dcache.gitcd dcache-tringsdaleixldictdtouch README.mdgit add README.mdgit commit -m &quot;add README&quot;git push -u origin master Existing folder or Git repository123456cd existing_foldergit initgit remote add origin git@192.168.1.66:wangxiaohua/dcache.gitgit add .git commitgit push -u origin master git clone http://wangxiaohua:6e2TFHqDt0ab@192.168.1.66/wangxiaohua/es-monitor.git]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>编程开发</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux内核日志输出文件]]></title>
    <url>%2F2018%2F09%2F05%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FLinux%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%2FLinux%E5%86%85%E6%A0%B8%E6%97%A5%E5%BF%97%E8%BE%93%E5%87%BA%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[需求背景需要将服务器的内核日志等输出到一个专门的文件，然后做日志收集。默认情况下，系统的内核日志都是输出到message中的，不便于查看。 日志配置修改配置文件vim /etc/rsyslog.conf 在文件末尾添加以下内容： 1kern.* /var/log/kern.log 重启服务然后重启syslog服务即可 1service rsyslog restart 使用service的方式是为了兼容centos6和7这2个系统。在centos7中会自动识别并转换为systemctl的方式。 补充因为历史遗留问题，服务器上有的开了有的没开，需要批量推送，因此使用下面的下脚本： 12345678#!/bin/bashres=`grep &apos;/var/log/kern.log&apos; /etc/rsyslog.conf`if [[ $? -eq 0 ]];then echo &quot;already&quot;else echo &quot;not exists&quot; echo &quot;kern.* /var/log/kern.log&quot; &gt;&gt; /etc/rsyslog.conf &amp;&amp; service rsyslog restartfi]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>Linux基础知识</category>
        <category>Linux系统管理</category>
      </categories>
      <tags>
        <tag>内核日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu更改apt源]]></title>
    <url>%2F2018%2F08%2F28%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FLinux%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%2Fubuntu%E6%9B%B4%E6%94%B9apt%E6%BA%90%2F</url>
    <content type="text"><![CDATA[在安装完ubuntu之后，我们一般都需要将apt源替换为阿里云等国内软件源站点，以便提高响应速度。 这里使用的是Ubuntu18.04发行版本 基础知识其实Ubuntu18.04版之前的任一版更改apt源为国内源方法早就有了，内容大同小异，我们应当掌握其规律了，其实每一版内容不同的地方就是版本号（或者官方一点的说：系统代号），所以我们先了解下新版本的系统代号： 使用如下命令查看： 1lsb_release -c 执行后输出如下： 12wxh@wxh-ThinkPad-E570:/etc/apt$ lsb_release -cCodename: bionic 我们可以看到新版本的Ubuntu系统代号为bionic 同样的我们也可以得到之前任意版本的系统代号： Ubuntu 12.04 (LTS)代号为precise。 Ubuntu 14.04 (LTS)代号为trusty。 Ubuntu 15.04 代号为vivid。 Ubuntu 15.10 代号为wily。 Ubuntu 16.04 (LTS)代号为xenial。 所以这也就解释了为什么我们百度出来的那么多方案里面内容不尽相同的原因，因为他们更改apt安装源时用的系统不一样。 下面开始实际操作 备份源文件我们要修改的文件是sources.list，它在目录/etc/apt/下，sources.list是包管理工具apt所用的记录软件包仓库位置的配置文件，同样类型的还有位于 同目录下sources.list.d文件下的各种.list后缀的各文件。 命令如下： 1sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak 编辑源文件内容1sudo vim /etc/apt/sources.list 将原有的内容注释或删除掉，添加以下内容 12345678910111213141516171819deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse 配置格式说明： 我们可以看到sources.list文件的条目都是有格式的（通过上面的内容大家也看的出来），一般有如下形式 12deb http://site.example.com/debian distribution component1 component2 component3deb-src http://site.example.com/debian distribution component1 component2 component3 后面几个参数是对软件包的分类（Ubuntu下是main， restricted，universe ，multiverse这四个） 补充-/etc/apt/sources.list 详解/etc/apt/sources.list 是包管理工具 apt 所用的记录软件包仓库位置的配置文件，同样的还有位于 /etc/apt/sources.list.d/*.list 的各文件。 sources.list 文件中的条目一般都有如下所示的形式： 12deb http://site.example.com/debian distribution component1 component2 component3deb-src http://site.example.com/debian distribution component1 component2 component3 档案类型 (Archive type)条目的第一个词 deb 或是 deb-src 表明了所获取的软件包档案类型。 其中： deb 档案类型为二进制预编译软件包，一般我们所用的档案类型。 deb-src 档案类型为用于编译二进制软件包的源代码。 仓库地址 (Repository URL)条目的第二个词则是软件包所在仓库的地址。我们可以更换仓库地址为其他地理位置更靠近自己的镜像来提高下载速度。 常用镜像地址列表： Debian https://www.debian.org/mirror/list Ubuntu http://wiki.ubuntu.org.cn/源列表 发行版 (Distribution)跟在仓库地址后的是发行版。发行版有两种分类方法，一类是发行版的具体代号，如 xenial,trusty, precise 等；还有一类则是发行版的发行类型，如oldstable, stable, testing 和 unstable。 另外，在发行版后还可能有进一步的指定，如 xenial-updates, trusty-security, stable-backports 等。 软件包分类 (Component)跟在发行版之后的就是软件包的具体分类了，可以有一个或多个。 不同的 Linux 发行版对软件有着不同的分类，如： Debian main 包含符合 DFSG 指导原则的自由软件包，而且这些软件包不依赖不符合该指导原则的软件包。这些软件包被视为 Debian 发型版的一部分。 contrib 包含符合 DFSG 指导原则的自由软件包，不过这些软件包依赖不在 main 分类中的软件包。 non-free 包含不符合 DFSG 指导原则的非自由软件包。 Ubuntu main 官方支持的自由软件。 restricted 官方支持的非完全自由的软件。 universe 社区维护的自由软件。 multiverse 非自由软件。 Ubuntu 对软件包的分类可以用下表来表示（参考自 Wikipedia）： 自由软件 非自由软件 官方支持的 Main Restricted 非官方支持的 Universe Multiverse 补充-update和upgrade每个LINUX的发行版，比如ubuntu、centos等，都会维护一个自己的软件仓库，我们常用的几乎所有软件都在这里面。这里面的软件绝对安全，而且绝对的能正常安装。 在UBUNTU下，我们维护一个源列表，源列表里面都是一些网址信息，这每一条网址就是一个源，这个地址指向的数据标识着这台源服务器上有哪些软件可以安装使用。 编辑源命令： 1sudo vim /etc/apt/sources.list 在这个文件里加入或者注释（加#）掉一些源后，保存。这时候，我们的源列表里指向的软件就会增加或减少一部分。 获得最近的软件包的列表:(列表中包含一些包的信息，比如这个包是否更新过) 1sudo apt-get update 这个命令，会访问源列表里的每个网址，并读取软件列表，然后保存在本地电脑。软件包管理器里看到的软件列表，都是通过update命令更新的。 update后，可能需要upgrade一下。 1sudo apt-get upgrade 这个命令，会把本地已安装的软件，与刚下载的软件列表里对应软件进行对比，如果发现已安装的软件版本太低，就会提示你更新。如果你的软件都是最新版本，会提示： 1升级了 0 个软件包，新安装了 0 个软件包，要卸载 0 个软件包，有 0 个软件包未被升级。 总而言之，update是更新软件列表，upgrade是更新软件。 注意：一般在执行 sudo apt-get upgrade 命令之前需要先执行一下 sudo apt-get update；这其实和windows下的软件检测更新是一样的，需要更新的会帮你自动更新并安装好 apt-get update 命令会同步使用者端和APT服务器的RPM 索引清单（package list），APT 服务器的RPM 索引清单置于base 资料夹内，使用者端电脑取得base 资料夹内的bz2 RPM 索引清单压缩档后，会将其解压置放于/var/state/apt/lists/，而使用者使用apt-get install 或apt-get dist-upgrade 指令的时候，就会将这个资料夹内的资料和使用者端电脑内的RPM 资料库比对，如此一来就可以知道那些RPM 已安装、未安装、或是可以升级的。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>Linux系统管理</category>
      </categories>
      <tags>
        <tag>ubuntu更改apt源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS记录类型简介]]></title>
    <url>%2F2018%2F08%2F28%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E5%8F%8A%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%2FDNS%2FDNS%E8%AE%B0%E5%BD%95%E7%B1%BB%E5%9E%8B%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[基础知识DNS：（Domain Name System，域名系统），因特网上作为域名和IP地址相互映射的一个分布式数据库，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。通过主机名，最终得到该主机名对应的IP地址的过程叫做域名解析（或主机名解析）。使用端口号53。 DNS服务器：用于对域名进行解析的域名解析服务器。 DNS代理：用于代理域名服务器，对客户端的查询请求进行响应（一般是本地查找，查找不到再向代理的服务器转发客户端的查询请求） dns记录类型介绍A记录说明：WEB服务器的IP指向 A （Address）记录是用来指定主机名（或域名）对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置域名的子域名。通俗来说A记录就是服务器的IP,域名绑定A记录就是告诉DNS,当你输入域名的时候给你引导向设置在DNS的A记录所对应的服务器。 简单的说，A记录是指定域名对应的IP地址。 AAAA记录作用同A记录，只不过该记录是将域名解析到一个指定的IPV6的IP上 CNAME记录通常称别名解析。可以将注册的不同域名都转到一个域名记录上，由这个域名记录统一解析管理 这种记录允许您将多个名字映射到同一台计算机。 通常用于同时提供WWW和MAIL服务的计算机。例如，有一台计算机名为“host.mydomain.com”（A记录）。 它同时提供WWW和MAIL服务，为了便于用户访问服务。可以为该计算机设置两个别名（CNAME）：WWW和MAIL。 这两个别名的全称就是“www.mydomain.com”和“mail.mydomain.com”。实际上他们都指向“host.mydomain.com”。 同样的方法可以用于当您拥有多个域名需要指向同一服务器IP，此时您就可以将一个域名做A记录指向服务器IP然后将其他的域名做别名到之前做A记录的域名上，那么当您的服务器IP地址变更时您就可以不必麻烦的一个一个域名更改指向了 只需要更改做A记录的那个域名其他做别名的那些域名的指向也将自动更改到新的IP地址上了。 NS记录解析服务器记录，用来表明由哪台服务器对该域名进行解析。 这里的NS记录只对子域名生效。 例如用户希望由12.34.56.78这台服务器解析news.mydomain.com，则需要设置news.mydomain.com的NS记录。 说明： “优先级”中的数字越小表示级别越高； “IP地址/主机名”中既可以填写IP地址，也可以填写像ns.mydomain.com这样的主机地址，但必须保证该主机地址有效。如，将 news.mydomain.com的NS记录指向到ns.mydomain.com，在设置NS记录的同时还需要设置ns.mydomain.com的 指向，否则NS记录将无法正常解析； NS记录优先于A记录。即，如果一个主机地址同时存在NS记录和A记录，则A记录不生效。这里的NS记录只对子域名生效。 MX记录MX（Mail Exchanger）记录是邮件交换记录，它指向一个邮件服务器，用于电子邮件系统发邮件时根据收信人的地址后缀来定位邮件服务器。例如，当Internet上的某用户要发一封信给 user@mydomain.com 时，该用户的邮件系统通过DNS查找mydomain.com这个域名的MX记录，如果MX记录存在， 用户计算机就将邮件发送到MX记录所指定的邮件服务器上。 其他的记录类型涉及到的时候再详细补充]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识及网络服务</category>
        <category>网络知识</category>
        <category>DNS</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu设置ssh服务]]></title>
    <url>%2F2018%2F08%2F26%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FLinux%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%2Fubuntu%E8%AE%BE%E7%BD%AEssh%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[默认情况下，安装完ubuntu之后，操作系统不会像centos一样会自动把openssh-server给安装上，也就是说，在系统安装完毕之后，我们还需要进行额外的操作，才能通过ssh的方式远程访问我们的ubuntu系统。 基础知识SSH分客户端openssh-client和openssh-server 如果你只是想登陆别的机器，那么只需要安装openssh-client（ubuntu有默认安装，如果没有则sudoapt-get install openssh-client），如果要使本机开放SSH服务就需要安装openssh-server。 安装配置在命令行中使用如下命令进行安装 安装1sudo apt-get install openssh-server 配置ssh-server配置文件位于/etc/ssh/sshd_config，在这里可以定义SSH的服务端口，默认端口是22，你可以自己定义成其他端口号。 通过修改配置文件/etc/ssh/sshd_config，可以进行修改ssh登录端口、禁止root登录等一系列操作，修改端口可以防止被端口扫描。 启动默认情况下，安装完毕之后将会自动启动，我们可用用过ps来查看 1ps -elf | grep sshd 如果没有启动的话，我们执行以下命令启动 1sudo /etc/init.d/ssh start]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>Linux系统管理</category>
      </categories>
      <tags>
        <tag>ubuntu设置ssh服务</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F23%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E8%99%9A%E6%8B%9F%E5%8C%96%2FDocker%2Bk8s%2FDocker%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[参考文献： 书籍：《Docker技术入门与实战 第2版》 第1章 基础知识Docker介绍有关虚拟化虚拟化技术是一个通用的概念，在不同领域有不同的理解，在计算领域，一般指的是计算虚拟化（computing Virtualization），或通常说的服务器虚拟化。 维基百科定义如下：“虚拟化是一种资源管理技术”，是将计算机的各种实体资源，例如服务器、网络、内存及存储等，予以抽象，转换后呈现出来的，打破实体结构间的不可切割的障碍，使用户可以比原来的组态更好的方式来应用这些资源 传统来看，虚拟化既可以通过硬件模拟来实现（xen、esxi等），也可以通过操作系统软件来实现（KVM等）。而容器技术则更为优雅，它充分利用了操作系统本身已有的机制和特性，可以实现远超传统虚拟机的轻量级虚拟化。因此，有人甚至把它称为“新一代的虚拟化”技术，并将基于容器打造的云平台亲切地称之为“容器云” 什么dockerDocker的构想是要实现“build,ship and run any app,anywhere”,即通过对应用程序的封装、分发、部署、运行生命周期。达到应用组件，“一次封装，到处运行”的目的。这里的应用组件，既可以是一个web应用、一个编译环境、也可以是一套数据库平台服务，甚至是一个操作系统或者集群 可以说，Docker首次为应用的开发、运行和部署提供了“一站式”的实用解决方案 IBM DeveloperWorks网站关于容器技术的描述十分准确：“容器技术有效地将由单个操作系统管理的资源划分到孤立的组中，以更好的在孤立的组之间平衡有冲突的资源使用需求” 总结：Docker是一项容器技术，是一个开源项目 从LXC从到DockerDocker技术的主要实现参考的是Linux容器技术（Linux Containers LXC） 在LXC的基础之上，Docker进一步优化了容器的使用体验，让它进入了寻常百姓家。 优化项目： 首先，Docker提供了各种容器管理工具（如分发、版本、移植等），让用户无需关注底层的操作，可以简单明了地管理和使用容器。 其次，docker引入了分层文件系统构建和高效的镜像机制，降低了迁移难度极大地提升了用户体验。用户操作docker就像操作应用自身一样简单。 使用Docker的好处在云时代，开发者创建的应用需要要很方便的在网络上传播，也就是说，应用必须要脱离底层物理硬件的限制，同时必须是“任何时间，任何地点”都可获取的。因此，开发者需要一种新型的创建分布式应用程序的方式，快速分发和部署，这正是docker所能够提供的最大优势。 举例来说，如果要部署LAMP平台，需要分别部署mysql，apache，php等，然后再进行一系列的配置，这样的配置非常繁琐并且容易出错。并且如果需要服务器迁移，往往需要重新部署。 docker提供了一种更为聪明的方式，通过容器来打包应用。解耦应用和运行平台。这意味着在进行应用迁移的时候，只需要再新的机器上面再启动容器就可以了，不需要再进行重新部署等操作。无论服务器是否是同一类型的平台架构。这节约了大量的时间，并降低了部署过程出现问题的风险。 Docker在开发和运维中的优势具体来说，Docker在开发和运维过程中，具有如下几个方面的优势： 更快速的交付和部署 使用镜像来快速构建一套标准环境， 更高效的资源利用 docker容器不需要额外的虚拟化管理程序支持，它是内核级别的虚拟化，可以实现更高的性能 更轻松的迁移和扩展 docker容器几乎可以在任何的平台上运行，包括物理机，虚拟机，公有云，私有云，个人电脑，服务器等。 更简单的更新管理 使用dockerfile，只需要小小的配置修改，就可以替代以往大量的更新工作，并且所有的修改都是以增量的方式被分发和更新，从而实现自动化并且高效的容器管理。 Docker与虚拟机的比较 docker容器很快，启动和停止可以在秒级实现，而传统的虚拟机方式需要数分钟 docker容器对系统资源的需求很少，一台主机可以同时运行数千个docker容器 docker通过类似git设计理念的操作来方便用户获取，分发和应用镜像，存储复用，增量更新。 docker通过dockerfile支持灵活的自动化创建和部署机制，提高工作效率，使流程标准化。 Docker和核心价值docker的核心价值在于，他很有可能改变传统的软件“交付”方式和运行方式。传统的交付源码或者交付软件的方式的最大问题在于，软件运行期间所“依赖的环境”是无法控制的、不能标准化的，IT人员常常需要耗费很多精力来解决因为“依赖的环境”而导致软件运行出现的各种问题。 而docker将软件与其“依赖的环境”打包在一起，以镜像的方式交付，让软件运行在“标准的环境中”，这非常符合云计算的要求。这种变革一旦被IT人员接受，可能会对产业链带来很大的冲击，我们熟悉的apt-get、yum是否会逐渐被docker pull取代？ 从这一点可以毫不夸张的说，docker是革命性的，它重新定义了软件开发、测试、交付和部署的流程。我们交付的不再是代码、配置文件、数据库定义等。而是整个应用程序运行环境：“OS+各种中间件、类库+应用程序代码” 有了标准化的运行环境，再加上对CPU、内存、磁盘、网络等动态资源的限制，docker构造了一个“轻量级虚拟环境”，传统虚拟机的绝大多数使用场景可以被docker取代，这将给IT基础设施带来一次更大的冲击；传统虚拟化（KVM、XEN、VMWare）将会何去何从？此外，docker秒级创建/删除虚拟机以及动态调整资源的能力，也非常契合云计算的“实例水平扩展”、“资源动态调整”的需求，docker很有可能成为云计算的基石。 Docker的应用场景通过上面的介绍，可以总结出以下几个使用docker的场景 业务高峰期通过启动大量容器进行横向扩展 应用需要经常迁移或者多环境运行 核心概念/名词解释只有理解了这三个核心概念，才能顺利的理解Docker容器的整个生命周期。 docker的大部分操作都围绕着它的三个核心概念：镜像、容器和仓库展开。因此准确把握这三大核心概念对于掌握docker技术尤为重要。 三剑客之镜像-ImageDocker镜像类似虚拟机镜像，可以将它理解为一个只读的模板。例如，一个镜像可以包含一个基本的操作系统环境，里面仅安装了apache应用程序（或者其他需要的程序），这个时候，可以把它称之为一个apache镜像。 镜像是创建Docker容器的基础。通过版本管理和增量的文件系统，Docker提供了一套十分简单的机制来创建和更新删除现有的镜像，用户甚至可以从网上下载一个已经做好的应用镜像，并且直接使用 功能总结 只读的模板（包含OS+应用） 版本管理和增量文件系统的机制在其之上 操作：自行创建或网上下载、更新、删除 三剑客之容器-Container我们可以将Docker的容器理解为一种轻量级的沙盒（sanbox）。 Docker利用容器来运行和隔离应用（镜像）。 容器是从镜像创建的应用运行实例，可以将指定的镜像启动、开始、停止、删除。而这些容器都是彼此相互隔离的，互不可见的。【容器是镜像的隔离运行单元】 每个容器内运行着一个应用，不同的容器相互隔离，容器之间也可以通过网络互相通信。容器的创建和停止都十分快速，几乎跟创建和终止原生应用一致；另外，容器自身对系统资源的额外需求也十分有限，远远低于传统虚拟机。 每个容器都是一个操作系统实例 容器的功能 通过容器打包运行应用 解耦应用和运行平台。这意味着在进行应用迁移的时候，只需要再新的机器上面再启动容器就可以了，不需要再进行重新部署等操作。无论服务器是否是同一类型的平台架构。 隔离应用 容器的组成结构 上层：实际应用 下层：操作系统环境（主要是内核+函数库） 三剑客之仓库-Repositorydocker仓库类似于代码仓库，它是docker集中存放镜像文件的场所 需要注意docker仓库和仓库注册服务器（Registry）的区别。 仓库注册服务器是存放仓库的地方，其中往往存放着多个仓库。 每个仓库集中存放某一类镜像，往往包括多个镜像文件，通过不同的tag（标签）来进行区分。 根据所存储的镜像公开与否，Docker仓库可以分为两种形式。目前，最大公开仓库是官方提供的docker Hub，其中存放了数量庞大的镜像供用户下载。国内不少云服务提供商（时速云，阿里云等）也提供了仓库的本地源，可以提供稳定的国内访问 docker也支持用户在本地网络内创建一个只能自己访问的私有仓库。当用户创建了自己的镜像之后就可以使用push命令将它上传到指定的公有或者私有仓库有。这样用户下次在另外一台机器上使用该镜像时，只需要将其从仓库上pull下来就可以。 Docker-EE和Docker-CEDocker Engine改为Docker CE（社区版）, Docker Community Edition Docker Data Center改为Docker EE（企业版）, Docker Enterprise Edition 在Docker三个定价层增加了额外的支付产品和支持 Docker社区版（CE）是为了开发人员或小团队创建基于容器的应用,与团队成员分享和自动化的开发管道。docker-ce提供了简单的安装和快速的安装，以便可以立即开始开发。docker-ce集成和优化，基础设施。 Docker企业版（EE）是专为企业的发展和IT团队建立谁。docker-ee为企业提供最安全的容器平台，以应用为中心的平台。 第2章 安装配置安装Docker系统要求docker目前只能运行在64为平台上，并且要求内核版本不低于3.10，实际上内核越新越好，过低的内核版本容易造成功能不稳定。 centos环境下安装docker安装依赖包 1$ sudo yum install -y yum-utils \ device-mapper-persistent-data \lvm2 添加yum源 12$ yum-config-manager \ --add-repo \https://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.repo 安装docker 12$ sudo yum makecache fast$ sudo yum -y install docker-ce ubuntu环境安装docker卸载老旧版本的docker Older versions of Docker were called docker or docker-engine. If these are installed, uninstall them: 1$ sudo apt-get remove docker docker-engine docker.io 安装软件源 Update the apt package index: 1$ sudo apt-get update Install packages to allow apt to use a repository over HTTPS: 12345$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common Add Docker’s official GPG key: 1$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Verify that you now have the key with the fingerprint 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88, by searching for the last 8 characters of the fingerprint. 123456$ sudo apt-key fingerprint 0EBFCD88pub 4096R/0EBFCD88 2017-02-22 Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88uid Docker Release (CE deb) &lt;docker@docker.com&gt;sub 4096R/F273FCD8 2017-02-22 Use the following command to set up the stable repository. You always need the stable repository, even if you want to install builds from the edge or test repositories as well. To add the edge or test repository, add the word edge or test (or both) after the word stable in the commands below. Note: The lsb_release -cs sub-command below returns the name of your Ubuntu distribution, such as xenial. Sometimes, in a distribution like Linux Mint, you might have to change $(lsb_release -cs) to your parent Ubuntu distribution. For example, if you are using Linux Mint Rafaela, you could use trusty. x86_64 / amd64 armhf IBM Power (ppc64le) IBM Z (s390x) 1234$ sudo add-apt-repository \ &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable&quot; 安装docker-ce Install the latest version of Docker CE, or go to the next step to install a specific version. Any existing installation of Docker is replaced. 1$ sudo apt-get install docker-ce On production systems, you should install a specific version of Docker CE instead of always using the latest. This output is truncated. List the available versions. 123$ apt-cache madison docker-cedocker-ce | 17.09.0~ce-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages 我这里的输出为： 1234$ apt-cache madison docker-ce docker-ce | 18.06.1~ce~3-0~ubuntu | https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages docker-ce | 18.06.0~ce~3-0~ubuntu | https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages docker-ce | 18.03.1~ce~3-0~ubuntu | https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages The contents of the list depend upon which repositories are enabled. Choose a specific version to install. The second column is the version string. The third column is the repository name, which indicates which repository the package is from and by extension its stability level. To install a specific version, append the version string to the package name and separate them by an equals sign (=): 1$ sudo apt-get install docker-ce=&lt;VERSION&gt; The Docker daemon starts automatically. Verify that Docker CE is installed correctly by running the hello-world image. 1$ sudo docker run hello-world 配置开机自启动 1$ sudo systemctl enable docker 卸载docker Uninstall the Docker CE package: 1$ sudo apt-get purge docker-ce Images, containers, volumes, or customized configuration files on your host are not automatically removed. To delete all images, containers, and volumes: 1$ sudo rm -rf /var/lib/docker You must delete any edited configuration files manually. 配置Docker服务权限配置为了避免每次使用docker命令都需要sudo使用特权身份，可以将当前用户加入安装中自动创建的docker用户组中。 1$ sudo usermod -aG docker [username] 关于这部分的内容-查看：Post-installation steps for Linux 后续需要单独写一篇文章写docker的启动权限问题 配置文件不存在问题docker服务的默认配置文件为： 12ubuntu中：/etc/default/dockercentos中：/etc/sysconfig/docker 在该配置文件中，我们可以通过修改其中的DOCKER_OPTS来修改服务启动的参数 但是实际情况，安装完docker之后，是没有改配置文件的 因此在docker的高版本之后，配置文件变成了：/etc/docker/key.json 官方参考配置文档：https://docs.docker.com/engine/reference/commandline/dockerd//#daemon-configuration-file 启动Docker服务 在Centos 7 中，我们可以使用如下命令启动docker 12345systemctl start docker# systemctl enable dockersystemctl daemon-reload #加载配置 第3章 使用Docker镜像镜像（image）是Docker三大核心概念中最为重要的，自Docker诞生之日起，“镜像”就是相关社区最为热门的关键词。 docker运行容器前需要本地存在对应的镜像，如果镜像没有保存在本地，docker会尝试先从默认镜像仓库下载（默认使用docker Hub公共注册服务器中的仓库），用户也可以通过配置，使用自定义的镜像仓库。 获取镜像镜像是运行容器的前提，官方的docker Hub网站已经提供了数十万个镜像供大家开放下载。 可以使用docker pull命令直接从docker Hub 镜像源来下载镜像。该命令的格式为docker pull NAME[:TAG]。其中，NAME是镜像仓库的名称（用来区分镜像），TAG是镜像的标签（往往用来表示版本信息）。例如：ubuntu系统是NAME，14.04是TAG 1docker pull ubuntu:14.04 注意： 如果不显性的制定TAG，该命令会自动选择latest标签，这会下载仓库中最新版本的镜像。 镜像的latest标签意味着该镜像的内容会跟踪最新的非稳定版本而发布，内部是不稳定的。 在生产环境中禁止忽略镜像的标签信息或者使用默认的latest标签 镜像的分层特性镜像文件一般由若干层（layer）构成，每一层都有一个唯一的id（完整的id有256比特，由64个十六进制字符组成）。 使用docker pull命令下载时会获取并输出镜像的各层信息。当不同的镜像包括相同的层时，本地仅存储该层的一份内容，减少了需要的存储空间。 镜像的重名问题在使用不同的镜像仓库时，可能会出现镜像重名的情况？ 严格来讲，镜像的仓库名称中还应该添加仓库地址（即仓库注册服务器（Registry））的地址作为前缀，默认我们使用的是docker Hub的服务，该前缀可以忽略不写。 例如：docker pull ubuntu:14.04 命令相当于docker pull registry.hub.docker.com/ubuntu:14.04命令。 运行镜像下载镜像到本地之后，即可随时使用该镜像，例如利用该镜像创建一个容器，在其中运行bash应用，执行ping localhost命令 1[root@master yum.repos.d]# docker run -it ubuntu:14.04 bash 查看镜像信息使用images命令列出镜像使用docker images命令可以列出本地主机上已有镜像的基本信息。、 1# docker images ​ 在列出的信息中，可以看到以下几个字段信息： 来自于哪个仓库 REPOSITORY 镜像的TAG（标签）信息，标签知识标记，并不能识别镜像内容 镜像的ID（唯一标识镜像）。IMAGE ID 创建时间 CREATED 镜像大小 SIZE 其中镜像的ID信息十分重要，它唯一标识了镜像。在使用镜像ID的时候，一般可以使用该ID的前若干个字符组成的可区分串来替代完整的ID。 镜像大小信息只是标识该镜像的逻辑体积的大小，实际上由于相同的镜像层本地只会存储一份，物理上占用的存储空间会小于各镜像的逻辑体积之和。 images支持的的选项参数： -a –all=true|false 列出所有的镜像文件（包括临时文件），默认为否 –digests=true|false 列出镜像的数字摘要值，默认为否 -f –filter=[] 过滤列出的镜像 ….. 具体可以通过man docker-images 进行查看。 使用tag命令添加镜像标签为了方便在后续的工作中使用特定镜像，还可以使用docker tag命令来为本地镜像任意添加新的标签 1[root@master ~]# docker tag ubuntu:14.04 ubuntu:my14.04 注意：docker tag命令添加的标签实际上起到了类似链接的作用 使用inspect命令查看详细信息使用docker inspect命令可以获取该镜像的详细信息，包括制作者，使用架构，各层的数字摘要等。 1[root@master ~]# docker inspect ubuntu:14.04 返回的是一个JSON格式的消息，如果我们只要其中一项内容时，可以使用参数-f来指定，例如，获取镜像的Architecture参数。 12[root@master ~]# docker inspect ubuntu:14.04 -f &#123;&#123;&quot;.Architecture&quot;&#125;&#125;amd64 使用history命令查看镜像历史既然镜像由多个层组成，那么怎么知道各个层的内容具体是什么呢？这时候可以使用history子命令，该命令将列出各层的创建信息。 123456789101112131415[root@master ~]# docker history ubuntu:14.04IMAGE CREATED CREATED BY SIZE COMMENTa35e70164dfb 12 days ago /bin/sh -c #(nop) CMD [&quot;/bin/bash&quot;] 0B &lt;missing&gt; 12 days ago /bin/sh -c mkdir -p /run/systemd &amp;&amp; echo &apos;do… 7B &lt;missing&gt; 12 days ago /bin/sh -c sed -i &apos;s/^#\s(deb.universe)$… 2.76kB &lt;missing&gt; 12 days ago /bin/sh -c rm -rf /var/lib/apt/lists/* 0B &lt;missing&gt; 12 days ago /bin/sh -c set -xe &amp;&amp; echo &apos;#!/bin/sh&apos; &gt; /… 195kB &lt;missing&gt; 12 days ago /bin/sh -c #(nop) ADD file:3900b83a46e97708a… 222MB 搜寻镜像使用docker search命令来搜索源端仓库中共享的镜像，默认搜索官方仓库中的镜像。 用法为： 1$ docker search TERM 支持的主要参数为： –automated=true|false：仅显示自动创建的镜像，默认为否。 –no-trubc=true|false：输出信息不截断显示，默认为否 -s –starts=X：指定仅显示评价为指定星级以上的镜像，默认为0，即输出所有的镜像。 删除镜像使用标签删除镜像使用docker rmi命令可以删除镜像，命令格式为： 123docker rmi IMAGE [IMAGE…]其中IMAGE可以为标签或ID,例如：ubuntu:my14.04 ​ 注意：当同一个镜像拥有多个标签的时候，docker rmi命令只是删除该镜像多个标签中的制定标签而已，并不影响镜像文件。因此上述操作相当于只是删除了镜像a35e70164dfb的一个标签而已。 但是，当镜像只是剩下一个标签的时候，此时再使用docker rmi 命令会彻底删除镜像。 使用镜像ID删除镜像当使用docker rmi（remove image）命令，并且后面跟上镜像的ID（也可以是能进行区分的部分ID串前缀）时，会先尝试删除所有指向该镜像，然后再删除该镜像文件本身 命令格式： 1docker rmi xxxx(ID) 注意：使用该命令可以彻底将该镜像删除，而不是只是删除对应的标签，请务必注意。 注意： 当有该镜像创建的容器存在时，镜像文件默认是无法被删除的 试图删除该镜像，docker会提示有容器正在运行，无法删除 如果要强行删除镜像，可以使用-f参数 创建镜像创建镜像的方法主要有三种： 基于已有镜像的容器创建 基于本地模板导入 基于dockerfile创建 基于已有镜像的容器创建当运行一个容器后，内部发生了变化，我们可以把这个发生了变化的容器做成一个新的镜像。 该方法主要是通过docker commit命令。 命令格式为： 1docker commit [options] container [repository[:tag]] 主要选项包括： -a –author=””：作者信息 -c –change=[]：提交的时候之心dockerfile指令， -m –message=””：提交消息 -p –pause=true 提交时暂停容器运行 123456789101112[root@master ~]# docker run -it ubuntu:14.04 /bin/bashroot@fdbe2f28b1d6:/# touch testroot@fdbe2f28b1d6:/# exitexit[root@master ~]# docker commit -m &quot;add a new file&quot; -a &quot;wxh&quot; fdbe2f28b1d6 test:0.1sha256:cd9c21826184f9e65e11644f826fb97918d40d469aa5e3fd8827cbcac19351ed[root@master ~]# docker images 查看 I’m just ​ 基于本地模板导入这部分不常用，详见书籍《docker技术入门与实践》-p32 基于dockerfile创建官方案例下面是docker官方案例，有一定的代表性 链接地址：https://docs.docker.com/v17.09/get-started/part2/#dockerfile Dockerfile will define what goes on in the environment inside your container. Access to resources like networking interfaces and disk drives is virtualized inside this environment, which is isolated from the rest of your system, so you have to map ports to the outside world, and be specific about what files you want to “copy in” to that environment. However, after doing that, you can expect that the build of your app defined in this Dockerfile will behave exactly the same wherever it runs. Dockerfile Create an empty directory. Change directories (cd) into the new directory, create a file called Dockerfile, copy-and-paste the following content into that file, and save it. Take note of the comments that explain each statement in your new Dockerfile. 1234567891011121314151617181920# Use an official Python runtime as a parent imageFROM python:2.7-slim# Set the working directory to /appWORKDIR /app# Copy the current directory contents into the container at /appADD . /app# Install any needed packages specified in requirements.txtRUN pip install --trusted-host pypi.python.org -r requirements.txt# Make port 80 available to the world outside this containerEXPOSE 80# Define environment variableENV NAME World# Run app.py when the container launchesCMD [&quot;python&quot;, &quot;app.py&quot;] Are you behind a proxy server? Proxy servers can block connections to your web app once it’s up and running. If you are behind a proxy server, add the following lines to your Dockerfile, using the ENV command to specify the host and port for your proxy servers: 1234&gt; # Set proxy server, replace host:port with values for your servers&gt; ENV http_proxy host:port&gt; ENV https_proxy host:port&gt; &gt; Add these lines before the call to pip so that the installation succeeds. This Dockerfile refers to a couple of files we haven’t created yet, namely app.py and requirements.txt. Let’s create those next. The app itself Create two more files, requirements.txt and app.py, and put them in the same folder with the Dockerfile. This completes our app, which as you can see is quite simple. When the above Dockerfile is built into an image, app.py and requirements.txt will be present because of that Dockerfile’s ADD command, and the output from app.py will be accessible over HTTP thanks to the EXPOSEcommand. requirements.txt 12FlaskRedis app.py 123456789101112131415161718192021222324from flask import Flaskfrom redis import Redis, RedisErrorimport osimport socket# Connect to Redisredis = Redis(host=&quot;redis&quot;, db=0, socket_connect_timeout=2, socket_timeout=2)app = Flask(__name__)@app.route(&quot;/&quot;)def hello(): try: visits = redis.incr(&quot;counter&quot;) except RedisError: visits = &quot;&lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;&quot; html = &quot;&lt;h3&gt;Hello &#123;name&#125;!&lt;/h3&gt;&quot; \ &quot;&lt;b&gt;Hostname:&lt;/b&gt; &#123;hostname&#125;&lt;br/&gt;&quot; \ &quot;&lt;b&gt;Visits:&lt;/b&gt; &#123;visits&#125;&quot; return html.format(name=os.getenv(&quot;NAME&quot;, &quot;world&quot;), hostname=socket.gethostname(), visits=visits)if __name__ == &quot;__main__&quot;: app.run(host=&apos;0.0.0.0&apos;, port=80) Now we see that pip install -r requirements.txt installs the Flask and Redis libraries for Python, and the app prints the environment variable NAME, as well as the output of a call to socket.gethostname(). Finally, because Redis isn’t running (as we’ve only installed the Python library, and not Redis itself), we should expect that the attempt to use it here will fail and produce the error message. Note: Accessing the name of the host when inside a container retrieves the container ID, which is like the process ID for a running executable. That’s it! You don’t need Python or anything in requirements.txt on your system, nor will building or running this image install them on your system. It doesn’t seem like you’ve really set up an environment with Python and Flask, but you have. Build the app We are ready to build the app. Make sure you are still at the top level of your new directory. Here’s what ls should show: 12$ lsDockerfile app.py requirements.txt Now run the build command. This creates a Docker image, which we’re going to tag using -t so it has a friendly name. 1docker build -t friendlyhello . Where is your built image? It’s in your machine’s local Docker image registry: 1234$ docker imagesREPOSITORY TAG IMAGE IDfriendlyhello latest 326387cea398 Run the app Run the app, mapping your machine’s port 4000 to the container’s published port 80 using -p: 1docker run -p 4000:80 friendlyhello You should see a message that Python is serving your app at http://0.0.0.0:80. But that message is coming from inside the container, which doesn’t know you mapped port 80 of that container to 4000, making the correct URL http://localhost:4000. Go to that URL in a web browser to see the display content served up on a web page. Now let’s run the app in the background, in detached mode: 1docker run -d -p 4000:80 friendlyhello You get the long container ID for your app and then are kicked back to your terminal. Your container is running in the background. You can also see the abbreviated container ID with docker container ls (and both work interchangeably when running commands): 123$ docker container lsCONTAINER ID IMAGE COMMAND CREATED1fa4ab2cf395 friendlyhello &quot;python app.py&quot; 28 seconds ago You’ll see that CONTAINER ID matches what’s on http://localhost:4000. Now use docker container stop to end the process, using the CONTAINER ID, like so: 1docker container stop 1fa4ab2cf395 载入和导出镜像用户可以使用docker save和docker load命令来存出和载入镜像 导出镜像如果要导出镜像到本地文件，可以使用docker save命令。例如导出本地的ubunt:14.04镜像为ubuntu_14.04.tar。如下图所示： 1[root@master ~]# docker save -o ubuntu:14.04.tar ubuntu:14.04 载入镜像可以使用docker load将导出的tar文件再导入到本地的镜像库，例如从上述文件导入镜像到本地镜像列表。 12345[root@master ~]# docker load --input ubuntu\:14.04.taror[root@master ~]# docker load &lt; ubuntu\:14.04.tar 该命令将导入镜像及其相关的元数据信息（包括标签等）。导入成功之后，可以使用docker images命令进行查看。 上传和下载镜像登录首先在docker hub网站完成注册 可以通过执行 docker login 命令交互式的输入用户名及密码来完成在命令行界面登录 Docker Hub。 你可以通过 docker logout 退出登录。 1234567891011wxh@wxh-ThinkPad-E570:~$ sudo docker login[sudo] password for wxh:Login with your Docker ID to push and pull images from Docker Hub. If you don&apos;t have a Docker ID, head over to https://hub.docker.com to create one.Username: watchmen1992Password:WARNING! Your password will be stored unencrypted in /home/wxh/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeededwxh@wxh-ThinkPad-E570:~$ 上传镜像可以使用docker push命令上传镜像到仓库，默认上传到docker Hub官方仓库（需要在前面进行登录） 在进行上传之前，我们需要将指定的image镜像打上tag标签 命令格式为： 12docker tag image username/repository:tagdocker push NAME[:TAG] | [REGISTRY_HOST[:REGISTRY_PORT]/]NAME[:TAG] 例如这里是： 12345wxh@wxh-ThinkPad-E570:~/.docker$ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEfriendlyhello latest 028a0a8d1a73 18 hours ago 132MBpython 2.7-slim 40792d8a2d6d 4 weeks ago 120MBhello-world latest 2cb0d9787c4d 7 weeks ago 1.85kB 1$ sudo docker tag friendlyhello watchmen1992/get_started:part1 然后push上传 12345678910wxh@wxh-ThinkPad-E570:~/.docker$ sudo docker push watchmen1992/get_started:part1The push refers to repository [docker.io/watchmen1992/get_started]88d851af045c: Pushedaa5497e6a355: Pushedd4f7cf378376: Pushed1ea4f6a807ba: Mounted from library/pythonfda4dc055a55: Mounted from library/pythone8fc09a140cf: Mounted from library/pythoncdb3f9544e4c: Mounted from library/pythonpart1: digest: sha256:ded675c46615053ef0a655b163228e50a0e58172003f78f319dad46ee09dac9a size: 1788 上传之后，我们在docker hub的页面就可以看到新增内容 1docker run -p 4000:80 username/repository:tag 例如： 1234567891011wxh@wxh-ThinkPad-E570:~/.docker$ sudo docker run -p 4000:80 watchmen1992/get_started:part1[sudo] password for wxh: * Serving Flask app &quot;app&quot; (lazy loading) * Environment: production WARNING: Do not use the development server in a production environment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)192.168.101.72 - - [30/Aug/2018 03:32:34] &quot;GET / HTTP/1.1&quot; 200 - 在上传之后，我们就能直接调用仓库的镜像去启动容器 下载镜像第4章 使用Docker容器容器是docker的另一个和细腻概念。简单来说，容器是镜像的一个运行实例。所不同的是，镜像是静态的只读文件，而容器带有运行时需要的可写文件层。 创建启动容器新建容器使用docker create命令新建一个容器 1格式：docker create -it ubuntu:latest 注意：使用该命令新建的容器处于停止状态，可以使用docker start命令来启动它。 启动容器使用docker start 命令来启动一个已经创建的容器，例如启动刚创建的ubuntu容器： 1$ docker start af 新建并启动容器除了创建容器后通过start命令来启动，也可以直接新建并启动容器。 所需要的命令主要为:docker run。等价于先执行docker create命令，再执行docker start命令 例如，下面的命令输出一个“hello world”之后容器终止 12[root@master ~]# docker run ubuntu:14.04 /bin/echo &quot;hello,world&quot;hello,world ​ 当利用docker run命令来创建并启动容器时，docker在后台运行的标准操作包括： 检查本地是否存在指定的镜像，不存在就从公有仓库中下载 利用镜像创建一个容器，并启动该容器 分配一个文件系统给容器，并在只读的镜像层外面挂载一层可读写层 从宿主主机配置的网桥接口中桥接一个虚拟接口道容器中 从网桥的地址池配置一个IP地址给容器 执行用户执行的应用程序 执行完毕后容器被自动终止 下面命令启动一个bash终端，允许用户进行交互 1[root@master ~]# docker run -it ubuntu:14.04 /bin/bash -t选项让docker分配一个伪终端，并绑定到容器的标准输入上，一则让容器的标准输入保持打开。更多的命令可以通过 man docker-run进行查看。 注意： 对于所创建的bash容器，当使用exit命令退出之后，容器就自动处于退出状态了，这是因为对于docker容器来说，当运行的应用退出之后，容器也就没有继续运行的必要了 守护态运行更多的时候，需要让docker容器在后台以守护态（daemonized）形式运行。此时可以添加-d参数来实现 1[root@master ~]# docker run -d ubuntu:14.04 /bin/bash -c &quot;while true;do echo hello,world;sleep 1;done&quot; 容器标准输出日志此时，要获取容器的标准输出信息，可以使用如下的docker logs命令 123[root@master ~]# docker logs 9e7或者[root@master ~]# docker logs -f 9e7 -f: 让 docker logs 像使用 tail -f 一样来输出容器内部的标准输出。 退出错误代码 125 docker daemon执行出错，例如指定了不支持的docker命令参数 126 所指定命令无法执行，例如权限出错 127 容器内命令无法找到 当命令执行出错时，会默认返回错误码 容器参数详见书籍《docker技术从入门到实践》p35-37 –rm 容器在终止后会立刻删除 -d 守护态启动，注意—rm和-d不能同时使用 终止容器可以使用docker stop来终止一个运行中的容器。 1命令格式为：docker stop -t|--time[=10] 该命令首先向容器发送SIGTERM信号，等待一段超时时间（默认为10秒）后，再发送SIGKILL信号来终止容器 ​ 当容器中指定的应用终结时，容器也会自动终止。 处于终止状态的容器，可以通过start命令来重新启动 此外，docker restart 命令会将一个运行态的容器先终止，然后再重新启动它 动车k 进入容器attach这种方法，同一时间只能有一个活动窗口。不建议 exec命令通过exec命令对容器执行操作是最为推荐的方式。 12345[root@master ~]# docker start 9e708d44e39a9e708d44e39a[root@master ~]# docker exec -it 9e7 /bin/bashroot@9e708d44e39a:/# 比较常用的参数有： -i –interactive=true|false 打开标准输入接受用户输入命令，默认为false。 –privileged=true|false 是否给执行命令以最高权限，默认为false。 -t –tty=true|false 分配伪终端，默认为false -u –user=”” 执行命令的用户名或者ID nsenter工具查看容器 docker ps 查看运行中的容器 docker ps -a 查看所有容器，包括运行的与停止的 docker ps -qa 查看所有容器ID 删除容器可以使用docker rm命令来删除处于终止或者退出状态的容器 命令格式为： 1docker rm -f|--force [-v|--volumes] container [container…] 主要支持的选项包括： -f –force=false 是否强行终止并删除一个运行中的容器 -l –link=false 删除容器的连接，但是保留容器 -v –volumes=false 删除容器挂载的数据卷 默认情况下，docker rm命令只能删除处于终止或者退出状态的容器，并不能删除还是处于运行状态的容器。 如果要删除一个运行中的容器，可以添加-f参数。该命令首先向容器发送SIGKILL信号给容器，终止其中的应用，之后强行删除。 导入和导出容器导出容器12345docker export -o test_for_run.tar ce5或者docker export e81 &gt; test_for_stop.tar 导入容器导出的文件可以使用docker import命令导入变成镜像 1docker import test_for_run.tar – test/ubuntu:v1.0 实际上，既可以使用docker load命令来导入镜像存储文件到本地镜像库，也可以用docker import命令来导入一个容器快照到本地镜像库。这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也更大。此外，从容器快照文件导入时可以重新制定标签等元数据信息 容器信息查看使用 docker inspect 来查看 Docker 的底层信息。它会返回一个 JSON 文件记录着 Docker 容器的配置和状态信息。 1234567891011121314151617181920212223runoob@runoob:~$ docker inspect wizardly_chandrasekhar[ &#123; &quot;Id&quot;: &quot;bf08b7f2cd897b5964943134aa6d373e355c286db9b9885b1f60b6e8f82b2b85&quot;, &quot;Created&quot;: &quot;2018-09-17T01:41:26.174228707Z&quot;, &quot;Path&quot;: &quot;python&quot;, &quot;Args&quot;: [ &quot;app.py&quot; ], &quot;State&quot;: &#123; &quot;Status&quot;: &quot;running&quot;, &quot;Running&quot;: true, &quot;Paused&quot;: false, &quot;Restarting&quot;: false, &quot;OOMKilled&quot;: false, &quot;Dead&quot;: false, &quot;Pid&quot;: 23245, &quot;ExitCode&quot;: 0, &quot;Error&quot;: &quot;&quot;, &quot;StartedAt&quot;: &quot;2018-09-17T01:41:26.494185806Z&quot;, &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot; &#125;,...... 第5章 访问docker仓库仓库（repositroy）是集中存放镜像的地方，分为公共仓库和私有仓库。 注意：注册服务器是存放仓库的服务器 docker hub公共镜像市场默认使用的镜像来源都是docker hub中的 1docker search lnmp 时速云镜像市场搭建本地私有仓库使用registry镜像创建私有仓库安装docker之后，可以通过官方提供的registry镜像来简单搭建一套本地私有仓库环境: 1docker run -d -p 5000:5000 registry 它将自动下载并且启动一个registry容器，创建本地的私有仓库服务 在默认情况下，会将仓库创建在容器的/tmp/registry目录下。可以通过-v参数来将镜像文件存放在本地的指定路径。 1docker run -d -p 5000:5000 -v /opt/data/registry:/tmp/registry registry 管理私有仓库-上传镜像因为docker images中的字段，REPOSITORY表示仓库信息，因此，我们在创建了私有仓库之后，要使用镜像，需要对镜像进行tag标签的操作 1[root@master ~]# docker tag ubuntu:14.04 47.93.54.101:5000/test 上述命令解释： 使用tag命令将ubuntu镜像标记为47.93.54.101:5000/test【也就是这个仓库下的】 47.93.54.101:5000是仓库注册服务器，test是仓库名称，test:[name]后面接的是tag信息，没有写的话，默认是latest 接下来使用docker push上传标记的镜像： 1[root@master ~]# docker push 47.93.54.101:5000/test 注意： 客户端采用https，docker registry未采用https服务，由于服务端没有采用https方式，因此客户端无法使用默认的https形式pull镜像，因此我们要做一些操作 服务端： 在/etc/docker/目录下，创建daemon.json文件。在文件中写入： 123[root@master docker]# cat daemon.json&#123;&quot;insecure-registries&quot;:[&quot;47.93.54.101:5000&quot;]&#125; 客户端 在/etc/docker/目录下，创建daemon.json文件。在文件中写入： 123[root@master docker]# cat daemon.json&#123;&quot;insecure-registries&quot;:[&quot;47.93.54.101:5000&quot;]&#125; 注意：不光服务端要配置，客户端也需要配置 配置完毕之后，我们就可以看到客户端能够正常的拉取我们私有仓库中的镜像 第6章 Docker数据管理生产环境中使用docker的过程中，往往需要对数据进行持久化，或者需要在多个容器之间进行数据共享，这必然涉及容器的数据管理操作。 容器中管理数据主要有2种方式 数据卷（data volumes）：容器内数据直接映射到本地主机环境 数据卷容器（data volumes containers）：使用特定容器维护数据卷。 数据卷数据卷是一个可以供容器使用的特殊目录，它将主机操作系统目录直接映射进容器，类似于linux的mount挂载操作。 数据卷可以提供很多有用的特性： 数据卷可以在容器之间共享和重用，容器键传递数据将变得高效方便。 对数据卷内数据的修改会立马生效，无论是容器内操作还是本地操作。 对数据卷的更新不会影响镜像，解耦了应用和数据 卷会一直存在，直到没有容器使用，可以安全的卸载它。 在容器内创建一个数据卷在用docker run命令的时候，使用-v标记可以在容器内创建一个数据卷。多次重复使用-v标记可以创建多个数据卷。 下面使用 training/webapp镜像创建一个web容器，并创建一个数据卷挂载到容器的/webapp目录。 1[root@master ~]# docker run -d -P --name web -v /webapp training/webapp python app.py ​ 注意：这里是在容器内部创建一个目录，而不是宿主机上创建 -P是将容器服务暴露的端口，是自动映射到本地主机的临时端口 挂载宿主机目录作为数据卷使用-v标记也可以指定挂载一个本地的已有目录到容器中去作为数据卷【推荐方式】 [root@master ~]# docker run -d -P –name web -v /src/webapp:/opt/webapp training/webapp python app.py 上述命令加载主机的/src/webapp目录到容器的/opt/webapp目录 本地目录的路径必须是绝对路径，如果目录不存在，docker会自动创建 注意：docker挂载数据卷的默认权限是读写（rw）,用户也可以通过ro指定为只读 1# docker run -d -P --name web -v /src/webapp:/opt/webapp:ro training/webapp python app.py 设置为ro之后，容器内对所挂载数据卷内的数据就无法修改了。 ​ 使用这个功能，在进行一些测试的时候十分方便，比如用户可以将一些程序或者数据放到本地目录中，然后再容器内运行和使用 挂载一个本地主机文件作为数据卷-v标记也可以从主机挂载单个文件到容器中作为数据卷（对应容器中的某一个文件） 1docker run –rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash 注意：如果直接挂载一个文件到容器中，使用文件编辑工具，包括vi或者sed等，可能会造成文件inode的概念，从docker 1.1.0版本起，这会导致报错，所以推荐的方式是直接挂载目录。 数据卷容器如果用户需要再多个容器之间共享一些持续更新的数据，最简单的方式是使用数据卷容器。数据卷容器也是一个容器，但是它的目的是专门用来提供数据卷供其他容器挂载。 首先，创建一个数据卷容器dbdata，并在其中创建一个数据卷挂载到/dbdata： 1docker run -it -v /dbdata --name dbdata ubuntu 【这个dbdata是在容器内部的一个目录，请注意】 作用是：定义一个数据卷 然后，可以在其他容器中使用–volumes-from来挂载dbdata容器中的数据卷 1docker run -it --volumes-from dbdata --name db1 ubuntu 在容器dbdata中查看，数据已经同步过来 可以多次使用 –volume-from参数来从多个容器挂载多个数据卷。还可以从其他已经挂载了容器卷的容器来挂载数据卷。 注意：如果要删除一个数据卷，必须在删除最后一个还挂载着它的容器时，显式使用docker rm -v命令来指定同时删除关联的容器。 使用数据卷容器可以让用户在容器之间自由地升级和移动数据卷。 利用数据卷容器来迁移可以利用数据卷容器对其中的数据卷进行备份、恢复、以实现数据的迁移。 备份使用下面的命令来备份dbdata数据卷容器内的数据卷 123456[root@master ~]# docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdatatar: Removing leading `/&apos; from member names/dbdata//dbdata/a/dbdata/db1 命令解析： 首先利用ubuntu镜像创建了一个容器worker，使用–volumes-from dbdata参数来让worker容器挂载dbdata容器的数据卷。 然后使用-v参数，挂载本地的当前目录到worker容器内部的/backup目录。 worker容器启动之后，使用tar命令，将/dbdata目录备份到容器内的/backup目录下，也就是宿主机的当前目录下。即可完成整个备份过程。 恢复数据操作都需要借助容器来完成，需要借助容器来打通一个通道。 恢复数据的思路（这里指的是恢复数据到另一个没有数据的数据卷容器中）： 创建一个新的数据卷容器 创建一个新的容器，挂载该容器（–volumes-from参数） 挂载本地的目录到/backup下（这时该目录下就会有本地的数据），然后解压其中的数据 命令如下： 123[root@master ~]# docker run -v /dbdata --name dbdata2 ubuntu /bin/bash[root@master ~]# docker run --volumes-from dbdata2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar 第7章 端口映射与容器互联在实际情况中，是需要多个服务组件容器共同协作的情况，因此往往需要多个容器之间能够互相访问到对方的服务。 除了通过网络来进行访问，docker还提供了两个功能满足服务访问的基本需求： 一个是允许映射容器内应用的服务端口到本地宿主主机 另一个是互联机制实现多个容器键通过容器名称来来快速访问 在启动容器的时候，如果不指定对应的参数，在容器外部是无法通过网络来访问容器内的网络应用和服务的。 端口映射实现访问容器所有接口的随机端口可以通过-P或者-p参数来指定端口映射。 当使用-P时，映射一个49000-49900的本机随机端口对应容器的端口 1[root@master ~]# docker run -d -P training/webapp python app.py ​ 可以通过docker logs -f [id]来查看应用信息 1[root@master ~]# docker logs -f 4336a2dbe777 所有接口的指定端口命令格式：hostport:containerport 1[root@master ~]# docker run -d -p 5000:5000 training/webapp python app.py 使用该命令映射所有接口地址 此时绑定所有接口上的所有地址，将本地的5000端口映射搭配容器的5000端口 指定地址的指定端口命令格式：IP:hostport:containerport 1[root@master ~]# docker run -d -p 127.0.0.1:5000:5000 training/webapp python app.py 指定使用本机的哪一个地址进行端口映射 指定地址的任意端口命令格式：IP::containerport 1[root@master ~]# docker run -d -p 127.0.0.1::5000 training/webapp python app.py 还可以使用udp标记来指定udp端口 1[root@master ~]# docker run -d -p 127.0.0.1::5000/udp training/webapp python app.py 查看映射端口配置命令语法：docker port 容id [port] 1234567[root@master ~]# docker port 4336a2dbe7775000/tcp -&gt; 0.0.0.0:32773[root@master ~]# docker port 4336a2dbe777 50000.0.0.0:32773 互联机制实现便捷互访容器的互联（link）是一种让多个容器中应用进行快速交互的方式。他会在源和接受容器之间创建连接关系，接受容器可以通过容器名称快速访问到源容器，而不用指定具体的IP地址。 自定义容器名称例如： 1[root@master ~]# docker run -v /dbdata --name dbdata2 ubuntu /bin/bash 连接系统通过容器名称来执行，因此需要给每个容器都定义一个名称。 容器互联使用—link参数可以让容器之间安全地进行交互 link参数格式为：–link name:alias docker通过两种方式为容器公开连接信息。 更新环境变量 更新/etc/hosts文件。 以下为参考案例： 首先创建一个新的数据库容器 1[root@master registry]# docker run -d --name db training/postgres 创建一个新的web容器，并将它连接到db容器 1[root@master registry]# docker run -d -P --name web --link db:db training/webapp python app.py ​ 查看容器之间的连接信息： 1[root@master registry]# docker run --rm --name web2 --link db:db training/webapp env 查看hosts信息(cat /etc/hosts) 可以看到，hosts文件中包含db的信息和自身的配置信息。 以上都是单机中多容器之间的互联，后续还会涉及跨主机之间的容器通信。 第8章 使用dockerfile创建镜像第9章 docker网络镜像仓库之-Harborgithub主页：https://github.com/goharbor/harbor Features Role based access control: Users and repositories are organized via ‘projects’ and a user can have different permission for images under a project. 用户和镜像仓库是通过项目关联起来的，不同用户在该项目下拥有不同的权限 Policy based image replication: Images can be replicated (synchronized) between multiple registry instances, with auto-retry on errors. Great for load balancing, high availability, multi-datacenter, hybrid and multi-cloud scenarios. 镜像将在多个注册实例中复制，实现高可用、负载均衡、多路选择等功能 Vulnerability Scanning: Harbor scans images regularly and warns users of vulnerabilities. 高危扫描：harbor将会在规律的扫描镜像并且提醒用户相关的危险 LDAP/AD support: Harbor integrates with existing enterprise LDAP/AD for user authentication and management. harbor可以聚合企业现在的LDAP/AD等实现用户认证和管理 Image deletion &amp; garbage collection: Images can be deleted and their space can be recycled. 镜像删除和垃圾收集 Notary: Image authenticity can be ensured. 可以保证镜像的可靠性 Graphical user portal: User can easily browse, search repositories and manage projects. 图形化的用户入口：可以浏览，检索，管理项目 Auditing: All the operations to the repositories are tracked. 审计：所有的操作都可以被追踪 RESTful API: RESTful APIs for most administrative operations, easy to integrate with external systems. 提供api Easy deployment: Provide both an online and offline installer. 部署简单，提供在线和离线两种安装方式 ### Architecture-体系结构 As depicted in the above diagram, Harbor comprises 6 components: Proxy: Components of Harbor, such as registry, UI and token services, are all behind a reversed proxy. The proxy forwards requests from browsers and Docker clients to various backend services. harbor使用代理结构，外部的客户端（浏览器或者docker client）访问调用是通过代理层去实现的 Registry: Responsible for storing Docker images and processing Docker push/pull commands. As Harbor needs to enforce access control to images, the Registry will direct clients to a token service to obtain a valid token for each pull or push request. 注册部分：响应操作docker镜像的请求 harbor为了确保安全性，客户端在调用的时候，需要取得valid token才可以进行操作 Core services: Harbor’s core functions, which mainly provides the following services: UI: a graphical user interface to help users manage images on the Registry Webhook: Webhook is a mechanism configured in the Registry so that image status changes in the Registry can be populated to the Webhook endpoint of Harbor. Harbor uses webhook to update logs, initiate replications, and some other functions. Token service: Responsible for issuing a token for every docker push/pull command according to a user’s role of a project. If there is no token in a request sent from a Docker client, the Registry will redirect the request to the token service. Database: Database stores the meta data of projects, users, roles, replication policies and images. 提供一个图形的用户入口，方便用户在注册钩子系统（registry webhook）中管理镜像。 Job services: used for image replication, local images can be replicated(synchronized) to other Harbor instances. Log collector: Responsible for collecting logs of other modules in a single place. 安装配置harbor参考文献：https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md 环境要求：Harbor is deployed as several Docker containers, and, therefore, can be deployed on any Linux distribution that supports Docker. The target host requires Python, Docker, and Docker Compose to be installed. Hardware Resource Capacity Description CPU minimal 2 CPU 4 CPU is prefered Mem minimal 4GB 8GB is prefered Disk minimal 40GB 160GB is prefered Software Software Version Description Python version 2.7 or higher Note that you may have to install Python on Linux distributions (Gentoo, Arch) that do not come with a Python interpreter installed by default Docker engine version 1.10 or higher For installation instructions, please refer to: https://docs.docker.com/engine/installation/ Docker Compose version 1.6.0 or higher For installation instructions, please refer to: https://docs.docker.com/compose/install/ Openssl latest is prefered Generate certificate and keys for Harbor Network ports Port Protocol Description 443 HTTPS Harbor UI and API will accept requests on this port for https protocol 4443 HTTPS Connections to the Docker Content Trust service for Harbor, only needed when Notary is enabled 80 HTTP Harbor UI and API will accept requests on this port for http protocol Installation Steps** The installation steps boil down to the following Download the installer; Configure harbor.cfg; Run install.sh to install and start Harbor; 实际操作： 下载1$ tar xvf harbor-online-installer-&lt;version&gt;.tgz 安装docker-ce 12345sudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.reposudo yum install docker-ce 启动docker 12[root@localhost tools]# systemctl start docker[root@localhost tools]# systemctl enable docker 安装Docker Compose 12345sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-composedocker-compose --version 配置Configuration parameters are located in the file harbor.cfg There are two categories of parameters in harbor.cfg, required parameters and optional parameters. required parameters: These parameters are required to be set in the configuration file. They will take effect if a user updates them in harbor.cfg and run the install.sh script to reinstall Harbor. optional parameters: These parameters are optional for updating, i.e. user can leave them as default and update them on Web UI after Harbor is started. If they are set in harbor.cfg, they only take effect in the first launch of Harbor. Subsequent update to these parameters in harbor.cfg will be ignored. Note: If you choose to set these parameters via the UI, be sure to do so right after Harbor is started. In particular, you must set the desired auth_mode before registering or creating any new users in Harbor. When there are users in the system (besides the default admin user), auth_mode cannot be changed. 配置文件中有2种配置内容，一种是必须配置的参数，一种是可选参数 可选参数：如果你将可选参数设置在配置文件里面，也可以在安装完毕只有在web UI上进行设置，如果在启动之前配置在配置文件里面，那么当启动之后再在web上修改，那么修改将会是无效的，不会被刷新到配置文件当中。 可选参数：可以再harbor启动之后，在web界面进行更新，如果这些参数是在在配置文件中的，那么不会即时生效。配置文件中的只会在启动之后生效 修改完毕之后的配置如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@app028-dev harbor]# less harbor.cfg | egrep -v &apos;^$|^#&apos;hostname = dhub-dev.dwbops.comui_url_protocol = httpsmax_job_workers = 3customize_crt = onssl_cert = /data/cert/server.cerssl_cert_key = /data/cert/server.keysecretkey_path = /dataadmiral_url = NAlog_rotate_count = 50log_rotate_size = 200Memail_identity =email_server = smtp.mydomain.comemail_server_port = 25email_username = sample_admin@mydomain.comemail_password = abcemail_from = admin &lt;sample_admin@mydomain.com&gt;email_ssl = falseemail_insecure = falseharbor_admin_password = dHarbor12345auth_mode = db_authldap_url = ldaps://ldap.mydomain.comldap_basedn = ou=people,dc=mydomain,dc=comldap_uid = uidldap_scope = 2ldap_timeout = 5ldap_verify_cert = trueself_registration = ontoken_expiration = 30project_creation_restriction = everyonedb_host = mysqldb_password = root123db_port = 3306db_user = rootredis_url =clair_db_host = postgresclair_db_password = passwordclair_db_port = 5432clair_db_username = postgresclair_db = postgresuaa_endpoint = uaa.mydomain.orguaa_clientid = iduaa_clientsecret = secretuaa_verify_cert = trueuaa_ca_cert = /path/to/ca.pemregistry_storage_provider_name = filesystemregistry_storage_provider_config = 自己做测试时，将url类型设置成为http，并且将域名设置成为：harbar.wxh.com 修改完配置之后： 1$ sudo ./install.sh 整个的安装过程如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119[root@localhost harbor]# ./install.sh[Step 0]: checking installation environment ...Note: docker version: 18.06.1Note: docker-compose version: 1.22.0[Step 1]: loading Harbor images ...651f69aef02c: Loading layer [==================================================&gt;] 135.8MB/135.8MB40a1aad64343: Loading layer [==================================================&gt;] 23.24MB/23.24MB3fe2713e4072: Loading layer [==================================================&gt;] 12.16MB/12.16MBba3a1eb0e375: Loading layer [==================================================&gt;] 17.3MB/17.3MB447427ec5e1a: Loading layer [==================================================&gt;] 15.87kB/15.87kB4ccb4026663c: Loading layer [==================================================&gt;] 3.072kB/3.072kB16faa95946a1: Loading layer [==================================================&gt;] 29.46MB/29.46MBLoaded image: vmware/notary-server-photon:v0.5.1-v1.4.0fa7ba9fd42c9: Loading layer [==================================================&gt;] 10.95MB/10.95MB4e400f9ae23e: Loading layer [==================================================&gt;] 17.3MB/17.3MB2802fb27c88b: Loading layer [==================================================&gt;] 15.87kB/15.87kBe6367a4e1e1e: Loading layer [==================================================&gt;] 3.072kB/3.072kB8ece8dfcdd98: Loading layer [==================================================&gt;] 28.24MB/28.24MBLoaded image: vmware/notary-signer-photon:v0.5.1-v1.4.0a7dd1a8afcaf: Loading layer [==================================================&gt;] 396.7MB/396.7MB05adebbe496f: Loading layer [==================================================&gt;] 9.216kB/9.216kB86eb534949fa: Loading layer [==================================================&gt;] 9.216kB/9.216kBd7f127c69380: Loading layer [==================================================&gt;] 7.68kB/7.68kB5ac1c4dc5ee9: Loading layer [==================================================&gt;] 1.536kB/1.536kBd0bec56b5b1a: Loading layer [==================================================&gt;] 9.728kB/9.728kB4bbe83860556: Loading layer [==================================================&gt;] 2.56kB/2.56kBe526f9e6769f: Loading layer [==================================================&gt;] 3.072kB/3.072kBLoaded image: vmware/harbor-db:v1.4.01cff102bbda2: Loading layer [==================================================&gt;] 154.1MB/154.1MB04c9f3e07de1: Loading layer [==================================================&gt;] 10.75MB/10.75MB7b6c7bf54f5c: Loading layer [==================================================&gt;] 2.048kB/2.048kB42f8acdb7fe3: Loading layer [==================================================&gt;] 48.13kB/48.13kB5b6299d0a1df: Loading layer [==================================================&gt;] 10.8MB/10.8MBLoaded image: vmware/clair-photon:v2.0.1-v1.4.06534131f457c: Loading layer [==================================================&gt;] 94.76MB/94.76MB73f582101e4b: Loading layer [==================================================&gt;] 6.656kB/6.656kB86d847823c48: Loading layer [==================================================&gt;] 6.656kB/6.656kBLoaded image: vmware/postgresql-photon:v1.4.05cd250d5a352: Loading layer [==================================================&gt;] 23.24MB/23.24MBad3fd52b54f3: Loading layer [==================================================&gt;] 14.99MB/14.99MB13b1e24cc368: Loading layer [==================================================&gt;] 14.99MB/14.99MBLoaded image: vmware/harbor-adminserver:v1.4.0c26c69706710: Loading layer [==================================================&gt;] 23.24MB/23.24MB223f6fe02cc8: Loading layer [==================================================&gt;] 23.45MB/23.45MB1fc843c8698a: Loading layer [==================================================&gt;] 7.168kB/7.168kBe09293610ee7: Loading layer [==================================================&gt;] 10.39MB/10.39MBd59f9780b1d8: Loading layer [==================================================&gt;] 23.44MB/23.44MBLoaded image: vmware/harbor-ui:v1.4.0dd4753242e59: Loading layer [==================================================&gt;] 73.07MB/73.07MB95aed61ca251: Loading layer [==================================================&gt;] 3.584kB/3.584kB1864f9818562: Loading layer [==================================================&gt;] 3.072kB/3.072kBda2a19f80b81: Loading layer [==================================================&gt;] 4.096kB/4.096kB058531639e75: Loading layer [==================================================&gt;] 3.584kB/3.584kBa84e69fb619b: Loading layer [==================================================&gt;] 10.24kB/10.24kBLoaded image: vmware/harbor-log:v1.4.0b1056051f246: Loading layer [==================================================&gt;] 23.24MB/23.24MB07678065e08b: Loading layer [==================================================&gt;] 19.19MB/19.19MBa2d9bdb8f5fb: Loading layer [==================================================&gt;] 19.19MB/19.19MBLoaded image: vmware/harbor-jobservice:v1.4.07f58ce57cd5e: Loading layer [==================================================&gt;] 4.805MB/4.805MBLoaded image: vmware/nginx-photon:v1.4.04c8965978b77: Loading layer [==================================================&gt;] 23.24MB/23.24MB1466c942edde: Loading layer [==================================================&gt;] 2.048kB/2.048kBac5c17331735: Loading layer [==================================================&gt;] 2.048kB/2.048kB86824c7c466a: Loading layer [==================================================&gt;] 2.048kB/2.048kBfd3bd0e70d67: Loading layer [==================================================&gt;] 22.8MB/22.8MBb02195d77636: Loading layer [==================================================&gt;] 22.8MB/22.8MBLoaded image: vmware/registry-photon:v2.6.2-v1.4.0Loaded image: vmware/photon:1.0Loaded image: vmware/mariadb-photon:v1.4.0454c81edbd3b: Loading layer [==================================================&gt;] 135.2MB/135.2MBe99db1275091: Loading layer [==================================================&gt;] 395.4MB/395.4MB051e4ee23882: Loading layer [==================================================&gt;] 9.216kB/9.216kB6cca4437b6f6: Loading layer [==================================================&gt;] 9.216kB/9.216kB1d48fc08c8bc: Loading layer [==================================================&gt;] 7.68kB/7.68kB0419724fd942: Loading layer [==================================================&gt;] 1.536kB/1.536kB526b2156bd7a: Loading layer [==================================================&gt;] 637.8MB/637.8MB9ebf6900ecbd: Loading layer [==================================================&gt;] 78.34kB/78.34kBLoaded image: vmware/harbor-db-migrator:1.4[Step 2]: preparing environment ...Generated and saved secret to file: /data/secretkeyGenerated configuration file: ./common/config/nginx/nginx.confGenerated configuration file: ./common/config/adminserver/envGenerated configuration file: ./common/config/ui/envGenerated configuration file: ./common/config/registry/config.ymlGenerated configuration file: ./common/config/db/envGenerated configuration file: ./common/config/jobservice/envGenerated configuration file: ./common/config/log/logrotate.confGenerated configuration file: ./common/config/jobservice/app.confGenerated configuration file: ./common/config/ui/app.confGenerated certificate, key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crtThe configuration files are ready, please use docker-compose to start the service.[Step 3]: checking existing instance of Harbor ...[Step 4]: starting Harbor ...Creating network &quot;harbor_harbor&quot; with the default driverCreating harbor-log ... doneCreating harbor-db ... doneCreating registry ... doneCreating harbor-adminserver ... doneCreating harbor-ui ... doneCreating nginx ... doneCreating harbor-jobservice ... done✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at http://harbar.wxh.com.For more details, please visit https://github.com/vmware/harbor .[root@localhost harbor]# 配置文件Required parameters: hostname: The target host’s hostname, which is used to access the UI and the registry service. It should be the IP address or the fully qualified domain name (FQDN) of your target machine, e.g., 192.168.1.10 or reg.yourdomain.com. Do NOT use localhost or 127.0.0.1 for the hostname - the registry service needs to be accessible by external clients! hostname一般配置为域名,也就是整个harbor的入口 ui_url_protocol: (http or https. Default is http) The protocol used to access the UI and the token/notification service. If Notary is enabled, this parameter has to be https. By default, this is http. To set up the https protocol, refer to Configuring Harbor with HTTPS Access. 走HTT还是HTTPS db_password: The root password for the MySQL database used for db_auth. Change this password for any production use! max_job_workers: (default value is 3) The maximum number of replication workers in job service. For each image replication job, a worker synchronizes all tags of a repository to the remote destination. Increasing this number allows more concurrent replication jobs in the system. However, since each worker consumes a certain amount of network/CPU/IO resources, please carefully pick the value of this attribute based on the hardware resource of the host. 做复制工作的进程数量，默认3个，这些进程，将这些镜像同步到远端的存储中 每个进程都需要消耗系统的资源，因此合理设置数量 customize_crt: (on or off. Default is on) When this attribute is on, the prepare script creates private key and root certificate for the generation/verification of the registry’s token. Set this attribute to off when the key and root certificate are supplied by external sources. Refer to Customize Key and Certificate of Harbor Token Service for more info. ssl_cert: The path of SSL certificate, it’s applied only when the protocol is set to https ssl_cert_key: The path of SSL key, it’s applied only when the protocol is set to https secretkey_path: The path of key for encrypt or decrypt the password of a remote registry in a replication policy. log_rotate_count: Log files are rotated log_rotate_count times before being removed. If count is 0, old versions are removed rather than rotated. log_rotate_size: Log files are rotated only if they grow bigger than log_rotate_size bytes. If size is followed by k, the size is assumed to be in kilobytes. If the M is used, the size is in megabytes, and if G is used, the size is in gigabytes. So size 100, size 100k, size 100M and size 100G are all valid. http_proxy: Config http proxy for Clair, e.g. http://my.proxy.com:3128. https_proxy: Config https proxy for Clair, e.g. http://my.proxy.com:3128. no_proxy: Config no proxy for Clair, e.g. 127.0.0.1,localhost,core,registry. 镜像操作上传镜像push的格式为：docker push reg.yourdomain.com/myproject/myrepo:mytag 注意首先需要登录 将要上传的镜像打上标志 123docker tag hello-world harbar.wxh.com/apps/hello-worlddocker push harbar.wxh.com/apps/hello-world 这种上传的话，默认是打上latest的标志 打上指定的标签： 12docker tag hello-world harbar.wxh.com/apps/hello-world:v1docker push harbar.wxh.com/apps/hello-world:v1 harbor镜像删除在web页面上删除镜像实际上只是执行的软删除，因为镜像存在很强的文件系统依赖关系 Harbor的UI界面上先删除镜像，但这个操作并没有删除磁盘上存放的镜像文件，只是镜像文件manifest的映射关系，还需要通过GC来删除。 CAUTION: If both tag A and tag B refer to the same image, after deleting tag A, B will also get deleted. if you enabled content trust, you need to use notary command line tool to delete the tag’s signature before you delete an image. 注意，如果标签A和B都指向都一个镜像（比如hello-world的2个镜像），那么删除一个之后，另外一个也会消失 先停止Harbor： 1docker-compose stop 通过带有–dry-run选项，可以查看到将要删除的镜像文件： 1docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.2-photon garbage-collect --dry-run /etc/registry/config.yml1 不带–dry-run选项，直接执行删除： 1docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.2-photon garbage-collect /etc/registry/config.yml1 再启动Harbor： 1docker-compose start 使用harbor-go-client项目地址：https://github.com/moooofly/harbor-go-client 操作步骤： 下载 1go get -u github.com/moooofly/harbor-go-client 安装 1make 在执行make的时候，可能会存在很多的依赖关系 有以下依赖关系需要解决： 1go get -u github.com/alecthomas/gometalinter 镜像清理策略需求： 暂时不做删除 repo 的处理【这部分手动处理】 保留 60 天内创建的所有 tag ，在 60 天之前创建的 tag ，额外保留 10 个； 标签数只有 1的镜像，不清理 保留最后一次更新的tag，有些image比较稳定，有可能超过60天都没有修改，但是却一直在用 针对一些特殊的（比如每天5个tag的镜像，那么60天就有300个），这个单独特殊处理 最终： 针对 tag ：保留 60 天内创建的所有 tag ，在 60 天之前创建的 tag ，额外保留 10 个； 针对 repo ：暂时不做删除 repo 的处理（不太好确定 repo 是否还在使用，理论上讲每个 repo 下至少应该有一个 tag 是被需要的；若打算删除，则建议 repo 负责人自行进行删除操作）； 私有仓库暂时不做处理； 具体实现： harbor 主要概念的关系：1 个 project -&gt; 每个 project 下具有 N 种不同的 repos &gt; 每个 repo 下具有 M 个 tags project 有创建时间，但这个对我们的处理策略来说没有用处； repo 有创建时间和 pull 时间，该 pull 时间对应 repo 下任意一个 tag ，最新一次，被拉取的那个时间 tag 有创建时间，但没有针对 tag 的 pull 时间（harbor 中定义的数据结构中不支持）； 因此 保留 60 天内的 tag”，这个根据 tag 的创建时间 “60 天之外的看 pull 的数量，关注 60 天之外是不是被 pull 过”，由于 pull 数量是针对 repo 整体的，无法对应到具体的 tag ，即在 API 层面无法方便的知道哪些 tag 最近被 pull 过（当然如果一定要做，就只能沟通分析 log 来搞，性价比不高），所以，只能根据 tag 创建时间的先后，“武断”的认为，后创建的 tag 应该是用户最想保留的； “如果没有被 pull 过，则只保留最新 5 个 tag”，根据上一条的说明，某个 tag 是否被 pull 过是无法知道的，但目前可以做到根据 tag 的创建时间进行保留（满足保留最新 N 个需求）； harbor镜像复制参考链接：https://github.com/goharbor/harbor/blob/master/docs/user_guide.md 该功能是面向项目的，系统管理员设置之后，匹配了过滤规则的项目，在触发了事先定义好的触发条件之后，这些项目就会被复制到远程的另一个仓库中。 如果在远程镜像仓库中，改项目不存在，那么就会自动创建这个项目 如果在远程仓库中，这个项目已经存在，并且配置的用户对这个项目没有写的权限，那么这个操作将会失败 注意：用户信息不会被复制 因为网络的原因，在复制传输的过程中，可能会出现一些延迟。如果复制job是因为网络原因而导致失败的，那么这个任务将会在几分钟之后再次尝试，一直尝试，知道网络恢复正常。 注意：因为api等原因，不同版本的镜像复制可能会失败，所以尽量使用同一个版本。 创建复制规则Click NEW REPLICATION RULE under Administration-&gt;Replications 注意，在创建endpoint的时候，直接test connection是会报错：“harbor Failed to ping endpoint” 这是因为网络问题导致，在内网访问的时候，还需要额外的添加hosts文件，详见注意事项 注意：在创建完毕之后，默认不会执行同步，需要手动点击一下replication 删除replication规则Only rules which have no pending/running/retrying jobs can be deleted. 只有当改规则下面没有正在运行或者等待运行或者正在重传的jobs时，才可以删除 注意事项在创建endpoint的时候，如果事先没有再容器内存配置对端的地址，那么会报连接错误 官方的issues：https://github.com/goharbor/harbor/issues/2221 Harborclient详见页面：https://github.com/int32bit/python-harborclient/blob/master/README.zh.md 常见问题1&gt; Why can not push image 192.168.0.1/hello-world:latest to Harbor? [A] At least two namespaces are needed for repository name in Harbor, so tag the image as 192.168.0.1/project_name/hello-world:latest should fix this. (Create the project on the web page first) 也就是说，在上传镜像的时候，应该是下面这种格式： 1docker push 192.168.0.1/project_name/hello-world:latest 因为在harbor中有项目的概念，也就是：访问地址/项目名称/镜像名称/版本标签 在使用dockerhub等进行镜像仓库的时候，用户名/id就是项目名称，因此不能创建多个项目名称，因为这种方式适合个人，但是不适用于企业。 企业中需要根据不同的项目类型进行分类存储，例如：apps、中间件等 证书问题证书生成： 1openssl req -sha256 -x509 -days 365 -nodes -newkey rsa:4096 -keyout harbar.wxh.com.key -out harbar.wxh.com.crt 注意，一些name的字段要配置成为域名harbar.wxh.com 生成之后，将证书存放到指定位置，然后修改配置文件指向这些证书文件 123less harbor.cfg | egrep -v &quot;^$|^#&quot;ssl_cert = /data/cert/harbar.wxh.com.crtssl_cert_key = /data/cert/harbar.wxh.com.key 然后需要对docker进行一些配置 mkdir -p /etc/docker/certs.d/harbar.wxh.com 然后将上面的文件复制到这个目录之下，并将/data/cert/harbar.wxh.com.crt重命名为/data/cert/harbar.wxh.com.cert 文件创建为目录问题/data1/harbor/data/secretkey secretkey为文件，而不是目录，在一些时候可能会出现这种问题，当出现这种问题的时候，将该目录清空，然后重新安装即可 汇总-docker常用命令12345678910111213141516docker build -t friendlyname . # Create image using this directory&apos;s Dockerfiledocker run -p 4000:80 friendlyname # Run &quot;friendlyname&quot; mapping port 4000 to 80docker run -d -p 4000:80 friendlyname # Same thing, but in detached modedocker container ls # List all running containersdocker container ls -a # List all containers, even those not runningdocker container stop &lt;hash&gt; # Gracefully stop the specified containerdocker container kill &lt;hash&gt; # Force shutdown of the specified containerdocker container rm &lt;hash&gt; # Remove specified container from this machinedocker container rm $(docker container ls -a -q) # Remove all containersdocker image ls -a # List all images on this machinedocker image rm &lt;image id&gt; # Remove specified image from this machinedocker image rm $(docker image ls -a -q) # Remove all images from this machinedocker login # Log in this CLI session using your Docker credentialsdocker tag &lt;image&gt; username/repository:tag # Tag &lt;image&gt; for upload to registrydocker push username/repository:tag # Upload tagged image to registrydocker run username/repository:tag # Run image from a registry]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>虚拟化</category>
        <category>Docker+k8s</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MAC操作技巧]]></title>
    <url>%2F2018%2F08%2F11%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FIT%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7%2FMAC%E6%93%8D%E4%BD%9C%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[美式键盘对应关系参考链接：https://support.apple.com/zh-cn/HT202676 在Mac的快捷键中经常会有一些符号，比如⌘、⌥、⇧、⌃等，而Mac下只有command键上有一个⌘的符号，而其他按键均没有符号，很多人可能不知道这是什么意思，之所以只有command键上有一个符号，而其他按键上没有，是因为： 只有command健才是Mac下唯一独有的一个特殊按键，而shift、alt（option）、control、caps lock、tab等在其他系统下都有，所以Mac在command键上做一个符号，用于表示这一按键的特殊性； 我们在生活中能接触到的外接键盘基本上都是美式键盘，下面是两种键盘的布局对应关系，在使用外接键盘的时候，需要有充分的了解。 文字描述： ⌘ —— Command (Windows键) ⌃ —— Control(Ctrl键) ⌥ —— Option (Alt) ⇧ —— Shift ⇪ —— Caps Lock FN —— FN(Insert) 括号里面是Windows对应的按键。以前Ctrl+C,Ctrl+V 现在要用Win+C,Win+V。 图形展示： Windows 标志：按下 Command (⌘) 键 退格或删除：按下 Delete 键 回车或 ⏎：按下 Return 键 Alt（左）：按下 Option 键 Alt GR（右）：按下 Option + Control 组合键 应用程序：Apple 键盘上没有这个按键 MAC常用快捷键概览先来一张图 全局操作 ⌃⌘ + f: 进入全屏模式 强制退出应用如果 Mac 上的某个应用停止响应，并且您无法正常退出该应用，则可以使用“强制退出”来关闭该应用。 同时按住三个按键：Option、Command 和 Esc (Escape) 键。这类似于在 PC 上按下 Control-Alt-Delete。或者，在屏幕左上角的苹果 () 菜单中选取“强制退出”。 强制刷新页面 正常刷新：command+r 强制刷新页面（刷新页面缓存）：command+shift+r 切换全屏页面ctrl+方向键的左右 控制中心ctrl+上箭头 软件包管理MAC上管理软件包我们一般使用Homebrew来实现 Homebrew是MAC必备神器之一，作为Mac OSX上的软件包管理工具，它能在Mac中方便的安装软件或者卸载软件， 简单到只需要一个命令。 安装： 1ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 命令参数： 123456brew install 安装软件brew list 列出已安装的软件brew update 更新brewbrew home 用浏览器打开brew的官方网站brew info 显示软件信息brew deps 显示包依赖 MAC下实现AutoHotKey功能在MAC下没有类似autohotkey的软件，但是它给我们提供了相应的工具，我们可以自定义的去实现功能 步骤1：从其他中进入automator（中文版的名称为：“自动操作”） 就是下面这位仁兄： 步骤2：创建服务 步骤3： 创建applescript，如下图所示，用鼠标将applescript选中并拖到右栏中 注意，需要先将服务收到一栏中从文本修改为没有输入 步骤4：编辑applescript 有关applescript的相关内容，网友可以自行查找，网上有非常多的资料。 简单说明下： tell语句，在这里调用谷歌浏览器程序（程序的实际名称可以打开终端，进入/Applications目录下ls查看） activate语句，这这里将调用出来的窗口显示在最前端 open location 语句，实际的操作，后面的参数是具体的链接地址 end tell语句，结束调用 步骤5：运行测试 点击两个运行中任一一个，可以看到网页会在最前端弹出，代码执行成功之后，缩进和颜色都会发生相应的变化。 步骤6： 保存为服务 输入command+s，在弹出的对话框中，输入自定义的名称 步骤7：设置快捷键 打开系统偏好设置–&gt;键盘—&gt;快捷键–&gt;服务–&gt;通用—&gt;选中刚才保存的服务–&gt;双击—&gt;在编辑器中，敲下自定义的快捷键，系统将会自动识别显示，如下图所示： 这个时候，设置就全部完毕，接下来我们就可以随时根据快捷键调用浏览器打开网页。 Iterm2操作快捷键标签 新建标签：command + t 关闭标签：command + w 切换标签：command + 数字 command + 左右方向键 切换全屏：command + enter 查找：command + f 分屏 垂直分屏：command + d 水平分屏：command + shift + d 切换屏幕：command + option + 方向键 或者 command + [ ] 命令 查看历史命令：command + ; 查看剪贴板历史：command + shift + h 上一条命令：ctrl + p 搜索命令历史：ctrl + r 行内 清除当前行：ctrl + u 到行首：ctrl + a 到行尾：ctrl + e 前进后退：ctrl + f/b (相当于左右方向键) 删除当前光标的字符：ctrl + d 删除光标之前的字符：ctrl + h 删除光标之前的单词：ctrl + w 删除到文本末尾：ctrl + k 交换光标处文本：ctrl + t 其他 清屏1：command + r 清屏2：ctrl + l 清屏3：clear 进入和退出全屏: Command + Enter 查看当前终端中光标的位置: Command + / 开启和关闭背景半透明: Command + u 清屏（重置当前终端）: Command + r 连接jumpserver Profile -&gt; Open Profiles… -&gt; Edit Profiles… 点击左下角+号 输入Profile Name，比如jumper 右边Command下选择Command，然后输入 1ssh -i /Users/yourname/.ssh/id_rsa username@ip -p port 关闭所有窗口 在Iterm2的一个窗口中选择右键New Tab或者command+o,在弹出的页面中选择刚创建的jumper，然后回车就登录上了。 注意，如果给RSA秘钥设置了密码，又不想每次在登录的时候都输出密码，这个时候我们可以在命令行中输入以下命令 1ssh-add -K /Users/yourname/.ssh/id_rsa 输入一次之后，后续就不用再次输入。 实现rz/sz功能Mac上iTerm原生不支持rz/sz命令，也就是不支持Zmodem来进行文件传输，不过只要通过简单的配置就可以实现。网上的教程一大把，这里就简单的记录一下过程。 安装lrzsz首先安装Homebrew(这里不写这个过程)，然后通过它先给Mac安装lrzsz。在终端下输入brew install lrzsz，静等一会即可安装完毕。 1brew install lrzsz 下载iTerm2辅助文件iTerm不能直接使用lrzsz，不过网上有大神提供了两个辅助脚本。我们只需要把文件下载到 /usr/local/bin/目录下并赋予可执行权限即可。 12345cd /usr/local/binwget https://raw.githubusercontent.com/mmastrac/iterm2-zmodem/master/iterm2-send-zmodem.shwget https://raw.githubusercontent.com/mmastrac/iterm2-zmodem/master/iterm2-recv-zmodem.shchmod +x iterm2-recv-zmodem.sh iterm2-send-zmodem.sh 这两个脚本实际是使用AppleScript来弹出文件选择窗口，然后把选中的文件名称传递给rzsz命令。我们打开其中一个看下代码。如果这一部分看不懂没关系，直接跳过即可，对后续的配置使用没有任何不良影响 配置iTerm2触发器这一步最关键，是在iTerm里面配置触发器，当监控到特定字符串的时候执行刚才下载的两个文件。为了使用方便，我专门建立了一个Profile配置，名字是Remote，并且配合后面的autossh使用。 打开iTerm2 -&gt; Preferences -&gt; Profiles 选择 Advanced 设置 Triggers ，点击 Edit 在弹出窗口中进行如下配置，最后的Instant一定要勾选上。 配置的具体内容在这里 1234567Regular expression: rz waiting to receive.\*\*B0100Action: Run Silent CoprocessParameters: /usr/local/bin/iterm2-send-zmodem.shRegular expression: \*\*B00000000000000Action: Run Silent CoprocessParameters: /usr/local/bin/iterm2-recv-zmodem.sh 重新启动iTerm之后，rz/sz就应该可以正常使用了。 Forklift操作在文件夹中搜索文件：command+s 常用文件夹-Favorites：alt+command+f ftp等传输工具：command+k 前进/后退：command+[/] 其他操作pycharm-光标变粗问题mac下默认的pycharm的光标是为粗体的改写模式，这是因为安装的时候装了ideaVim插件，改为竖线光标的方法：把ideaVim插件去掉（点击pycharm–&gt;preference–&gt;plugins–&gt;搜索ideavim，然后将该插件勾除掉即可） MAC版本snipaste截图后无法输入中文问题参考资料：https://jingyan.baidu.com/article/c1a3101e635d6ade646deb56.html 点击菜单栏截图软件的图标，选择退出软件。 接着按键盘上面的control+space ，选择拼音输入法。 这个时候就可以启动截图软件了，按键盘上的cmmand+space，在黑色框输入软件的名字回车。 点击软件图标，选择截图或者是按fn+f1，进行桌面的截图。 在截图上面，按空格键调出截图软件工具条，然后点击工具条上面的T，这个时候就可以在图片上面进行中文的标注了。总结就是用不了重启软件即可。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>IT基础知识</category>
        <category>常用软件工具</category>
      </categories>
      <tags>
        <tag>MAC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内核模块操作命令-lsmod+rmmod+modinfo+modprobe]]></title>
    <url>%2F2018%2F07%2F13%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FLinux%E5%91%BD%E4%BB%A4%2F%E5%86%85%E6%A0%B8%E6%A8%A1%E5%9D%97%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4-lsmod%2Brmmod%2Bmodinfo%2Bmodprobe%2F</url>
    <content type="text"><![CDATA[本篇主要讲解和Linux内核模块相关的操作命令 lsmod-查看内核模块信息lsmod命令用于显示已经加载到内核中的模块的状态信息。执行lsmod命令后会列出所有已载入系统的模块。 Linux操作系统的核心具有模块化的特性，应此在编译核心时，可以不用把全部的功能都放入核心，而是将这些功能编译成一个个单独的模块，待需要时再分别载入使用。 命令的输出如下： 1234567[root@lvs001 modprobe.d]# lsmod Module Size Used byiptable_nat 5923 0 nf_nat 22676 1 iptable_natnf_conntrack_ipv4 9186 3 iptable_nat,nf_natnf_conntrack 79537 3 iptable_nat,nf_nat,nf_conntrack_ipv4nf_defrag_ipv4 1483 1 nf_conntrack_ipv4 12345[root@lvs001 modprobe.d]# lsmod | grep ip_vsip_vs_rr 1420 0 ip_vs 126705 2 ip_vs_rrlibcrc32c 1246 1 ip_vsipv6 336368 913 ip_vs,ib_ipoib,ib_addr 说明： 第1列：表示模块的名称。 第2列：表示模块的大小。 第3列：表示该模块调用其他模块的个数 第4列：显示该模块被其他什么模块调用 通常在使用lsmod命令时，都会采用类似lsmod | grep -i ipvs这样的命令来查询当前系统是否加载了某些模块。 modinfo-查看内核模块信息modinfo会显示kernel模块的对象文件，以显示该模块的相关信息。 modinfo列出Linux内核中命令行指定的模块的信息。若模块名不是一个文件名，则会在/lib/modules/version 目录中搜索，就像modprobe一样。 modinfo默认情况下，为了便于阅读，以下面的格式列出模块的每个属性：fieldname : value。 123456参 数： -a或--author 显示模块开发人员。 -d或--description 显示模块的说明。 -h或--help 显示modinfo的参数使用方法。 -p或--parameters 显示模块所支持的参数。 -V或--version 显示版本信息。 1234567[root@lvs001 modprobe.d]# modinfo ip_vsfilename: /lib/modules/2.6.32-696.el6.x86_64/kernel/net/netfilter/ipvs/ip_vs.kolicense: GPLsrcversion: 0FB85919D62C4255E412E5Cdepends: ipv6,libcrc32cvermagic: 2.6.32-696.el6.x86_64 SMP mod_unload modversions parm: conn_tab_bits:Set connections&apos; hash size (int) 注意，使用lsmod不能看到内核的相关参数配置，而使用modinfo命令则可以显示 rmmod-卸载内核模块rmmod命令 用于从当前运行的内核中移除指定的内核模块。 执行rmmod指令，可删除不需要的模块。 12345选项信息：-v：显示指令执行的详细信息；-f：强制移除模块，使用此选项比较危险；-w：等待着，直到模块能够被除时在移除模块；-s：向系统日志（syslog）发送错误信息。 12[root@lvs001 modprobe.d]# rmmod ip_vsERROR: Module ip_vs is in use by ip_vs_rr 使用rmmod卸载模块的时候，提示信息会比使用modprobe -r 的输出更详细，此时会显示该模块的被调用情况 insmod-载入内核模块insmod(install module)命令用于载入模块。 Linux有许多功能是通过模块的方式，在需要时才载入kernel。如此可使kernel较为精简，进而提高效率，以及保有较大的弹性。这类可载入的模块，通常是设备驱动程序。 语法: 1insmod [-fkmpsvxX][-o &lt;模块名称&gt;][模块文件][符号名称 = 符号值] 1234567891011参数说明：-f 不检查目前kernel版本与模块编译时的kernel版本是否一致，强制将模块载入。-k 将模块设置为自动卸除。-m 输出模块的载入信息。-o&lt;模块名称&gt; 指定模块的名称，可使用模块文件的文件名。-p 测试模块是否能正确地载入kernel。-s 将所有信息记录在系统记录文件中。-v 执行时显示详细的信息。-x 不要汇出模块的外部符号。-X 汇出模块所有的外部符号，此为预设置。 在Linux中，modprobe和insmod都可以用来加载module，不过现在一般都推荐使用modprobe而不是insmod了。modprobe和insmod的区别是什么呢？ modprobe可以解决load module时的依赖关系，比如load moudleA就必须先load mouduleB之类的，它是通过/lib/modules//modules.dep文件来查找依赖关系的。而insmod不能解决依赖问题。 modprobe默认会去/lib/modules/目录下面查找module，而insmod只在给它的参数中去找module（默认在当前目录找）。 但是insmod也有它的有用之处，举个例子吧。 有/root/my-mod.ko这个module，cd /root/，然后用insmod my-mod.ko(insmod /root/my-mod.ko)就可以insert这个module了， 但是用modprobe my-mod.ko(modprobe /root/my-mod.ko)却提示”FATAL: Module my-mod.ko not found”，这就是因为modprobe是到/lib/modules/uname -r/下去找module的，如果没找到就是这样了。 depmod-分析模块依赖性modprobe-内核模块操作modprobe命令用于智能地向内核中加载模块或者从内核中移除模块。 modprobe可载入指定的个别模块，或是载入一组相依的模块。 modprobe会根据depmod所产生的相依关系，决定要载入哪些模块。若在载入过程中发生错误，在modprobe会卸载整组的模块。 1234567891011参数选项-a或--all：载入全部的模块/指定模块；-c或--show-conf：显示所有模块的设置信息；-d或--debug：使用排错模式；-l或--list：显示可用的模块；-r或--remove：卸载模块；-t或--type：指定模块类型；-v或--verbose：执行时显示详细的信息；-V或--version：显示版本信息；-help：显示帮助。 例如： 12卸载：modprobe -r ip_vs 载入：modprobe -a ip_vs get_module需要安装sysfsutils包之后才能产生该命令 1yum -y install sysfsutils 12345678910111213141516171819202122232425262728293031323334[root@lvs001 modprobe.d]# get_module ip_vs initstate : live refcnt : 2 srcversion : 0FB85919D62C4255E412E5CParameters: conn_tab_bits : 12Sections: .altinstr_replacement : 0xffffffffa039c2b8 .altinstructions : 0xffffffffa039ef98 .bss : 0xffffffffa03a1c60 .data : 0xffffffffa039ff00 .data.cacheline_aligned : 0xffffffffa03a1580 .data.read_mostly : 0xffffffffa03a1040 .exit.text : 0xffffffffa039c40e .gnu.linkonce.this_module : 0xffffffffa03a1a20 .init.text : 0xffffffffa03aa000 .note.gnu.build-id : 0xffffffffa039c454 .rheldata : 0xffffffffa039fee0 .rodata : 0xffffffffa039c480 .rodata.str1.1 : 0xffffffffa039d6e8 .rodata.str1.8 : 0xffffffffa039dc90 .smp_locks : 0xffffffffa039eb68 .strtab : 0xffffffffa03ae2f8 .symtab : 0xffffffffa03aa4d0 .text : 0xffffffffa038a000 __kcrctab_gpl : 0xffffffffa039f540 __kcrctab : 0xffffffffa039fe80 __ksymtab_gpl : 0xffffffffa039f4f0 __ksymtab_strings : 0xffffffffa039f590 __ksymtab : 0xffffffffa039fdc0 __mcount_loc : 0xffffffffa039f6e8 __param : 0xffffffffa039f568 __verbose : 0xffffffffa03a1980]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>Linux命令</category>
      </categories>
      <tags>
        <tag>lsmod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网卡中断与CPU绑定]]></title>
    <url>%2F2018%2F07%2F11%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%2F%E7%BD%91%E7%BB%9C%E8%B0%83%E4%BC%98%2F%E7%BD%91%E5%8D%A1%E4%B8%AD%E6%96%AD%E4%B8%8ECPU%E7%BB%91%E5%AE%9A%2F</url>
    <content type="text"><![CDATA[参考文献： Linux 性能调优] 网卡中断与CPU的绑定问题 简单介绍下linux下的中断（interrupt）- 一切皆有可能 - 51CTO技术博客 把网卡中断绑定到CPU,最大化网卡的吞吐量 - SegmentFault Linux 多核下绑定硬件中断到不同 CPU（IRQ Affinity） | vpsee.com Linux系统CPU的性能监控及调优 - 简书 Tech Items - Linux NIC Interrupts Overloading Single CPUs - ChinaNetCloud Linux网卡中断使单个CPU过载 - 推酷 网卡软中断调优 - deven的博客 - 51CTO技术博客 除了不让多个中断集中到单个CPU，还有更进一步的方法: 调整网卡驱动参数使之采用多个队列，这样多个CPU可以各自处理一个队列。 当然，这依赖于网卡是否支持 多队列网卡及网卡中断绑定阐述 – 运维那点事 (这篇文章讲得很全面，推荐阅读） 网卡多队列及中断绑定 - wyaibyn的专栏 - CSDN博客 8.6. Receive-Side Scaling (RSS) - Performance Tuning Guide - Red Hat Enterprise Linux 6 即使网卡只支持单个队列，我们可以在系统层面模拟层多个队列，这个涉及到被称为Receive Packet Steering (RFS)和Receive Flow Steering (RFS）的两个技术 多队列网卡及网卡中断绑定阐述 – 运维那点事 8.7. Receive Packet Steering (RPS) - Performance Tuning Guide - Red Hat Enterprise Linux 6 8.8. Receive Flow Steering (RFS) - Performance Tuning Guide - Red Hat Enterprise Linux 6 在Linux的网络调优方面，如果你发现网络流量上不去，那么有一个方面需要去查一下：网卡处理网络请求的中断是否被绑定到单个CPU（或者说跟处理其它中断的是同一个CPU）。 背景网卡与操作系统的交互一般有两种方式： 一种是中断（IRQ，网卡在收到了网络信号之后，主动发送中断到CPU，而CPU将会立即停下手边的活以便对这个中断信号进行分析）， 另一种叫DMA（Direct Memory Access, 也就是允许硬件在无CPU干预的情况下将数据缓存在指定的内存空间内，在CPU合适的时候才处理） 在网卡方面，大部分还是在用IRQ方式（据说DMA技术仅仅被应用在少数高端网卡上; 另一个说法是：DMA方式会使外部设备的控制器独占PCI总线，从而CPU无法与外部设备进行交互，这对通用型操作系统Linux来说，是很难接收的，所以DMA方式在Linux内核里使用得很少）。 但是（再来一个但是），在现在的对称多核处理器（SMP）上，一块网卡的IRQ还是只有一个CPU来响应，其它CPU无法参与，如果这个CPU还要忙其它的中断（其它网卡或者其它使用中断的外设（比如磁盘）），那么就会形成瓶颈。 问题判定网上不少讲这个问题的文章都是直接让查询IRQ跟CPU的绑定情况，甚至直接修改。但我们应该先判断我们的系统是不是受这个问题影响，然后再来看怎么解决。 首先，让你的网络跑满（比如对于MySQL/MongoDB服务，可以通过客户端发起密集的读操作; 或者执行一个i大文件传送任务） 第一个要查明的是：是不是某个CPU在一直忙着处理IRQ？ 这个问题我们可以从 mpstat -P ALL 1 的输出中查明：里面的 %irq一列即说明了CPU忙于处理中断的时间占比 12345618:20:33 CPU %user %nice %sys %iowait %irq %soft %steal %idle intr/s18:20:33 all 0,23 0,00 0,08 0,11 6,41 0,02 0,00 93,16 2149,2918:20:33 0 0,25 0,00 0,12 0,07 0,01 0,05 0,00 99,49 127,0818:20:33 1 0,14 0,00 0,03 0,04 0,00 0,00 0,00 99,78 0,0018:20:33 2 0,23 0,00 0,02 0,03 0,00 0,00 0,00 99,72 0,0218:20:33 3 0,28 0,00 0,15 0,28 25,63 0,03 0,00 73,64 2022,19 上面的例子中，第四个CPU有25.63%时间在忙于处理中断（这个数值还不算高，如果高达80%（而同时其它CPU这个数值很低）以上就说明有问题了），后面那个 intr/s 也说明了CPU每秒处理的中断数（从上面的数据也可以看出，其它几个CPU都不怎么处理中断）。 然后我们就要接着查另外一个问题：这个忙于处理中断的CPU都在处理哪个（些）中断？ 1234567891011121314cat /proc/interrupts CPU0 CPU1 CPU2 CPU3 0: 245 0 0 7134094 IO-APIC-edge timer 8: 0 0 49 0 IO-APIC-edge rtc 9: 0 0 0 0 IO-APIC-level acpi 66: 67 0 0 0 IO-APIC-level ehci_hcd:usb2 74: 902214 0 0 0 PCI-MSI eth0169: 0 0 79 0 IO-APIC-level ehci_hcd:usb1177: 0 0 0 7170885 IO-APIC-level ata_piix, b4xxp185: 0 0 0 59375 IO-APIC-level ata_piixNMI: 0 0 0 0 LOC: 7104234 7104239 7104243 7104218 ERR: 0MIS: 0 这里记录的是自启动以来，每个CPU处理各类中断的数量（第一列是中断号，最后一列是对应的设备名）[详细说明: E.2.10 /proc/interrupts - Deployment Guide - RedHat Enterprise Linux 6 )，从上面可以看到： eth0所出发的中断全部都是 CPU0在处理，而CPU0所处理的中断请求中，主要是eth0和LOC中断。 （有时我们会看到几个CPU对同一个中断类型所处理的的请求数相差无几（比如上面的LOC一行），这并不一定是说多个CPU会轮流处理同一个中断，而是因为这里记录的是“自启动以来”的统计，中间可能因为irq balancer重新分配过处理中断的CPU——当然，也可能是谁手工调节过）。 解决问题首先说明几点： 首先应该根据上面的诊断方法查明当前系统是不是受这个原因影响，如果不是，那么就没有必要往下看了; 现在的多数Linux系统中已经有了IRQ Balance这个服务（服务程序一般是 /usr/sbin/irqbalance），它可以自动调节分配各个中断与CPU的绑定关系，以避免所有中断的处理都集中在少数几个CPU上; 在某些情况下，这个IRQ Balance反而会导致问题，会出现 irqbalance 这个进程反而自身占用了较高的CPU（当然也就影响了业务系统的性能） 下面来说手工将中断限定到少数几个CPU的方法。 首先当然要查明，该网卡的中断当前是否已经限定到某些CPU了？具体是哪些CPU？ 根据上面 /proc/interrupts 的内容我们可以看到 eth0 的中断号是74，然后我们来看看该中断号的CPU绑定情况（或者说叫亲和性 affinity） 12$ sudo cat /proc/irq/74/smp_affinityffffff 这个输出是一个16进制的数值，0xffffff = ‘0b111111111111111111111111’，这就意味着这里有24个CPU，所有位都为1表示所有CPU都可以被该中断干扰。 另一个例子: 12$ sudo cat /proc/irq/67/smp_affinity00000001 这个例子说明，只有CPU0处理编号为67的中断。 修改配置的方法： 我们可以用 echo 2 &gt; /proc/irq/74/smp_affinity 的方法来修改这个设置（设置为2表示将该中断绑定到CPU1上，0x2 = 0b10，而第一个CPU为CPU0）]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>性能调优</category>
        <category>网络调优</category>
      </categories>
      <tags>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVS性能指标及监控]]></title>
    <url>%2F2018%2F07%2F10%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E8%BF%90%E7%BB%B4%E6%9E%B6%E6%9E%84%2F%E9%AB%98%E5%B9%B6%E5%8F%91%2F%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F4%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1-LVS%2FLVS%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87%E5%8F%8A%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[参考文献： Performance and Tuning - LVSKB 性能参数LVS 的性能主要通过以下几个方面来提高 ipvs connection table size-最大连接数官方的解释如下： 12345678910111213141516171819202122232425The IPVS connection hash table uses the chaining scheme to handlehash collisions. Using a big IPVS connection hash table will greatlyreduce conflicts when there are hundreds of thousands of connectionsin the hash table.Note the table size must be power of 2. The table size will be thevalue of 2 to the your input number power. The number to choose isfrom 8 to 20, the default number is 12, which means the table sizeis 4096. Don&apos;t input the number too small, otherwise you will loseperformance on it. You can adapt the table size yourself, accordingto your virtual server application. It is good to set the table sizenot far less than the number of connections per second multiplyingaverage lasting time of connection in the table. For example, yourvirtual server gets 200 connections per second, the connection lastsfor 200 seconds in average in the connection table, the table sizeshould be not far less than 200x200, it is good to set the tablesize 32768 (2**15).Another note that each connection occupies 128 bytes effectively andeach hash entry uses 8 bytes, so you can estimate how much memory isneeded for your box.You can overwrite this number setting conn_tab_bits module parameteror by appending ip_vs.conn_tab_bits=? to the kernel command lineif IP VS was compiled built-in. 说明： LVS的连接信息使用IPVS connection hash table这个哈希表去保存，它记录每个进来的连接及路由去向的信息 任何一个报文到达都需要查找连接Hash表。Hash表的查找复杂度为O(n/m)，其中n为Hash表中对象的个数，m为Hash表的桶个数。当对象在Hash表中均匀分布和Hash表的桶个数与对象个数一样多时，Hash表的查找复杂度可以接近O(1)。 table size使用2的幂次方进行配置指定，范围为8-20，也就是说连接数的取值范围为：2^8-2^20 默认配置为2^12，也就是4096个连接数上限 在生产环境中，我们一般设置为最大值，也就是2^20（1048576） 注意，这些连接是需要占用内存的，因此要考虑到内存大小的因素 每一个TCP连接需要占用约128字节，哈希表的每个条目需要占用8字节 以设置为最大值为例，那么，这些连接以及条目共占用内存如下： 2^20*(128byte+8byte) = 142606336byte = 136MB 配置： 在/etc/modprobe.d/目录下添加文件ip_vs.conf，内容为： options ip_vs conn_tab_bits=22（文档中写的上限是20，但是实际配置的时候发现22也是可以的） 123echo &apos;options ip_vs conn_tab_bits=22&apos; &gt; /etc/modprobe.d/ipvs.confmodprobe -r ip_vs &amp;&amp; modprobe -a ip_vsipvsadm -Ln 注意，在卸载内核模块的时候，可能会有依赖关系，这时候使用lsmod先查看依赖调用关系，将调用的模块卸载之后再进行操作，例如，这里的操作如下： 1234[root@lvs001 modprobe.d]# modprobe -r ip_vs_rr[root@lvs001 modprobe.d]# modprobe -r ip_vs[root@lvs001 modprobe.d]# modprobe -a ip_vs[root@lvs001 modprobe.d]# modprobe -a ip_vs_rr CPU Soft Interrupt -CPU软中断在Linux的网络调优方面，如果你发现网络流量上不去，那么有一个方面需要去查一下：网卡处理网络请求的中断是否被绑定/发送到单个CPU，导致只有一个CPU处理网络请求 但是，在当前的对称多核处理器服务器上，一块网卡的IRQ还是只有一个CPU来响应，其它CPU无法参与，如果这个CPU还要忙其它的中断（其它网卡或者其它使用中断的外设（比如磁盘）），那么就会形成瓶颈。 动态查看CPU的irq情况 12命令：mpstat -P ALL 1 %irq一列即说明了CPU忙于处理中断的时间占比 查看CPU处理中断的情况 12cat /proc/interrupts 这里记录的是自启动以来，每个CPU处理各类中断的数量（第一列是中断号，最后一列是对应的设备名） 我们进行过滤，获取网卡的中断号，然后再分析CPU的中断情况 获取对应的中断号 123456789101112131415161718192021[root@lvs001 ~]# cat /proc/interrupts | egrep &apos;em1|em2|p1p1|p1p2&apos; | awk &apos;&#123;print $1&#125;&apos;157:158:159:160:161:162:163:164:165:166:167:168:169:170:171:172:173:174:175:176: 获取CPU的处理信息 1234567891011121314151617181920212223[root@lvs001 ~]# for i in &#123;157..176&#125;;do cat /proc/interrupts | egrep -w $i ;done 157: 13529132 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge em1-tx-0 158: 32642550 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge em1-rx-1 159: 30481981 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge em1-rx-2 160: 15555217 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge em1-rx-3 161: 25509530 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge em1-rx-4 162: 13538297 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge em2-tx-0 163: 25653580 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge em2-rx-1 164: 25741710 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge em2-rx-2 165: 35448970 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge em2-rx-3 166: 25494937 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge em2-rx-4 167: 256824 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge p1p1-tx-0 168: 281534 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge p1p1-rx-1 CAL: 133 863797 189 189 189 189 188 189 189 189 189 189 189 189 189 189 188 188 168 185 188 186 188 188 188 188 188 188 188 189 187 130 Function call interrupts 169: 64639 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge p1p1-rx-2 170: 65879 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge p1p1-rx-3 171: 425700 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge p1p1-rx-4 172: 256754 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge p1p2-tx-0 173: 43230 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge p1p2-rx-1 174: 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge p1p2-rx-2 175: 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge p1p2-rx-3 176: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IR-PCI-MSI-edge p1p2-rx-4[root@lvs001 ~]# 这里显示不友好，建议复制到编辑器中查看 可以看到，这些网卡的中断都是由CPU0来处理的 查看 1[root@lvs002 ~]# for i in &#123;157..176&#125;;do cat /proc/irq/$i/smp_affinity;done 配置 1for i in &#123;157..176&#125;;do echo ffffffff &gt; /proc/irq/$i/smp_affinity;done Netfilter Connection Track-连接跟踪原文： IPVS uses its own simple and fast connection tracking for performance reasons, instead of using netfilter connection tracking. So, if you don’t use firewalling feature at load balancer and you need an extremely fast load balancer, do not load netfilter conntrack modules into you system, because there is no need to do double tracking. Note that LVS/NAT should work too without the conntrack modules. Julian compared the performance of IPVS with ip_conntrack and without ip_conntrack. See http://archive.linuxvirtualserver.org/html/lvs-users/2001-12/msg00141.html 默认情况下LVS自身会记录连接信息，但是 iptables 也会记录 connection 的状态，但是很多情况下，我们并不需要 iptables 来做这件事， 我们可以告诉它 NOTRACK，不要记录这些信息。 配置： 增加raw表，在其他表处理之前，-j NOTRACK跳过其它表处理 123456iptables -t raw -A PREROUTING -d 103.13.244.16/29 -p tcp --dport 80 -j NOTRACK iptables -t raw -A OUTPUT -d 103.13.244.16/29 -p tcp --dport 80 -j NOTRACK iptables -t raw -A PREROUTING -d 103.13.244.16/29 -p tcp --dport 443 -j NOTRACK iptables -t raw -A OUTPUT -d 103.13.244.16/29 -p tcp --dport 443 -j NOTRACK [root@lvs002 ~]# /etc/init.d/iptables save 与之同时，因为涉及到内网之间的通信，因此这里也将连接跟踪表进行调大 123456# vim /etc/sysctl.confnet.netfilter.nf_conntrack_max = 3065536 net.nf_conntrack_max = 3065536 # sysctl -p Real Server - syn cookie参数参考链接： lvs-users IPVS SYN-cookies SYN Cookie是对TCP服务器端的三次握手协议作一些修改，专门用来防范SYN Flood攻击的一种手段。它的原理是，在TCP服务器收到TCP SYN包并返回TCP SYN+ACK包时，不分配一个专门的数据区，而是根据这个SYN包计算出一个cookie值。在收到TCP ACK包时，TCP服务器在根据那个cookie值检查这个TCP ACK包的合法性。如果合法，再分配专门的数据区进行处理未来的TCP连接。 SYN Flood攻击利用的是IPv4中TCP协议的三次握手（Three-Way Handshake）过程进行的攻击。TCP协议规定，一端向另一端发起TCP连接时，它需要首先发送SYN 包到对方，对方收到后发送一个SYN+ACK包回来，发起方再发送 ACK包回去，这样三次握手就结束了。我们把TCP连接的发起方叫作”TCP客户机（TCP Client）”，TCP连接的接收方叫作”TCP服务器（TCP Server）”。值得注意的是在TCP服务器收到TCP SYN request包时，在发送TCP SYN+ACK包回TCP客户机前，TCP服务器要先分配好一个数据区专门服务于这个即将形成的TCP连接。一般把收到SYN包而还未收到ACK包时的连接状态称为半开连接（Half-open Connection）。 在最常见的SYN Flood攻击中，攻击者在短时间内发送大量的TCP SYN包给受害者，这时攻击者是TCP客户机，受害者是TCP服务器。根据上面的描述，受害者会为每个TCP SYN包分配一个特定的数据区，只要这些SYN包具有不同的源地址（这一点对于攻击者来说是很容易伪造的）。这将给TCP服务器系统造成很大的系统负担，最终导致系统不能正常工作。 内核文档说明 123456789101112131415161718192021222324tcp_syncookies - BOOLEAN Only valid when the kernel was compiled with CONFIG_SYN_COOKIES Send out syncookies when the syn backlog queue of a socket overflows. This is to prevent against the common &apos;SYN flood attack&apos; Default: 1 Note, that syncookies is fallback facility. It MUST NOT be used to help highly loaded servers to stand against legal connection rate. If you see SYN flood warnings in your logs, but investigation shows that they occur because of overload with legal connections, you should tune another parameters until this warning disappear. See: tcp_max_syn_backlog, tcp_synack_retries, tcp_abort_on_overflow. syncookies seriously violate TCP protocol, do not allow to use TCP extensions, can result in serious degradation of some services (f.e. SMTP relaying), visible not by you, but your clients and relays, contacting you. While you see SYN flood warnings in logs not being really flooded, your server is seriously misconfigured. If you want to test which effects syncookies have to your network connections you can set this knob to 2 to enable unconditionally generation of syncookies. 注意，即使开启该机制并不意味着所有的连接都是用SYN cookies机制来完成连接的建立，只有在半连接队列已满的情况下才会触发SYN cookies机制。由于SYN cookies机制严重违背TCP协议，不允许使用TCP扩展，可能对某些服务造成严重的性能影响（如SMTP转发），对于防御SYN flood攻击的确有效。对于没有收到攻击的高负载服务器，不要开启此选项，可以通过修改tcp_max_syn_backlog、tcp_synack_retries和tcp_abort_on_overflow系统参数来调节。 tcp_max_syn_backlog变量告诉你在内存中可以缓存多少个SYN请求。该变量需要打开tcp_syncookies才有效。如果服务器负载很高，可以尝试提高该变量的值。 tcp_synack_retries变量用于TCP三次握手机制中第二次握手，当收到客户端发来的SYN连接请求后，服务端将回复SYN+ACK包，这时服务端处于SYN_RCVD状态，并等 待客户端发来的回复ACK包。如果服务端没有收到客户端的ACK包，会重新发送SYN+ACK包，直到收到客户端的ACK包。该变量设置发送 SYN+ACK包的次数，超过这个次数，服务端将放弃连接。默认值是5。 tcp_abort_on_overflow变量的值是个布尔值，默认值为0（FALSE关闭）。如果开启，当服务端接收新连接的速度变慢时，服务端会发送RST包（reset包）给客户端，令客户端 重新连接。这意味着如果突然发生溢出，将重获连接。仅当你真的确定不能通过调整监听进程使接收连接的速度变快，可以启用该选项。该选项会影响到客户的连接。 配置： 12345678# vim /etc/sysctl.conf net.ipv4.tcp_syncookies = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_max_syn_backlog = 2048保存退出后，执行：# sysctl -p 参数说明如下： net.ipv4.tcp_syncookies = 1#表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN*，默认为0，表示关闭； net.ipv4.tcp_tw_reuse = 1#表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；为1，开启； 这个酌情开启，这里暂时不开启 net.ipv4.tcp_tw_recycle = 1#表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭；为1，开启； net.ipv4.tcp_fin_timeout #修改系統默认的 TIMEOUT 时间，这里根据服务器的实际情况设置。默认为60秒 另外细心的朋友可能发现了，报错信息： Possible SYN flooding on port 13370. Sending cookies.后面跟了句”Check SNMP counters”。这句我当时差点被误导，因为我的服务器上正好跑了一个snmp抓流量的服务，开始以为是它导致的，后来一想那是udp的协议，和tcp没关系呀。查了kernel的代码发现，原来那是print打印的固定info输出： 1234567891011121314151617181920212223static bool tcp_syn_flood_action(const struct sock *sk, const struct sk_buff *skb, const char *proto)&#123; struct request_sock_queue *queue = &amp;inet_csk(sk)-&gt;icsk_accept_queue; const char *msg = &quot;Dropping request&quot;; bool want_cookie = false; struct net *net = sock_net(sk);#ifdef CONFIG_SYN_COOKIES if (net-&gt;ipv4.sysctl_tcp_syncookies) &#123; msg = &quot;Sending cookies&quot;; want_cookie = true; __NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPREQQFULLDOCOOKIES); &#125; else#endif __NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPREQQFULLDROP); if (!queue-&gt;synflood_warned &amp;&amp; net-&gt;ipv4.sysctl_tcp_syncookies != 2 &amp;&amp; xchg(&amp;queue-&gt;synflood_warned, 1) == 0) pr_info(&quot;%s: Possible SYN flooding on port %d. %s. Check SNMP counters.\n&quot;, proto, ntohs(tcp_hdr(skb)-&gt;dest), msg); return want_cookie;&#125; 关闭网卡LRO和GRO现在大多数网卡都具有LRO/GRO功能，即 网卡收包时将同一流的小包合并成大包 （tcpdump抓包可以看到&gt;MTU 1500bytes的数据包）交给 内核协议栈；LVS内核模块在处理&gt;MTU的数据包时，会丢弃； 因此，如果我们用LVS来传输大文件，很容易出现丢包，传输速度慢； 解决方法，关闭LRO/GRO功能，命令： 12345678910111213ethtool -k eth0 查看LRO/GRO当前是否打开ethtool -K eth0 lro off 关闭GROethtool -K eth0 gro off 关闭GRO查看：[root@lvs001 ~]# ethtool -k p1p1 | grep offload[root@lvs001 ~]# ethtool -k p1p2 | grep offload配置：[root@lvs001 ~]# ethtool -K p1p1 lro off[root@lvs001 ~]# ethtool -K p1p1 gro off[root@lvs001 ~]# ethtool -K p1p2 lro off[root@lvs001 ~]# ethtool -K p1p2 gro off offload特性，主要是指将原本在协议栈中进行的IP分片、TCP分段、重组、checksum校验等操作，转移到网卡硬件中进行，降低系统CPU的消耗，提高处理性能。 包括 LSO/LRO、GSO/GRO、TSO/UFO 等。 LSO/LRO 分别对应到发送和接收两个方向，是 Large Segment Offload 和 Large Receive Offload。 首先来看 LSO。我们知道计算机网络上传输的数据基本单位是离散的网包，既然是网包，就有大小限制，这个限制就是 MTU（Maximum Transmission Unit）的大小，一般是1518字节。比如我们想发送很多数据出去，经过os协议栈的时候，会自动帮你拆分成几个不超过MTU的网包。然而，这个拆分是比较费计算资源的（比如很多时候还要计算分别的checksum），由 CPU 来做的话，往往会造成使用率过高。那可不可以把这些简单重复的操作 offload 到网卡上呢？ 于是就有了 LSO，在发送数据超过 MTU 限制的时候（太容易发生了），OS 只需要提交一次传输请求给网卡，网卡会自动的把数据拿过来，然后进行切，并封包发出，发出的网包不超过 MTU 限制。 接下来看 LSO，当网卡收到很多碎片包的时候，LRO 可以辅助自动组合成一段较大的数据，一次性提交给 OS处理。 一般的，LSO 和 LRO 主要面向 TCP 报文。 GSO/GRO Generic Segmentation Offload 和 Generic Receive Offload，分别比 LSO 和 LRO 更通用，自动检测网卡支持特性，支持分包则直接发给网卡，否则先分包后发给网卡。新的驱动一般用 GSO/GRO。 TSO/UFO TCP Segmentation Offload 和 UDP fragmentation offload，分别对应 TCP 报文和 UDP 报文。 很典型的，TCP 协议中就考虑了分片存在的情况，往往是切分 TCP 的数据包，叫做 TSO。而一般的情况，则称为 LSO 或者 GSO。 对于其他不支持切片的协议例如 UDP，则只能进行 IP 层上的切片。 检查与开关 可以通过 ethtool -k eth0 命令来查看各个选项的当前状态，注意输出中各种 off-load 选项的状态。 总结 也就是说，在将数据包转发出去的时候，包的大小必须小于1500字节，但是在处理收到的数据包的时候，包的大小没有1500字节的限制 发送模式： TSO GSO UFO 接收模式： LRO GRO RSS 注意 目前常用的抓包工具大部分都是从协议栈中（如数据链路层）捕获数据包，而网卡的offload特性会将数据包的分片、重组等工作转移到协议栈以下的硬件层面进行，因此在开启TSO、GRO等机制的情况下，我们使用tcpdump、wireshark等工具抓取到的数据包往往不能真实反应链路上实际的数据帧，给网络流量特征的分析造成不利影响。 在某些情况下，例如分片攻击等攻击方式，甚至可能会因为网卡设备的offload机制处理，而规避防火墙、IDS以及人工的检查。针对这些情况，可以选择关闭网卡offload的相关选项，或者在链路的其他节点进行抓包。 /proc下的IP_VS参数设置根据前文的介绍，可以通过ipvsadm命令和LVS内核打交道； 除此之外，我们还可以通过proc参数，来 配置全局参数 和 获取统计信息； 配置全局参数，位于目录/proc/sys/net/ipv4/vs/下； 获取统计信息，位于目录/proc/net/下； 参考资料：官方内核文档 1234567891011121314151617[root@lvs001 ~]# ll /proc/sys/net/ipv4/vs | awk &apos;&#123;print $9&#125;&apos;am_droprateamemthreshcache_bypassconn_reuse_modedrop_entrydrop_packetexpire_nodest_connexpire_quiescent_templatenat_icmp_sendsecure_tcpsync_qlen_maxsync_refresh_periodsync_retriessync_sock_sizesync_thresholdsync_version 有一些几个参数需要进行调整 cache_bypass 12345678cache_bypass - BOOLEAN 0 - disabled (default) not 0 - enabled If it is enabled, forward packets to the original destination directly when no cache server is available and destination address is not local (iph-&gt;daddr is RTN_UNICAST). It is mostly used in transparent web cache cluster. 主要用于缓存体系，enable之后，当后端配置的是缓存系统的时候，当没有可用的sever时，直接将数据包转发给后端的数据产生节点 conn_reuse_mode 1234567891011121314151617181920conn_reuse_mode - INTEGER 1 - default Controls how ipvs will deal with connections that are detected port reuse. It is a bitmap, with the values being: 0: disable any special handling on port reuse. The new connection will be delivered to the same real server that was servicing the previous connection. This will effectively disable expire_nodest_conn. bit 1: enable rescheduling of new connections when it is safe. That is, whenever expire_nodest_conn and for TCP sockets, when the connection is in TIME_WAIT state (which is only possible if you use NAT mode). bit 2: it is bit 1 plus, for TCP connections, when connections are in FIN_WAIT state, as this is the last state seen by load balancer in Direct Routing mode. This bit helps on adding new real servers to a very busy cluster. 用户后端server开启端口reuse（端口复用，服务器上启动多个进程监听同一个端口，在tenginx中使用时能够极大的提高性能）的情况。 当设置enable的时候，接受到新连接之后，将进行重新调度，将连接请求分发到启动该端口的其他进程上 expire_nodest_conn 1234567891011121314151617expire_nodest_conn - BOOLEAN 0 - disabled (default) not 0 - enabled The default value is 0, the load balancer will silently drop packets when its destination server is not available. It may be useful, when user-space monitoring program deletes the destination server (because of server overload or wrong detection) and add back the server later, and the connections to the server can continue. If this feature is enabled, the load balancer will expire the connection immediately when a packet arrives and its destination server is not available, then the client program will be notified that the connection is closed. This is equivalent to the feature some people requires to flush connections when its destination is not available. 设置为0时，当后端的server被检测为不可用时，不会立即将连接断开，而是会保持一段时间，让其自然过期失效，如果在这个过程当中，server又恢复正常，那么将继续使用这个连接 当设置为为enable（非0）时，当检测到后端的server不可用时，将会立即将这个连接关闭。 expire_quiescent_template 123456789101112131415expire_quiescent_template - BOOLEAN 0 - disabled (default) not 0 - enabled When set to a non-zero value, the load balancer will expire persistent templates when the destination server is quiescent. This may be useful, when a user makes a destination server quiescent by setting its weight to 0 and it is desired that subsequent otherwise persistent connections are sent to a different destination server. By default new persistent connections are allowed to quiescent destination servers. If this feature is enabled, the load balancer will expire the persistence template if it is to be used to schedule a new connection and the destination server is quiescent. 默认值为0，当RS的weight为0时（例如健康监测失败时，LB会将RS的权重重置为0），会话保持的新建连接还会继续调度到该RS上 如果设置为非0，那么当weight为0时，LB会将话保持的连接模板置为无效，重新调度新的RS； sync_threshold 123456789101112sync_threshold - vector of 2 INTEGERs: sync_threshold, sync_period default 3 50 It sets synchronization threshold, which is the minimum number of incoming packets that a connection needs to receive before the connection will be synchronized. A connection will be synchronized, every time the number of its incoming packets modulus sync_period equals the threshold. The range of the threshold is from 0 to sync_period. When sync_period and sync_refresh_period are 0, send sync only for state changes or only once when pkts matches sync_threshold 同步阈值设置，该文件中的值为两个整数，默认为3 50 数值表示含义如下（以3 50为例）：接受到3个数据包及以上，该连接就可以被同步 Linux系统调优-网络内核参数12345678910net.ipv4.tcp_tw_recyle=1net.ipv4.tcp_tw_reuse=1net.ipv4.tcp_max_syn_backlog=8192net.ipv4.tcp_keepalive_time=1800net.ipv4.tcp_fin_timeout=30net.core.rmem_max=16777216net.core.wmem_max=16777216net.ipv4.tcp_rmem=4096 87380 16777216net.ipv4.tcp_wmem=4096 65536 16777216net.core.netdev_max_backlog=3000 算法优化SH调度算法-尽量不要采用 一些业务为了支持会话保持，选择SH调度算法，以实现 同一源ip的请求调度到同一台RS上；但 SH算法本省没有实现一致性hash，一旦一台RS down，当前所有连接都会断掉；如果配置了inhibit_on_failure，那就更悲剧了，调度到该RS上的流量会一直损失； 实际线上使用时，如需会话保持，建议配置persistence_timeout参数，保证一段时间同一源ip的请求到同一RS上； WLC调度算法-注意RS donw-&gt;up的影响 WLC算法下，RS一旦出现down后up的情况，瞬间所有的新建连接都会调度到该RS上，可能会超过该RS处理请求的上限； 快速配置[root@lvs002 ~]# vim /etc/sysctl.conf [root@lvs002 ~]# sysctl -p 1234567891011121314151617181920212223242526net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0kernel.sysrq = 0kernel.core_uses_pid = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.shmmax = 68719476736kernel.shmall = 4294967296net.core.netdev_max_backlog = 2048net.ipv4.ip_local_port_range = 10000 65000net.ipv4.tcp_max_tw_buckets = 462144vm.swappiness = 1net.ipv4.tcp_max_syn_backlog = 65535net.core.somaxconn = 32768net.ipv4.ip_forward = 1net.netfilter.nf_conntrack_max = 3065536net.nf_conntrack_max = 3065536net.ipv4.tcp_syncookies = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 1800net.core.rmem_max = 16777216net.core.wmem_max = 16777216net.ipv4.tcp_rmem = 4096 87380 16777216net.ipv4.tcp_wmem = 4096 65536 16777216net.core.netdev_max_backlog = 3000 LVS监控一般情况下，我们可以通过watch ipvsadm -ln来监视lvs的当前状态，但如果我们想分析一段时间（一周，一月或者更长）的连接数情况，ipvsadm就无能为力了。我们可以借助一个叫lvs-rrd的小工具来达到这个目的。 lvs-rrd官网链接：http://tepedino.org/lvs-rrd/ 但是在这里，由于这个工具只能收集连接数的数据，因此我们还是采用zabbix进行集中监控 使用lvs-rrd监控lvs状态lvs_rrd工具实现了网页的形式来查看lvs状态功能。 其主要有两个脚本组成：信息收集脚本和图像绘制脚本。 信息收集脚本是将lvs的信息生成rrd格式的数据文件，然后利用图像绘制脚本生成图像，并生成一个php页面，这个页面中引用其所生成的图像，这样我们可以通过web页面的形式查看生成的php页面，就可以时时的查看lvs的状态信息。 lvs_rrd需要部署在LVS-Master和LVS-Backup上，更准确的说lvs_rrd中的信息收集脚本一定要在LVS director 上运行（不能安装在其他服务器上）。 但是通过配置图像生成脚本和图像的生成目录，我们也可以将源数据时时的复制到其他的服务器中，再在其他服务器上生成图像展示 下面简单的介绍部署的步骤 下载安装rrdtool（画图）工具 123456789wget https://oss.oetiker.ch/rrdtool/pub/rrdtool-1.4.7.tar.gzyum -y install cairo-devel libxml2-devel pango-devel pango libpng-devel freetype freetype-devel libart_lgpl-devel perl-ExtUtils-CBuilder perl-ExtUtils-MakeMaker dejavu-lgc-sans-fonts./configure --prefix=/usr/local/rrdtoolmake &amp;&amp; make install echo &quot;/usr/local/rrdtool/lib&quot; &gt;&gt; /etc/ld.so.confldconfig 安装nginx 123456789yum -y install pcre pcre-devel php php-fpmuseradd -s /sbin/nologin nginxwget https://nginx.org/download/nginx-1.14.0.tar.gz./configure --prefix=/usr/local/nginx --user=nginx --group=nginx --with-http_ssl_module --with-http_stub_status_module --with-pcremake &amp;&amp; make install 注意修改nginx的监听端口为非80 nginx+php配置 12/etc/init.d/php-fpm startchkconfig php-fpm on 在nginx配置文件中添加以下内容 location ~ \.php$ { root html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } 下载安装lvs-rrd工具 这里使用最新的0.7版本，该版本要求rrdtool版本最低为： 1.2.x 将lvs-rrd-v0.7.tar.gz解压后将文件夹复制到/data/www/目录下并更名为lvs 1234wget http://tepedino.org/lvs-rrd/lvs-rrd-v0.7.tar.gztar -zxvf lvs-rrd-v0.7.tar.gzmv lvs-rrd-v0.7 /usr/local/nginx/html/lvs-rrd 修改相应的脚本文件： 12345vim lvs.rrd.update 修改以下内容RRDTOOL=&quot;/usr/local/rrdtool/bin/rrdtool&quot; #rrdtool可执行程序路径IPVSADM=&quot;/sbin/ipvsadm&quot; #ipvsadm命令路径WORKDIR=&quot;/data1/lvs-rrd&quot; #rrdtool收集的数据的存放路径 12345678vim graph-lvs.sh 修改以下内容# WORKDIR must match the directory used in the update script.WORKDIR=&quot;/data1/lvs-rrd&quot; #rrdtool收集的数据的存放路径,同上面一致RRDTOOL=&quot;/usr/local/rrdtool/bin/rrdtool&quot; #rrdtool可执行程序路径# Where to put the graphs. GRAPHS=&quot;/data1/lvs-rrd/graphs&quot; #生成的图片保存路径WEBPATH=&quot;/lvs-rrd/graphs&quot; #web访问的路径 123456vim lvs-rrd.php&lt;?phpheader(&quot;Cache-Control: max-age=300, must-revalidate&quot;);system(&quot;/usr/local/nginx/html/lvs-rrd/graph-lvs.sh -H&quot;);?&gt; 注意：WEBPATH的配置是浏览器实际访问时图片的访问路径，也就是http://ip:port/webpath/xxx.gif 在日志中的输出显示为： /usr/local/nginx/html/lvs-rrd/graphs/lvs.All.All.All.All.All-year.gif 因此需要手动在站点目录下创建该目录并创建软链接，将生成的图片保存路径链接到该目录 12mkdir -p /usr/local/nginx/html/lvs-rrd/graphsln -s /data1/lvs-rrd/graphs /usr/local/nginx/html/lvs-rrd/graphs 配置nginx认证 在nginx配置文件的server中配置如下两行 12auth_basic &quot;dwd&quot;;auth_basic_user_file htpasswd; 然后执行以下命令创建加密文件 1htpasswd -bc htpasswd ops-lvs Dwd_Ops_123 配置计划任务 这里，将更新数据的间隔时间设置为30s 1234* * * * * /usr/local/nginx/html/lvs-rrd/lvs.rrd.update &gt;/dev/null 2&gt;&amp;1* * * * * /usr/local/nginx/html/lvs-rrd/graph-lvs.sh -H &gt; /dev/null 2&gt;&amp;1* * * * * sleep 30 ; /usr/local/nginx/html/lvs-rrd/lvs.rrd.update &gt;/dev/null 2&gt;&amp;1* * * * * sleep 30 ; /usr/local/nginx/html/lvs-rrd/graph-lvs.sh -H &gt; /dev/null 2&gt;&amp;1 Zabbix监控LVS监控指标： 动态的数据： cps(connect per second) ，每秒的连接数情况 InPPS(input packge per second)，每秒的入向数据包数量情况 OutPPS(output packge per second)，每秒的出向数据包数量情况 InBPS（input byte per second）,每秒的流入字节数情况 OutBPS(output byte per second)，每秒的流出字节数情况 ActiveConn，处于ESAT的连接（使用系统的netstat无法看到） InActConn，处于非ESAT的连接（使用系统的netstat无法看到） 静态统计数据： Conns，自启动之后的总连接数 InPkts，自启动之后的总入向数据包数量统计 OutPkts，自启动之后的总出向数据包数量统计 InBytes，自启动之后的总入向字节数统计 OutBytes，自启动之后的总出向字节数统计 监控逻辑： 使用ipvsadm命令从服务器中采集数据 所使用的命令分别为： ipvsadm -Ln –rate ipvsadm -Ln –stats 注意：zabbix配置文件中需要打开sudo的权限，拥有root的权限之后才能执行ipvsadm命令去获取数据]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维架构</category>
        <category>高并发</category>
        <category>负载均衡</category>
        <category>4层负载均衡-LVS</category>
      </categories>
      <tags>
        <tag>LVS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux服务器双网卡bond配置]]></title>
    <url>%2F2018%2F06%2F26%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E8%BF%90%E7%BB%B4%E6%9E%B6%E6%9E%84%2F%E9%AB%98%E5%B9%B6%E5%8F%91%2F%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F2%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2FLinux%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8F%8C%E7%BD%91%E5%8D%A1bond%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[参考文献： linux 网卡绑定 bonding Linux网卡绑定探析 Linux下网卡bonding配置 LINUX-网卡Bond Linux双网卡绑定bond详解 Linux网卡bond的七种模式详解 基础知识概述什么是bond网卡bond是通过多张网卡绑定为一个逻辑网卡，实现本地网卡的冗余，带宽扩容和负载均衡，在生产场景中是一种常用的技术。 通俗点讲就是两块网卡具有相同的IP地址而并行链接聚合成一个逻辑链路工作。 其实这项技术在Sun和Cisco中早已存在，被称为Trunking和Etherchannel 技术，在Linux的2.4.x的内核中开始采用这这种技术，被称为bonding。 内核支持在Linux Kernels 2.4.12及以后的版本均供bonding模块，以前的版本可以通过patch实现。可以通过以下命令确定内核是否支持 bonding： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@nginx001 ~]# cat /boot/config-2.6.32-696.el6.x86_64 | grep -i bond# CONFIG_PATA_WINBOND is not setCONFIG_BONDING=mCONFIG_WINBOND_840=mCONFIG_INPUT_WINBOND_CIR=mCONFIG_IR_WINBOND_CIR=mor[root@nginx001 ~]# cat /boot/config-2.6.32-696.el6.x86_64 | grep -i bondingCONFIG_BONDING=mor[root@ network-scripts]# modinfo bondingfilename: /lib/modules/2.6.32-696.el6.x86_64/kernel/drivers/net/bonding/bonding.koauthor: Thomas Davis, tadavis@lbl.gov and many othersdescription: Ethernet Channel Bonding Driver, v3.7.1version: 3.7.1license: GPLalias: rtnl-link-bondsrcversion: 454FF5806F146AD7FB41356depends: vermagic: 2.6.32-696.el6.x86_64 SMP mod_unload modversions parm: max_bonds:Max number of bonded devices (int)parm: tx_queues:Max number of transmit queues (default = 16) (int)parm: num_grat_arp:Number of peer notifications to send on failover event (alias of num_unsol_na) (int)parm: num_unsol_na:Number of peer notifications to send on failover event (alias of num_grat_arp) (int)parm: miimon:Link check interval in milliseconds (int)parm: updelay:Delay before considering link up, in milliseconds (int)parm: downdelay:Delay before considering link down, in milliseconds (int)parm: use_carrier:Use netif_carrier_ok (vs MII ioctls) in miimon; 0 for off, 1 for on (default) (int)parm: mode:Mode of operation; 0 for balance-rr, 1 for active-backup, 2 for balance-xor, 3 for broadcast, 4 for 802.3ad, 5 for balance-tlb, 6 for balance-alb (charp)parm: primary:Primary network device to use (charp)parm: primary_reselect:Reselect primary slave once it comes up; 0 for always (default), 1 for only if speed of primary is better, 2 for only on active slave failure (charp)parm: lacp_rate:LACPDU tx rate to request from 802.3ad partner; 0 for slow, 1 for fast (charp)parm: ad_select:803.ad aggregation selection logic; 0 for stable (default), 1 for bandwidth, 2 for count (charp)parm: min_links:Minimum number of available links before turning on carrier (int)parm: xmit_hash_policy:balance-xor and 802.3ad hashing method; 0 for layer 2 (default), 1 for layer 3+4, 2 for layer 2+3 (charp)parm: arp_interval:arp interval in milliseconds (int)parm: arp_ip_target:arp targets in n.n.n.n form (array of charp)parm: arp_validate:validate src/dst of ARP probes; 0 for none (default), 1 for active, 2 for backup, 3 for all (charp)parm: arp_all_targets:fail on any/all arp targets timeout; 0 for any (default), 1 for all (charp)parm: fail_over_mac:For active-backup, do not set all slaves to the same MAC; 0 for none (default), 1 for active, 2 for follow (charp)parm: all_slaves_active:Keep all frames received on an interface by setting active flag for all slaves; 0 for never (default), 1 for always. (int)parm: resend_igmp:Number of IGMP membership reports to send on link failure (int)parm: packets_per_slave:Packets to send per slave in balance-rr mode; 0 for a random slave, 1 packet per slave (default), &gt;1 packets per slave. (int)parm: lp_interval:The number of seconds between instances where the bonding driver sends learning packets to each slaves peer switch. The default is 1. (uint) 当看到有相关配置输出的时候则说明当前操作系统的内核版本是支持bond的 bond模式bonding的七种工作模式: bonding技术提供了七种工作模式，在使用的时候需要指定一种，每种有各自的优缺点. balance-rr (mode=0) 默认, 有高可用 (容错) 和负载均衡的功能, 需要交换机的配置，每块网卡轮询发包 (流量分发比较均衡). active-backup (mode=1) 只有高可用 (容错) 功能, 不需要交换机配置, 这种模式只有一块网卡工作, 对外只有一个mac地址。缺点是端口利用率比较低 balance-xor (mode=2) 不常用 broadcast (mode=3) 不常用 802.3ad (mode=4) IEEE 802.3ad 动态链路聚合，需要交换机配置，没用过 balance-tlb (mode=5) 不常用 balance-alb (mode=6) 有高可用 ( 容错 )和负载均衡的功能，不需要交换机配置 (流量分发到每个接口不是特别均衡) 具体的网上有很多资料，了解每种模式的特点根据自己的选择就行, 一般会用到0、1、4、6这几种模式。 一般常用的常用的有两种： mode=0（balance-rr） 表示负载分担round-robin，并且是轮询的方式比如第一个包走eth0，第二个包走eth1，直到数据包发送完毕。 优点：流量提高一倍 缺点：需要接入交换机做端口聚合，否则可能无法使用 mode=1（active-backup） 表示主备模式，即同时只有1块网卡在工作。 优点：冗余性高 缺点：链路利用率低，两块网卡只有1块在工作 实践操作配置子网卡源文件内容： 1234567891011[root@nginx001 network-scripts]# cat ifcfg-p1p1DEVICE=&quot;p1p1&quot;BOOTPROTO=&quot;dhcp&quot;DHCP_HOSTNAME=&quot;bigdata&quot;HWADDR=&quot;D0:94:66:5B:76:89&quot;IPV6INIT=&quot;yes&quot;IPV6_AUTOCONF=&quot;yes&quot;NM_CONTROLLED=&quot;yes&quot;ONBOOT=&quot;no&quot;TYPE=&quot;Ethernet&quot;UUID=&quot;9126f785-f642-4ce4-84d8-558284f17623&quot; 修改后的文件内容如下： 12345678910111213[root@nginx001 network-scripts]# cat ifcfg-p1p1DEVICE=&quot;p1p1&quot;BOOTPROTO=&quot;static&quot;DHCP_HOSTNAME=&quot;bigdata&quot;HWADDR=&quot;D0:94:66:5B:76:89&quot;IPV6INIT=&quot;yes&quot;IPV6_AUTOCONF=&quot;yes&quot;NM_CONTROLLED=&quot;yes&quot;ONBOOT=&quot;yes&quot;TYPE=&quot;Ethernet&quot;UUID=&quot;9126f785-f642-4ce4-84d8-558284f17623&quot;MASTER=bond1SLAVE=yes 同样的，在第二块网卡上进行配置，配置之后的文件内容如下所示： 12345678910111213[root@nginx001 network-scripts]# cat ifcfg-p1p2DEVICE=&quot;p1p2&quot;BOOTPROTO=&quot;static&quot;DHCP_HOSTNAME=&quot;bigdata&quot;HWADDR=&quot;D0:94:66:5B:76:8A&quot;IPV6INIT=&quot;yes&quot;IPV6_AUTOCONF=&quot;yes&quot;NM_CONTROLLED=&quot;yes&quot;ONBOOT=&quot;yes&quot;TYPE=&quot;Ethernet&quot;UUID=&quot;a89099a9-0852-4c43-bef3-07e3999ec597&quot;MASTER=bond1SLAVE=yes 配置bond网卡子网卡配置完毕之后，我们开始配置bond网卡，vim创建文件，填入以下内容之后，保存退出： 1234567891011121314[root@nginx001 network-scripts]# vim ifcfg-bond1DEVICE=&quot;bond1&quot;BOOTPROTO=&quot;static&quot;IPV6INIT=&quot;yes&quot;IPV6_AUTOCONF=&quot;yes&quot;MTU=&quot;1500&quot;NM_CONTROLLED=&quot;yes&quot;ONBOOT=&quot;yes&quot;TYPE=&quot;Ethernet&quot;IPADDR=103.13.244.21NETMASK=255.255.255.248GATEWAY=103.13.244.17DNS1=223.5.5.5DNS2=223.6.6.6]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维架构</category>
        <category>高并发</category>
        <category>负载均衡</category>
        <category>2层负载均衡</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chrome常用操作]]></title>
    <url>%2F2018%2F06%2F25%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FIT%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7%2FChrome%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[参考文献： chrome键盘快捷键 Windows和Linux标签页和窗口快捷键 操作 快捷键 打开新窗口 Ctrl + n 在无痕模式下打开新窗口 Ctrl + Shift + n 打开新的标签页，并跳转到该标签页 Ctrl + t 重新打开最后关闭的标签页，并跳转到该标签页 Ctrl + Shift + t 跳转到下一个打开的标签页 Ctrl + Tab 或 Ctrl + PgDn 跳转到上一个打开的标签页 Ctrl + Shift + Tab 或 Ctrl + PgUp 跳转到特定标签页 Ctrl + 1 到 Ctrl + 8 跳转到最后一个标签页 Ctrl + 9 在当前标签页中打开主页 Alt + Home 打开当前标签页浏览记录中记录的上一个页面 Alt + 向左箭头键 打开当前标签页浏览记录中记录的下一个页面 Alt + 向右箭头键 关闭当前标签页 Ctrl + w 或 Ctrl + F4 关闭当前窗口 Ctrl + Shift + w 最小化当前窗口 Alt + 空格键 + n 最大化当前窗口 Alt + 空格键 + x 关闭当前窗口 Alt + F4 退出 Google Chrome Ctrl + Shift + q Google Chrome 功能快捷键 操作 快捷键 打开 Chrome 菜单 Alt + f 或 Alt + e 或 F10 + Enter 键 显示或隐藏书签栏 Ctrl + Shift + b 打开书签管理器 Ctrl + Shift + o 在新标签页中打开“历史记录”页 Ctrl + h 在新标签页中打开“下载内容”页 Ctrl + j 打开 Chrome 任务管理器 Shift + Esc 将焦点放置在 Chrome 工具栏中的第一项上 Shift + Alt + t 将焦点放置在 Chrome 工具栏中的最后一项上 F10 将焦点移到未聚焦于的对话框（如果显示）中 F6 打开查找栏搜索当前网页 Ctrl + f 或 F3 跳转到与查找栏中搜索字词相匹配的下一条内容 Ctrl + g 跳转到与查找栏中搜索字词相匹配的上一条内容 Ctrl + Shift + g 打开“开发者工具” Ctrl + Shift + j 或 F12 打开“清除浏览数据”选项 Ctrl + Shift + Delete 在新标签页中打开 Chrome 帮助中心 F1 使用其他帐号登录或以访客身份浏览 Ctrl + Shift + m 打开反馈表单 Alt + Shift + i 地址栏快捷键 在地址栏中可使用以下快捷键： 操作 快捷键 使用默认搜索引擎进行搜索 输入搜索字词并按 Enter 键 使用其他搜索引擎进行搜索 输入搜索引擎名称，然后按 Tab 键 为网站名称添加 www. 和 .com，并在当前标签页中打开该网站 输入网站名称并按 Ctrl + Enter 键 打开新的标签页并执行 Google 搜索 输入搜索字词并按 Alt + Enter 键 跳转到地址栏 Ctrl + l、Alt + d 或 F6 从页面中的任意位置搜索 Ctrl + k 或 Ctrl + e 从地址栏中移除联想查询内容 按向下箭头键以突出显示相应内容，然后按 Shift + Delete 键 网页快捷键 操作 快捷键 打开选项以打印当前网页 Ctrl + p 打开选项以保存当前网页 Ctrl + s 重新加载当前网页 F5 或 Ctrl + r 重新加载当前网页（忽略缓存的内容） Shift + F5 或 Ctrl + Shift + r 停止加载网页 Esc 浏览下一个可点击项 Tab 浏览上一个可点击项 Shift + Tab 使用 Chrome 打开计算机中的文件 按住 Ctrl + o 键并选择文件 显示当前网页的 HTML 源代码（不可修改） Ctrl + u 将当前网页保存为书签 Ctrl + d 将所有打开的标签页以书签的形式保存在新文件夹中 Ctrl + Shift + d 开启或关闭全屏模式 F11 放大网页上的所有内容 Ctrl 和 + 缩小网页上的所有内容 Ctrl 和 - 将网页上的所有内容恢复到默认大小 Ctrl + 0 向下滚动网页，一次一个屏幕 空格键或 PgDn 向上滚动网页，一次一个屏幕 Shift + 空格键或 PgUp 转到网页顶部 首页 转到网页底部 末尾 在网页上水平滚动 按住 Shift 键并滚动鼠标滚轮 将光标移到文本字段中的上一个字词前面 Ctrl + 向左箭头键 将光标移到文本字段中的上一个字词后面 Ctrl + 向右箭头键 删除文本字段中的上一个字词 Ctrl + Backspace 在当前标签页中打开主页 Alt + Home 鼠标快捷键 以下快捷键要求您使用鼠标： 操作 快捷键 在当前标签页中打开链接（仅限鼠标） 将链接拖到标签页中 在新的后台标签页中打开链接 按住 Ctrl 键的同时点击链接 打开链接，并跳转到该链接 按住 Ctrl + Shift 键的同时点击链接 打开链接，并跳转到该链接（仅使用鼠标） 将链接拖到标签栏的空白区域 在新窗口中打开链接 按住 Shift 键的同时点击链接 在新窗口中打开标签页（仅使用鼠标） 将标签页拖出标签栏 将标签页移至当前窗口（仅限鼠标） 将标签页拖到现有窗口中 将标签页移回其原始位置 拖动标签页的同时按 Esc 将当前网页保存为书签 将相应网址拖动到书签栏中 下载链接目标 按住 Alt 键的同时点击链接 显示浏览记录 右键点击“后退”箭头 或“前进”箭头 ，或者点击（按住鼠标按键别松手）“后退”箭头 或“前进”箭头 在最大化模式和窗口模式间切换 双击标签栏的空白区域 放大网页上的所有内容 按住 Ctrl 键的同时向上滚动鼠标滚轮 缩小网页上的所有内容 按住 Ctrl 键的同时向下滚动鼠标滚轮 MAC标签页和窗口快捷键 操作 快捷键 打开新窗口 ⌘ + n 在无痕模式下打开新窗口 ⌘ + Shift + n 打开新的标签页，并跳转到该标签页 ⌘ + t 重新打开最后关闭的标签页，并跳转到该标签页 ⌘ + Shift + t 跳转到下一个打开的标签页 ⌘ + Option + 向右箭头键 跳转到上一个打开的标签页 ⌘ + Option + 向左箭头键 跳转到特定标签页 ⌘ + 1 到 ⌘ + 8 跳转到最后一个标签页 ⌘ + 9 打开当前标签页浏览记录中记录的上一个页面 ⌘ + [ 或 ⌘ + 向左箭头键 打开当前标签页浏览记录中记录的下一个页面 ⌘ + ] 或 ⌘ + 向右箭头键 关闭当前标签页或弹出式窗口 ⌘ + w 关闭当前窗口 ⌘ + Shift + w 最小化窗口 ⌘ + m 隐藏 Google Chrome ⌘ + h 退出 Google Chrome ⌘ + q Google Chrome 功能快捷键 操作 快捷键 显示或隐藏书签栏 ⌘ + Shift + b 打开书签管理器 ⌘ + Option + b 在新标签页中打开“设置”页 ⌘ + , 在新标签页中打开“历史记录”页 ⌘ + y 在新标签页中打开“下载内容”页 ⌘ + Shift + j 打开查找栏搜索当前网页 ⌘ + f 跳转到与查找栏中搜索字词相匹配的下一条内容 ⌘ + g 跳转到与查找栏中搜索字词相匹配的上一条内容 ⌘ + Shift + g 打开查找栏后，搜索选定文本 ⌘ + e 打开“开发者工具” ⌘ + Option + i 打开“清除浏览数据”选项 ⌘ + Shift + Delete 使用其他帐号登录或以访客身份浏览 ⌘ + Shift + m 地址栏快捷键 在地址栏中可使用以下快捷键： 操作 快捷键 使用默认搜索引擎进行搜索 输入搜索字词并按 Enter 键 使用其他搜索引擎进行搜索 输入搜索引擎名称，然后按 Tab 键 为网站名称添加 www. 和 .com，并在当前标签页中打开该网站 输入网站名称并按 Control + Enter 键 为网站名称添加 www. 和 .com，并在新标签页中打开该网站 输入网站名称并按 Control + Shift + Enter 键 在新的后台标签页中打开网站 输入网址并按 ⌘ + Enter 键 跳转到地址栏 ⌘ + l 从地址栏中移除联想查询内容 按向下箭头键以突出显示相应内容，然后按 Shift + fn + Delete 键 网页快捷键 操作 快捷键 打开选项以打印当前网页 ⌘ + p 打开选项以保存当前网页 ⌘ + s 打开“页面设置”对话框 ⌘ + Option + p 重新加载当前网页（忽略缓存的内容） ⌘ + Shift + r 停止加载网页 Esc 浏览下一个可点击项 Tab 浏览上一个可点击项 Shift + Tab 使用 Google Chrome 打开计算机中的文件 按住 ⌘ + o 键并选择文件 显示当前网页的 HTML 源代码（不可修改） ⌘ + Option + u 打开 JavaScript 控制台 ⌘ + Option + j 将当前网页保存为书签 ⌘ + d 将所有打开的标签页以书签的形式保存在新文件夹中 ⌘ + Shift + d 开启或关闭全屏模式 ⌘ + Ctrl + f 放大网页上的所有内容 ⌘ 和 + 缩小网页上的所有内容 ⌘ 和 - 将网页上的所有内容恢复到默认大小 ⌘ + 0 向下滚动网页，一次一个屏幕 空格键 向上滚动网页，一次一个屏幕 Shift + 空格键 搜索网络 ⌘ + Option + f 将光标移到文本字段中的上一个字词前面 Option + 向左箭头键 将光标移到文本字段中的上一个字词后面 Option + 向右箭头键 删除文本字段中的上一个字词 Option + Delete 在当前标签页中打开主页 ⌘ + Shift + h 鼠标快捷键 以下快捷键要求您使用鼠标： 操作 快捷键 在当前标签页中打开链接（仅限鼠标） 将链接拖到标签页中 在新的后台标签页中打开链接 按住 ⌘ 键的同时点击链接 打开链接，并跳转到该链接 按住 ⌘ + Shift 键的同时点击链接 打开链接，并跳转到该链接（仅使用鼠标） 将链接拖到标签栏的空白区域 在新窗口中打开链接 按住 Shift 键的同时点击链接 在新窗口中打开标签页（仅使用鼠标） 将标签页拖出标签栏 将标签页移至当前窗口（仅限鼠标） 将标签页拖到现有窗口中 将标签页移回其原始位置 拖动标签页的同时按 Esc 将当前网页保存为书签 将相应网址拖动到书签栏中 下载链接目标 按住 Option 键的同时点击链接 显示浏览记录 右键点击“后退”箭头 或“前进”箭头 ，或者点击（按住鼠标按键别松手）“后退”箭头 或“前进”箭头 将窗口高度最大化 双击标签栏的空白区域]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>IT基础知识</category>
        <category>常用软件工具</category>
      </categories>
      <tags>
        <tag>Chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[英语学习方法及路线]]></title>
    <url>%2F2018%2F06%2F24%2F%E4%B8%AA%E4%BA%BA%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%2F%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%8F%8A%E8%B7%AF%E7%BA%BF%2F%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%8F%8A%E8%B7%AF%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[中国人学习英语的误区神功大力丸思想大部分人幻想通过一套教材就能够解决英语的所有问题，幻想简单、高效、快速、直接的一种方法，解决学习英语中遇到的所有难题 国内这种大力丸的思想的起源基本是来自于老师群体，培训机构的老师群体，把这种理念和方法进行包装，试图以一种简单的策略来解决所有的英语问题 语言学习是一个复杂的整体，分为不同的阶段，每个阶段的重点都不一样，需要选用的教材，方法，策略也都不一样。并且，因为每个人的学习习惯和作息习惯都不一样，因此选用的学习策略自然也不应该相同 在整个英语学习过程当中，需要输入大量的听力和阅读材料，不能只靠一套课本就彻底解决所有问题，你需要认真分析你所处的水平基础（每个人的水平和基础不一样），灵活选择不同的教材和不同的方法策略 国内的学习者总是喜欢想当然，一定要在某种方法和达到某种水平之间建立因果关系，也就是说，他们做了某事，就一定可以收获什么，如果说我不能收获到成果，可能不是因为方法材料不适合，而只是因为我不够努力和坚持。盲目应用不适合自己的方法和教材 坚持一切以考试为核心]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>英语学习</category>
        <category>英语学习方法及路线</category>
      </categories>
      <tags>
        <tag>英语学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Typora]]></title>
    <url>%2F2018%2F06%2F24%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FIT%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7%2FTypora%2F</url>
    <content type="text"><![CDATA[参考文献： Typora官方网站 基础知识软件介绍markdown编辑器有很多的选择，对比使用之后，目前在用的是一款叫Typora编辑器，特写文章记录一下。本文只是简单介绍，待亲自上手体验之后方可体会它的美感。 官方说明： Typora will give you a seamless experience as both a reader and a writer. It removes the preview window, mode switcher, syntax symbols of markdown source code, and all other unnecessary distractions. Replace them with a real live preview feature to help you concentrate the content itself. Markdown 编辑器，比较常见的是双栏布局：左边敲源码，右边显示渲染结果。 但是Typora 是单栏布局，是真正意义上的所见即所得，摒弃了传统的markdown编辑器的分栏设置（例如markdown pad 2左边是源码，右边是渲染之后的显示效果）以及其他非必须的内容，书写时直接显示成效图。光标一离开，就立刻显示为想要的样子，并且由用户选择何时进入源码模式（输入ctrl+/即可切换源码编辑模式） 入门实践快捷键操作标题操作插入标题： 一级标题：ctrl+1 二级标题：ctrl+2 三级标题：ctrl+3 四级标题：ctrl+4 五级标题：ctrl+5 增大标题级别：ctrl + = 减小标题级别：ctrl + - 表格操作插入表格： 编辑器中插入 直接ctrl+t插入表格，并且在弹出的提示框中可以自由选择表格的行和列数 外部复制（excel等专业表格软件中复制） 直接复制即可，Typora会自动把excel的格式转换为markdown的语法格式 代码块 行内代码块 使用``号将内容包含即可，快捷键为：ctrl+shift+反引号 行代码块 在开头输入```然后回车，在下方输入内容，内容将会自动的变成代码块的形式 指定格式的行代码块 在开头输入```bash/python/ruby然后回车，在下方输入内容，内容将会自动的变成指定代码块的形式 无序列表 基础无序列表： 使用 - content 即可 完成/未完成列表：-[] content or - [x] content 无序列表缩进问题 增大列表缩进（也就是右移，子列表）：ctrl+] 减小列表缩进（也就是左移，父列表）：ctrl+[ 网页链接与图片 编辑器中插入网页超链接 直接ctrl+k即可出现相对应的格式 给文字加上超链接 只需拷贝链接，然后选中文字，按一下ctrl + k，链接就添上了 编辑器中插入图片 ctrl+shift+i 图片拖拽 这个功能可以将拖入图片转化为插入图片，但是这个功能默认是关闭的，需要在设置中手动开启，设置的路径为: Preferences -&gt; Editor 切换源码/预览模式输入ctrl+/可以再源码模式和成像预览模式之间进行来回切换 其他常用操作 文字加粗：ctrl+b 文字倾斜：ctrl+i 文字下划线：ctrl+u 文字删除线：两个波浪线分别在文字的两边~~content~~ 清除格式：ctrl+\ 生成目录输出[toc]然后回车，将会自动产生一个目录，这个目录抽取了文章的所有标题，自动更新内容 软件本身操作文件快速打开输入ctrl+p之后，输入关键字，它会在当前文件夹下进行搜索 打开文件位置点击文件之后，在侧边栏中会有打开文件位置的选项，它能进入该文件所在的目录，这个功能十分的方便。 编辑查找ctrl+f 进行查找 替换ctrl+h进行替换 首行缩进在其中的空格与换行中，有首行缩进的选项，选中之后，会把所有内容中的首行内容进行缩进两个空格。 格式代码ctrl+shitf+` 删除线alt+shift+5 高亮==content== 效果如下： ==content== 清除样式ctrl+\]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>IT基础知识</category>
        <category>常用软件工具</category>
      </categories>
      <tags>
        <tag>Typora</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[换行符报警问题]]></title>
    <url>%2F2018%2F06%2F23%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%2FGit%2F%E6%8D%A2%E8%A1%8C%E7%AC%A6%E6%8A%A5%E8%AD%A6%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在Windows环境下使用git进行add的时候，会提示： “warning:LF will be replacee by CRLF” 基本信息： CRLF – Carriage-Return Line-Feed 回车换行 回车(CR, 对应 ASCII 13, \r) 换行(LF, ASCII 10, \n)。 这里放上ASCII字符代码表的前半部分，可以直观的看到 注意： 这两个标识换行的ACSII字符不会在屏幕有任何输出，在Windows中广泛使用\r\n来标识一行的结束。而在Linux/UNIX系统中只使用\n来标识一行的结束。 这一点经常在windows和linux平台上进行操作的小伙伴基本都知道 也就是说在windows中的换行符为 CRLF， 而在linux下的换行符为：LF 原因分析： git配置中设置 core.autocrlf=true 后： 当我们执行git add将文件转入到暂存区时，系统将会把LF转换成CRLF 当我们执行commit提交时，会把暂存区的内容(也就是我们对工作区做的改动)再重新转化为LF然后放入版本库(repository) 从工作区转化暂存区时，如果发现里面存在 LF 换行符，LF 会被转化成 CRLF，并给出提到的那条警告：”LF will be replaced by CRLF” 这里有一个重要的知识点：git创建的项目，暂存区和运行平台挂钩，但是最终项目文件在版本库(repository)中的换行符是为LF【因为git最终是运行在Linux平台之上】 其实这句警告的下面其实还有一句很重要的话: warning: LF will be replaced by CRLF in . The file will have its original line endings in your working directory. (翻译下就是:“在工作区里,这个文件会保持它原本的换行符，也就是LF和CRLF混合存在。”) 深入延伸扩展 简单来说，在windows平台，我们工作区的文件都应该用 CRLF 来换行。如果改动文件时引入了 LF,或者设置 core.autocrlf 之前,工作区已经有 LF 换行符。那么提交改动时,git 会警告你哪些文件不是纯 CRLF 文件,但 git 不会擅自修改工作区的那些文件,而是对暂存区(我们对工作区的改动)进行修改。 也因此,当我们进行 git add 的操作时,只要 git 发现改动的内容里有 LF 换行符,就还会出现这个警告。 设置 core.autocrlf=true, 只要保持工作区都是纯 CRLF 文件,编辑器用 CRLF 换行,就不会出现警告。 git 默认让版本库里用 LF 换行,只要保持这条规则,多人协作就不会出什么大问题。 git 的 Windows 客户端基本都会默认设置 core.autocrlf=trueLinux 最好不要重新设置,因为这个配置算是为 Windows 平台定制。 如果 Windows 上设置 core.autocrlf=false,仓库里也没有配置 .gitattributes,很容易引入 CRLF 或者混合换行符(Mixed Line Endings,一个文件里既有 LF 又有CRLF)到版本库,这样就可能产生各种奇怪的问题。 如果有换行符不匹配本地平台的情况,建议你用 dos2unix 之类的工具转换下换行符,因为很多配置文件是严格要求文件编码和换行符的,谨慎一点比较好。 问题解决： 因为如果 Windows 上设置 core.autocrlf=false,仓库里也没有配置 .gitattributes,很容易引入 CRLF 或者混合换行符(Mixed Line Endings,一个文件里既有 LF 又有CRLF)到版本库,这样就可能产生各种奇怪的问题。 所以，解决最好保持工作区都是纯 CRLF 文件,编辑器用 CRLF 换行,就不会出现警告。 首先core.autocrlf = true在windows上才是正确的选择，不建议将其修改为false（网上大部分的解决方法都是：rm -rf .git &amp;&amp; git config –global core.autocrlf false &amp;&amp; git init &amp;&amp; git add . &amp;&amp; git remote add xx 这种解决方式，其实不是太友好），如果实在忍受不了，想要避免这些warning，那么执行下面的操作： 添加.gitattributes 设置core.safecrlf = true 使用dos2unix、notepad++等工具来将LF转换成CRLF 所以，建议保持默认效果]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>编程开发</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改主题超链接样式]]></title>
    <url>%2F2018%2F06%2F23%2F%E4%B8%AA%E4%BA%BA%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2FHexo%2F%E4%BF%AE%E6%94%B9%E4%B8%BB%E9%A2%98%E8%B6%85%E9%93%BE%E6%8E%A5%E6%A0%B7%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[因为我使用的是Next主题，这里说下Next主题的修改，其他主题的操作也都是一致的 Next默认对超链接只有下划线样式，在查看文章内容的时候很容易被忽略 主题样式是在\hexoBlog\themes\next\source\css,这里面保存了Muse,Mist和Pisces等主题的css文件 例如,字体和边框的颜色还有字体,图片的大小等保存在next\source\css_variables里. 而我们要修改的body超链接的样式在themes\next\source\css_common\components\post\post.styl里,编辑文件，在文件中添加以下内容: .post-body a { color: #428BCA; font-weight: bold; } 添加以后，我们需要执行clean操作生效 hexo clean hexo g -d 强制刷新我们的文章内容，就能看到超链接已经变成蓝色粗体的形式 如下图所示：]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>个人博客</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iperf命令]]></title>
    <url>%2F2018%2F06%2F12%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FLinux%E5%91%BD%E4%BB%A4%2Fiperf%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[参考文献： github主页 基础知识安装配置使用iperf时，涉及服务端和客户端的概念 因为iperf是使用C语言编写的，因此在安装之前服务器上要安装gcc编译器。 安装# wget http://downloads.es.net/pub/iperf/iperf-3.5.tar.gz # tar -zxvf iperf-3.5.tar.gz # cd iperf-3.5 # ./configure; make; make install 启动服务端执行即可，客户端不需要执行 # iperf3 -s -D iperf3版本启动之后，默认的监听端口为：5201 实际测试客户端指定发送一个5GB的数据包，每隔5秒钟输出一次传输状态，输出结果的显示单位为MB显示，并发3个线程发送 客户端侧执行 # iperf3 -c 10.11.6.3 -n 5000000000 -p 5201 -i 5 -f M -P 3]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>Linux命令</category>
      </categories>
      <tags>
        <tag>iperf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高速通道从入门到实践]]></title>
    <url>%2F2018%2F06%2F08%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E5%85%AC%E6%9C%89%E4%BA%91%E4%BA%A7%E5%93%81%2F%E9%98%BF%E9%87%8C%E4%BA%91%2F%E9%AB%98%E9%80%9F%E9%80%9A%E9%81%93%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[基础知识阿里云高速通道（Express Connect）服务，帮助您在VPC间、VPC与本地数据中心间搭建私网通信通道，提高网络拓扑的灵活性和跨网络通信的质量和安全性。使用高速通道可以使您避免绕行公网带来的网络质量不稳定问题，同时可以免去数据在传输过程中被窃取的风险。 VPC间内网通信 高速通道支持位于相同地域或不同地域，同一账号或不同账号的VPC之间进行内网互通。 阿里云通过在两侧VPC的路由器上分别创建路由器接口，以及自有的骨干传输网络来搭建高速通道，轻松实现两个VPC之间安全可靠，方便快捷的通信。 本地数据中心和阿里云上VPC间内网通信 您可以通过物理专线在物理层面上连接您的本地数据中心到阿里云，然后建立边界路由器和路由器接口来连接数据中心与阿里云VPC。 基础架构基于软件自定义网络（Software Defined Network，简称SDN）架构下的三层Overlay技术和交换机虚拟化技术，阿里云将客户的物理专线接入的端口隔离起来，并抽象成边界路由器。通过目前主流的隧道技术，阿里云将客户的数据包在交换机内部进行封装，在用户的物理专线和VPC的路由器之间加上隧道封装，然后将数据传输到VPC内。 路由器接口路由器接口是一种虚拟设备，具备搭建通信通道并控制其工作状态的功能。 高速通道通过在两侧的VPC路由器上分别创建路由器接口为两个VPC之间搭建内网通信通道。 在两个路由器接口建立连接后，两侧的路由器可以通过建立的通道相互发送消息。因此，两个VPC中的资源（比如ECS实例）就可以通过内网进行通信了。 发起端和接受端当两个路由器接口进行互连时，一个扮演连接发起端角色，另一个扮演连接接受端角色。只有发起端路由器接口才可以发起连接，接受端路由器接口只能等待发起端发起连接。发起端和接受端仅用于控制连接建立的过程，在实际进行网络通信时，通信链路是双向的，发起端和接受端没有任何差别。 对于同账号VPC互通，高速通道提供了同时创建两端的选项。在这种情况下，您不需要手动发起连接，系统会自动发起并建立连接。对于跨账号VPC互通，您必须手动发起路由器接口间的连接。 发起端与接受端的对比如下表所示。 注意： 和我们传统的路由器的发起和接受概念有点不一样，这里需要明确的指定 发起端需要指定接受端的路由器接口 接收端需要指定发起端的路由器接口 也就是说，这两个接口只能是给这一条通道使用 连接过程和连接状态路由器接口的连接过程为：发起端路由器接口发起连接 &gt; 接受端路由器接口接受连接 &gt; 连接成功。 在不同的连接过程和阶段，路由器接口的状态也不同如下表所示。路由器接口创建后的初始状态为未连接。 说明：在创建路由器接口时，如果您选择了同时创建两端，系统会自动发起并建立连接，此种情况下路由器接口直接变为已激活状态。 整个连接过程如下所示： 路由器接口规格高速通道提供小型（10MB-50MB）、中型（100MB-900MB）和大型（1GB-4.5GB）三种规格的路由器接口。 可选择的路由器接口规格在不同连接场景和不同地域中并不相同。您可以根据具体的配置在购买页面选择合适的路由器接口规格。同地域间VPC互连的路由器接口规格默认为大型2档（2GB）。 使用限制 两个VPC之间只能有一对连接成功的路由器接口。 路由器接口创建后无法修改连接角色。 边界路由器（VBR）必须是发起端。 物理专线物理专线是对阿里云接入点和本地数据中心之间建立的网络线路的抽象。您需要通过租用一条运营商的专线将本地数据中心连接到阿里云接入点，建立专线连接。 专线接入后，您可以创建一个边界路由器（VBR）将您本地数据中心和阿里云连接起来，构建混合云环境，使云上资源可以绕过公网通过私网访问本地数据中心。 物理专线的私网连接不通过公网，因此与传统的公网连接相比，物理专线连接更加安全、可靠、速度更快、延迟更低。 功能高速通道物理专线提供以下功能： 多种连接方式 您可以选择使用点对点以太网连接或MPLS VPN连接。物理专线支持以太格式的RJ45电口和LC模式光口, 可以提供1Mbps至10Gbps的传输速率。 冗余连接 物理专线通过等价路由实现两条物理线路冗余： 如果两条专线接入同地域下不同接入点，则两条线路形成天然冗余。 如果两条专线接入同地域下同一个接入点，您可以在申请第二条物理专线时，将第一条物理专线作为冗余线路。 使用限制物理专线使用限制如下： 物理专线不支持SDH的G.703、V.35格式接口。 阿里云在每个可接入的地域提供一个或多个接入点，不同的接入点有运营商限制。在申请专线接入前，您需要提交工单获取接入点以及运营商限制信息。 总结在物理专线接入之后，你会得到一个边界路由器 边界路由器（VBR）必须是发起端。 购买物理专线之后，其实不是直接连接到阿里云的机房，而是连接到这个边界路由器，边界路由器的对端再连接VPC的路由器 边界路由器边界路由器（Virtual border router, VBR）是您申请的物理专线接入交换机的产品映射，可以看做是CPE（Customer-premises equipment）设备和VPC之间的一个路由器，作为数据从VPC到本地数据中心的转发桥梁。 边界路由器同VPC中的路由器一样，同样管理一个路由表。在该路由表中配置路由条目，可以对边界路由器中的流量转发进行管理。 功能边界路由器提供如下功能： 作为VPC和本地数据中心的中间路由器，交换数据包。 在三层子接口模式下，可以识别或附加VLAN(Virtual Local Area Network)标签。 决定物理专线端口模式：三层路由接口或基于VLAN的三层子接口。 支持添加BGP动态路由。 使用限制 目前不支持源地址策略路由。 每个边界路由器有且只有1个路由表。 每个路由表支持48条自定义路由条目。 使用场景VPC私网互连您可以使用高速通道实现两个VPC间的的私网通信需求，既可以避免绕行公网带来的网络质量不稳定问题，也可以免去数据在传输过程中被窃取的风险。详情请参考跨地域VPC互连和跨账号VPC互连。 本地数据中心专线接入VPC如果您的本地数据中心需要与VPC进行私网通信，您可以使用高速通道的物理专线功能实现两侧的私网通信，您可以选择自行搭建专线接入阿里云或让阿里巴巴的合作伙伴为您搭建物理专线。通过物理专线可以实现本地数据中心和VPC间高质量、高可靠且安全性高的私网通信。您可以使用高速通道实现两个VPC间的的私网通信需求，既可以避免绕行公网带来的网络质量不稳定问题，也可以免去数据在传输过程中被窃取的风险。详情请参考同账号专线接入和跨账号专线接入。 两个VPC共用NAT网关如果您需要两个VPC共用一个NAT网关进行公网通信，您可以使用高速通道实现同两个VPC使用同一个NAT网关来访问公网。 使用限制 同一个路由器上的路由器接口不能互连。 边界路由器上的路由器接口只能作为发起端。 一对VPC之间只能同时存在一对互连的路由器接口。 一条物理专线上最多可以存在的边界路由器个数：50个。 一个用户名下最多可以存在的已激活的路由器接口个数：5个。 一个路由器上最多可以存在的已激活的路由器接口个数：5个。 一个账号最多可以在一个接入点接入的物理专线条数：2条。 一个账号下最多可以存在的空闲边界路由器（没有接口的边界路由器）个数：2个。 入门实践跨地域VPC互连本操作以如下同一个账号下的两个VPC为例演示如何使用高速通道实现VPC私网互通。 说明：同账号下同地域和跨地域VPC互连的操作步骤一样。 前提条件 确保要进行互连的VPC或交换机的网段不冲突。 文章内容：跨地域VPC互连 跨账号VPC互连前提条件 两个VPC中交换机地址不能冲突。 已获取双方的阿里云账号ID和路由器ID。 文章内容：跨账号VPC互连 物理专线文章内容：物理专线接入]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>公有云产品</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>高速通道</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络专线]]></title>
    <url>%2F2018%2F06%2F06%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E5%8F%8A%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%2F%E7%BD%91%E7%BB%9C%E7%A7%91%E6%99%AE%E7%9F%A5%E8%AF%86%2F%E7%BD%91%E7%BB%9C%E4%B8%93%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[参考文献： 百度百科 概述什么是网络专线？笼统来说，网络专线就是为某个机构拉一条独立的网线，也就是一个独立的局域网，例如军事，银行等，让用户的数据传输变得可靠可信，专线的优点就是安全性好，QoS（ quality of service 服务质量）可以得到保证。不过，专线租用价格也相对比较高，而且管理也需要专业人员。 网络专线主要有两种信道： 物理专用信道。物理专用信道就是在服务商到用户之间铺设有一条专用的线路，线路只给用户独立使用，其他的数据不能进入此线路，而一般的线路就允许多用户共享信道； 【独享物理线路的形式】 虚拟专用信道；虚拟专用信道就是在一般的信道上为用户保留一定的带宽，使用户可以独享这部分带宽，就像在公用信道上又开了一个通道，只让相应用户使用，而且用户的数据是加密的，以此来保证可靠性与安全性；【在共享物理上创建逻辑独享线路】 这里连接的通道是用户端的出口网关设备（一般是路由器）到ISP的接入端这一段的线路。后续的上网还是通过ISP去实现 目前市面上的信道有： 帧中继（Frame Relay） 数字数据网（DDN Digital Data Network） 异步传输模式（ATM Asynchronous Transfer Mode） X.25（分组交换业务网） 第三代ADSL（非对称用户数字链路） 虚拟专用网络（VPN Virtual Private Network）以及E1等。 什么是互联网专线？互联网专线接入业务是指为客户提供各种速率的专用链路（主要提供传输速率为2M及以上速率），直接连接IP骨干网络，实现方便快捷的高速互联网上网服务。互联网专线接入业务按照客户需求可提供更高速率的专线接入，主要有2Mb/s、10Mb/s、100Mb/s、1000Mb/s等等。 和网络专线的区别 互联网专线跳过了ISP的环节，直接连接Internet骨干网络 主要特点 1.与普通互联网接入相比，其特点是客户通过相对永久的通信线路接入Internet。 2.与拨号上网的最大区别是专线与Internet之间保持着永久、高速、稳定的连接，客户可以实现24小时对Internet的访问，随时获取全球信息资源，提高商务交易的效率。 3.专线客户拥有固定的真实IP地址，可以相对方便地向Internet上的其他客户提供信息服务。 4.专线具有误码率低，时延小的特点。 5.专有带宽的整条电路资源仅为一个客户服务，全程带宽完全独享。 什么是裸光纤？裸光纤就是指专线光纤。通俗又权威的说法：裸光纤就是中间没有连接/经过任何传输设备的光纤，也就是直通光缆。 一般来讲，用户向电信或其他公司租用裸光纤，就是指电信或其他公司只提供光纤物理通道，不提供数据处理等服务，整条光纤干线也不经过任何数据处理设备，由用户自行配置两地的收发设备。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识及网络服务</category>
        <category>网络知识</category>
        <category>网络科普知识</category>
      </categories>
      <tags>
        <tag>网络专线</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ECS从入门到实践]]></title>
    <url>%2F2018%2F06%2F03%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E5%85%AC%E6%9C%89%E4%BA%91%E4%BA%A7%E5%93%81%2F%E9%98%BF%E9%87%8C%E4%BA%91%2FECS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[参考文献： 阿里云官方文档 ECS基础知识ECS 概述云服务器Elastic Compute Service（ECS）是阿里云提供的一种基础云计算服务。使用云服务器ECS就像使用水、电、煤气等资源一样便捷、高效。您无需提前采购硬件设备，而是根据业务需要，随时创建所需数量的云服务器ECS实例。在使用过程中，随着业务的扩展，您可以随时扩容磁盘、增加带宽。如果不再需要云服务器，也能随时释放资源，节省费用。 名词解释在使用ECS之前，需要了解以下概念 地域和可用区：是指ECS资源所在的物理位置。 实例：等同于一台虚拟机，包含CPU、内存、操作系统、网络、磁盘等最基础的计算组件。 实例规格：是指实例的不同配置，包括vCPU核数、内存、网络性能等。实例规格决定了ECS实例的计算和存储能力。 镜像：是指ECS实例运行环境的模板，一般包括操作系统和预装的软件。操作系统支持多种Linux发行版本和不同的Windows版本。 块存储：包括基于分布式存储架构的 弹性块存储，以及基于物理机本地硬盘的 本地存储。 快照：是指某一个时间点上一块弹性块存储的数据备份。 网络类型：包括 - 专有网络：基于阿里云构建的一个隔离的网络环境，专有网络之间逻辑上彻底隔离。更多信息，请参考专有网络VPC。 - 经典网络：统一部署在阿里云公共基础内，规划和管理由阿里云负责。 安全组：由同一地域内具有相同保护需求并相互信任的实例组成，是一种虚拟防火墙，用于设置不同实例的网络访问控制。 SSH密钥对：远程登录Linux ECS实例的验证方式，阿里云存储公钥，您需要自己妥善保管私钥。您也可以选择使用 用户名密码 验证登录Linux ECS实例。 IP地址：包括用于 内网通信 的内网IP或私有IP，以及用于访问Internet的公网IP。 弹性公网IP：可以与实例反复绑定或解绑的静态公网IP地址。 云服务器管理控制台：是指ECS的Web操作界面。 ECS的优势与普通的IDC机房或服务器厂商相比，阿里云提供的云服务器ECS具有以下优势： 高可用性 安全 弹性 高可用性 相较于普通的IDC机房以及服务器厂商，阿里云会使用更严格的IDC标准、服务器准入标准以及运维标准，以保证云计算整个基础框架的高可用性、数据的可靠性以及云服务器的高可用性。 在此基础之上，阿里云所提供的每个地域都存在多可用区。当您需要更高的可用性时，可以利用阿里云的多可用区搭建自己的主备服务或者双活服务。对于面向金融领域的两地三中心的解决方案，您也可以通过多地域和多可用区搭建出更高的可用性服务。其中包括容灾、备份等服务，阿里云都有非常成熟的解决方案。 在阿里云的整个框架下，这些服务可以非常平滑地进行切换，相关的信息可以在阿里云行业解决方案中找到。无论是两地三中心，还是电子商务以及视频服务等，都可以在阿里云找到对应的行业解决方案。 此外，阿里云提供了如下三项支持： 提升可用性的产品和服务，包括云服务器、负载均衡、多备份数据库服务以及数据迁移服务DTS等。 行业合作伙伴以及生态合作伙伴，帮助您完成更高、更稳定的架构，并且保证服务的永续性。 多种多样的培训服务，让您从业务端到底层的基础服务端，在整条链路上实现高可用。 安全性 选择了云计算，最关心的问题就是云计算的安全与稳定。阿里云近期通过了很多的国际安全标准认证，包括ISO27001、MTCS等，这些所有的安全合规都要求对于用户数据的私密性、用户信息的私密性以及用户隐私的保护都有非常严格的要求。对于云计算，推荐您使用阿里云专有网络。 在阿里云专有网络之上，可以产生更多的业务可能性。您只需进行简单配置，就可在自己的业务环境下，与全球所有机房进行串接，从而提高了业务的灵活性、稳定性以及业务的可发展性。 对于原来拥有自建的IDC机房，也不会产生问题。阿里云专有网络可以拉专线到原有的IDC机房，形成混合云的架构。阿里云可以提供各种混合云的解决方案和非常多的网络产品，形成强大的网络功能，让您的业务更加灵活。结合阿里云的生态，您可以在云上发展出意想不到的业务生态。 阿里云专有网络更加稳定和安全。 稳定性：业务搭建在专有网络上，而网络的基础设施将会不停进化，使您每天都拥有更新的网络架构以及更新的网络功能，使得您的业务永远保持在一个稳定的状态。专有网络允许您自由地分割、配置和管理自己的网络。 安全性：面对互联网上不断的攻击流量，专有网络天然就具备流量隔离以及攻击隔离的功能。业务搭建在专有网络上后，专有网络会为业务筑起第一道防线。 总之，专有网络提供了稳定、安全、快速交付、自主可控的网络环境。对于传统行业以及未接触到云计算的行业和企业而言，借助专有网络混合云的能力和混合云的架构，它们将享受云计算所带来的技术红利。 弹性 云计算最大的优势就在于弹性。目前，阿里云已拥有在数分钟内开出一家中型互联网公司所需要的IT资源的能力，这就能够保证大部分企业在云上所构建的业务都能够承受巨大的业务量压力。 计算弹性 纵向弹性，即单个服务器的配置变更。传统IDC模式下，很难做到对单个服务器进行变更配置。而对于阿里云，当您购买了云服务器或者存储的容量后，可以根据业务量的增长或者减少自由变更自己的配置。关于纵向弹性的具体应用，详情请参考 升降配。 横向弹性。对于游戏应用或直播平台出现的高峰期，若在传统的IDC模式下，您根本无法立即准备资源；而云计算却可以使用弹性的方式帮助客户度过这样的高峰。当业务高峰消失时，您可以将多余的资源释放掉，以减少业务成本的开支。利用横向的扩展和缩减，配合阿里云的弹性伸缩，完全可以做到定时定量的伸缩，或者按照业务的负载进行伸缩。关于横向弹性的具体应用，详情请参考 弹性伸缩。 存储弹性。阿里云拥有很强的存储弹性。当存储量增多时，对于传统的IDC方案，您只能不断去增加服务器，而这样扩展的服务器数量是有限的。在云计算模式下，将为您提供海量的存储，当您需要时可以直接购买，为存储提供最大保障。关于存储弹性的具体应用，详情请参考磁盘扩容。 网络弹性。云上的网络也具有非常大的灵活性。只要您购买了阿里云的专有网络，那么所有的网络配置与线下IDC机房配置可以是完全相同的，并且可以拥有更多的可能性。可以实现各个机房之间的互联互通，各个机房之间的安全域隔离，对于专有网络内所有的网络配置和规划都会非常灵活。关于网络弹性的具体应用，详情请参考专有网络。 总之，对于阿里云的弹性而言，是计算的弹性、存储的弹性、网络的弹性以及您对于业务架构重新规划的弹性。您可以使用任意方式去组合自己的业务，阿里云都能够满足您的需求。 块存储概念阿里云为您的云服务器ECS提供了丰富的块存储产品类型，包括基于分布式存储架构的弹性块存储产品，以及基于物理机本地硬盘的本地存储产品。其中： 弹性块存储，是阿里云为云服务器ECS提供的数据块级别的随机存储，具有低时延、持久性、高可靠等性能，采用 三副本的分布式机制，为ECS实例提供99.9999999%的数据可靠性保证。可以随时创建或释放，也可以随时扩容。 本地存储，也称为本地盘，是指挂载在ECS云服务器所在物理机（宿主机）上的本地硬盘，是一种临时块存储。是专为对存储I/O性能有极高要求的业务场景而设计的存储产品。该类存储为实例提供块级别的数据访问能力，具有低时延、高随机IOPS、高吞吐量的I/O能力。 块存储、对象存储、文件存储的区别阿里云目前主要提供三种数据存储产品，分别是块存储、文件存储（NAS）和 对象存储（OSS）。 三者区别如下： 块存储：是阿里云为ECS云服务器提供的块设备，高性能、低时延，满足随机读写，可以像使用物理硬盘一样格式化建文件系统使用。可用于大部分通用业务场景下的数据存储。 对象存储（OSS，Object Storage Service）：可以理解是一个海量的存储空间，最适合存储互联网上产生的图片、短视频、音频等海量非结构化数据，您可以通过API在任何时间、任何地点访问对象存储里的数据。常用于互联网业务网站搭建、动静资源分离、CDN加速等业务场景。 文件存储（NAS，Network Attached Storage）：类似于对象存储，适合存储非结构化的海量数据。但是您需要通过标准的文件访问协议访问这些数据，比如 Linux 系统需要使用Network File System (NFS)协议，Windows系统需要使用Common Internet File System (CIFS)协议。您可以通过设置权限让不同的客户端同时访问同一份文件。文件存储适合企业部门间文件共享、广电非线编、高性能计算、Docker等业务场景。 块存储性能衡量块存储产品的性能指标主要包括：IOPS、吞吐量和访问时延。 IOPS IOPS是Input/Output Operations per Second，即每秒能处理的I/O个数，用于表示块存储处理读写（输出/输入）的能力。如果要部署事务密集型应用，需要关注IOPS性能。 最普遍的IOPS性能指标是顺序操作和随机操作，如下表所示。 吞吐量 吞吐量是指单位时间内可以成功传输的数据数量。 如果要部署大量顺序读写的应用，需要关注吞吐量。 访问延迟 访问时延是指块存储处理一个I/O需要的时间。 如果您的应用对时延比较敏感，比如数据库（过高的时延会导致应用报错），建议您使用固态硬盘介质的SSD云盘、SSD共享块存储或本地SSD盘类产品。 如果您的应用更偏重存储吞吐能力，对时延不太敏感，比如Hadoop离线计算等吞吐密集型应用，建议您使用本地HDD盘类产品，如d1或d1ne大数据型实例。 不同云盘之间的性能测试对比请看文档：云盘性能对比部分 弹性块存储弹性块存储，是阿里云为云服务器ECS提供的数据块级别的随机存储，具有低时延、持久性、高可靠等性能，采用 分布式三副本机制，为ECS实例提供99.9999999%的数据可靠性保证。弹性块存储支持在可用区内自动复制您的数据，防止意外硬件故障导致的数据不可用，保护您的业务免于组件故障的威胁。就像硬盘一样，您可以对挂载到ECS实例上的弹性块存储做分区、创建文件系统等操作，并持久存储数据。 您可以根据业务需要随时扩容弹性块存储。具体操作，请参见 扩容数据盘 和 扩容系统盘。您也可以为弹性块存储创建快照，备份数据。关于快照的更多信息，参见 快照。 根据是否可挂载到多台ECS实例，弹性块存储可以分为： 云盘：一块云盘只能挂载到同一地域、同一可用区的一台ECS实例。 共享块存储：一块共享块存储可以同时挂载到同一地域、同一可用区的16台ECS实例。 说明：共享块存储目前仍处于公测阶段，公测期间支持最多同时挂载到4台ECS实例上。 总结： 也就是说弹性块存储在使用的时候，可以被当做是本地的磁盘，也可以是当做网络存储，类似NFS等挂载到多台ECS主机上使用区分云盘和共享块存储的方式是能否被多台ECS同时挂载 云盘根据性能分类 根据性能不同，云盘可以分为： ESSD云盘：又称增强型SSD云盘，是阿里云全新推出的超高性能的云盘产品。基于新一代分布式块存储架构，结合25GE网络和RDMA技术，为您提供单盘高达100万的随机读写能力和低至100μs的单路时延能力。ESSD云盘处于邀测阶段，更多信息，请参见 ESSD云盘FAQ。 SSD云盘：采用固态硬盘作为存储介质，能够提供稳定的高随机I/O、高数据可靠性的高性能存储。 高效云盘：采用固态硬盘与机械硬盘的混合介质作为存储介质。 普通云盘：采用机械磁盘作为存储介质 根据用途分类 根据用途不同，云盘可以作： 系统盘：生命周期与系统盘所挂载的ECS实例相同，随实例一起创建和释放。不可共享访问。系统盘可选的容量范围与实例所选的镜像有关： Linux（不包括CoreOS）+ FreeBSD：20 GiB ~ 500 GiB CoreOS：30 GiB ~ 500 GiB Windows：40 GiB ~ 500 GiB 数据盘：可以与ECS实例同时创建，也可以 单独创建，不可共享访问。与ECS实例同时创建的数据盘，生命同期与实例相同，随实例一起创建和释放。单独创建的数据盘，可以 单独释放，也可以 设置为随ECS实例一起释放。数据盘的容量由云盘类型决定，详细信息，请参见 块存储性能。作数据盘用时，云盘与共享块存储共享数据盘配额，即一台实例最多挂载16块数据盘。 共享块存储共享块存储是一种支持多台ECS实例并发读写访问的数据块级存储设备，具备多并发、高性能、高可靠等特性，数据可靠性可以达到 99.9999999%。单块共享块存储最多可以同时挂载到16台ECS实例。目前尚处于公测阶段（申请公测资格），最多同时挂载到4台ECS实例。 共享块存储只能作数据盘用，只能单独创建，可以共享访问。您可以 设置共享块存储与挂载的ECS实例一起释放。 根据性能不同，共享块存储可以分为： SSD共享块存储：采用固态硬盘作为存储介质，能够提供稳定的高随机I/O、高数据可靠性的高性能存储。 高效共享块存储：采用固态硬盘与机械硬盘的混合介质作为存储介质。 挂载到实例上时，共享块存储与云盘共享数据盘配额，即一台实例最多挂载16块数据盘。 更多共享块存储的信息，请参见 共享块存储FAQ。 网络和安全性内网目前阿里云的云服务器ECS内网间，非I/O优化的实例为千兆共享的带宽，I/O优化的实例为万兆共享的带宽，没有特殊限制。由于是共享网络，因此无法保证带宽速度是不变的。 如果两台同地域的ECS实例之间需要传输数据，一般建议使用内网连接。同时，云数据库RDS、负载均衡（SLB） 以及 对象存储（OSS） 相关的内网速度也都是千兆共享的环境。这些产品间也都可以使用内网相互连接使用。 目前只要是相同地域下，SLB、云数据库RDS、OSS与ECS之间都可以直接内网互通连接使用。 弹性网卡弹性网卡（ENI）是一种可以附加到专有网络VPC类型ECS实例上的虚拟网卡，通过弹性网卡，您可以实现高可用集群搭建、低成本故障转移和精细化的网络管理。所有地域均支持弹性网卡。 使用场景弹性网卡适用于以下几种场景： 搭建高可用集群 满足系统高可用架构对于单实例多网卡的需求。 低成本故障迁移 通过将弹性网卡从ECS实例分离后再附加到另外一台ECS实例，将故障实例上的业务流量快速迁移到备用实例，实现服务快速恢复。 精细化网络管理 可以为实例配置多个弹性网卡，例如用于内部管理的弹性网卡及用于面向公网业务访问的弹性网卡等，完成管理数据和业务数据间的隔离。可以根据源IP、协议、端口等对每张弹性网卡配置精准的安全组规则，从而对每张弹性网卡的流量进行安全访问控制。 弹性网卡类型 弹性网卡分为两种类型： 主网卡 在创建专有网络实例时随实例默认创建的弹性网卡称作主网卡。主网卡的生命周期和实例保持一致，您无法分离主网卡与实例。 辅助网卡 您可以创建辅助网卡，并将其附加到实例上或从实例上分离。每个实例能附加的网卡上限与实例规格相关，详细信息，请参考 实例规格族。 弹性网卡属性 属性 数量 主私有IP地址 1个 MAC地址 1个 安全组 至少1个，最多5个 描述信息 1个 网卡名称 1个 限制约束 使用弹性网卡有如下限制： 一个账号在一个地域内默认最多可创建100个弹性网卡。如果需要更多，请 提交工单 申请。 ECS实例与弹性网卡必须在同一VPC的同一可用区中，可以分属于不同交换机。 每台实例允许附加的弹性网卡数量由实例规格决定。详细信息，请参见 实例规格族。 非I/O优化实例规格不支持弹性网卡。 您不能在一个实例上附加多个弹性网卡来提高实例带宽。 说明：实例的带宽能力由实例规格决定。 安全组安全组是一个逻辑上的分组，这个分组是由同一个地域（Region）内具有相同安全保护需求并相互信任的实例组成。每个实例至少属于一个安全组，在创建的时候就需要指定。同一安全组内的实例之间网络互通，不同安全组的实例之间默认内网不通。可以授权两个安全组之间互访。 安全组是一种虚拟防火墙，具备状态检测包过滤功能。安全组用于设置单台或多台云服务器的网络访问控制，它是重要的网络安全隔离手段，用于在云端划分安全域。 安全组限制 单个安全组内的实例个数不能超过 1000。如果您有超过 1000 个实例需要内网互访，可以将他们分配到多个安全组内，并通过互相授权的方式允许互访。 每个实例最多可以加入 5 个安全组。 每个用户的安全组最多 100 个。 对安全组的调整操作，对用户的服务连续性没有影响。 安全组是有状态的。如果数据包在 Outbound 方向是被允许的，那么对应的此连接在 Inbound 方向也是允许的。 安全组的网络类型分为经典网络和专有网络。 - 经典网络类型的实例可以加入同一地域（Region）下经典网络类型的安全组。 - 专有网络类型的实例可以加入同一专有网络（VPC）下的安全组。 安全组规则 安全组规则可以允许或者禁止与安全组相关联的云服务器 ECS 实例的公网和内网的入出方向的访问。 您可以随时授权和取消安全组规则。您的变更安全组规则会自动应用于与安全组相关联的ECS实例上。 在设置安全组规则的时候，安全组的规则务必简洁。如果您给一个实例分配多个安全组，则该实例可能会应用多达数百条规则。访问该实例时，可能会出现网络不通的问题。 安全组规则限制 每个安全组最多有 100 条安全组规则。 DDOS基础防护阿里云云盾默认为ECS实例免费提供5 Gbit/s恶意流量攻击，即 DDoS基础防护能力。这一功能可以有效防止云服务器ECS实例受到恶意攻击，从而保证ECS系统的稳定，即当流入ECS实例的流量超出实例规格对应的限制时，云盾就会帮助ECS实例限流，避免ECS系统出现问题。 企业版入门企业级用户在购买和使用云服务器ECS实例时，通常需考虑如下几点： 配置选型 估算成本 网络规划 配置安全组 制定快照策略 镜像迁移 用负载均衡实现ECS的高可用性 配置选型参考资料：阿里云官方资料 实例规格族实例是能够为您的业务提供计算服务的最小单位，它是以一定的规格来为您提供相应的计算能力的。 根据业务场景和使用场景，ECS实例可以分为多种规格族。同一个规格族里，根据CPU和内存的配置，可以分为多种不同的规格。 ECS实例规格定义了实例的CPU和内存（包括CPU型号、主频等）这两个基本属性。但是，ECS实例只有同时配合块存储、镜像和网络类型，才能唯一确定一台实例的具体服务形态。 用户指南安全组安全组限制 单个安全组内的实例个数不能超过 1000。如果您有超过 1000 个实例需要内网互访，可以将他们分配到多个安全组内，并通过互相授权的方式允许互访。 每个实例最多可以加入 5 个安全组。 每个用户的安全组最多 100 个。 对安全组的调整操作，对用户的服务连续性没有影响。 安全组是有状态的。如果数据包在 Outbound 方向是被允许的，那么对应的此连接在 Inbound 方向也是允许的。 安全组的网络类型分为经典网络和专有网络。 - 经典网络类型的实例可以加入同一地域（Region）下经典网络类型的安全组。 - 专有网络类型的实例可以加入同一专有网络（VPC）下的安全组。 每个安全组最多有 100 条安全组规则。 安全组注意事项 出方向的端口25默认受限，无法通过安全组规则打开，但是您可以 申请解封端口25。 安全组实践的基本建议在开始安全组的实践之前，下面有一些基本的建议： 最重要的规则：安全组应作为白名单使用。 开放应用出入规则时应遵循“最小授权”原则，例如，您可以选择开放具体的端口（如 80 端口）。 不应使用一个安全组管理所有应用，因为不同的分层一定有不同的需求。 对于分布式应用来说，不同的应用类型应该使用不同的安全组，例如，您应对 Web、Service、Database、Cache 层使用不同的安全组，暴露不同的出入规则和权限。 没有必要为每个实例单独设置一个安全组，控制管理成本。 优先考虑 VPC 网络。 不需要公网访问的资源不应提供公网 IP。 尽可能保持单个安全组的规则简洁。因为一个实例最多可以加入 5 个安全组，一个安全组最多可以包括 100 个安全组规则，所以一个实例可能同时应用数百条安全组规则。您可以聚合所有分配的安全规则以判断是否允许流入或留出，但是，如果单个安全组规则很复杂，就会增加管理的复杂度。所以，应尽可能地保持单个安全组的规则简洁。 调整线上的安全组的出入规则是比较危险的动作。如果您无法确定，不应随意更新安全组出入规则的设置。阿里云的控制台提供了克隆安全组和安全组规则的功能。如果您想要修改线上的安全组和规则，您应先克隆一个安全组，再在克隆的安全组上进行调试，从而避免直接影响线上应用。 设置安全组规则设置安全组的入网规则不要使用 0.0.0.0/0 的入网规则 允许全部入网访问是经常犯的错误。使用 0.0.0.0/0 意味着所有的端口都对外暴露了访问权限。这是非常不安全的。正确的做法是，先拒绝所有的端口对外开放。安全组应该是白名单访问。例如，如果您需要暴露 Web 服务，默认情况下可以只开放 80、8080 和 443 之类的常用TCP端口，其它的端口都应关闭。 { &quot;IpProtocol&quot; : &quot;tcp&quot;, &quot;FromPort&quot; : &quot;80&quot;, &quot;ToPort&quot; : &quot;80&quot;, &quot;SourceCidrIp&quot; : &quot;0.0.0.0/0&quot;, &quot;Policy&quot;: &quot;accept&quot;} , { &quot;IpProtocol&quot; : &quot;tcp&quot;, &quot;FromPort&quot; : &quot;8080&quot;, &quot;ToPort&quot; : &quot;8080&quot;, &quot;SourceCidrIp&quot; : &quot;0.0.0.0/0&quot;, &quot;Policy&quot;: &quot;accept&quot;} , { &quot;IpProtocol&quot; : &quot;tcp&quot;, &quot;FromPort&quot; : &quot;443&quot;, &quot;ToPort&quot; : &quot;443&quot;, &quot;SourceCidrIp&quot; : &quot;0.0.0.0/0&quot;, &quot;Policy&quot;: &quot;accept&quot;} , 关闭不需要的入网规则 如果您当前使用的入规则已经包含了 0.0.0.0/0，您需要重新审视自己的应用需要对外暴露的端口和服务。如果确定不想让某些端口直接对外提供服务，您可以加一条拒绝的规则。比如，如果您的服务器上安装了 MySQL 数据库服务，默认情况下您不应该将 3306 端口暴露到公网，此时，您可以添加一条拒绝规则，如下所示，并将其优先级设为100，即优先级最低。 { &quot;IpProtocol&quot; : &quot;tcp&quot;, &quot;FromPort&quot; : &quot;3306&quot;, &quot;ToPort&quot; : &quot;3306&quot;, &quot;SourceCidrIp&quot; : &quot;0.0.0.0/0&quot;, &quot;Policy&quot;: &quot;drop&quot;, Priority: 100} , 上面的调整会导致所有的端口都不能访问 3306 端口，极有可能会阻止您正常的业务需求。此时，您可以通过授权另外一个安全组的资源进行入规则访问。 授权另外一个安全组入网访问 不同的安全组按照最小原则开放相应的出入规则。对于不同的应用分层应该使用不同的安全组，不同的安全组应有相应的出入规则。 例如，如果是分布式应用，您会区分不同的安全组，但是，不同的安全组可能网络不通，此时您不应该直接授权 IP 或者 CIDR 网段，而是直接授权另外一个安全组 ID 的所有的资源都可以直接访问。比如，您的应用对 Web、Database 分别创建了不同的安全组：sg-web 和 sg-database。在sg-database 中，您可以添加如下规则，授权所有的 sg-web 安全组的资源访问您的 3306 端口。 { &quot;IpProtocol&quot; : &quot;tcp&quot;, &quot;FromPort&quot; : &quot;3306&quot;, &quot;ToPort&quot; : &quot;3306&quot;, &quot;SourceGroupId&quot; : &quot;sg-web&quot;, &quot;Policy&quot;: &quot;accept&quot;, Priority: 2} , 授权另外一个 CIDR 可以入网访问 经典网络中，因为网段不太可控，建议您使用安全组 ID 来授信入网规则。 VPC 网络中，您可以自己通过不同的 VSwitch 设置不同的 IP 域，规划 IP 地址。所以，在 VPC 网络中，您可以默认拒绝所有的访问，再授信自己的专有网络的网段访问，直接授信可以相信的 CIDR 网段。 { &quot;IpProtocol&quot; : &quot;icmp&quot;, &quot;FromPort&quot; : &quot;-1&quot;, &quot;ToPort&quot; : &quot;-1&quot;, &quot;SourceCidrIp&quot; : &quot;10.0.0.0/24&quot;, Priority: 2} , { &quot;IpProtocol&quot; : &quot;tcp&quot;, &quot;FromPort&quot; : &quot;0&quot;, &quot;ToPort&quot; : &quot;65535&quot;, &quot;SourceCidrIp&quot; : &quot;10.0.0.0/24&quot;, Priority: 2} , { &quot;IpProtocol&quot; : &quot;udp&quot;, &quot;FromPort&quot; : &quot;0&quot;, &quot;ToPort&quot; : &quot;65535&quot;, &quot;SourceCidrIp&quot; : &quot;10.0.0.0/24&quot;, Priority: 2} , 总结： 安全组的实质是白名单 不适用0.0.0.0/0的入网规则 如果已经存在了0.0.0.0/0这种规则，那么需要设置关闭不安全的入网规则，例如关闭3306端口等，这里需要设置优先级为最小值100 不同应用使用不同的安全组，在这种情况下，需要在入向规则中添加安全组授权，比如数据库的安全组中授权web的安全组 最佳实践安全常见问题]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>公有云产品</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>ECS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[带宽计算方法及B与b说明]]></title>
    <url>%2F2018%2F05%2F30%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FIT%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2FIT%E7%A7%91%E6%99%AE%E7%9F%A5%E8%AF%86%2F%E5%B8%A6%E5%AE%BD%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E5%8F%8AB%E4%B8%8Eb%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[参考文献： 带宽计算方法及大B与小b说明 bit与Byte的关系在计算机科学中，bit（比特）是表示信息的最小单位，叫做二进制位；一般用0和1表示。 Byte叫做字节，由8个比特位（8bit）组成一个字节(1Byte)，用于表示计算机中的一个字符。 bit与Byte之间可以进行换算，其换算关系为：1Byte=8bit（或简写为：1B=8b） 在实际应用中一般用简称， 即1bit简写为1b(注意是小写英文字母b)，1Byte简写为1B（注意是大写英文字母B）。 网络带宽在计算机网络或者是网络运营商(Internet service provider)中，一般宽带速率的表示单位用bps(或b/s，小b)表示：bps表示比特每秒即表示每秒钟传输多少位信息，是bit per second的缩写。 实际所说的1M带宽的意思是1Mbps（是兆比特每秒Mbps，而不是兆字节每秒MBps） 换算公式: 在速度传输环境中，Milionbit=1000Kilobit=1000000bit； 1Mbps = 1000kbps = 1000 000bps 然后我们通过bit和byte的转换可以得到： 1Mbps = 1000kbps/8 = 125 kBps 也就是说：1Mbps的带宽速率，每秒可以传输1个M比特，1000个千比特，125个千字节 注意： 在实际书写中，B应表示Byte(字节)，b应表示bit(比特)，但是我们在实际书写中很容易把bit和Byte都混写为b ，如把Mb/s和MB/s都混写为Mb/s，导致人们在实际计算中因单位的混淆而出错。 下载速度在实际上网应用中，下载软件时常常看到诸如下载速度显示为128KB（KB/s），103KB/s等等宽带速率大小字样，因为ISP提供的线路带宽使用的单位是比特（bit，即小b），而一般下载软件显示的是字节（byte，1byte＝8bits），所以要通过换算，才能得实际值。 所以，我们可以按照公式换算一下： 128KB/s=128×8(Kb/s)=1024Kb/s=1.024Mb/s 也就是说：下载速度为128KB/s，对应的网络带宽是1.024Mb/s。 1Mbps = 1000 kbps/8 = 125 kBps 因此1M的带宽下载的速度最大为125KB每秒。 同样的，2M、3M带宽分别对应的是250KBps、375KBps； 也就是说：2M、3M带宽的下载速度分别不会超过250KB、375KB每秒。 在一些软件的带宽的显示页面，通常的显示页面也是以bps的方式来显示，这个时候，我们就需要进行一下换算，例如下面的页面截图（阿里云带宽使用情况） 图中所选的这个值是：13272120 bps(bits/s)，我们下面进行换算： 13272120 bps = 13272120/1000 Kbps = 13272120/1000/1000 Mbps = 12.65727 Mbps 换算之后，我们可以看到这里显示的带宽是12.6M 总结在网络运营商提供的宽带速率单位中，”bps”是指”bit per second” 而我们在日常生活中，使用的一般是”Byte persecond”(Bps) 我们说的带宽几M几M指的是 2Mbps、8Mbps这种格式，为了便于更加直观的查看，我们会转成B的形式，也就是说，我们拿到这个数字之后，需要先*1000，将M变成K，然后再/8，最后的单位就是我们最常使用的单位了 举个栗子： 1M的带宽，理论的下载速度为：1*1000/8= 125KB/s 8M的带宽，理论的下载速度为：8*1000/8 = 1000KB/s = 1MB/s 在8M带宽之后，我们的换算，可以直接除以8来得到结果 100M的带宽，理论的下载速度为： 100/8 = 12.5MB/s 常见问题1024和1000的区别 在网络传输中使用的计算单位是：1000（带宽计算是使用1000） 在存储领域（内存或者磁盘等）使用的计算单位是：1024。 数据传输速率的衡量单位K是十进制含义,但数据存储的K是2进制含义。 1Mbps与1m/s的区别 我们上面说过，1Mbps指的是1000/8KB/S也就是125KB/S， 而m/s、KB/S等属于字节每秒的单位，1m/s指的是是1024KB/s 补充：ADSL宽带上下行知识ADSL（Asymmetric Digital Subscriber Loop）技术是一种不对称数字用户线实现宽带接入互连网的技术，ADSL作为一种传输层的技术，充分利用现有的铜线资源，在一对双绞线上提供上行640kbps（理论上行1Mbps）下行8Mbps的带宽，从而克服了传统用户在”最后一公里”的”瓶颈”，实现了真正意义上的宽带接入。 上行速率：是指用户电脑向网络发送信息时的数据传输速率。 下行速率： 是指网络向用户电脑发送信息时的传输速率。比如用 FTP上传文件到网上去，影响上传速度的就是“上行速率”；而从网上下载文件，影响下载速度的就是“下行速率”。 当然，在实际上传下载过程中，线路、设备 (含计算机及其他设备)等的质量也会对速度造成或多或少的影响。 上行速率对上行速率有影响 TCP/IP规定，每一个封包，都需要有acknowledge信息的回传，也就是说，传输的资料，需要有一个收到资料的信息回复，才能决定后面的传输速度，并决定是否重新传输遗失的资料。 上行的带宽一部分就是用来传输这些acknowledge(确认)资料的，当上行负载过大的时候，就会影响acknowledge资料的传送速度，并进而影响到下载速度。这对非对称数字环路也就是ADSL这种上行带宽远小于下载带宽的连接来说影响尤为明显。 有试验证明，当上传满载时，下载速度讲变为理想速度的40%，这就可以解释为什么很多朋友用BT下载的时候稍微限速反而能够获得更大的下载速度。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>IT基础知识</category>
        <category>IT科普知识</category>
      </categories>
      <tags>
        <tag>带宽计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7安装Python3]]></title>
    <url>%2F2018%2F05%2F20%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2FPython%2FCentos7%E5%AE%89%E8%A3%85Python3%2F</url>
    <content type="text"><![CDATA[在centos 7中，默认安装的python版本为2.7,一般情况下，我们都需要对python进行升级 [root@master ~]# cat /etc/redhat-release CentOS Linux release 7.3.1611 (Core) [root@master ~]# python --version Python 2.7.5 环境说明[root@master ~]# which python /usr/bin/python [root@master ~]# ll /usr/bin/python lrwxrwxrwx 1 root root 7 Apr 13 16:50 /usr/bin/python -&gt; python2 [root@master ~]# ll /usr/bin/python2 lrwxrwxrwx 1 root root 9 Apr 13 16:50 /usr/bin/python2 -&gt; python2.7 [root@master ~]# ll /usr/bin/python2.7 -rwxr-xr-x 1 root root 7136 Aug 4 2017 /usr/bin/python2.7 我们知道我们的python命令是在/usr/bin目录下 可以看到，python指向的是python2，python2指向的是python2.7 因此我们可以装个python3，然后将python指向python3，然后python2指向python2.7，那么两个版本的python就能共存了。 正式安装下载python3的源码包 wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz 解压编译安装 [root@master software]# tar -zxvf Python-3.6.5.tgz [root@master software]# cd Python-3.6.5 ​ [root@master Python-3.6.5]# ./configure –prefix=/usr/local/python3 [root@master Python-3.6.5]# make &amp;&amp; make install 添加软链接 [root@master Python-3.6.5]# mv /usr/bin/python /usr/bin/python.bak ​ [root@master Python-3.6.5]# ln -s /usr/local/python3/bin/python3.6 /usr/bin/python [root@master Python-3.6.5]# python --version Python 3.6.5 补充操作更改yum配置安装完毕之后，我们需要修改yum的配置，因为其要使用python2执行，此时我们修改了python的指向路径，不修改则会导致yum无法正常使用。 vim /usr/bin/yum 把#! /usr/bin/python修改为#! /usr/bin/python2 vim /usr/libexec/urlgrabber-ext-down 把#! /usr/bin/python 修改为#! /usr/bin/python2]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>基础环境配置</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDS从入门到实践]]></title>
    <url>%2F2018%2F05%2F14%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E5%85%AC%E6%9C%89%E4%BA%91%E4%BA%A7%E5%93%81%2F%E9%98%BF%E9%87%8C%E4%BA%91%2FRDS%2F</url>
    <content type="text"><![CDATA[官方文档 基础知识阿里云关系型数据库（Relational Database Service，简称 RDS）是一种稳定可靠、可弹性伸缩的在线数据库服务。基于阿里云分布式文件系统和SSD盘高性能存储，RDS 支持 MySQL、SQL Server、PostgreSQL 和 PPAS（Postgre Plus Advanced Server，一种高度兼容 Oracle 的数据库）引擎，并且提供了容灾、备份、恢复、监控、迁移等方面的全套解决方案，彻底解决数据库运维的烦恼。 RDS的特点云数据库RDS作为一个公共的关系型数据库，高可用和高安全是其首要优势，其次才是高性能，因为没人会使用既不稳定又不安全的服务。RDS的优势主要体现在如下几点： RDS提供了主备双节点的实例，双节点可以在同一地域的不同可用区，MySQL实例的双节点还可以在不同地域，当主实例出现故障时可快速切换到备实例，保障了RDS的稳定性。 RDS在数据的存取上加入了中间层，所有请求都会经过中间层，而且有SQL注入的请求都会被中间层拦截掉。在底层数据写入上，RDS采用了最高安全级别的写入，保证在主机异常掉电的情况下数据不会出现丢失。以此来保障数据库的高安全性。 RDS源码团队持续对MySQL进行源码优化，在标准的基准测试中性能和稳定性上都是高于社区版本的。 关于这部分内容可以查看：对比ECS自建数据库与RDS性能时的注意事项 访问控制数据库账号 当用户创建实例后，RDS并不会为用户创建任何初始的数据库账号。 有如下两种方式来创建数据库帐号： 用户可以通过控制台或者API来创建普通数据库账号，并设置数据库级别的读写权限。 如果用户需要更细粒度的权限控制，比如表、视图，字段级别的权限，也可以通过控制台或者API先创建高权限数据库账号，并使用数据库客户端和高权限数据库账号来创建普通数据库账号。高权限数据库账号可以为普通数据库账号设置表级别的读写权限。 说明：通过高权限数据库账号创建的普通数据库账号，无法通过控制台或者API进行管理。 IP白名单 虽然RDS不支持ECS的安全组功能，但是RDS提供了IP白名单来实现网络安全访问控制。 默认情况下，RDS实例被设置为不允许任何IP访问，即127.0.0.1。 用户可以通过控制台的数据安全性模块或者API来添加IP白名单规则。IP白名单的更新无需重启RDS实例，因此不会影响用户的使用。 IP白名单可以设置多个分组，每个分组可配置1000个IP或IP段。 设置白名单后，只有以下服务器才能访问RDS实例： 白名单中 IP 地址所属的服务器 白名单中 ECS 安全组内的 ECS 实例 注意事项： 系统会给每个实例创建一个默认的default白名单分组，该白名单分组只能被修改或清空，但不能被删除。 对于新建的RDS实例，系统默认会将回送地址127.0.0.1添加到default白名单分组中，IP地址127.0.0.1代表禁止所有IP地址或IP段访问该RDS实例。所以，在您设置白名单时，需要先将127.0.0.1删除，然后再添加您允许访问该RDS实例的IP地址或IP段。 若将白名单设置为%或者0.0.0.0/0，代表允许任何IP访问RDS实例。该设置将极大降低数据库的安全性，如非必要请勿使用。 安全组 目前仅杭州、青岛、香港地域支持 ECS 安全组。 目前仅支持添加一个安全组。 对白名单中的 ECS 安全组的更新将实时应用到白名单中。 系统安全 RDS 处于多层防火墙的保护之下，可以有力地抗击各种恶意攻击，保证数据的安全。 RDS 服务器不允许直接登录，只开放特定的数据库服务所需要的端口。 RDS 服务器不允许主动向外发起连接，只能接受被动访问。 数据链路服务阿里云数据库提供全数据链路服务，包括 DNS、负载均衡、Proxy 等。因为 RDS 使用原生的 DB Engine，对数据库的操作高度类似，基本没有学习成本。 DNS DNS 模块提供域名到 IP 的动态解析功能，以便规避 RDS 实例 IP 地址改变带来的影响。在连接池中设置域名后，即使对应的IP地址发生了变化，仍然可以正常访问 RDS 实例。 例如，某 RDS 实例的域名为 test.rds.aliyun.com，对应的 IP 地址为 10.10.10.1。某程序连接池中设置为 test.rds.aliyun.com 或 10.10.10.1 都可以正常访问 RDS 实例。 一旦该 RDS 实例发生了可用区迁移或者版本升级后，IP 地址可能变为 10.10.10.2。如果程序连接池中设置的是域名 test.rds.aliyun.com，则仍然可以正常访问 RDS 实例。但是如果程序连接池中设置的是IP地址 10.10.10.1，就无法访问 RDS 实例了。 负载均衡 负载均衡 模块提供实例 IP 地址（包括内网 IP 和外网 IP），以便屏蔽物理服务器变化带来的影响。 例如，某 RDS 实例的内网 IP 地址为 10.1.1.1，对应的 Proxy 或者 DB Engine 运行在 192.168.0.1 上。在正常情况下，负载均衡 模块会将访问 10.1.1.1 的流量重定向到 192.168.0.1 上。当 192.168.0.1 发生了故障，处于热备状态的 192.168.0.2 接替了 192.168.0.1 的工作。此时 负载均衡 模块会将访问 10.1.1.1 的流量重定向到 192.168.0.2 上，RDS 实例仍旧正常提供服务。 Proxy Proxy 模块提供数据路由、流量探测和会话保持等功能。 数据路由功能：支持大数据场景下的分布式复杂查询聚合和相应的容量管理。 流量探测功能：降低 SQL 注入的风险，在必要情况下支持 SQL 日志的回溯。 会话保持功能：解决故障场景下的数据库连接中断问题。 高可用服务高可用服务由 Detection、Repair、Notice 等模块组成，主要保障数据链路服务的可用性，除此之外还负责处理数据库内部的异常。 另外，RDS 还通过迁移到支持多可用区的地域和采用适当的高可用策略，提升 RDS 的高可用服务。 Detection Detection 模块负责检测 DB Engine 的主节点和备节点是否提供了正常的服务。通过间隔为 8~10 秒的心跳信息，HA 节点可以轻易获得主节点的健康情况，结合备节点的健康情况和其它 HA 节点的心跳信息，Detection 模块可以排除网络抖动等异常引入的误判风险，在 30 秒内完成异常切换操作。 Repair Repair 模块负责维护 DB Engine 的主节点和备节点之间的复制关系，还会修复主节点或者备节点在日常运行中出现的错误。 例如： 主备复制异常断开的自动修复 主备节点表级别损坏的自动修复 主备节点 Crash 的现场保存和自动修复 Notice Notice 模块负责将主备节点的状态变动通知到 负载均衡 或者 Proxy，保证用户访问正确的节点。 例如：Detection 模块发现主节点异常，并通知 Repair 模块进行修复。Repair 模块进行了尝试后无法修复主节点，通知 Notice 进行流量切换。Notice 模块将切换请求转发至 负载均衡 或者Proxy，此时用户流量全部指向备节点。与此同时，Repair 在别的物理服务器上重建了新的备节点，并将变动同步给 Detection 模块。Detection 模块开始重新检测实例的健康状态。 多可用区 RDS在特定地域提供了多可用区部署的能力，也就是将RDS的主备实例分别部署于同一地域的不同可用区。相对于单可用区 RDS 实例，多可用区 RDS 实例可以承受更高级别的灾难。 目前多可用区 RDS 不额外收取任何费用，用户可以直接在已开通多可用区的地域购买多可用区 RDS 实例，也可以通过跨可用区迁移将单可用区 RDS 实例转化成多可用区 RDS 实例。 注意： 因为多可用区之间存在一定的网络延迟，因此多可用区 RDS 实例在采用半同步数据复制方案的时候，对于单个更新的响应时间会比单可用区实例长。这种情况最好通过提高并发量的方式来实现整体吞吐量的提高。 高可用策略 高可用策略是根据用户自身业务的特点，采用服务优先级和数据复制方式之间的不同组合，以组合出适合自身业务特点的高可用策略。 服务优先级有以下两个级别： RTO（Recovery Time Objective）优先：数据库应该尽快恢复服务，即可用时间最长。对于数据库在线时间要求比较高的用户应该使用 RTO 优先策略。 RPO（Recovery Point Objective）优先：数据库应该尽可能保障数据的可靠性，即数据丢失量最少。对于数据一致性要求比较高的用户应该使用 RPO 优先策略。 数据复制方式有以下三种方式： 异步复制（Async）：应用发起更新（含增加、删除、修改操作）请求，Master 完成相应操作后立即响应应用，Master 向 Slave 异步复制数据。因此异步复制方式下，Slave 不可用不影响主库上的操作，而 Master 不可用有较小概率会引起数据不一致。 强同步复制（Sync）：应用发起更新（含增加、删除、修改操作）请求，Master 完成操作后向 Slave 复制数据，Slave 接收到数据后向 Master 返回成功信息，Master 接到 Slave 的反馈后再响应应用。Master 向 Slave 复制数据是同步进行的，因此 Slave 不可用会影响 Master 上的操作，而 Master 不可用不会引起数据不一致。 半同步复制（Semi-Sync）：正常情况下数据复制方式采用强同步复制方式，当 Master 向 Slave 复制数据出现异常的时候（Slave 不可用或者双节点间的网络异常），Master 会暂停对应用的响应，直到复制方式超时退化成异步复制。如果允许应用在此时更新数据，则 Master 不可用会引起数据不一致。当双节点间的数据复制恢复正常（Slave 恢复或者网络恢复），异步复制会恢复成强同步复制。恢复成强同步复制的时间取决于半同步复制的实现方式，阿里云数据库 MySQL 5.5 版和 MySQL 5.6 版有所不同。 实际操作MySQL数据库版本阿里云上的MySQL提供基础版、高可用版和金融版三种版本 基础版一般就是用于个人学习、或开发测试时使用。目前基础版只提供MySQL 5.7版本，并且只提供单节点部署，性价比非常高。基础版采用计算节点与存储分离的实现方式，也就是说假如计算节点宕机，MySQL就不可用啦，但数据都存在云盘里面不会丢，数据一致性还是可以得到保证，不用担心数据丢失。可用性不高这是基础版的最大问题，反正只是用于不重要的场景，生产环境大家是不会选用基础版的。 高可用版顾名思义，为应用提供了数据库的高可用保障，也就是说至少要用双节点。RDS MySQL高可用版采用一主一备的经典高可用架构，采用基于binlog的数据复制技术维护数据库的可用性和数据一致性。同时，高可用版从性能上也可以保障业务生产环境的需求，配置上采用物理服务器部署，本地SSD硬盘，提供最佳性能，各方面表现均衡。 最高级的是金融版，针对像金融、证券、保险等行业的核心数据库，他们对数据安全性、可用性要求非常高。金融版采用三节点，实现一主两备的部署架构，通过binlog日志多副本多级别复制，确保数据的强一致性，可提供金融级的数据可靠性和跨机房容灾能力。 规格阿里云上MySQL有三种规格类型：通用型、独享型和独占型。 其中通用型和独享型都是在一台物理服务器上划分多个资源隔离的区域，为不同用户提供MySQL数据库实例。他们的不同点在于，通用型对于CPU和存储空间采用了复用的技术。当部署在同一台服务器上的所有MySQL 实例都很繁忙的情况下，有可能会出现实例间的CPU争抢，或存储的争抢；而独享型虽然也是多个数据库实例共享一台物理服务器，但资源隔离策略上确保每个用户都可以独享所分配到的CPU、内存、I/O、存储，不会出现多个实例发生资源争抢的情况。 最高级别的一种是独占型，是指一个MySQL实例独占一台服务器，会获得最好的性能，当然价格也最贵。最求极致性能但对价格不敏感的客户一般会在重要业务系统采用独占型实例。 使用流程通常，从新购实例到可以开始使用实例，需要完成如下操作： 使用限制高权限账号数据库连接注意：目前只支持同一个可用区的连接，不同可用区无法连接，如果需要跨越可用区，需要进行设置 目前RDS连接可以使用DMS连接或者第三方工具连接 跨可用区访问管理工具-DMSDMS 是一款访问管理云端数据库的Web服务，支持Redis、 MySQL、SQL Server、PostgreSQL和MongoDB等数据源。DMS提供了数据管理、对象管理、数据流转和实例管理四部分功能。DMS使用也非常简单： 数据迁移-DTS相关资料： 文档中心 帮助中心 ECS自建数据库迁移到RDS DTS概述数据传输(Data Transmission)服务DTS是阿里云提供的一种支持RDBMS(关系型数据库)、NoSQL、OLAP等多种数据源之间数据交互的数据服务。它提供了数据迁移、实时数据订阅及数据实时同步等多种数据传输能力。通过数据传输可实现不停服数据迁移、数据异地灾备、跨境数据同步、缓存更新策略等多种业务应用场景，助您构建安全、可扩展、高可用的数据架构。 数据传输服务DTS的目标是帮用户将复杂的数据交互工作承担下来，让用户可以专注于上层的业务开发，数据传输服务承诺99.95%的链路稳定性。 数据传输服务DTS支持多种数据源类型，例如： 关系型数据库：Oracle、MySQL、SQLServer、PostgreSQL NoSQL: Redis OLAP: 分析型数据库AnalyticDB 迁移服务主要帮助用户把数据从本地数据库迁移到阿里云数据库，或者把阿里云数据库的一个实例迁移到另一实例中。阿里云数据库提供了数据传输服务DTS（Data Transfer Service）工具，方便用户快速的迁移数据库。 DTS是一个云上的数据传输服务，能快速的将本地数据库或者RDS中的实例迁移到另一个RDS实例中。关于DTS简介，请参见DTS产品概述。 DTS提供了三种迁移模式，分别为结构迁移、全量迁移和增量迁移： 结构迁移：DTS会将迁移对象的结构定义迁移到目标实例，目前支持结构迁移的对象有表、视图、触发器、存储过程和存储函数。 全量迁移：DTS会将源数据库迁移对象已有数据全部迁移到目标实例中。 注意：在全量迁移过程中，为了保证数据一致性，无主键的非事务表会被锁定。锁定期间这些表无法写入，锁定时长依赖于这些表的数据量大小。在这些无主键非事务表迁移完成后，锁才会释放。 增量迁移：DTS会将迁移过程中数据变更同步到目标实例。 , 注意：如果迁移期间进行了DDL操作，这些结构变更不会同步到目标实例。 源及目标数据迁移支持的源实例类型包括: (1) RDS实例 (2) 本地自建数据库 (3) ECS自建数据库 数据迁移支持的目标实例包括： (1) RDS实例 (2) ECS自建数据库 (3) Redis实例 Mysql迁移限制对于本地 MySQL-&gt;RDS for MySQL 的数据迁移，DTS 支持结构迁移、全量数据迁移及增量数据迁移，各迁移类型的功能及限制如下： 迁移过程中，不支持 DDL 操作。 结构迁移不支持 event 的迁移。 如果使用了对象名映射功能后，依赖这个对象的其他对象可能迁移失败。 当选择增量迁移时，源端的本地 MySQL 实例需要按照要求开启 binlog。 当选择增量迁移时，源库的 binlog_format 需要设置为 row。 当选择增量迁移且源 MySQL 实例如果为 5.6 或以上版本时，它的 binlog_row_image 必须为 full。 数据同步数据实时同步功能旨在帮助用户实现两个数据源之间的数据实时同步。通过数据实时同步功能可实现数据异地灾备、本地数据灾备、跨境数据同步及在线离线数据打通(OLTP-&gt;OLAP数据同步)等多种业务场景。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>公有云产品</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>RDS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP请求]]></title>
    <url>%2F2018%2F05%2F13%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E5%8F%8A%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%2FHTTP%2FHTTP%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[本文转载自：HTTP深入浅出 http请求 概述HTTP(HyperText Transfer Protocol)是一套计算机通过网络进行通信的规则。 计算机专家设计出HTTP，使HTTP客户（如Web浏览器）能够从HTTP服务器(Web服务器)请求信息和服务，HTTP目前协议的版本是1.1。 HTTP是一种无状态的协议，无状态是指Web浏览器和Web服务器之间不需要建立持久的连接，这意味着当一个客户端向服务器端发出请求，然后Web服务器返回响应(response)，连接就被关闭了，在服务器端不保留连接的有关信息.HTTP遵循请求(Request)/应答(Response)模型。Web浏览器向Web服务器发送请求，Web服务器处理请求并返回适当的应答。所有HTTP连接都被构造成一套请求和应答。 HTTP使用内容类型，是指Web服务器向Web浏览器返回的文件都有与之相关的类型。所有这些类型在MIME Internet邮件协议上模型化，即Web服务器告诉Web浏览器该文件所具有的种类，是HTML文档、GIF格式图像、声音文件还是独立的应用程序。大多数Web浏览器都拥有一系列的可配置的辅助应用程序，它们告诉浏览器应该如何处理Web服务器发送过来的各种内容类型。 通信过程在一次完整的HTTP通信过程中，Web浏览器与Web服务器之间将完成下列7个步骤： 1. 建立TCP连接 在HTTP工作开始之前，Web浏览器首先要通过网络与Web服务器建立连接，该连接是通过TCP来完成的，该协议与IP协议共同构建Internet，即著名的TCP/IP协议族，因此Internet又被称作是TCP/IP网络。HTTP是比TCP更高层次的应用层协议，根据规则，只有低层协议建立之后才能，才能进行更层协议的连接，因此，首先要建立TCP连接，一般TCP连接的端口号是80 2. Web浏览器向Web服务器发送请求命令 一旦建立了TCP连接，Web浏览器就会向Web服务器发送请求命令 例如：GET/sample/hello.jsp HTTP/1.1 3. Web浏览器发送请求头信息 浏览器发送其请求命令之后，还要以头信息的形式向Web服务器发送一些别的信息，之后浏览器发送了一空白行来通知服务器，它已经结束了该头信息的发送。 4. Web服务器应答 客户机向服务器发出请求后，服务器会客户机回送应答， HTTP/1.1 200 OK 应答的第一部分是协议的版本号和应答状态码 5. Web服务器发送应答头信息 正如客户端会随同请求发送关于自身的信息一样，服务器也会随同应答向用户发送关于它自己的数据及被请求的文档。 6. Web服务器向浏览器发送数据 Web服务器向浏览器发送头信息后，它会发送一个空白行来表示头信息的发送到此为结束，接着，它就以Content-Type应答头信息所描述的格式发送用户所请求的实际数据 7. Web服务器关闭TCP连接 一般情况下，一旦Web服务器向浏览器发送了请求数据，它就要关闭TCP连接，然后如果浏览器或者服务器在其头信息加入了这行代码Connection:keep-alive 添加之后，TCP连接在发送后将仍然保持打开状态，于是，浏览器可以继续通过相同的连接发送请求。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带宽。 HTTP请求HTTP请求格式当浏览器向Web服务器发出请求时，它向服务器传递了一个数据块，也就是请求信息，HTTP请求信息由3部分组成： 请求方法URI协议/版本 请求头(Request Header) 请求正文 下面是一个HTTP请求的例子： GET/sample.jspHTTP/1.1 Accept:image/gif.image/jpeg,*/* Accept-Language:zh-cn Connection:Keep-Alive Host:localhost User-Agent:Mozila/4.0(compatible;MSIE5.01;Window NT5.0) Accept-Encoding:gzip,deflate username=jinqiao&amp;password=1234 说明： （1）请求方法URI协议/版本 请求的第一行是“方法URL议/版本”：GET/sample.jsp HTTP/1.1 以上代码中“GET”代表请求方法，“/sample.jsp”表示URI，“HTTP/1.1代表协议和协议的版本。 根据HTTP标准，HTTP请求可以使用多种请求方法。例如：HTTP1.1支持7种请求方法：GET、POST、HEAD、OPTIONS、PUT、DELETE和TARCE。在Internet应用中，最常用的方法是GET和POST。 URL完整地指定了要访问的网络资源，通常只要给出相对于服务器的根目录的相对目录即可，因此总是以“/”开头，最后，协议版本声明了通信过程中使用HTTP的版本。 （2）请求头(Request Header) 请求头包含许多有关的客户端环境和请求正文的有用信息。例如，请求头可以声明浏览器所用的语言，请求正文的长度等。 Accept:image/gif.image/jpeg.*/* Accept-Language:zh-cn Connection:Keep-Alive Host:localhost User-Agent:Mozila/4.0(compatible:MSIE5.01:Windows NT5.0) Accept-Encoding:gzip,deflate. （3）请求正文 请求头和请求正文之间是一个空行，这个行非常重要，它表示请求头已经结束，接下来的是请求正文。 请求正文中可以包含客户提交的查询字符串信息：username=jinqiao&amp;password=1234 在以上的例子的HTTP请求中，请求的正文只有一行内容。当然，在实际应用中，HTTP请求正文可以包含更多的内容。 HTTP请求方法我这里只讨论GET方法与POST方法 GET方法 GET方法是默认的HTTP请求方法，我们日常用GET方法来提交表单数据，然而用GET方法提交的表单数据只经过了简单的编码，同时它将作为URL的一部分向Web服务器发送，因此，如果使用GET方法来提交表单数据就存在着安全隐患上。例如Http://127.0.0.1/login.jsp?Name=zhangshi&amp;Age=30&amp;Submit=%cc%E+%BD%BB从上面的URL请求中，很容易就可以辩认出表单提交的内容。（？之后的内容）另外由于GET方法提交的数据是作为URL请求的一部分所以提交的数据量不能太大 POST方法 POST方法是GET方法的一个替代方法，它主要是向Web服务器提交表单数据，尤其是大批量的数据。POST方法克服了GET方法的一些缺点。通过POST方法提交表单数据时，数据不是作为URL请求的一部分而是作为标准数据传送给Web服务器，这就克服了GET方法中的信息无法保密和数据量太小的缺点。因此，出于安全的考虑以及对用户隐私的尊重，通常表单提交时采用POST方法。 从编程的角度来讲，如果用户通过GET方法提交数据，则数据存放在QUERY＿STRING环境变量中，而POST方法提交的数据则可以从标准输入流中获取。 HTTP响应HTTP应答与HTTP请求相似，HTTP响应也由3个部分构成，分别是： 协议状态版本代码描述 响应头(Response Header) 响应正文 下面是一个HTTP响应的例子： HTTP/1.1 200 OK Server:Apache Tomcat/5.0.12 Date:Mon,6Oct2003 13:23:42 GMT Content-Length:112 &lt;html&gt; &lt;head&gt; &lt;title&gt;HTTP响应示例&lt;title&gt; &lt;/head&gt; &lt;body&gt; Hello HTTP! &lt;/body&gt; &lt;/html&gt; （1）协议状态版本代码描述 协议状态代码描述HTTP响应的第一行类似于HTTP请求的第一行，它表示通信所用的协议是HTTP1.1服务器已经成功的处理了客户端发出的请求（200表示成功）: HTTP/1.1 200 OK （2）响应头 响应头(Response Header)响应头也和请求头一样包含许多有用的信息，例如服务器类型、日期时间、内容类型和长度等： Server:Apache Tomcat/5.0.12 Date:Mon,6Oct2003 13:13:33 GMT Content-Type:text/html Last-Moified:Mon,6 Oct 2003 13:23:42 GMT Content-Length:112 （3）响应正文 响应正文响应正文就是服务器返回的HTML页面： &lt;html&gt; &lt;head&gt; &lt;title&gt;HTTP响应示例&lt;title&gt; &lt;/head&gt; &lt;body&gt; Hello HTTP! &lt;/body&gt; &lt;/html&gt; 响应头和正文之间也必须用空行分隔。 HTTP应答码 HTTP应答码也称为状态码，它反映了Web服务器处理HTTP请求状态。HTTP应答码由3位数字构成，其中首位数字定义了应答码的类型： 1XX－信息类(Information),表示收到Web浏览器请求，正在进一步的处理中 2XX－成功类（Successful）,表示用户请求被正确接收，理解和处理例如：200 OK 3XX-重定向类(Redirection),表示请求没有成功，客户必须采取进一步的动作。 4XX-客户端错误(Client Error)，表示客户端提交的请求有错误 例如：404 NOT Found，意味着请求中所引用的文档不存在。 5XX-服务器错误(Server Error)表示服务器不能完成对请求的处理：如 500 常见请求方法 GET 通过请求URI得到资源 POST用于添加新的内容 PUT用于修改某个内容 DELETE删除某个内容 CONNECT,用于代理进行传输，如使用SSL OPTIONS询问可以执行哪些方法 PATCH,部分文档更改 PROPFIND, (wedav)查看属性 PROPPATCH, (wedav)设置属性 MKCOL, (wedav)创建集合（文件夹） COPY, (wedav)拷贝 MOVE, (wedav)移动 LOCK, (wedav)加锁 UNLOCK (wedav)解锁 TRACE用于远程诊断服务器 HEAD类似于GET, 但是不返回body信息，用于检查对象是否存在，以及得到对象的元数据]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识及网络服务</category>
        <category>网络知识</category>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>HTTP请求</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL普通用户无法本地登录的解决方法及MySQL的用户认证算法]]></title>
    <url>%2F2018%2F05%2F02%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FMysql%2FMySQL%E9%97%AE%E9%A2%98%2FMySQL%E6%99%AE%E9%80%9A%E7%94%A8%E6%88%B7%E6%97%A0%E6%B3%95%E6%9C%AC%E5%9C%B0%E7%99%BB%E5%BD%95%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%E5%8F%8AMySQL%E7%9A%84%E7%94%A8%E6%88%B7%E8%AE%A4%E8%AF%81%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[参考文献： MySQL普通用户无法本地登录的解决方法及MySQL的用户认证算法 问题在启动cachecloud项目的时候，发现日志中出现大量的连接数据库报错 我的授权命令为： mysql&gt; grant all privileges on cache_cloud.* to &apos;cachecloud&apos;@&apos;%&apos; identified by &apos;Cache_cloud123&apos;; Query OK, 0 rows affected (0.00 sec) 这样的配置，按道理来说，是不应该出现连不上的（%代表任意的主机来源，并且已经排查了防火墙等因素） 在本地登录发现发现存在如下问题： 当输入之前设置的密码时，将会一直提示：“ERROR 1045 (28000): Access denied”，而当我们不输入密码，也就是说输入密码为空时则能正常进入数据库。 我们使用USER()和CURRENT_USER()两个函数查看所使用的用户。 mysql&gt; SELECT USER(), CURRENT_USER(); +----------------------+----------------+ | USER() | CURRENT_USER() | +----------------------+----------------+ | cachecloud@localhost | @localhost | +----------------------+----------------+ 1 row in set (0.00 sec) mysql&gt; USER()函数返回你在客户端登陆时指定的用户名和主机名。 CURRENT_USER()函数返回的是MySQL使用授权表中的哪个用户来认证你的登录请求。 这里发现，之前设置的授权规则并没有生效，是数据库使用的是’’@’localhost’这个来源信息来进行登录认证，而’’@’localhost’这个匿名用户是没有密码的，因此我输入空密码登录成功了。但是登录后，所对应的用户的匿名用户。 一般在MySQL在安装完毕后，我们使用mysql_install_db这个脚本生成授权表，会默认创建’’@’localhost’这个匿名用户。正是因为这个匿名用户，影响了其他用户从本地登录的认证。 原因那么MySQL是如何进行用户身份认证呢？ MySQL的简要认证算法如下： 当用户从客户端请求登陆时，MySQL将授权表中的条目与客户端所提供的条目进行比较，包括用户的用户名，密码和主机。 授权表中的Host字段是可以使用通配符作为模式进行匹配的，如test.example.com, %.example.com, %.com和%都可以匹配test.example.com这个主机。 授权表中的User字段不允许使用模式匹配，但是可以有一个空字符的用户名代表匿名用户，并且空字符串可以匹配所有的用户名，就像通配符一样。 当user表中的Host和User有多个值可以匹配客户端提供的主机和用户名时，MySQL将user表读入内存，并且按照一定规则排序，按照排序规则读取到的第一个匹配客户端用户名和主机名的条目对客户端进行身份验证。 排序规则： 对于Host字段，按照匹配的精确程度进行排序，越精确的排序越前，例如当匹配test.example.com这个主机时, %.example.com比%.com更精确，而test.example.com比%.example.com更精确。 对于User字段，非空的字符串用户名比空字符串匹配的用户名排序更靠前。 User和Host字段都有多个匹配值，MySQL使用主机名排序最前的条目，在主机名字段相同时再选取用户名排序更前的条目。 因此，如果User和Host字段都有多个匹配值，主机名最精确匹配的条目被用户对用户进行认证。 了解了这个规则之后，我们就知道为什么cachecloud登录失败了。 在使用该用户进行本机登录的时候，mysql中有2个匹配条目 ‘cachecloud’@’%’ ‘’@’localhost’ 匿名用户能够匹配的原因上面说过，空字符串可以匹配所有的用户名，就像通配符一样。 根据MySQL认证时的排序规则，第一个条目的用户名排序更前，第二个条目的主机名更精确，排序更前。 而MySQL会优先使用主机名排序第一的条目进行身份认证，因此’’@’localhost’被用户对客户端进行认证。因此，只有使用匿名用户的空密码才能登录进数据库。就会出现刚才上面的情况了。 解决删除匿名用户【仅仅是为了安全也有这个必要】 为什么root用户不会受影响，而只有普通用户不能从本地登录？ 因为mysql_install_db脚本会在授权表中生成’root’@’localhost’这个账户。同样的，使用root登录MySQL时，’root’@’localhost’和’’@’localhost’都能匹配登录的账户，但是根据排序规则，主机名相同，而用户名非空字符串优先，因此’roo’@’localhost’这个条目的排序更靠前。使用root本地登录是不会被匿名用户遮盖。 [root@qa1-common004 ~]# mysql -u root -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 942 Server version: 5.6.40 MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. mysql&gt; mysql&gt; select user,host from mysql.user; +------------+---------------------------+ | user | host | +------------+---------------------------+ | cachecloud | % | | root | 127.0.0.1 | | root | ::1 | | | localhost | | root | localhost | | | qa1-common004.ecs.east1-b | | root | qa1-common004.ecs.east1-b | +------------+---------------------------+ 7 rows in set (0.00 sec) mysql&gt; delete from mysql.user where user=&apos;&apos;; Query OK, 2 rows affected (0.00 sec) mysql&gt; select user,host from mysql.user; +------------+---------------------------+ | user | host | +------------+---------------------------+ | cachecloud | % | | root | 127.0.0.1 | | root | ::1 | | root | localhost | | root | qa1-common004.ecs.east1-b | +------------+---------------------------+ 5 rows in set (0.00 sec) mysql&gt; flush privileges; Query OK, 0 rows affected (0.00 sec) mysql&gt; exit 退出之后再次登录，问题得到解决。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>数据库</category>
        <category>MySQL</category>
        <category>MySQL问题</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下JDK的安装配置]]></title>
    <url>%2F2018%2F04%2F28%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2FJDK%2FLinux%E4%B8%8BJDK%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在Linux中JDK的配置主要分为以下几个步骤： 下载 解压 软链接 配置系统/用户环境变量 下载：官方下载链接：下载 JAVA环境的配置主要分为两种，一种是由root用户操作，针对所有用户全局生效的配置，一种是由具体普通用户操作，仅针对该用户生效的配置 因此，以下的配置根据实际需求。 全局生效-管理员权限操作解压+软链接 # tar -zxvf jdk-7u75-linux-x64.tar.gz -C /usr/local/ # cd /usr/local/ # ln -s /usr/local/jdk1.7.0_75/ /usr/local/JDK 创建软链接目的：灵活的版本升级切换 配置系统环境变量： # vim /etc/profile 在文件末尾添加以下内容 export JAVA_HOME=/usr/local/JDK export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib export PATH=$JAVA_HOME/bin:$JRE_HOME/bin::$CLASSPATH:$PATH # source /etc/profile 用户局部生效-用户环境变量**注意提示符的变化，这里以appdev用户为例 解压+软链接 $ tar -zxvf jdk-7u75-linux-x64.tar.gz $ ln -s /home/appdev/jdk1.7.0_75/ /home/appdev/JDK 创建软链接目的：灵活的版本升级切换 配置系统环境变量： $ vim .bash_profile 在文件末尾添加以下内容 export JAVA_HOME=/home/appdev/JDK export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib export PATH=$JAVA_HOME/bin:$JRE_HOME/bin::$CLASSPATH:$PATH $ source .bash_profile]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>基础环境配置</category>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>JDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cachecloud-Redis云平台]]></title>
    <url>%2F2018%2F04%2F28%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FRedis%2FCacheCloud-Redis%E4%BA%91%E5%B9%B3%E5%8F%B0%2FCachecloud%2F</url>
    <content type="text"><![CDATA[Cachecloud介绍有关cachecloud的一些基础知识，官方都有非常详细的文档，这里不再花费篇幅进行复述，下面是相关的资料链接，请自行查看。 github官网： https://github.com/sohutv/cachecloud Wiki: https://github.com/sohutv/cachecloud/wiki 博客： https://cachecloud.github.io/ 官方视频： http://my.tv.sohu.com/pl/9100280/index.shtml 简介： CacheCloud提供一个Redis云管理平台：实现多种类型(Redis Standalone、Redis Sentinel、Redis Cluster)自动部署、解决Redis实例碎片化现象、提供完善统计、监控、运维功能、减少开发人员的运维成本和误操作，提高机器的利用率，提供灵活的伸缩性，提供方便的接入客户端。 提供的功能： 监控统计： 提供了机器、应用、实例下各个维度数据的监控和统计界面。 一键开启： Redis Standalone、Redis Sentinel、Redis Cluster三种类型的应用，无需手动配置初始化。 Failover： 支持哨兵,集群的高可用模式。 伸缩： 提供完善的垂直和水平在线伸缩功能。 完善运维： 提供自动运维和简化运维操作功能，避免纯手工运维出错。 方便的客户端 方便快捷的客户端接入。 元数据管理： 提供机器、应用、实例、用户信息管理。 流程化： 提供申请，运维，伸缩，修改等完善的处理流程 一键导入： 一键导入已经存在Redis 须知： Redis集群、redis哨兵集群、Redis单实例等在CacheCloud中都是以应用的形式存在，一个应用对应一个appid 一个redis集群是一个应用，分配一个appid（不管其中有几个节点） 一个哨兵集群是一个应用，分配一个appid（不管其中有几个主从节点和哨兵节点） 一个单实例是一个应用，分配一个appid 如何使用： 我们在平台上的执行任何操作都需要**账号**，创建的单节点、哨兵、集群等都是以用户申请的应用形式存在的。普通用户的主要工单有 注册用户申请 应用申请 应用扩容 应用配置修改 管理员的界面可操作的选项较多，此处不做详细说明。 客户端如何连接： 客户端在第一次启动的时候去CacheCloud通过appId拿到Redis的节点信息，之后不会与CacheCloud打交道了。 流程图如下所示： 安装部署这里只说单机环境，高可用环境将在下面章节说明：CacheCloud高可用架构 环境要求： JDK 7+ Maven 3+ MySQL 5.5+ Redis 3+ 基础环境JDK+MavenJDK： 步骤： 下载 解压 软链接 配置系统环境变量 操作如下： [root@qa1-common004 local]# java -version java version &quot;1.8.0_77&quot; Java(TM) SE Runtime Environment (build 1.8.0_77-b03) Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode) [root@qa1-common004 local]# which java /usr/local/jdk/bin/java [root@qa1-common004 local]# ll /usr/local/jdk lrwxrwxrwx 1 root root 11 Apr 17 17:01 /usr/local/jdk -&gt; jdk1.8.0_77 这里我使用的是1.8版本。 详细操作请看文章：Linux下JDK的安装配置 Maven 步骤： 下载 下载链接 解压 软链接 配置系统环境变量 操作如下： # wget http://www-eu.apache.org/dist/maven/maven-3/3.5.3/binaries/apache-maven-3.5.3-bin.tar.gz # tar -zxvf apache-maven-3.5.3-bin.tar.gz -C /usr/local/ # cd /usr/local/ # ln -s /usr/local/apache-maven-3.5.3/ /usr/local/maven # vim /etc/profile 在文件末尾添加以下内容，保存退出 M3_HOME=/usr/local/maven export PATH=$M3_HOME/bin:$PATH [root@host-192-168-8-37 ~]# source /etc/profile 下载CacheCloud项目# yum -y install git # git clone https://github.com/sohutv/cachecloud.git # ls cachecloud/ cachecloud-open-client cachecloud-open-common cachecloud-open-web LICENSE pom.xml README.md script MySQL这里安装mysql5.7版本 配置yum源并安装 centos6.8 【6.8安装5.6版本，安装5.7时涉及依赖关系过多】 # wget http://repo.mysql.com/mysql-community-release-el6-5.noarch.rpm # rpm -ivh mysql-community-release-el6-5.noarch.rpm # yum -y install mysql-community-server centos 7.x 【5.7版本】 # wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm # rpm -ivh mysql57-community-release-el7-11.noarch.rpm # yum -y install mysql-server 修改mysql配置文件 # vim /etc/my.cnf [mysqld] character-set-server=utf8 启动 # /etc/init.d/mysqld start 数据库配置创建数据库 mysql&gt; create database cache_cloud default charset utf8; Query OK, 1 row affected (0.00 sec) 创建cachecloud用户 mysql&gt; grant all privileges on cache_cloud.* to &apos;cachecloud&apos;@&apos;%&apos; identified by &apos;Cache_cloud123&apos;; Query OK, 0 rows affected (0.00 sec) 导入初始化数据注意，这里已经不是在数据库中了 [root@qa1-common004 script]# pwd /root/software/cachecloud/script [root@qa1-common004 script]# mysql -u root -p cache_cloud &lt; cachecloud.sql Enter password: 修改cachecloud配置数据库设置 [root@qa1-common004 swap]# pwd /root/software/cachecloud/cachecloud-open-web/src/main/swap ​​ [root@qa1-common004 swap]# cat online.properties​ cachecloud.db.url = jdbc:mysql://172.24.64.132:3306/cache_cloud?useUnicode=true&amp;characterEncoding=UTF-8​ cachecloud.db.user = cachecloud​ cachecloud.db.password = Cache_cloud123​ cachecloud.maxPoolSize = 20 isClustered = true isDebug = false spring-file=classpath:spring/spring-online.xml log_base=/opt/cachecloud-web/logs web.port=8585 log.level=WARN 注意这里需要提前在数据库中删除匿名用户 开启机器监控功能 # pwd /root/software/cachecloud/cachecloud-open-web/src/main/java/com/sohu/cache/schedule/jobs # vim ServerJob.java 将稳中的注释去掉，修改之后的文件如下所示： 如果公司已经有完善的监控，那么不建议开启机器监控，能够一定程度上减小数据库的压力。 cachecloud构建及启动项目构建 在cachecloud的根目录下执行以下maven命令，该命令会进行项目的构建 [root@qa1-common004 cachecloud]# pwd /root/software/cachecloud [root@qa1-common004 cachecloud]# [root@host-192-168-8-37 cachecloud]# mvn clean compile install -Ponline [root@host-192-168-8-37 cachecloud]# cd script/ [root@host-192-168-8-37 script]# sh deploy.sh /root/software/ 启动 # sh /opt/cachecloud-web/start.sh 启动成功之后的web页面如下图所示： 实际使用redis数据节点初始化执行初始化脚本 sh cachecloud-init.sh cachecloud 添加主机redis应用模板配置注意：在部署redis相关应用之前，一定要先进行模板的配置，因为默认配置下，redis的守护进程模式为关系，保护模式也是开启的 修改配置： 配置名称：daemonize；配置值：yes;配置说明：是否守护进程 新增配置： 配置名称：protected-mode；配置值：no;配置说明：保护模式 配置名称：bind；配置值：0.0.0.0;配置说明：绑定ip 注意：哨兵的配置模板中只需要新增protected-mode参数即可。 部署哨兵应用导入已经存在的redis实例redis哨兵cachecloud使用优化哨兵复用问题： 使用cachecloud部署哨兵集群时，每次生成的哨兵节点都是不一样的，这种情况，会造成一定的资源浪费（每一对主从都需要至少3个哨兵节点，对服务器的端口资源、内存资源等都会造成一定的浪费） 因此，我们采取复用哨兵节点的方式来实现redis的主从高可用 实现步骤： 哨兵模板中设置端口，将端口固定，为了后续的配置方便 手动创建主从节点 哨兵中添加新建的主从节点 在cachecloud平台上导入这个应用 相当于其实是导入redis哨兵的方式 哨兵配置： redis-cli -p 6388 sentinel monitor master-test-qa1 172.24.64.134 6385 2&amp;&amp;redis-cli -p 6388 SENTINEL set master-test-qa1 auth-pass redis123&amp;&amp;redis-cli -p 6388 sentinel set master-test-qa1 down-after-milliseconds 20000&amp;&amp;redis-cli -p 6388 sentinel set master-test-qa1 failover-timeout 60000 关闭master节点的持久化 AOF持久化：appendonly 配置为no cachecloud坑- CacheCloud安装部署及使用常见问题及注意事项 数据库版本问题： 如果使用mysql5.7，则需要进行针对sql文件做一些设置（only_full_group_by模式设置等） cachecloud平台乱码问题： 需要修改online.properties配置文件中的连接串（使用这种方式：jdbc:mysql://127.0.0.1:3306/cache_cloud?useUnicode=true&amp;characterEncoding=UTF-8） cachecloud后台配置模板：默认配置下，redis没有开启守护进程运行方式、开启了保护模式等，需要做一些配置修改之后才可以正常启动 机器监控数据无法展示问题：除了在程序文件中去掉相应的代码注释，还需要将cachecloud-open-web/nmon下指定系统版本的nmon文件放到/opt/cachecloud/soft/目录下 密码配置问题：密码配置栏中，输入密码之后，还需要点击更新才可以生效 Jedis支持redis版本问题：Jedis暂时无法稳定支持redis4.x版本，因此涉及到的集群水平扩容等功能是无法实现的（集群创建等还是可以支持的），因此我们建议使用3版本，后续关注Jedis的版本发布情况。 应用导入时提示：节点不是存活的 cachecloud节点上需要安装redis，因为他会使用redis-cli 去ping指定的节点，没有返回pong时，则会报错 哨兵导入问题： 如果哨兵是复用的，也就是说一组哨兵节点监听了多对主从节点，那么在导入的时候回出现问题，目前导入功能只支持一个mastername 主机名设置问题： 注意hosts文件需要进行配置]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>数据库</category>
        <category>Redis</category>
        <category>CacheCloud-Redis云平台</category>
      </categories>
      <tags>
        <tag>cachecloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[湿气的产生及预防治疗]]></title>
    <url>%2F2018%2F04%2F22%2F%E4%B8%AA%E4%BA%BA%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F%E8%BF%90%E5%8A%A8%E8%90%A5%E5%85%BB%E4%B8%8E%E5%81%A5%E5%BA%B7%2F%E8%BA%AB%E4%BD%93%E4%B9%8B%E9%81%93%2F%E6%B9%BF%E6%B0%94%E7%9A%84%E4%BA%A7%E7%94%9F%E5%8F%8A%E9%A2%84%E9%98%B2%E6%B2%BB%E7%96%97%2F</url>
    <content type="text"><![CDATA[文章结构： 第一部分：是什么-湿气的概念 &amp;&amp; 为什么-湿气的产生原因 第二部分：怎么做-如何预防及治疗 参考文献： 知乎-湿气是怎么回事，人为什么会有湿气？ 湿气（中医理论概念） 1. 湿气概念及产生原因 1.1 概念1.1.1 湿 我们在日常生活中，感受到湿的时候一般是物体含水量超出一定范围，这个水分可以依附到很多物体上，比如湿巾、湿木头、湿衣服等等（无法正常排出的水）。 但含水量也不能超过一定限度，依附不住的水就不叫做湿，而是自由的水，比如湿衣服滴下来的水，这就不称为湿。 1.1.2 湿气 湿气是一种中医理论中的概念。通俗的来说，就是人体内有多余的水份无法正常代谢排出，堆积在身体之内，从而影响身体健康。（具体原因可能是个人体质、疾病或生活习惯不良，造成体内水分调控系统失衡） 就像我们日常生活中所看见的：食物放在潮湿的地方相比干燥的地方，很快就会发霉，类比到人上，当人的湿气较重之后，会产生一系列疾病。 一般也把湿气称之为：湿邪 在致病的风、寒、暑、湿、燥、火这“六淫邪气” 中，中医最怕湿邪。 湿是最容易渗透的。湿邪从来不孤军奋战，总是要与别的邪气狼狈为奸。 湿气遇寒则成为寒湿，这就好比冬天的时候，如果气候干燥，不管怎么冷，人都还是能接受的，但如果湿气重，人就很难受了。南方的冬天比北方的冬天更令人难受，就是因为南方湿气比较重，寒湿袭人。 湿气遇热则成为湿热，这就好比夏天的桑拿天，又热又湿，让人喘不过气来，明显不如烈日当空、气候干燥的时候来得痛快。 湿气遇风则成为风湿，驱风很容易，但一旦成了风湿，就往往是慢性疾病，一时半会儿治不好了。 湿气在皮下，就形成肥胖，也是不好处理的健康问题…… 为什么现代人的病那么复杂，那么难治？因为他们体内有湿，体外的邪气总是和体内的湿气里应外合，纠缠不清！以前仅仅盛行于我国西南的川菜，风行全国，就是因为川味是辛辣的，以前只有生活在湿邪比较重的西南一带人需要用它来化解体内的湿气；全国人体内都有湿气了，这就需要辛辣来化解。 主导湿气的人体器官是：脾 因此，湿气问题的根本原因是各种原因导致的脾功能下降（也有可能是其他器官导致，因为脾在工作时要需要借助胃肝肾等器官），具体见下文。 1.2 产生原因-湿气是怎么来的？1.2.1 外在原因 一个是因为外在的环境，也就是湿邪进入到了身体。 比如长期居住在湿气重的地方，比如淋了雨还不及时擦干，比如晚上洗头没吹干就睡觉，让外界的湿气进入到体内。 湿气进入身体后常常奔着脾胃去，导致脾的运化能力下降，而这又会容易导致体内生湿。 1.2.2 内因 另外一个就是饮食习惯差，导致脾运化能力下降而生湿。【饮食不当，伤害脾胃，这是产生湿气的罪归祸首】 此外夏天的时候狂开空调，狂吃冷饮，硬生生的把要出来的水份给逼回去了。还有缺乏运动，没有及时的增强脾的工作能力。 脾主运化，吃进来的食物通过它来运化出精微物质，剩下的糟粕排出体外。当因为各种原因导致脾虚、运化能力下降的时候，精微物质就没法完全提炼出来。 1.2.3 原因解析 从微观的角度讲，物质没有完全被消化时，就成了携带营养物质的“垃圾”，成分复杂且分子比较大，没法被人体吸收，但又不像糟粕那么大块头好分辨，那么容易把它们驱逐。 它们的分子量和体积远大于水分子，潜伏着，聚集起来，极其容易把周围的水分子吸附住、束缚住，使含水量超出正常的生理水平，于是形成了湿。 脾被湿气困住，更加影响它的运化工作，导致湿气加重。湿一直凝聚不化，时间长了就成为痰，身体出于自保自救，把其中一部分水、二氧化碳和营养垃圾打包成了脂肪。所以中医常说胖人多痰湿，就是这个道理。【So，减肥先去湿气】 1.3 湿气的特点1.3.1 笨重并且混浊 湿气依附在身体某些地方，和身边的物体紧紧结合，难舍难离。物体湿的状态时会比干燥的时候重很多,所以体内有湿气的时候，我们往往觉得身体或头部沉重；湿气浊会导致身体气血流通不畅，长期聚集身体又没法整治它,导致有湿气的地方脏乱差，滋生各种毒害。 1.3.2 难缠粘人 什么东西被湿邪盯上，就好像被缠上了粘液，各种不爽，比如小便不畅，大便黏腻不爽等。此外它还很难去除，经常和你缠缠绵绵，病程较长，比如风湿病、温湿病。 1.3.3 阻遏气机、损伤阳气 湿气本质上属于阴邪，靠着它黏腻难缠的劲头，赖在脏腑经络上不走，导致气机升降无能，于是阳气就没法正常生发了。所以一般被湿邪困住的人，阳气都不太旺，会有脸色淡白，精力不济的现象。 1.4 湿气重的表现 头发爱出油、面部油亮, 小肚子大(常有胀气)，身体浮肿。 身体发沉、发重，浑身无力。 皮肤上会有湿疹，胃口不好，嘴里发黏。 常感到疲倦，精力不集中睡觉打呼噜，痰多，咳嗽,睡觉留口水、口臭、身体有异味，耳内湿（耳禅湿）毛发粗糙，易脱落。 舌质很胖，颜色偏淡。症状严重的，舌头边上会有齿痕，这叫“裙边舌”。 眼袋下垂，黑圆圈严重，肥胖，减肥后反弹，机能衰退，对房事不感兴趣质量不高等。 大便溏稀不成型，正常的大便是光滑的呈圆柱体，每次大便之后，不会粘在光滑的马桶壁上，如果你每次上完厕所，大便冲不干净，那么一定是体内湿气在作怪。而且，湿气会让便秘如影随形。下一次，当你大便的时候，很可能就会出现便秘。 等等等等 当湿气演变成为顽固性湿气的时候，身体会出现数十种不适： 以上症状，如果你占了2种以上，要引起注意了，这说明体内有湿气。湿气不除，是引发及恶化疾病的关键。 并且现代人由于工作强度、压力等都更大，因此运动量也原来越小，体内阴盛阳虚从而湿邪内郁。这也是当前越来越多的年轻人有湿气相关疾病的原因。 2. 如何预防及治疗 在这里，我们将预防和治疗两者结合在一起说明，因为光靠预防不能完全杜绝，或多或少肯定都还是会产生湿气。 湿气很重，不要只会傻傻拔罐。 2.1 药物目前没有什么比较好的药物，一般采用饮食结合运动的方式来预防和治疗湿气。 2.2 饮食这里只说该吃什么，至于不该吃什么，请看日常生活章节 薏米赤小豆桂圆粥 薏米：性寒。因此要用赤小豆来中和，并且每次的量不宜太多 赤小豆：性，。注意赤小豆是扁的，红豆是圆的 桂圆/枣：桂圆甘温。有的人体质偏寒，里面可以加一点温补的食物，像桂圆、大枣都可以 如果着凉感冒了，或是体内有寒，胃中寒痛，食欲不佳，可在薏米赤小豆汤中加几片生姜。生姜性温，能温中祛寒，健脾和胃。 肾虚的人，可在薏米赤小豆汤中加一些黑豆。因为黑色入肾，豆的形状也跟肾十分相似，以形补形，是补肾的佳品。 人们常说的脚气病，是典型的湿热下注。可在薏米赤小豆汤中加点碎黄豆，用熬出来的汤泡脚，这是治脚气的一个小秘方。 学会薏米赤小豆汤的加减变化，使用得当可以对生活中大部分常见病起到很好的治疗效果。 如下图所示： 2.3 运动现代人动脑多、体力消耗少，加上长期待在密闭空调内，很少流汗，身体调控湿度的能力变差。因此这也为产生湿气创造了条件。 运动出汗是很好的去湿气方式 2.4 日常生活2.4.1 不宜 不过食生冷肥甘厚腻甜辛辣， 避开生冷食物。这里说的生冷食物指的是冷饮、凉拌菜等，而不是水果。【这一点在夏天的时候最为明显，一些人在夏天时喝冷饮、和冰镇啤酒、吃冰镇西瓜、吃凉菜等毫无节制】 夏天尽量不吹空调 睡前务必吹干头发 饮食口味重，日常饮食口味经常过重的话，由于细胞渗透压的作用，浓度低的会向浓度高的一方渗透，力求平衡，从而会使身体处于不正常状态 不宜久坐，一小时不动两小时不动三小时不动，身体以为你不会动了，它的运行也会慢下来慢下来 不宜大量吃水果。 2.4.2 宜 晚上用热水泡脚。 每天晚上坚持用热水泡脚半小时（注意：时间是半小时），泡到微微出汗。 泡脚的同时敲打肘窝、腘和腋窝各5分钟。这三个地方是排湿气的重要部位。腋窝都知道，肘窝就是手肘后面弯曲部位，腘就是膝盖后面弯曲部位。 天气好的日子，勤晒衣物和被子，减少病菌，降低生病的可能。 夏天时家中易闷热潮湿，每天要适度开窗换气，新鲜的空气可以减少细菌病毒的滋生，以傍晚最适宜。 清淡饮食 保持衣物干爽,不要穿潮湿未干的衣服、盖潮湿的被子，被子(垫絮)要经常晒。 夏天不要贪凉睡地板 2.5 总结食疗、运动最多只能暂时缓解症状，找到自身湿气产生的原因，才能从根上断绝它。 我们不能怪罪脾胃太虚弱，吃那么多它累死也消化不完啊；不要怪它懒罢工不干活，湿气困着它，它也很无奈；别说它工作不到位，身体消耗少，营养物质只能不断堆积。 不形成良好的生活习惯，喝再多薏米粥、吃再多健脾祛湿的方药都是白搭！所以与其总是寻医问药寻找除湿气的方法，不如老老实实先好好吃饭、合理饮食、不贪凉不贪酒、加强体育锻炼.多动少吃清淡平衡饮食,这才是正确的姿势。]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>运动营养与健康</category>
        <category>身体之道</category>
      </categories>
      <tags>
        <tag>湿气</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django项目实践]]></title>
    <url>%2F2018%2F04%2F17%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%2FPython%2Fdjango%2FDjango%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[基础环境： Python3系列（针对环境变量做好软链接，将python3链接为python） sudo apt-get -y install python3 ln -s /usr/bin/python3 /usr/bin/python pip3(pip2不支持Python3.x，因此我们要安装pip来支持python3) sudo apt-get install python3-pip ln -s /usr/bin/pip3 /usr/bin/pip pip的升级：pip install –upgrade pip 依赖关系（python3-venv） sudo apt-get -y install python3-venv 环境准备创建激活虚拟环境要使用django，首先需要建立一个虚拟工作环境。虚拟环境是系统的一个位置，你可以在其中安装包，并将其与其他python包隔离。将项目的库与其他项目分离是有益的。 创建虚拟环境： wxh@wxh-virtual-machine:/opt$ python -m venv ll_env 激活虚拟环境 (ll_env) wxh@wxh-virtual-machine:/opt$ source ll_env/bin/activate 停止虚拟环境 (ll_env) wxh@wxh-virtual-machine:/opt$ deactivate ​ 安装django安装django(python3之后对应的django版本为1.8.1，因此在安装的时候需要额外注意)： (ll_env) wxh@wxh-virtual-machine:/opt$ pip install Django django项目创建项目：(ll_env) wxh@wxh-virtual-machine:/opt$ sudo django-admin.py startproject learning_log . 创建完毕之后的目录结构如下所示： 注意事项： 命令末尾有有一个句点的存在，如果遗忘，可能出现一些问题。 manage.py文件是一个简单的程序，它接受命令并将其交给django的相关部分去运行，我们将会使用这些命令来管理诸如使用数据库和运行服务器等任务 目录learning_log有4个文件，其中最重要的的是setting.py、urls.py、wsgi.py。 setting.py指定django如何与系统交互以及如何管理项目。在开发项目的过程中，我们将修改其中的一些设置，并添加一些设置。 urls.py告诉django应该创建哪些网页来响应浏览器请求。 wsgi.py帮助django提供它创建的文件。（web server gateway interface）web服务器网关接口的缩写 在一个目录下，只能创建一个django项目，因为一个目录下不允许存在2个manage.py 创建数据库django将大部分与项目有关的信息都存储在数据库中，因此我们需要创建一个供django使用的数据库。为给项目“学习笔记”创建数据库，在处于活动虚拟环境中的情况下执行下面的命令： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py migrate 启动/停止项目核实django项目是否正确的创建，执行下面的命令启动： python manage.py runserver [port] 创建应用程序django由一系列应用程序组成（也可以看成是一系列的功能组件），他们协同工作，让项目成为一个整体。现在我们暂时只创建一个应用程序，它将完成项目的大部分工作。在后面，我们还将再添加一个管理用户账户的应用程序 (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py startapp learning_logs 命令startapp让django创建应用程序所需的基础设施。如果现在查看项目目录，将看到其中新增了一个文件夹learning_logs 其中最重要的文件是models、admin和views，我们将使用models来定义我们要在应用程序中管理的数据 注意：learning_logs是learning_log项目中的一个应用程序，这个概念要分清。 定义模型每位用户都需要在学习笔记中创建很多的主题（例如：数学、英语、语文等等）。用户输入的每个条目（文章，每篇笔记）都要与特定的主题相关联。这些条目将以文本的形式显示。我们还需要存储每个目录的时间戳，以便能够告诉用户每个条目都是什么时候创建的。​​ (ll_env) wxh@wxh-virtual-machine:/opt$ cat learning_logs/models.py​ from django.db import models​ # Create your models here. 我们可以看到这个文件的内容。django事先导入了模块models，让我们自己创建模型。 模型告诉django如何处理应用程序中存储的数据。在代码层面，模型就是一个类，就像前面讨论的每个类一样，包含属性和方法。 我们编写完毕之后的代码如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim models.py from django.db import models class Topic(models.Model): &quot;&quot;&quot;用户学习的主题&quot;&quot;&quot; text = models.CharField(max_length=200) date_added = models.DateTimeField(auto_now_add=True) def __str__(self): &quot;&quot;&quot;返回模型的字符串表示&quot;&quot;&quot; return self.text 我们创建了一个名为Topic的类，它继承了models模块中的类Model-django中一个定义了模型基本功能的类。新建的这个Topic类中，我们只定义了两个属性：text和data_added（也就是拿来保存主题和时间戳） 属性text是一个CharField——由字符或文本组成的数据（见）。需要存储少量的文本，如名称、标题或城市时，可使用CharField。定义CharField属性时，必须告诉Django该在数据库中预留多少空间。在这里，我们将max_length设置成了200（即200个字符），这对存储大多数主题名来说足够了。 属性date_added是一个DateTimeField——记录日期和时间的数据（见）。我们传递了实参auto_add_now=True，每当用户创建新主题时，这都让Django将这个属性自动设置成当前日期和时间 我们需要告诉django，默认应该使用哪个属性来显示有关主题的信息。django调用方法str()来显示模型的简单表示，在这里，我们编写了方法str(),它返回存储在属性text中的字符串。 激活模型要使用模型，必须让Django将应用程序包含到项目中。为此，打开settings.py（它位于项目learning_log目录下），你将看到一个这样的片段，这一段的配置告诉Django使用哪些应用程序安装在项目中： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo vim learning_log/settings.py 33 INSTALLED_APPS = [ 34 &apos;django.contrib.admin&apos;, 35 &apos;django.contrib.auth&apos;, 36 &apos;django.contrib.contenttypes&apos;, 37 &apos;django.contrib.sessions&apos;, 38 &apos;django.contrib.messages&apos;, 39 &apos;django.contrib.staticfiles&apos;, 40 ] 我们将新建的应用程序加载进项目之中，修改之后的配置如下所示： 33 INSTALLED_APPS = [ 34 &apos;django.contrib.admin&apos;, 35 &apos;django.contrib.auth&apos;, 36 &apos;django.contrib.contenttypes&apos;, 37 &apos;django.contrib.sessions&apos;, 38 &apos;django.contrib.messages&apos;, 39 &apos;django.contrib.staticfiles&apos;, 40 # my appalication 41 &apos;learning_logs&apos; 42 ] 通过这种将应用程序编组的方式，在项目不断增大，包含更多的应用程序时，可以有效的对应用程序进行跟踪。 数据库配置： 接下来，需要让django修改数据库，使其能够存储与模型Topic相关的信息，执行以下命令：​​ (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py makemigrations learning_logs​ Migrations for ‘learning_logs’:​ learning_logs/migrations/0001_initial.py​ - Create model Topic​ (ll_env) wxh@wxh-virtual-machine:/opt$ 命令makemigrations让Django确定该如何修改数据库，使其能够存储与我们定义的新模型相关联的数据。 输出表明Django创建了一个名为0001_initial.py的迁移文件，这个文件将在数据库中为模型Topic创建一个表。 下面来应用这种迁移，让Django替我们修改数据库：​​ (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py migrate​ Operations to perform:​ Apply all migrations: admin, auth, contenttypes, learning_logs, sessions​ Running migrations:​ Applying learning_logs.0001_initial… OK 注意：每当需要修改该应用程序管理的数据时（在这里是learning_logs应用程序），都采取如下三个步骤：​ 修改models.py； 对learning_logs调用makemigrations; 让django迁移项目** django管理网站一个网站，需要有管理员来管理网站，django提供的管理网站（admin site）能够轻松的实现。 接下来，我们将建立管理网站，并通过它使用模型Topic来添加一些主题 创建超级用户创建具备所有权限的用户—超级用户，执行以下命令： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py createsuperuser Username (leave blank to use &apos;root&apos;): ll_admin Email address: Password: Password (again): Superuser created successfully. 注意：django并不存储实际输入的明文密码，而是存储该密码的散列值，每当你输入密码的时候，django都将计算其散列值，并将结果与存储的散列值进行比较。 向管理网站注册模型Django自动在管理网站中添加了一些模型，如User和Group，但对于我们创建的模型，必须手工进行注册。 我们创建应用程序learning_logs时， Django在models.py所在的目录中创建了一个名为admin.py的文件：​​ (ll_env) wxh@wxh-virtual-machine:/opt$ cat learning_logs/admin.py​ from django.contrib import admin​ # Register your models here. 修改之后的文件为：​​ (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim admin.py​ from django.contrib import admin from learning_logs.models import Topic admin.site.register(Topic) 导入我们要注册的模型Topic，再使用admin.site.register方法让django通过管理网站管理我们的模型 接下来访问：http://127.0.0.1:8000/admin可以直接使用我们刚才创建的超级管理员用户的用户名和密码进行登录。 登录之后的页面如下所示： 这个网页能够让你添加和修改用户和用户组，还可以管理刚才定义的模型Topic相关的数据。 我们能够看到刚才定义的模型Topic及其相关的数据（当前没有数据） 添加主题向管理网站注册了topic之后，我们需要添加主题，这里添加Chess和Rock Climbing主题。添加完毕之后，如下图所示： 定义模型Entry当前的模型只是定义了2个属性（主题和时间戳），并不能实际的保存数据，因此我们需要添加模型Entry 关系：每个条目都会与特定的主题相关联，即多对一的关系。 修改之后的代码如下图所示： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim models.py from django.db import models class Topic(models.Model): &quot;&quot;&quot;用户学习的主题&quot;&quot;&quot; text = models.CharField(max_length=200) date_added = models.DateTimeField(auto_now_add=True) def __str__(self): &quot;&quot;&quot;返回模型的字符串表示&quot;&quot;&quot; return self.text ​​ class Entry(models.Model):​ “””学到的有关某个主题的具体知识”””​ topic = models.ForeignKey(Topic,on_delete=models.CASCADE)​ text = models.TextField()​ date_added = models.DateTimeField(auto_now_add=True)​​ class Meta:​ verbose_name_plural = ‘entries’ ​​ def str(self):​ “””返回模型的字符串表示”””​ return self.text[:50] + “…” 像Topic一样， Entry也继承了Django基类Model。 第一个属性topic是一个ForeignKey实例。外键是一个数据库术语，它引用了数据库中的另一条记录；这些代码将每个条目关联到特定的主题。每个主题创建时，都给它分配了一个键（或ID）。需要在两项数据之间建立联系时，Django使用与每项信息相关联的键。稍后我们将根据这些联系获取与特定主题相关联的所有条目。 接下来是属性text，它是一个TextField实例（见）。这种字段不需要长度限制，因为我们不想限制条目的长度。 属性date_added让我们能够按创建顺序呈现条目，并在每个条目旁边放置时间戳。 我们在Entry类中嵌套了Meta类。 Meta存储用于管理模型的额外信息，在这里，它让我们能够设置一个特殊属性，让Django在需要时使用Entries来表示多个条目。如果没有这个类，Django将使用Entrys来表示多个条目。 最后，方法str()告诉Django，呈现条目时应显示哪些信息。由于条目包含的文本可能很长，我们让Django只显示text的前50个字符（见）。我们还添加了一个省略号，指出显示的并非整个条目。 迁移模型Entry由于我们添加了一个新模型，因此需要再次迁移数据库 步骤： 修改models.py makemigrations参数 mkigrate参数 命令及输出如下所示： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py makemigrations learning_logs Migrations for &apos;learning_logs&apos;: learning_logs/migrations/0002_entry.py - Create model Entry (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py migrate Operations to perform: Apply all migrations: admin, auth, contenttypes, learning_logs, sessions Running migrations: Applying learning_logs.0002_entry... OK 生成了一个新的迁移文件0002_entry.py，它告诉django如何修改数据库，使其能够存储与模型Entry相关的信息。 执行命令migrate，我们发现django应用了这种迁移且一切顺利。 向管理网站注册Entry首先需要修改admin.py (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim admin.py from django.contrib import admin from learning_logs.models import Topic,Entry admin.site.register(Topic) admin.site.register(Entry) 然后刷新页面，可以看到新的内容 接下来，我们添加条目 为当前两个主题都添加相应的条目： django shell输入一些数据后，就可通过交互式终端会话以编程方式查看这些数据了。这种交互式环境称。为Django shell，是测试项目和排除其故障的理想之地。下面是一个交互式shell会话示例： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py shell [sudo] password for wxh: Python 3.5.2 (default, Nov 23 2017, 16:37:01) [GCC 5.4.0 20160609] on linux Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. (InteractiveConsole) &gt;&gt;&gt; from learning_logs.models import Topic &gt; &gt;&gt;&gt; Topic.objects.all() &lt;QuerySet [&lt;Topic: Chess&gt;, &lt;Topic: Rock Climbing&gt;]&gt; &gt;&gt;&gt; topics = Topic.objects.all() &gt;&gt;&gt; for topic in topics: ... print (topic.id,topic) ... 8 Chess 9 Rock Climbing &gt;&gt;&gt; t = Topic.objects.get(id=8) &gt;&gt;&gt; t.text &apos;Chess&apos; &gt;&gt;&gt; t.date_added datetime.datetime(2018, 4, 23, 12, 51, 16, 430242, tzinfo=&lt;UTC&gt;) 命令python manage.py shell启动一个Python解释器，可使用它来探索存储在项目数据库中的数据 在这里，我们导入了模块learning_logs.models中的模型Topic，然后使用方法Topic.objects.all()来获取模型Topic的所有实例；它返回的是一个列表，称为查询集（queryset）。 我们可以像遍历列表一样遍历查询集。 我们将返回的查询集存储在topics中，然后打印每个主题的id属性和字符串表示。从输出可知，主题Chess的ID为2，而Rock Climbing的ID为9。 知道对象的ID后，就可获取该对象并查看其任何属性。 我们还可以查看与主题相关联的条目。前面我们给模型Entry定义了属性topic，这是一个ForeignKey，将条目与主题关联起来。利用这种关联， Django能够获取与特定主题相关联的所有条目，如下所示： &gt;&gt;&gt; t.entry_set.all() &lt;QuerySet [&lt;Entry: The opening is the first part of the game, roughly...&gt;]&gt; 为通过外键关系获取数据，可使用相关模型的小写名称、下划线和单词set。例如，假设你有模型Pizza和Topping，而Topping通过一个外键关联到Pizza；如果你有一个名为my_pizza的对象，表示一张比萨，就可使用代码my_pizza.topping_set.all()来获取这张比萨的所有配料。 编写用户可请求的网页时，我们将使用这种语法。确认代码能获取所需的数据时， shell很有帮助。如果代码在shell中的行为符合预期，那么它们在项目文件中也能正确地工作。如果代码引发了错误或获取的数据不符合预期，那么在简单的shell环境中排除 故障要比在生成网页的文件中排除故障容易得多。我们不会太多地使用shell，但应继续使用它来熟悉对存储在项目中的数据进行访问的Django语法。 注意：每次修改模型后，都需要重启shell,执行Ctr + D django APIOnce you’ve created your data models, Django automatically gives you a database-abstraction API that lets you create, retrieve, update and deleteobjects. Creating objects &gt;&gt;&gt; from learning_logs.models import Topic &gt;&gt;&gt; b = Topic(text=&apos;badminton&apos;) &gt;&gt;&gt; b.save() 注意： This performs an INSERT SQL statement behind the scenes. Django doesn’t hit the database until you explicitly call save() The save() method has no return value 其他操作，查看：官方资料 创建网页：学习笔记主页使用django创建网页的过程通常分为四个阶段： 定义模型 这里我们有Topic模型和Entry模型，模型我们可以理解为就是一个数据库，其中定义了几个字段以及其对应的数据类型，后续我们都是根据这些数据进行操作。 定义URL URL模式描述了URL是如何设计的，让Django知道如何将浏览器请求与网站URL匹配，以确定返回哪个网页 编写视图 每个URL都被映射到特定的视图——视图函数获取并处理网页所需的数据。【也就是URL与内容的映射关系】 编写模板 视图函数通常调用一个模板，后者生成浏览器能够理解的网页。【也就是内容数据的显示方式，视图前端显示模板】 接下来，我们来创建学习笔记的主页。我们将定义该主页的URL、编写其视图函数并创建一个简单的模板 逻辑关系： 项目中定义调用的应用程序的模块，然后在具体应用程序中配置URL和视图 映射URL用户通过在浏览器中输入URL以及单击链接来请求网页，因此我们需要确定项目需要哪些URL主 页 的 URL 最 重 要 ， 它 是 用 户 用 来 访 问 项 目 的 基 础 URL 。 当 前 ， 基 础 URL（http://localhost:8000）返回默认的Django网站，让我们知道正确地建立了项目。我们将修改这一点，将这个基础URL映射到“学习笔记”的主页。 打开项目主目录中的urls.py文件（该文件针对整个项目的url配置），默认的内容为： 前两行导入了为项目和管理网站管理URL的函数和模块，在这个针对整个项目的urls.py文件中，变量urlpatterns包含项目中的应用程序的URL。 admin.site.urls模块定义了可在管理网站中请求的所有URL。 修改之后的配置文件如下图所示： 现在，我们需要在learning_logs目录下创建urls.py文件文件内容如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim urls.py (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ cat urls.py &quot;&quot;&quot;定义learning_logs的URL模式&quot;&quot;&quot; from django.conf.urls import url from . import views app_name = &apos;learning_logs&apos; urlpatterns = [ #主页 url(r&apos;^$&apos;,views.index,name=&apos;index&apos;) ] 讲解： 实际的URL模式其实是对函数url()的调用，这个函数接受3个实参 这里的正则表达式让python查找开头和末尾之间没有任何东西的url。 python忽略项目基础的URL（在这里是:http://localhost:8000/），因此这个正则表达式与基础URL相匹配，其他的非基础URL都不与这个正则表达式匹配，如果请求的是其他的URL页面，django将会返回一个错误页面。 url()的第2个实参指定了要调用的视图函数。请求的URL与前面的正则表达式匹配时，django将会调用views.index（这个index视图函数稍后编写） 第3个实参，将这个url模式的名称指定为index(相当于是alias别名的形式)，让我们在代码的其他地方引用它。每当我们需要提供这个主页的链接时，我们可以直接使用这个名称，而不用编写URL。 编写视图每个URL都被映射到特定的视图函数——视图函数获取并处理网页所需的数据。【也就是URL与内容的映射关系】 视图函数接受请求中的信息，准备好生成网页所需的数据，再讲这些数据发送给浏览器—这通常还涉及到网页的模板 learning_logs中的文件views.py是执行命令python manage.py startapp时自动生成的，当前的文件内容如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ cat views.py from django.shortcuts import render # Create your views here. 现在，这个文件只导入了函数render()，它根据视图提供的数据渲染响应。修改之后的文件内容如下如所示：​​ from django.shortcuts import render​ def index(request): “””学习笔记的主页””” return render(request,’learning_logs/index.html’) 当URL被刚才定义的模式匹配之后，django会在views.py文件中查找函数index()，再将请求对象传递给这个视图函数（也就是这里的request）。 接下来向函数render进行传2个实参，原始的请求对象以及一个用于创建网页的模板（模板目录下的learning_logs目录下的index.html文件） 下面我们来编写这个模板。 编写模板模板定义了网页的结构，也就是说模板指定了网页是什么样子的。 每当网页被请求时，django将会填入相关的数据。模板让你能够访问视图提供的任何数据。我们主页视图没有提供任何数据，因此相应的模板非常简单。 创建目录在目录learning_logs下创建templates目录，用户保存网页模板文件 然后创建子目录learninig_logs，并在该子目录下新建文件index.html 文件内容为： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo mkdir -p templates/learning_logs (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ cd templates/learning_logs (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim index.html &lt;p&gt;Learning Log&lt;/p&gt; &lt;p&gt;Learning Log helps you keep track of your learning, for any topic you&apos;re learning about.&lt;/p&gt; 现在重新访问网站，会发现显示的是刚才自定义的这个网页，不再显示django网页。 如下图所示： 总结创建网页的过程看起来会有点复杂，但是将URL、视图和模板分离，效果实际上会更好，这让我们可以分别考虑项目的不同方面。 例如：数据库专家可以专注于模型，程序员可以专注于视图代码，而web涉及人员可以专注于模板。 关系： 访问请求–&gt;url（入：定义url匹配规则，出：视图函数。同时还定义这个url的调用别名） –&gt;视图（入：url请求对象，出：创建网页的模板，包含路径和名称信息。动态内容在这里生成） –&gt;模板（html网页，其中包含具体的网页内容，这里只是静态内容，接受视图中的动态数据然后组装显示） 创建其他网页接下来，我们创建两个显示数据的网页。 其中一个列出所有的主题 另一个显示特定主题的所有条目 对于每一个网页，我们都将指定URL模式，编写一个视图函数，并编写一个模板。 为了效率，我们可以先编写一个父模板 模板继承父模板：base.html 我们在/opt/learning_logs/templates/learning_logs目录下创建base.html【在项目中，每个网页都将继承base.html】 内容如下： &lt;p&gt; &lt;a href=&quot;{ % url &apos;learning_logs:index&apos;% }&quot;&gt;Learning Log&lt;/a&gt; &lt;/p&gt; { % block content % }{ % endblock content % } 实际效果为： Learning Log { % block content % }{ % endblock content % } 说明： 在当前，所有页面都包含的元素只有顶端的标题。我们将在每个页面中包含这个模板，因此我们将这个标题设置到主页的链接。 模板标签是用大括号和百分号{ %% }表示的。模板标签是一小段代码，生成要在网页中显示的信息。 在这里，模板标签{ % url ‘learning_logs:index’% }生成一个URL，该URL与learning_logs/urls.py中定义的名为index的URL模式匹配，也就是说learning_logs是一个命名空间，而index是该命名空间中一个名称独特的URL模式。 在HTML页面中，链接是使用锚（mao）标签来定义了：​​ 格式为： link text 这里也就是类似markdown的链接使用方式，也可以说是类似Linux中软链接的方式，可以隐藏后端真实的URL串，并且让对外的链接保持最新也要容易得多。 子模板现在我们需要重新编写index.html，使其继承base.html，修改之后的代码如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim index.html { % extends &quot;learning_logs/base.html&quot; % } { % block content % } &lt;p&gt;Learning Log helps you keep track of your learning, for any topic you&apos;re learning about.&lt;/p&gt; { % endblock content % } 子模板的第一行必须包含标签{ % extends % }，让Django知道它继承了哪个父模板。 注意：这时候虽然两个文件的目录结构都是一样，但是还是需要制定目录learning_logs。 这行代码导入了base.html页面的所有内容，让index.html能够指定要在content块中预留的空间中添加的内容。 在子模板中，只需要包含当前网页特有的内容，这不仅简化了每个模板，还使得网站修改起来容易得多，要修改很多网页都包含的元素，只需要在父模板中修改该元素。 在大型项目中，我们可以定义多级父模板，有一个用于整个网站的父模板，并且网站的每个主要部分都有一个父模板，每个部分中又去继承这个模板。 接下来，我们专注于另外两个网页： 显示全部所有主题的网页 显示特定主题中条目的网页 显示所有主题的页面URL模式首先定义显示所有主题页面的URL。在这里我们使用topics来表示 完整的URL应该是：http://localhost:8000/topics 我们修改urls.py​​ “””定义learning_logs的URL模式”””​ from django.conf.urls import url from . import views app_name = &apos;learning_logs&apos; urlpatterns = [ #主页 url(r&apos;^$&apos;,views.index,name=&apos;index&apos;), url(r&apos;^topics/$&apos;,views.topics,name=&apos;topics&apos;) ] 视图函数修改views.py，添加上面指定的topics视图函数 from django.shortcuts import render from .models import Topic def index(request): &quot;&quot;&quot;学习笔记的主页&quot;&quot;&quot; return render(request,&apos;learning_logs/index.html&apos;) def topics(request): &quot;&quot;&quot;显示所有的主题&quot;&quot;&quot; topics = Topic.object.order_by(&apos;date_added&apos;) context = {&apos;topics&apos;:topics} return render(request,&apos;learning_logs/topics.html&apos;,context) 说明： 需要导入与所需数据相关联的模型，也就是导入Topic类 视图函数中包含一个形参（django从服务器收到的访问request对象） 第一行中，我们查询数据库，请求提供Topic对象，并且按照属性date_added进行排序，然后将返回的查询集存储在topics中 context定义了一个将要发送给模板的上下文。上下文是一个字典类型，其中的键是我们将在模板中用来访问数据的名称，而值是我们要发送给模板的数据。在这里，我们暂时只是定义了一个键值对，包含我们将要在网页中显示的主题。 创建使用数据的网页时，除了对象request和模板的路径之外，我们还将变量context传递给render() 模板在这里，我们需要定义上面指定的topics网页。页面接受字典context，以便能使用topics()提供的数据。 创建的topics.html页面内容如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim topics.html { % extends &quot;learning_logs/base.html&quot; % } { % block content % } &lt;p&gt;Topics&lt;/p&gt; &lt;ul&gt; { % for topic in topics % } &lt;li&gt;{{ topic }}&lt;/li&gt; { % empty % } &lt;li&gt; No topics have been added yet.&lt;/li&gt; { % endfor % } &lt;/ul&gt; ​​ { % endblock content % } 说明： 就像index.html一样，我们首先使用标签{ % extends % }来继承base.html，再开始定义content块。这个网页的主体是一个项目列表，其中列出了用户输入的主题。在标准的HTML中，项目列表被称为无序列表，用标签&lt;ul&gt;&lt;/ul&gt;表示。 我们使用了一个for循环的模板标签【注意：pytho使用缩进来指出哪些代码是for循环的组成部分，而在模板中，每个for循环都必须使用{ % endfor %标签来显示地指出其结束位置}】 在这里topics这个变量是从视图函数中传递过来的，因此不需要我们再指定 在HTML中，for循环的格式为： { % for item in list % } do something with each item { % endfor % } 在循环中，我们要将每个主题转换为一个项目列表项。要在模板中打印变量，需要将变量名用双花括号括起来。每次循环时，代码都被替换为topic的当前值。这些花括号不会出现在网页中，它们只是用于告诉Django我们使用了一个模板变量。 HTML标签表示一个项目列表项，在标签对内部，位于标签和之间的内容都是一个项目列表项。 修改父模板 我们现在需要修改父模板(base.html)，使其包含到显示所有主题的页面的链接修改之后的内容如下所示： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ cat base.html &lt;p&gt; &lt;a href=&quot;{%url 'learning_logs:index'%}&quot;&gt;Learning Log&lt;/a&gt; - &lt;a href=&quot;{%url 'learning_logs:topics'%}&quot;&gt;Topics&lt;/a&gt; &lt;/p&gt; {% block content%}{% endblock content%} 这个时候刷新页面，显示如下： ![topics](http://picture.watchmen.xin/python-django/alltopics.png) ### 显示特定主题主所有条目的页面 ### 接下来，我们需要创建一个专注于显示特定主题的页面-**`显示该主题的名称以及该主题的所有条目`** 同样，我们的顺序还是： 1. 定义URL模式 2. 编写视图函数 3. 创建网页模板 此外，我们还需要修改上一个网页（显示所有主题的网页），让每个项目列表都是一个链接，点击之后可以显示相应主题的所有条目 #### URL模式 #### (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim urls.py """定义learning_logs的URL模式""" from django.conf.urls import url from . import views app_name = 'learning_logs' urlpatterns = [ #主页 url(r'^$',views.index,name='index'), url(r'^topics/$',views.topics,name='topics'), url(r'^topics/(?P\d+)/$',views.topic,name='topic') ] **说明：** r让django将这个字符串视为原始字符串，并指出正则表达式包含在引号内。这个表达式的第二部分**`/(?P\d+)/`**与包含在两个//内的整数内容进行匹配，并将这个整数存储在一个名为topic_id的实参中，这部分表达式捕获URL中的值； ?P将匹配到的值存储到topic_id中，而表达式\d+与包含在两个斜杆内的任何数字都匹配，不管这个数字为多少位 当发现URL与这个模式匹配的时候，django将会调用视图函数topiic()，并将存储在topic_id中的值作为实参传递给它。在这个函数中，我们使用topic_id的值来获取相应的主题 #### 视图 #### 视图topic()需要从数据库中获取指定的主题以及与之相关联的所有条目。如下所示： ​ (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim views.py from django.shortcuts import render from .models import Topic def index(request): """学习笔记的主页""" return render(request,'learning_logs/index.html') def topics(request): """显示所有的主题""" topics = Topic.objects.order_by('date_added') context = {'topics':topics} return render(request,'learning_logs/topics.html',context) def topic(request,topic_id): """显示单个主题及其所有的条目""" topic = Topic.objects.get(id=topic_id) entries = topic.entry_set.order_by('-date_added') context = {'topic':topic,'entries':entries} return render(request,'learning_logs/topic.html',context) 在这个视图函数中，我们有两个形参。这个函数接受正则表达式(?P\d+)捕获的值，并将其存储到topic_id这个形参当中 接下来，我们使用get()来获取指定的主题 下一行中，我们获取与该主题相关联的条目并将它们按照date_add进行排序：date_added前面的减号（-）指定按照降序进行排序，也就是先显示最近的条目，我们将主题和条目都存储在字典context当中，再将这个字典发送给模板topic.html。 注意： > 第一行和第二行（get和order_by）的代码都被称之为查询，因为它们会向数据中查询特定的信息，在自己的项目中编写这样的查询时，可以现在django shell中先进行试验 > 相比于直接编写视图和网页模板，再在浏览器中检查结果，在shell中执行代码可以更加快速的获得反馈 #### 模板 #### 这个模板需要显示每个主题的名称和其对应条目的内容，如果当前主题不包含任何条目，我们还需要向用户指出这一点。 代码如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim topic.html {% extends "learning_logs/base.html" %} {% block content %} Topic:{{ topic }} Entries: {% for entry in entries%} {{ entry.date_added|date:'M d, Y H:i'}} {{ entry.text|linebreaks }} { % empty % } There are no entries for this topic yet. {% endfor %} {% endblock content %} 说明： 是一个模板变量，在这里变量都是这种表示方式，这里的变量topic，是存储在字典context中的在&lt;ul&gt;&lt;/ul&gt;中我们利用for循环定义一个显示每个条目的项目列表（无序列表）&gt; 每个列表项目都将列出两项信息：条目的时间戳和完整的文本。&gt;&gt; 第一行时间戳：在django中的模板中，竖线（|）表示模板过滤器–【对模板变量的值进行修改的函数】。后面的部分我们称之为：过滤器&gt;&gt; 过滤之后的时间戳显示格式将是：’M d, Y H:i’以这样的格式显示时间戳： January 1, 2015 23:00&gt;&gt; 第二行显示text的完整值，而不仅仅是entry的前50个字符。过滤器linebreaks将包含换行符的长条目转换为浏览器能够理解的格式，避免显示一个不间断的文本块。&gt; 在最后，我们使用模板标签{ % empty % }打印一条消息，告诉用户当前主题还没有条目注意：代码和说明中的{和%中其实是没有空格的，在这里故意加上空格是因为hexo博客框架无法正常处理。#### 将显示所有主题的页面中的每个主题都设置为链接 ####在浏览器中查看显示特定主题的页面前，我们需要修改模板topics.html，让每个主题都链接到相应的网页如下所示：(ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim topics.html​这个时候我们再次刷新页面，再单击其中的一个主题，将会看到类似于下图的页面：—# 用户账户 #在这一章节，我们将创建对用户友好而直观的网页，让用户无需通过管理网站就能添加新的主题和条目，以及编辑既有的条目。我们还将添加一个用户注册系统，让用户能够创建账户和自己的学习笔记。让任意数量的用户都能与之交互，是Web应用程序的核心所在。## 让用户能够输入数据 ##当前，只有超级用户能够通过管理网站输入数据。我们不想让用户与管理网站进行交互，因此我们将使用django的表单创建工具来创建工具让用户能够输入数据的页面这部分，主要会涉及以下几种数据- 添加新主题- 添加新条目- 编辑条目### 添加新主题 ###在添加新主题时，需要使用到一个输入框【在这里是一个基于表单的页面】#### 用户添加主题的表单我们需要让用户输入并提交信息的页面是表单&gt; 并且在用户输入数据的时候，我们还需要进行验证，确认提供的信息是否是正确的数据类型&gt; 而且信息不是恶意的信息，例如终端服务器的代码。&gt; 然后，我们再对这些有效信息进行处理，并将其保存到数据库的合适地方，这些工作都是由django自动完成。在django中，创建表单最简单的方式是使用ModeForm，它根据我们在第18章定义的模型中的信息自动创建表单。创建一个forms.py文件，并存储到models.py所在目录中代码如下：(ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim forms.py​​ from django import forms​ from .models import Topic class TopicForm(forms.ModelForm): class Meta: model = Topic fields = [‘text’] labels = {‘text’:’’}最简单的ModelForm版本只包含一个内嵌的Meta类，它告诉django根据哪个模型创建表单，以及在表单中包含哪些字段。&gt; 在这里，我们根据模型Topic创建一个表单，该表单只包含字段text，下面的代码让django不要为字段text生产标签。#### URL定义在这里我们创建的url是：http://localhost:8000/new_topic/代码如下： “””定义learning_logs的URL模式””” from django.conf.urls import url from . import views app_name = ‘learning_logs’ urlpatterns = [ #主页 url(r’^$’,views.index,name=’index’), url(r’^topics/$’,views.topics,name=’topics’), url(r’^topics/(?P\d+)/$’,views.topic,name=’topic’), # 用户添加新主题的网页 url(r’^new_topic/$’,views.’new_topic’,name=’new_topic’) ]这个url模式将请求交给视图函数new_topic(),接下来我们将编写这个函数#### 视图函数new_topic() ####函数new_topic()需要处理两种情形- 第一种情况是刚进入new_topic网页（在这种情况下，它应该显示一个空表单）- 第二种情况是对提交的表单数据进行处理，并将用户重定向到网页topics代码如下： from django.shortcuts import render from django.http import HttpResponseRedirect from django.urls import reverse from .models import Topic from .forms import TopicForm def index(request): “””学习笔记的主页””” return render(request,’learning_logs/index.html’) def topics(request): “””显示所有的主题””” topics = Topic.objects.order_by(‘date_added’) context = {‘topics’:topics} return render(request,’learning_logs/topics.html’,context) def topic(request,topic_id): “””显示单个主题及其所有的条目””” topic = Topic.objects.get(id=topic_id) entries = topic.entry_set.order_by(‘-date_added’) context = {‘topic’:topic,’entries’:entries} return render(request,’learning_logs/topic.html’,context) def new_topic(request): “””添加新主题””” if request.method != ‘POST’: # 未提交数据：创建一个新表单 form = TopicForm() else: # POST提交的数据，对数据进行处理 form = TopicForm(request.POST) if form.is_valid(): form.save() return HttpResponseRedirect(reverse(‘learning_logs:topics’)) context = {‘form’: form} return render(request,’learning_logs/new_topic.html’,context)说明：在这里，我们我们导入了HttpResponseRedirect类，用户提交主题后，我们会使用这个类将用户重定向到网页topics。函数reverse()根据指定的URL模型确定URL，这意味着django将在页面被请求时生成URL。我们还导入了刚才创建的表单TopicFrom#### 补充信息：GET请求与POST ####创建Web应用程序时，将用到的两种主要请求类型是GET请求和POST请求。对于只是从服务器读取数据的页面，使用GET请求；在用户需要通过表单提交信息时，通常使用POST请求。处理所有表单时，我们都将指定使用POST方法。还有一些其他类型的请求，但这个项目没有使用。函数new_topic()将请求对象作为参数。用户初次请求该网页时，其浏览器将发送GET请求；用户填写并提交表单时，其浏览器将发送POST请求。根据请求的类型，我们可以确定用户请求的是空表单（GET请求）还是要求对填写好的表单进行处理（POST请求）。如果请求方法不是POST，请求就可能是GET，因此我们需要返回一个空表单（即便请求是其他类型的，返回一个空表单也不会有任何问题）。我们创建一个TopicForm实例，将其存储在变量form中，再通过上下文字典将这个表单发送给模板。由于实例化TopicForm时我们没有指定任何实参， Django将创建一个可供用户填写的空表单。如果请求方法为POST，将执行else代码块，对提交的表单数据进行处理。我们使用用户输入的数据（它们存储在request.POST中）创建一个TopicForm实例，这样对象form将包含用户提交的信息。要将提交的信息保存到数据库，必须先通过检查确定它们是有效的。函数is_valid()核实用户填写了所有必不可少的字段（表单字段默认都是必不可少的），且输入的数据与要求的字段类型一致（例如，字段text少于200个字符，这是我们在第18章中的models.py中指定的）。这种自动验证避免了我们去做大量的工作。如果所有字段都有效，我们就可调用save()，将表单中的数据写入数据库。保存数据后，就可离开这个页面了。我们使用reverse()获取页面topics的URL，并将其传递给HttpResponseRedirect()，后者将用户的浏览器重定向到页面topics。在页面topics中，用户将在主题列表中看到他刚输入的主题。#### 模板new_topic ####下面来创建模板new_topic.html代码如下：(ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim new_topic.html#### 链接到页面new_topic ####接下来，我们在页面topics中添加一个到页面new_topic的链接：(ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim topics.html这个时候访问指定页面，可以看到有输入框出现### 添加新条目 ###现在用户可以添加新主题了，但是他们还想添加新的条目。我们再次定义URL，编写视图函数和模板，并连接到添加新条目的网页，但在此之前，我们需要好在forms.py中再添加一个类1. 用于添加新条目的表单我们需要创建一个与模型Entry相关联的表单，但是这个表单的定制程度比TopicForm要高些(ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim forms.py from django import forms from .models import Topic,Entry class TopicForm(forms.ModelForm): class Meta: model = Topic fields = [‘text’] labels = {‘text’:’’} class EntryForm(forms.ModelForm): class Meta: model = Entry fields = [‘text’] labels = {‘text’:’’} widgets = {‘text’:forms.Textarea(attrs={‘cols’:80})}说明：在这里，再导入了Entry。我们定义了属性widgets。小部件（widgets）是一个HTML表单元素，例如单行文本框、多行文本区域或者下拉列表。通过设置属性widgets，可覆盖Django选择的默认小部件。通过让Django使用forms.Textarea，我们定制了字段’text’的输入小部件，将文本区域的宽度设置为80列，而不是默认的40列。这给用户提供了足够的空间，可以编写有意义的条目。2. URL模式new_entry在用户添加新条目的页面的URL模式中，需要包含实参topic_id,因为条目必须与特定的主题相关联。该URL模式如下：(ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim urls.py “””定义learning_logs的URL模式””” from django.conf.urls import url from . import views app_name = ‘learning_logs’ urlpatterns = [ #主页 url(r’^$’,views.index,name=’index’), url(r’^topics/$’,views.topics,name=’topics’), url(r’^topics/(?P\d+)/$’,views.topic,name=’topic’), # 用户添加新主题的网页 url(r’^new_topic/$’,views.new_topic,name=’new_topic’), # 用户添加新条目的页面 url(r’^new_entry/(?P\d+)/$’,views.new_entry,name=’new_entry’) ]3. 视图函数new_entry# django安全相关默认情况下，使用delete等http方法的时候会返回报错信息：12Forbidden (CSRF cookie not set.): /api/projects/repo[19/Mar/2019 06:59:48] &quot;DELETE /api/projects/repo?region=dev&amp;repo_name=watchmen/test HTTP/1.1&quot; 403 2868 解决： 到setting里面注释掉下面的注释位置就好、 例如： 12345678910MIDDLEWARE = [ &apos;django.middleware.security.SecurityMiddleware&apos;, &apos;django.contrib.sessions.middleware.SessionMiddleware&apos;, &apos;django.middleware.common.CommonMiddleware&apos;, #&apos;django.middleware.csrf.CsrfViewMiddleware&apos;, &apos;django.contrib.auth.middleware.AuthenticationMiddleware&apos;, &apos;django.contrib.messages.middleware.MessageMiddleware&apos;, &apos;django.middleware.clickjacking.XFrameOptionsMiddleware&apos;,] sqlite3版本问题在服务器上运行django项目的时候，会出现版本过低的问题 报错： 1django.db.utils.NotSupportedError: URIs not supported 解决：升级sqlite3版本 找到 python3.6/site-packages/django/db/backends/sqlite3/base.py 搜索修改内容 1kwargs.update(&#123;&apos;uri&apos;: False&#125;) #这里原来是True，修改为False就可以了 django监控所有接口问题使用以下命令启动时， 1nohup /application/python3/bin/python3 manage.py runserver 0.0.0.0:8000 &gt; /dev/null 2&gt;&amp;1 &amp; django定时任务在编写django项目的时候，很多时候会有定时任务的需求。 安装配置步骤1：安装django-crontab库1pip install django-crontab 再在settings.py中添加app: 1234INSTALLED_APPS = ( ... &apos;django_crontab&apos;, ) 步骤2：创建定时任务在app内新建py文件，文件名称随意。 例如我们在名为bind_ops的app下新建了一个bind_crontab.py文件。 文件内容： 1234567from lib.logger import loggerimport datetimetime = datetime.datetime.now()time = str(time)def crontab_test(): print (111) logger.info(&quot;bind crontab&quot; +time ) 因为要看定时任务的效果，所以采用了直接输出和记录到文件的形式 经过测试发现：当定时任务在执行时，如果你只是输出一些语句，那么你将看不到任何内容，所以请不要怀疑这个定时任务没有执行。 然后在 settings.py中增加： 1234567# 最简单配置CRONJOBS = [ # 表示每天2：01执行 (&apos;01 2 * * *&apos;, &apos;bind_ops.bind_crontab.crontab_test&apos;)]# 参数及字段说明： 第一个参数（表示时间），前5个字段分别表示： 分钟：0-59 小时：1-23 日期：1-31 月份：1-12 星期：0-6（0表示周日） 一些特殊符号： *： 表示任何时刻 ,： 表示分割 -：表示一个段，如在第二段里： 1-5，就表示1到5点 /n : 表示每个n的单位执行一次，如第二段里，*/1, 就表示每隔1个小时执行一次命令。也可以写成1-23/1. 第二个参数（表示路径）： 格式：app名称.文件名.函数名 如果想生成日志，那就再加一个字符串类型的参数：’&gt;&gt; path/name.log’， path路径，name文件名。’&gt;&gt;’表示追加写入，’&gt;’表示覆盖写入。 提示： 如果你有多个定时任务，以逗号隔开，都放入CORNJOBS的列表中即可。 路径必须写绝对路径，写相对路径是不识别的。 步骤3：启动任务以上都完成后，需要执行以下命令将任务添加并生效 1python manage.py crontab add 显示当前的定时任务 1python manage.py crontab show 删除所有定时任务 1python manage.py crontab remove 重启django服务执行（可能不需要，因为并没有用，也正常使用了。） 1python manage.py corntab -e 实际案例dname项目-bind定时获取主机名具体看项目源码 django配置uWSGI启动命令： 12345/usr/local/bin/uwsgi --http :8000 --chdir /Users/wangxiaohua/PycharmProjects/dnamed --wsgi-file dnamed_django/wsgi.py/application/python3/bin/uwsgi --http :8000 --chdir /application/dnamed --wsgi-file dnamed_django/wsgi.py/application/dnamed 其中： –chdir：项目目录 –wsgi-file：项目中wsgi.py文件的目录，相对于项目目录 还可以是本地使用socket的方式： 1/application/python/bin/uwsgi --socket 127.0.0.1:9093 --chdir /application/saltapi_django_qa_prod --wsgi-file /application/saltapi_django_qa_prod/saltapi_django/wsgi.py --master --processes 4 --threads 50 nginx中的配置文件： 这是salt-api的接口配置 12345678910111213141516171819202122232425[root@salt-jumpserver-dev conf.d]# cat saltapi_qa_prod_http.conf## The default server#server &#123; listen 10080; location /static &#123; root /data1/saltapi/static_qa_prod; &#125; location / &#123; uwsgi_pass 127.0.0.1:9093; include uwsgi_params; &#125; error_page 404 /404.html; location = /40x.html &#123; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125;&#125; django自定义404页面 编辑setting.py文件 1234567891011DEBUG = False ALLOWED_HOSTS = [&apos;*&apos;,] TEMPLATES = [ &#123; ... &apos;DIRS&apos;: [os.path.join(BASE_DIR, &apos;templates&apos;)] ... &#125;,] 在templates文件夹下新建404.html文件 123456789&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;404&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;404 NOT FOUND&lt;/h1&gt;&lt;/body&gt; 编辑urls.py文件 12345678910#from app_name import views # app_name是应用名from bind_ops import views as bind_ops urlpatterns = [ ...]#handler404 = views.page_not_foundhandler404 = bind_ops.page_not_found 编辑views.py文件 123456789def page_not_found(request): return render(request, &quot;404.html&quot;) def error_403(request, exception): ...或者是这种形式def error_403(request, *args, **kwargs): ... 当出现异常，需要调试的时候，可以再settings.py文件中添加下面这行： 1DEBUG_PROPAGATE_EXCEPTIONS = True 生产环境django配置在settings.py中修改以下内容： DEBUG =True 改为 DEBUG =False ALLOWED_HOSTS = []修改具体的域名 自定义404页面]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>编程开发</category>
        <category>Python</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VPC从入门到实践]]></title>
    <url>%2F2018%2F04%2F17%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E5%85%AC%E6%9C%89%E4%BA%91%E4%BA%A7%E5%93%81%2F%E9%98%BF%E9%87%8C%E4%BA%91%2FVPC%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[参考文献： 阿里云-VPC官方帮助文档 VPC基础知识VPC概述专有网络VPC（Virtual Private Cloud）是用户基于阿里云创建的自定义私有网络, 不同的专有网络之间二层逻辑隔离，用户可以在自己创建的专有网络内创建和管理云产品实例，比如ECS、负载均衡、RDS等。 特点 专有网络VPC（Virtual Private Cloud）是基于阿里云构建的一个隔离的网络环境。每个专有网络（也就是每个VPC）之间逻辑上彻底隔离。 专有网络是独有的的云上私有网络。用户可以完全掌控自己的专有网络，例如选择IP地址范围、配置路由表和网关等，可以在自己定义的专有网络中使用阿里云资源如ECS、RDS、SLB等。 可以将专有网络连接到其他专有网络，或本地网络（这里的本地网络包括IDC和公司内部机房），形成一个按需定制的网络环境，实现应用的平滑迁移上云和对数据中心的扩展。 【默认情况下，VPC内主机是无法与外网连接的。这里需要借助阿里云的公网介入产品，例如弹性公网IP、NAT网关、负载均衡等】 VPC重点内容小结： 不同用户的云服务器部署在不同的专有网络里。 不同专有网络之间通过隧道ID进行隔离。专有网络内部由于交换机和路由器的存在，所以可以像传统网络环境一样划分子网，每一个子网内部的不同云服务器使用同一个交换机互联，不同子网间使用路由器互联。 不同专有网络之间内部网络完全隔离，只能通过对外映射的IP（弹性公网IP和NAT IP）互联。 由于使用隧道封装技术对云服务器的IP报文进行封装，所以云服务器的数据链路层（二层MAC地址）信息不会进入物理网络，实现了不同云服务器间二层网络隔离，因此也实现了不同专有网络间二层网络隔离。 专有网络内的ECS使用安全组防火墙进行三层网络访问控制。 VPC中的路由器和交换机路由器（VRouter）是专有网络的枢纽。作为专有网络中重要的功能组件，它可以连接VPC内的各个交换机，同时也是连接VPC和其他网络的网关设备。每个专有网络创建成功后，系统会自动创建一个路由器。每个路由器关联一张路由表。更多信息，参见路由。 交换机（VSwitch）是组成专有网络的基础网络设备，用来连接不同的云产品实例。创建专有网络之后，您可以通过创建交换机为专有网络划分一个或多个子网。同一专有网络内的不同交换机之间内网互通。您可以将应用部署在不同可用区的交换机内，提高应用的可用性。 说明：交换机不支持组播和广播。您可以通过阿里云提供的组播代理工具实现组播代理。详情参见组播代理概述。 整体拓扑架构如下图所示： VPC可以使用的私网地址范围： 在创建专有网络和交换机时，您需要以CIDR地址块的形式指定专有网络使用的私网网段。关于CIDR的相关信息，参见维基百科上的Classless Inter-Domain Routing条目说明。 您可以使用下表中标准的私网网段及其子网作为VPC的私网地址。专有网络创建成功之后，无法修改网段。建议使用比较大的网段，尽量避免后续扩容。 网段 可用私网IP数量 （不包括系统保留） IP地址范围 192.168.0.0/16 65532 192.168.0.0-192.168.255.255 172.16.0.0/12 1048572 172.16.0.0-172.31.255.255 10.0.0.0/8 16777212 10.0.0.0-10.255.255.255 最终每个网段的可用IP数量要总数减去4（系统去掉了0以及保留了最后3个） 交换机的网段不能和所属的专有网络的网段重叠，可以是其子集或者相同，网段大小在16位网络掩码与29位网络掩码之间。 如果交换机的网段和专有网络的网段相同，您只能创建一个交换机。 更多网络规划的信息，参考VPC网络规划。 关系梳理： 一个VPC（专有网络）是一个大网段，例如：192.168.0.0/16 如果掩码是24位，那么这里一共可以划分出来2^8=256个子网 每一个可用区的可用主机数量为：2^8=256个（可用的为252个，剔除了0和255、254、253) 一个交换机在一个可用区中，一个交换机也就是一个子网，路由器连接的是每个交换机，对外的网段是VPC的网段。 基础架构及实现原理问题随着云计算的不断发展，对虚拟化网络的要求越来越高，比如弹性（scalability）、安全性（security）、可靠性（reliability）和私密性（privacy），并且还有极高的互联性能（performance）需求，因此催生了多种多样的网络虚拟化技术。 比较早的解决方案，是将虚拟机的网络和物理网络融合在一起，形成一个扁平的网络架构，例如大二层网络。随着虚拟化网络规模的扩大，这种方案中的ARP欺骗、广播风暴、主机扫描等问题会越来越严重。为了解决这些问题，出现了各种网络隔离技术，把物理网络和虚拟网络彻底隔开。其中一种技术是用户之间用VLAN进行隔离，但是VLAN的数量最大只能支持到4096个，无法支撑公共云的巨大用户量。 解决方案基于目前主流的隧道技术，专有网络（Virtual Private Cloud，简称VPC）隔离了虚拟网络。每个VPC都有一个独立的隧道号，一个隧道号对应着一个虚拟化网络。一个VPC内的ECS（Elastic Compute Service）实例之间的传输数据包都会加上隧道封装，带有唯一的隧道ID标识，然后送到物理网络上进行传输。不同VPC内的ECS实例因为所在的隧道ID不同，本身处于两个不同的路由平面，所以不同VPC内的ECS实例无法进行通信，天然地进行了隔离。 基于隧道技术和软件定义网络（Software Defined Network，简称SDN）技术，阿里云的研发在硬件网关和自研交换机设备的基础上实现了VPC产品。 逻辑架构如下图所示，VPC包含交换机、网关和控制器三个重要的组件。交换机和网关组成了数据通路的关键路径，控制器使用自研的协议下发转发表到网关和交换机，完成了配置通路的关键路径。整体架构里面，配置通路和数据通路互相分离。交换机是分布式的结点，网关和控制器都是集群部署并且是多机房互备的，并且所有链路上都有冗余容灾，提升了VPC产品的整体可用性。 交换机和网关性能在业界都是领先的。自研的SDN协议和控制器，能轻松管控公共云成千上万张虚拟网络。 VPC通信专有网络是完全隔离的网络环境。默认情况下，相同专有云网络内的ECS和云服务可以进行私网通信，但VPC与VPC之间、VPC与经典网络或公网不能互通。您可以使用弹性公网IP、高速通道、NAT、VPN网关或公网负载均衡等公网产品实现专有网络间的通信。 VPC与VPC通信 VPC与经典网络通信 VPC与Internet通信 VPC与本地IDC通信 VPC与VPC通信默认情况，不同专有网络间的ECS不能直接进行私网通信。 您可以使用高速通道的路由器接口，在两侧VPC的路由器上分别创建路由器接口，以及自有的骨干传输网络来搭建高速通道，轻松实现VPC之间安全可靠、方便快捷的通信。配置详情参考同账号下专有网络内网互通。 VPC与经典网络通信默认专有网络内的ECS不能访问经典网络的ECS或者云服务。 VPC访问经典网络 VPC与经典网络可以通过公网IP进行通信。只要VPC和经典网络中的ECS实例或云实例的公网IP符合下表中的任意一一条要求，专有网络就可以访问经典网络的云服务。 经典网络访问VPC 经典网络也可以通过公网IP访问VPC，只要VPC中的ECS实例或云服务也配置了公网IP。 VPC与Internel通信默认情况下，专有网络内的ECS不能与公网互通。您可以通过以下途径中的一种打通VPC与公网的通信： 为专有网络中的ECS实例分配公网IP，实现专有网络与公网的通信。详情参考分配公网IP。 您可以通过在ECS上绑定弹性公网IP（EIP），实现专有网络与公网的互通。详情参考弹性公网IP。 在专有网络的ECS上设置NAT网关，实现专有网络与公网的互通。详情参考端口映射和SNAT设置。 如果您有多台ECS需要和公网互通，您可以使用NAT网关的共享带宽包，一个带宽包内的所有ECS实例共享带宽，以节省您的费用。 专有网络的ECS实例添加到一个公网负载均衡实例中。详情参考配置公网负载均衡。 此情形下，专有网络中的ECS实例不能访问公网，只能接收负载均衡转发的公网请求。 VPC与本地IDC通信默认情况下，本地IDC网络中心和专有网络之间不能通信，您可以通过以下途径打通本地IDC与VPC之间的通信： 您可以使用高速通道的物理专线来连通本地IDC到阿里云的专线接入点，并建立虚拟边界路由器作为VPC到IDC的数据转发桥梁。详情参考专线接入。 您可以使用VPN网关来实现本地IDC网络中心与专有网络的互通，详情参考搭建VPN网关。 名词解释 注意： 路由条目包括系统路由和自定义路由两种类型。 使用限制 注意： 每个VPC只有一个路由器，也就只有一张路由表 每个VPC可以容纳的云产品数量最多为15000个 路由器不支持动态路由协议 交换机不支持二层的广播个组播 VPC应用场景此部分介绍了专有网络常用的使用场景和架构： 场景一：本地数据中心+云上业务的混合云模式 场景二：多租户的安全隔离 场景三：主动访问公网的抓取类业务 场景四：多个应用流量波动大——共享带宽包 场景一：本地数据中心+云上对外业务的混合云模式如果您有以下业务需求，建议您使用VPC+高速通道+ECS+RDS的配置架构。 将内部核心系统与核心数据放置在自建数据中心以确保核心数据的安全； 云上部署对外客户的应用系统，实时应对业务访问量激增。 架构解读： 使用VPC、RDS、ECS搭建云上业务系统，核心数据部署在云下自建数据中心，使用高速通道专线接入保证云上数据快速同步，实现云上云下数据互通，搭建一个混合云使用环境。 场景二：多租户的安全隔离如果您有以下业务需求，建议使用VPC+ECS+RDS+SLB的配置架构。 希望在云上构建一个完全隔离的业务环境，因为传统云架构的多租户共享机制不能保证数据安全； 自主定义私有网络配置。 架构解读：您可以在阿里云上创建一个专有网络，和其他租户的网络完全隔离。您可以完全掌控自己的虚拟网络，例如选择自己的IP地址范围、划分网段、配置路由表和网关等，从而实现安全而轻松的资源访问和应用程序访问。此外，您也可以通过专线或VPN等连接方式将您的专有网络与传统数据中心相连，形成一个按需定制的网络环境，实现应用的平滑迁移上云和对数据中心的扩展。 场景三：主动访问公网的抓取类业务如果您有以下业务需求，建议使用VPC+ECS+NAT网关的配置架构。 专有网络中的多个服务器可以主动访问互联网； 避免这些服务器的公网IP暴露在公网上。 架构解读： 您可以对专有网络中的同一虚拟交换机下的所有ECS做SNAT配置，多台ECS通过同一公网IP访问互联网，并可随时进行公网IP替换，避免被外界攻击。 场景四：多个应用流量波动大——共享带宽包如果您有以下需求，建设使用VPC+NAT网关+ECS的配置架构。 系统中同时存在多个面向互联网的应用； 各个应用都需要对外提供服务，并且其波峰时间点不一致。 架构解读：您可以购买多个专有网络类型的ECS，分别承载不同的应用业务，前端挂载NAT网关，通过配置DNAT IP转发规则实现多IP共享带宽功能，减轻波峰波谷效应，从而减少您的成本。 NAT网关SNAT：在NAT网关上操作，接受到后端ECS主机发送来的数据包时，将源IP地址，修改为自身的IP地址，堆外隐藏内部的信息，是内部主动对外的 DNAT：在NAT网关上操作，接受到internet发送过来的数据包时，将目的IP地址进行修改（这个时候是NAT网关的地址），修改成为内部ECS主机的IP地址。用于后端多套系统共享带宽。 S：源是NAT网关 D:目的地是NAT网关 入门实践搭建专有网络本教程将指引您搭建一个专有网络，并为专有网络中的ECS实例绑定一个弹性公网IP（EIP）进行公网访问。 参考文献: 官方帮助文档-搭建专有网络 步骤1 创建专有网络和交换机在专有网络中部署云资源，您必须至少创建一台交换机。完成以下操作步骤，创建专有网络和交换机： 登录专有网络管理控制台。 在顶部菜单栏，选择专有网络的地域。 注意：专有网络的地域和要部署的云资源的地域必须相同，本操作选择华北2（也就是北京地域）。 单击创建专有网络，根据以下信息配置专有网络和交换机，然后单击确定。 配置对照表： 创建如下图所示： 注意： 每个交换机的第一个和最后三个IP地址为系统保留地址。 例如/20的掩码，可以使用的IP范围应该是4094个（2^12-2=4096-2=4094），但是实际平台上显示可分配使用IP为4092个，因为有4个已经被系统占用（以172.16.0.0/20为例，172.176.0.0、172.31.255.253、172.31.255.254和172.31.255.255这些地址是系统保留地址。） 步骤二 创建ECS实例完成以下操作，在已创建的VPC中创建一个ECS实例： 在专有网络控制台的左侧导航栏，单击交换机。 选择交换机的地域，本操作选择华北1。 找到已创建的交换机，然后单击购买 &gt; ECS实例。 配置ECS实例后，单击立即购买。 本操作中ECS实例的网络配置如下： 网络：选择已创建的专有网络和交换机。 公网IP地址：选择不分配。 返回ECS管理控制台，查看已创建的ECS实例。 步骤三 创建EIP弹性公网IP（EIP）是可以独立购买和持有的公网IP地址资源。完成以下操作，创建EIP： 在专有网络控制台的左侧导航栏，单击弹性公网IP。 选择EIP的地域，然后单击申请弹性公网IP。本操作，选择华北1。 配置EIP，完成支付。 步骤四 绑定EIP完成以下操作，将EIP绑定到已创建的ECS实例上： 在专有网络控制台的左侧导航栏，单击弹性公网IP。 选择EIP的地域。 找到已创建的EIP，然后单击绑定。 在弹出的对话框中，实例类型选择ECS实例，然后选择已创建的ECS实例。 单击确定。 步骤五 公网访问测试以上4和步骤均省略，可以自行查看文档 ECS安全组配置这部分内容见官方文档： 官方文档1 官方文档2 交换机交换机（VSwitch）是组成专有网络的基础网络设备，用来连接不同的云产品实例。创建专有网络之后，您可以通过创建交换机为专有网络划分一个或多个子网，同一专有网络内的不同交换机之间内网互通。 您可以将应用部署在不同可用区的交换机内，提高应用的可用性。 交换机不支持组播和广播。您可以通过阿里云提供的组播代理工具实现组播代理。详情参见组播代理概述。 交换机的网段在创建交换机时，您需要以CIDR地址块的形式指定交换机的私网网段，交换机的网段限制如下： 交换机的网段可以和其所属的VPC网段相同或者是其VPC网段的子集。 例如，VPC的网段是192.168.0.0/16，那么该VPC内的交换机的网段可以是192.168.0.0/16，也可以是192.168.0.0/17，一直到192.168.0.0/29。 说明：如果您的交换机网段和所属VPC网段相同，您只能在该VPC下创建一台交换机。 交换机的网段的大小在16位网络掩码与29位网络掩码之间，可提供8-65536个地址。 每个交换机的第一个和最后三个IP地址为系统保留地址。 以192.168.1.0/24为例，192.168.1.0、 192.168.1.253、 192.168.1.254和192.168.1.255这些地址是系统保留地址。 交换机网段的确定还需要考虑该交换机下容纳的主机的数量。 如果该交换机有和其他专有网络的交换机，或本地数据中心通信的需求，确保交换机的网段和要通信的网段不冲突。 默认的专有网络和交换机当创建一个云产品实例时，如果您没有提前创建专有网络和交换机，您可以使用系统提供的默认专有网络配置。在实例创建后，一个默认的专有网络和交换机也会随之创建好。 说明：每个地域只有一个默认专有网络，但每个专有网络内的每个可用区都可创建一个默认交换机。 默认专有网和交换机的配置如下表所示。 路由表和路由条目创建专有网络时，系统会为该专有网络自动创建一个路由器和一张路由表。 路由表中的每一项是一条路由条目。路由条目指定了网络流量的导向目的地，由目标网段、下一跳类型、下一跳三部分组成。路由条目包括系统路由和自定义路由。 您不可以直接删除专有网络的路由器或路由表，但可以在路由表中添加自定义路由条目转发流量。删除VPC后，关联的路由器和路由表也会随之删除。 系统路由创建VPC时，系统会自动添加一条目标网段为100.64.0.0/10的系统路由用于VPC内的云产品通信。另外，阿里云也会为交换机自动添加一条以交换机网段为目标网段的系统路由。 我们不可以创建，也不能删除系统路由。 比如我创建了一个网段为192.168.0.0/16的专有网络，并在该专有网络下创建了两个网段为192.168.1.0/24和192.168.0.0/24的交换机，则该专有网络的路由表中会有如下三条系统路由： 目标网段 下一跳类型 下一跳 类型 100.64.0.0/10 - - 系统 192.168.1.0/24 - - 系统 192.168.0.0/24 - - 系统 自定义路由您可以根据需要，添加自定义路由。针对不同的功能，专有网络提供如下下一跳类型的路由： ECS实例：将指向目标网段的流量转发到专有网络内的一台ECS实例上。 当需要通过该ECS实例部署的应用访问互联网或其他应用时，配置此类型的路由。【一般是网关管理类应用服务器】 适用于将指定网络访问路由至ECS实例进行流量统一转发和管理的场景，例如将一台ECS实例配置为公网网关管理其他ECS实例访问公网。 VPN网关：将指向目标网段的流量转发到一个VPN网关上。 当需要通过VPN网关连接本地网络或者其他专有网络时，配置此类型的路由。 专有网络：将来指向标网段的流量转发到一个专有网络内。 当需要使用高速通道连接两个专有网络时，配置此类型的路由。 边界路由器：将指向目标网段的流量转发到一个边界路由器上。 当需要使用高速通道连接本地网络（物理专线接入）时，才需要配置此类型的路由。 选路规则路由表采用最长前缀匹配原则作为流量的路由选路规则。最长前缀匹配是指当路由表中有多条条目可以匹配目的IP时，采用掩码最长（最精确）的一条路由作为匹配项并确定下一跳。 路由实例VPC内网路由当您在VPC内的一台ECS实例（ECS01）自建了NAT网关或绑定了弹性公网IP，您需要专有网络内的云资源通过该ECS实例访问公网时（实际上是被外部访问），可以添加如下一条自定义路由： 目标网段 下一跳类型 下一跳 0.0.0.0/0 ECS实例 ECS01 VPC互连VPC之间的互连又可以分为两种情况，一种是直接使用高速通道进行连接，一种是使用VPN网关进行连接。 第一种：使用高速通道进行连接 当使用高速通道连接两个VPC（VPC1 172.16.0.0/12和VPC2 192.168.0.0/16）时，创建完两个互相连接的路由器接口后，您还需要在两个VPC中分别添加如下一条路由： VPC1的路由配置: 目标网段 下一跳类型 下一跳 192.168.0.0/16 路由器接口（专有网络方向） VPC2的路由配置: 目标网段 下一跳类型 下一跳 172.16.0.0/12 路由器接口（专有网络方向） 第二种：使用VPN网关进行连接 如下图所示，当使用VPN网关连接两个VPC（VPC1 172.16.0.0/12和VPC2 10.0.0.0/8）时，配置完VPN网关后，需要在VPC中分别添加如下路由： VPC1的路由配置 目标网段 下一跳类型 下一跳 10.0.0.0/8 VPN网关 VPN网关1 VPC2的路由配置 目标网段 下一跳类型 下一跳 172.16.0.0/12 VPN网关 VPN网关2 VPC连接本地网络-（自建机房或者IDC）VPC连接本地网络又可以分为两种情况，一种是直接使用高速通道进行连接，一种是使用VPN网关进行连接。 第一种：使用高速通道连接本地网络 如下图所示，当使用高速通道物理专线连接专有网络和本地网络时，配置完专线和边界路由器后，需要配置如下路由： VPC端的路由配置 目标网段 下一跳类型 下一跳 192.168.0.0/16 路由器接口（普通路由） RI1 边界路由器端的配置 目标网段 下一跳类型 下一跳 192.168.0.0/16 指向专线 RI3 172.16.0.0/12 指向VPC RI2 本地网络的路由配置 目标网段 下一跳类型 下一跳 172.16.0.0/12 — 本地网关设备 第二种：使用VPN网关连接本地网络 如下图所示，当使用VPN网关连接VPC（网段：172.16.0.0/12）和本地网络（网段：192.168.0.0/16）时，配置好VPN网关后，需要在VPC内添加如下一条路由： 目标网段 下一跳类型 下一跳 192.168.0.0/16 VPN网关 已创建的VPN网关 添加自定义路由步骤如下所示： 登录专有网络管理控制台。 在左侧导航栏，单击路由表。 选择路由表所属的VPC的地域，然后单击目标路由表的ID链接。 单击添加路由条目。 在弹出的对话框，配置路由条目： 最佳实践网络规划在进行网络规划的时候，有几个问题需要去解决： 问题1：应该使用几个VPC？ 问题2：应该使用几个交换机？ 问题3：应该选择什么网段？ 问题4：VPC与VPC互通或者与线下IDC互通时，如何规划网段？ 问题1：应该使用几个VPC？单个VPC： 如果没有多地域部署系统的要求且各系统之间也不需要通过VPC进行隔离，那么推荐使用一个VPC。 目前，单个VPC内运行的云产品实例可达15000个，这样的容量基本上可以满足需求。 ps：可用区是指在同一地域内，电力和网络互相独立的物理区域，在同一地域内可用区与可用区之间内网互通。 单个VPC的拓扑如下所示： 多个VPC： 有以下需求时，那么建议使用多个VPC 多地域部署系统 VPC是地域级别的资源，是不能跨地域部署的。当您有多地域部署系统的需求时，就必然需要使用多个VPC。基于阿里巴巴骨干网构建的高速通道产品能轻松实现跨地域，跨国VPC间的互通。详情参考高速通道VPC互通。 多业务系统隔离 如果在一个地域的多个业务系统需要通过VPC进行严格隔离，比如生产环境和测试环境，那么也需要使用多个VPC，如下图所示。 问题2：应该使用几个交换机？首先，即使只使用一个VPC，也尽量使用至少两个交换机，并且将两个交换机分布在不同可用区，这样可以实现跨可用区容灾。 同一地域不同可用区之间的网络通信延迟很小，但也需要经过业务系统的适配和验证。由于系统调用复杂加上系统处理时间、跨可用区调用等原因可能产生期望之外的网络延迟。建议您进行系统优化和适配，在高可用和低延迟之间找到平衡。 其次，使用多少个交换机还和系统规模、系统规划有关。如果前端系统可以被公网访问并且有主动访问公网的需求，考虑到容灾可以将不同的前端系统部署在不同的交换机下，将后端系统部署在另外的交换机下。 问题3：应该选择什么网段？在创建VPC和交换机时，您必须以无类域间路由块 (CIDR block) 的形式为您的专有网络划分私网网段。 VPC网段规划 网段范围 网段 可用IP地址数量 备注 192.168.0.0/16 65532 去除系统占用地址 172.16.0.0/12 1048572 去除系统占用地址 10.0.0.0/8 16777212 去除系统占用地址 注意：如果有除此之外的特殊网段要求，也可以提工单或者通过客户经理申请开通。 如果有多个VPC，或者有VPC和线下IDC构建混合云的需求，建议使用上面这些标准网段的子网作为VPC的网段，掩码建议不超过16位。 如果云上只有一个VPC并且不需要和本地IDC互通时，可以选择上表中的任何一个网段或其子网。 VPC网段的选择还需要考虑到是否使用了经典网络。如果您使用了经典网络，并且计划将经典网络的ECS实例和VPC网络连通，那么，建议您选择非10.0.0.0/8作为VPC的网段，因为经典网络的网段也是10.0.0.0/8。 交换机网段 可以根据以下建议规划交换机网段。同样，交换机创建成功后，网段无法再修改。 交换机的网段的大小在16位网络掩码与29位网络掩码之间，可提供8-65536个地址。16位掩码能支持65532个ECS实例，而小于29位掩码又太小，没有意义。 交换机的网段可以和其所属的VPC网段相同，或者是其VPC网段的子网。比如VPC的网段是192.168.0.0/16，那么该VPC下的虚拟交换机的网段可以是192.168.0.0/16，也可以是192.168.0.0/17一直到192.168.0.0/29。 如果交换机网段和所属VPC网段相同，您在该VPC下只能创建一台交换机。 每个交换机的第一个和最后三个IP地址为系统保留地址。以192.168.1.0/24为例，192.168.1.0、 192.168.1.253、192.168.1.254和192.168.1.255这些地址是系统保留地址。 ClassicLink功能允许经典网络的ECS和192.168.0.0/16，10.0.0.0/8，172.16.0.0/12这三个VPC网段的ECS通信。例如，如果要和经典网络通信的VPC网段是10.0.0.0/8，则要和经典网络ECS通信的交换机的网段必须是10.111.0.0/16。详情参考ClassicLink。 交换机网段的确定还需要考虑该交换机下容纳ECS的数量。 问题4：VPC与VPC互通或者与本地数据中心互通时，如何规划网段？如下图所示，比如您在华东1、华北2、华南1三个地域分别有VPC1、VPC2和VPC3三个VPC。VPC1和VPC2通过高速通道内网互通，VPC3目前没有和其他VPC通信的需求，将来可能需要和VPC2通信。另外，您在上海还有一个自建IDC，需要通过高速通道（专线功能）和华东1的VPC1私网互通。 此例中VPC1和VPC2使用了不同的网段，而VPC3暂时没有和其他VPC互通的需求，所以VPC3的网段和VPC2的网段相同。但考虑到将来VPC2和VPC3之间有私网互通的需求，所以两个VPC中的交换机的网段都不相同。VPC互通要求互通的交换机的网段不能一样，但VPC的网段可以一样。 在多VPC的情况下，建议遵循如下网段规划原则： 尽可能做到不同VPC的网段不同，不同VPC可以使用标准网段的子网来增加VPC可用的网段数。 如果不能做到不同VPC的网段不同，则尽量保证不同VPC的交换机网段不同。 如果也不能做到交换机网段不同，则保证要通信的交换机网段不同。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>公有云产品</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>VPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IT常用英语记录]]></title>
    <url>%2F2018%2F04%2F16%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FIT%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2FIT%E8%8B%B1%E8%AF%AD%2FIT%E5%B8%B8%E7%94%A8%E8%8B%B1%E8%AF%AD%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[术语对照表 单词短语 释义 CURD CRUD (创建：Create， 读取：Read，更新：Update，删除： Delete) 是对于存储的信息可以进行操作的同义词。是一个对四种操作持久化信息的基本操作的助记符。CRUD 通常是指适用于存于数据库或数据存储器上的信息的操作，不过也可以应用在高层级的应用操作，例如通过在设置状态字段并标记删除的而并非移除数据的伪删除。 SDK RDS Remote Data Services。远程数据服务。云数据库 ECS 云服务器 ECS（Elastic Compute Service）是一种弹性可伸缩的计算服务 VPC 专有网络VPC（Virtual Private Cloud）是基于阿里云构建的一个隔离的网络环境，专有网络之间逻辑上彻底隔离。 PM Product Manager，产品经理 RD Research and Development 研究与开发 QA Qualtiy Assurance 品质保证。QA的主要职责就是质量保证工作。 OP Operator，操作员，管理员。 href hypertext reference 超文本连接 DML 数据变更是指DML(包含insert/delete/update) DDL 结构变更是指DDL(例如：create/drop/alter table) pki 公钥基础设施（Public Key Infrastructure） PV page view TPS transactions per second 每秒传输的事物处理个数，即服务器每秒处理的事务数 QPS queries per second RPS requests per second RPS 并发数/平均响应时间]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>IT基础知识</category>
        <category>IT英语</category>
      </categories>
      <tags>
        <tag>IT常用英语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Http Server 网络处理模型的进化之路]]></title>
    <url>%2F2018%2F04%2F02%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E5%8F%8A%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%2FHTTP%2FHttp-Server-%E7%BD%91%E7%BB%9C%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96%E4%B9%8B%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[缘起我刚毕业那会儿，国家还是包分配工作的，我的死党小明被分配到了一个叫数据库的大城市，天天都可以坐在高端大气上档次的机房里，在那里专门执行 SQL查询优化，工作稳定又舒适； 隔壁宿舍的小白被送到了编译器镇，在那里专门把 C 源文件编译成 EXE 程序，虽然累，但是技术含量非常高，工资高，假期多。 我成绩不太好，典型的差生，四级补考了两次才过，被发配到了一个不知道什么名字的村庄，据说要处理什么 HTTP请求，这个村庄其实就是一个破旧的电脑，令我欣慰的是可以上网，时不时能和死党们通个信什么的。 不过辅导员说了，我们都有光明的前途。 Http Server 1.0HTTP是个新鲜的事物，能够激起我一点点工作的兴趣，不至于沉沦下去。 一上班，操作系统老大扔给我一大堆文档： “这是 HTTP协议， 两天看完！” 我这样的英文水平，这几十页的英文 HTTP协议我不吃不喝不睡两天也看不完， 死猪不怕开水烫，慢慢磨吧。 两个星期以后，我终于大概明白了这 HTTP是怎么回事：无非是有些电脑上的浏览器向我这个破电脑发送一个预先定义好的文本（Http request）, 然后我这边处理一下（通常是从硬盘上取一个后缀名是 html的文件，然后再把这个文件通过文本方式发回去（http response），就这么简单。 唯一麻烦的实现，我得请操作系统给我建立 Http 层下面的 TCP 连接通道， 因为所有的文本数据都得通过这些 TCP通道接收和发送，这个通道是用 socket建立的。 弄明白了原理，我很快就搞出了第一版程序，这个程序长这个样子： 看看， 这些 socket, bind, listen , accept… 都是操作系统老大提供的接口， 我能做的也就是把他们组装起来：先在 80端口监听，然后进入无限循环，如果有连接请求来了，就接受 (accept)，创建新的 socket，最后才可以通过这个 socket来接收，发送 http 数据。 老大给我的程序起了个名称，Http Server 版本 1.0 。 这个名字听起来挺高端的，我喜欢。 我兴冲冲的拿来实验，程序启动了，在 80端口“蹲守”，过了一会儿就有连接请求了， 赶紧 Accept ,建立新的 socket ，成功 ！接下来就需要从 socket 中读取 Http Request 了。 可是这个 receive 调用好慢，我足足等了 100 毫秒还没有响应！我被阻塞 (block) 住了！ 操作系统老大说：“别急啊，我也在等着从网卡那里读数据，读完以后就会复制给你。” 我乐的清闲，可以休息一下。 可是操作系统老大说：“别介啊，后边还有很多浏览器要发起连接，你不能在这儿歇着啊。” 我说不歇着怎么办？receive调用在你这里阻塞着，我除了加入阻塞队列，让出 CPU 让别人用还能干什么？ 老大说： “唉，大学里没听说过多进程吗？你现在很明显是单进程，一旦阻塞就完蛋了，想办法用下多进程，每个进程处理一个请求！” 老大教训的是，我忘了多进程并发编程了。 Http Server 2.0 ：多进程多进程的思路非常简单，当 accept连接以后，对于这个新的 socket ，不在主进程里处理，而是新创建子进程来接管。这样主进程就不会阻塞在 receive 上，可以继续接受新的连接了。 我改写了代码，把 Http server 升级为 V2.0，这次运行顺畅了很多，能并发的处理很多连接了。 这个时候 Web 刚刚兴起，我这个 Http Server 访问的人还不多，每分钟也就那么几十个连接发过来，我轻松应对。 由于是新鲜事物，我还有资本给搞数据库的小明和做编译的小白吹吹牛，告诉他们我可是网络高手。 没过几年，Web迅速发展，我所在的破旧机器也不行了，换成了一个性能强悍的服务器，也搬到了四季如春的机房里。 现在每秒中都有上百个连接请求了，有些连接持续的时间还相当的长，所以我经常得创建成百上千的进程来处理他们，每个进程都得耗费大量的系统资源，很明显操作系统老大已经不堪重负了。 他说：“咱们不能这么干了，这么多进程，光是做进程切换就把我累死了。” “要不对每个 Socket 连接我不用进程了，使用线程？ ” “可能好一点，但我还是得切换线程啊，你想想办法限制一下数量吧。” 我怎么限制？我只能说同一时刻，我只能支持 x个连接，其他的连接只能排队等待了。 这肯定不是一个好的办法。 Http Server 3.0 : Select模型老大说：“我们仔细合计合计，对我来说，一个 Socket连接就是一个所谓的文件描述符（File Descriptor ,简称 fd , 是个整数），这个 fd 背后是一个简单的数据结构，但是我们用了一个非常重量级的东西 – 进程 –来表示对它的读写操作，有点浪费啊。” 我说：“要不咱们还切换回单进程模型？但是又会回到老路上去，一个 receive 的阻塞就什么事都干不了了。” “单进程也不是不可以，但是我们要改变一下工作方式。” “改成什么？” 我想不透老大在卖什么关子。 “你想想你阻塞的本质原因，还不是因为人家浏览器还没有把数据发过来，我自然也没法给你，而你又迫不及待的想去读，我只好把你阻塞。在单进程情况下，一阻塞，别的事儿都干不了。“ “对，就是这样” “所以你接受了客户端连接以后，不能那么着急的去读，咱们这么办，你的每个 socket fd 都有编号，你把这些编号告诉我，就可以阻塞休息了 。” 我问道：“这不和以前一样吗？原来是调用 receive 时阻塞，现在还是阻塞。” “听我说完，我会在后台检查这些编号的 socket，如果发现这些 socket 可以读写，我会把对应的 socket 做个标记，把你唤醒去处理这些 socket 的数据，你处理完了，再把你的那些 socket fd 告诉我，再次进入阻塞，如此循环往复。” 我有点明白了：“ 这是我们俩的一种通信方式，我告诉你我要等待什么东西，然后阻塞，如果事件发生了，你就把我唤醒，让我做事情。” “对，关键点是你等我的通知，我把你从阻塞状态唤醒后，你一定要去遍历一遍所有的 socket fd，看看谁有标记，有标记的做相应处理。我把这种方式叫做 select 。” 我用 select 的方式改写了 Http server，抛弃了一个 socket 请求对于一个进程的模式，现在我用一个进程就可以处理所有的 socket了。 Http Server4.0 : epoll这种称为 select 的方式运行了一段时间，效果还不错，我只管把 socket fd 告诉老大，然后等着他通知我就行了。 有一次我无意中问老大：“我每次最多可以告诉你多少个 socket fd？” “1024个。” “那就是说我一个进程最多只能监控 1024 个 socket 了？ ” “是的，你可以考虑多用几个进程啊！” 这倒是一个办法，不过”select”的方式用的多了，我就发现了弊端，最大的问题就是我从阻塞中恢复以后，需要遍历这 1000 多个 socket fd，看看有没有标志位需要处理。 实际的情况是， 很多 socket 并不活跃， 在一段时间内浏览器并没有数据发过来， 这 1000 多个 socket 可能只有那么几十个需要真正的处理，但是我不得不查看所有的 socket fd，这挺烦人的。 难道老大不能把那些发生了变化的 socket 告诉我吗？ 我把这个想法给老大说了下，他说：“嗯，现在访问量越来越大， select 方式已经不满足要求，我们需要与时俱进了，我想了一个新的方式，叫做 epoll。” “看到没有，使用 epoll 和 select 其实类似“ 老大接着说 ：”不同的地方是第 3 步和第 4 步，我只会告诉你那些可以读写的 socket , 你呢只需要处理这些 ‘ready’ 的 socket 就可以了“ “看来老大想的很周全， 这种方式对我来说就简单的多了。 ” 我用 epoll 把 Http Server 再次升级，由于不需要遍历全部集合，只需要处理哪些有变化的，活跃的 socket 文件描述符，系统的处理能力有了飞跃的提升。 我的 Http Server 受到了广泛的欢迎，全世界有无数人在使用，最后死党数据库小明也知道了，他问我：“ 大家都说你能轻松的支持好几万的并发连接， 真是这样吗？ ” 我谦虚的说：“过奖，其实还得做系统的优化啦。” 他说：“厉害啊，你小子走了狗屎运了啊。” 我回答： “毕业那会儿辅导员不是说过吗， 每个人都有光明的前途。”]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识及网络服务</category>
        <category>网络知识</category>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>epoll模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[终端常用快捷键]]></title>
    <url>%2F2018%2F04%2F01%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FLinux%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F%E7%BB%88%E7%AB%AF%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F%E7%BB%88%E7%AB%AF%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[常用快捷键Tab键： 命令、文件名等自动补全功能。 Ctrl+a： 光标回到命令行首。 （a：ahead） Ctrl+e： 光标回到命令行尾。 （e：end） Ctrl+b： 光标向行首移动一个字符。 （b：backwards） Ctrl+f： 光标向行尾移动一个字符。 （f：forwards） Ctrl+w： 删除光标处到行首的字符，也就是删除光标前面的所有内容。 Ctrl+k： 删除光标处到行尾的字符，也就是删除光标后面的所有内容。 Ctrl+u： 删除整个命令行文本字符，删除整行命令。 Ctrl+h： 向行首删除一个字符，向前删除一个字符，相当于Backspace。 Ctrl+d： 向行尾删除一个字符，向后删除一个字符，相当于Delete。 Ctrl+y： 粘贴Ctrl+u，Ctrl+k，Ctrl+w删除的文本。 Ctrl+p： 上一个使用的历史命令。 （p：previous） Ctrl+n： 下一个使用的历史命令。（n：next ） Ctrl+t： 交换光标所在字符和其前的字符。 Ctrl+i： 相当于Tab键。 Shift+Insert： 粘贴鼠标所复制的内容 Ctrl+d: 在空命令行的情况下可以退出终端。 Shift+c： 删除之后的所有内容并进入编辑模式 Ctrl+c： 中断终端中正在执行的任务。 Ctrl+z： 使正在运行在终端的任务，运行于后台。 （可用fg恢复到前台） 非常用快捷键Ctrl+s： 使终端发呆，静止，可以使快速输出的终端屏幕停下来。 Ctrl+q： 退出Ctrl+s引起的发呆。 Ctrl+[： 相当于Esc键。 Esc键： 连续按3次显示所有的支持的终端命令，相当于Tab键。 Ctrl+r： 快速检索历史命令。（r：retrieve）。 Ctrl+o： =Ctrl+m：相当Enter键。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>Linux基础知识</category>
        <category>终端常用快捷键</category>
      </categories>
      <tags>
        <tag>终端常用快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux运维工程师面试常见问题]]></title>
    <url>%2F2018%2F03%2F28%2F%E4%B8%AA%E4%BA%BA%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F%E8%81%8C%E5%9C%BA%2FLinux%E8%BF%90%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%2FLinux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[redhat、centos、suse、ubuntu等发行版本的区别这些发行版本本质上没有什么区别，都是类unix系统。redhat的系统本身是免费的，但是它的服务和一些特定的组件是收费的而centos是redhat社区版本，国内使用较多，社区相当活跃。SUSE也是分为两种，一种企业版本的SLES，一种是opensuse，sles主要是一些金融国企在使用，安全性较好，opensuse是社区版本，suse使用起来和centos相差不多，软件包形式使用源自redhat的rpm，但是管理工具使用的是zypper。ubuntu源自于debain，国外使用的较多，也是相当热门的一个发型版本，在桌面领域有绝对技术优势，适合开发人员使用。 OSI7层模型和TCP/IP模型的区别联系OSI7层模型OSI 7层模式主要是由国际标准化组织（ISO）创建的，是一个国际通用的标准，它被开发出作为一个参照标准，用于指导如何设计网络通信系统。说的简单一点就是统一网络设备商的协议标准，实现多网络设备商环境（主机，路由器，交换机等等都是网络设置，都要遵循同一套的通信标准）它一共分为7层，每一层在网络通信数据传输过程中都定义了不同的功能。 OSI7层模型主要分为(从下到上)：物理层，数据链路层，网络层，传输层，会话层，表示层，应用层。每一层说明： layer function Application data flow；离用户最近的一层，它使一个网络应用能和另一个网络应用相互通信 Presentation 定义数据格式；数据压缩、加解密等。 Session 定义如何建立和终止连接，是告诉4层怎么做，4层只管被动的接受然后去做。 Transport 数据段；将上层的数据流进行分段；建立和终止网络连接；常用于流量控制和数据恢复 Network 数据包；使用IP地址在Internet上唯一确定一台设备；定义设备路由，寻址 Data link 数据帧；将上层数据封装成数据帧，其中包含源目MAC地址以及帧校验字段（用于检测传输错误）；它包含2个子层（LLC和MAC） Physical 比特流；定义了比特流如何在两台设备之间流通；主要涉及线缆，网卡， 下面是每一层常见的对应协议 layer protocol Application HTTP,FTP,Telnet,SMTP,SNMP Presentation MIME,TIFF,GIF,JPEG,PICT,ASCII,EBCDIC,encryption,MPEG,MIDI,HTML Session SSl/TLS,NetBIOS,RPC Transport TCP,UDP Network IP,ICMP,ARP,RARP Data link PPP,HDLC,IEEE 802.3/802.2,FDDI,ATM,IEEE 802.5/802.2 Physical Ethernet TCP/IP协议族TCP/IP模型类似OSI模型，作用也是描述一套指导标准，实现网络设备之间的通信，它被设计成4层 Application Transport Internet Network Access 其对应关系为： TCP/IP model OSI model Application Application Presentation Session Transport Transport Internet Network Network Access Data link Physical 区别除了层数的区别之外，它们之间最大的区别就是： OSI模型规定了在一个网络上传输数据所需要的步骤，并且它是非常具体的，定义了每一层使用什么协议以及如何使用；而TCP/IP模型是不特定的。另外一个区别就是，目前TCP/IP是所有网络设备上既定的协议事实，一般分析问题使用OSI模型。 路由，交换技术的基本原理路由技术我们在这里说的路由技术一般是指，路由转发。主要涉及设备为路由器或者三层交换机。这些设备上会维护一张路由表，其中的信息可以是通过动态路由协议（例如OSPF，EIGRP，ISIS，RIP，BGP，静态路由，默认路由等）获取组成路由表的内容是：出口接口和对应网段 路由设备接收到一个数据包之后，会解封装，获取其中的目的IP地址信息（网段信息），然后查找路由表，选择最优路由去转发。路由设备上也会有一张ARP表，根据广播域 交换技术在一个局域网内，也就是一个广播域内使用的技术。通常会涉及到的设备就是交换机。交换机上会维护一张MAC地址转发表，其中的信息是MAC地址和端口的映射关系。交换机根据数据帧中的目的MAC地址进行数据包的转发 注意：在一个广播域内的数据流动是依靠二层MAC来实现的，因为在第一次会涉及到ARP，有了记录之后，交换机会记录他的MAC地址表，后续的速度就会非常快 PS：有关网络模型和路由技术可以结合一个小案例在白板上演示一下，效果会更好，也就是将上述两部分的内容有机的整合成为一个整体 脚本部分运维知识优化运维思想运维核心是什么稳定性-网站/平台不宕机【核心】集群 负载均衡 高可用 解耦，微服务 数据不丢失【核心】数据备份，异地容灾 避免人为错误 避免人为错误，主要分为两个方面，一个是开发不严谨产生的错误，这部分通过流程可以控制。【比如测试不严谨，开发人员的代码有问题，直接把服务器资源跑没了】 一个是自己操作产生的问题，这部分通过一些智能化的自动化工具来尽量避免【避免在执行命令的时候误操作等等】 解决：建立完善的流程制度，对运维来说，包括标准化，对开发测试来说，包括上线的流程化 运维效率管理平台运维脚本化，工具化，自动化，人工智能化 如何做好运维工作运维职业规划你为什么离职你对加班的看法个人最大的优点和缺点问题应答0. 自我介绍面试官 早上/下午好 我叫汪小华 大学就读于晋中学院 网络工程专业。目前一共有三年工作经验， 实习和第一份工作都是在亿阳信通在这期间从最基础的桌面运维干起，一直到最后独立接手负责了一个项目从0到1的这么一个整体过程，有很多的收获。这一期间对如何做好运维工作有了一些的感悟。 第二份工作是被内推到创世漫道，主要负责公司linux平台的调整，主要和我对接的是公司的架构师，因此在这个过程中对运维思想这方面有了很大的提升。 有关这两部分具体的内容我会在后面和您聊如何通过运维思想做好运维工作时谈及 我的优点是有一定的网络基础，平时喜欢问为什么，和同事交流谈论的时候喜欢拿纸笔写写画画。最强的技能部分应该是进程管理，这一点我觉得对运维工作来说，至关重要，关于这部分稍后我们可以一块交流探讨。 我的缺点目前是如何将所学知识有机的结合成为一个整体，构成一张知识之网这种能力还不够，这一点也是在后续的工作中需要去刻意修炼的。 以上是我的自我介绍，今天我要应聘的岗位是Linux运维工程师，谢谢！ 1. 发行版本区别及shell、python这些发行版本本质上没有什么区别，都是类unix系统。redhat的系统是免费的，但是他服务是收费的，并且有一些类似RHCS等服务只有收费版才支持。而centos是redhat社区版本，国内使用较多，社区相当活跃。SUSE也是分为两种，一种企业版本的SLES，一种是opensuse，sles主要是一些金融国企在使用，安全性较好，opensuse是社区版本，suse使用起来和centos相差不多，软件包形式使用源自redhat的rpm，但是管理工具使用的是zypper。ubuntu源自于debain，国外使用的较多，也是相当热门的一个发型版本，在桌面领域有绝对技术优势，适合开发人员使用。 1**有关shell的问题，做面试题，看abs【每天看一点】。** python目前正在学习，目前基础部分已经学完了，正在学习django项目，学完之后，要花钱买一套马哥或者老男孩的python视频来补充，预计下半年能够做项目。 2. OSI模型、TCP/IP部分；路由交换基本原理OSI7层模型OSI 7层模式主要是由国际标准化组织（ISO）创建的，是一个国际通用的标准，它被开发出作为一个参照标准，用于指导如何设计网络通信系统。说的简单一点就是统一网络设备商的协议标准，实现不同网络设备和谐共存的环境（主机，路由器，交换机等等都是网络设置，都要遵循同一套的通信标准）它一共分为7层，每一层在网络通信数据传输过程中都定义了不同的功能。 OSI7层模型主要分为(从下到上)：物理层，数据链路层，网络层，传输层，会话层，表示层，应用层。每一层说明： layer function Application data flow；离用户最近的一层，它使一个网络应用能和另一个网络应用相互通信 Presentation 定义数据格式；数据压缩、加解密等。 Session 定义如何建立和终止连接，是告诉4层怎么做，4层只管被动的接受然后去做。 Transport 数据段；将上层的数据流进行分段；建立和终止网络连接；常用于流量控制和数据恢复 Network 数据包；使用IP地址在Internet上唯一确定一台设备；定义设备路由，寻址 Data link 数据帧；将上层数据封装成数据帧，其中包含源目MAC地址以及帧校验字段（用于检测传输错误）；它包含2个子层（LLC和MAC） Physical 比特流；定义了比特流如何在两台设备之间流通；主要涉及线缆，网卡， 下面是每一层常见的对应协议 layer protocol Application HTTP,FTP,Telnet,SMTP,SNMP Presentation MIME,TIFF,GIF,JPEG,PICT,ASCII,EBCDIC,encryption,MPEG,MIDI,HTML Session SSl/TLS,NetBIOS,RPC Transport TCP,UDP Network IP,ICMP,ARP,RARP Data link PPP,HDLC,IEEE 802.3/802.2,FDDI,ATM,IEEE 802.5/802.2 Physical Ethernet TCP/IP协议族TCP/IP模型类似OSI模型，作用也是描述一套指导标准，实现网络设备之间的通信，它被设计成4层 1234ApplicationTransportInternetNetwork Access 其对应关系为： TCP/IP model OSI model Application Application Presentation Session Transport Transport Internet Network Network Access Data link Physical OSI和TCP/IP的区别除了层数的区别之外，它们之间最大的区别就是： OSI模型规定了在一个网络上传输数据所需要的步骤，并且它是非常具体的，定义了每一层使用什么协议以及如何使用；而TCP/IP模型是不特定的。另外一个区别就是，目前TCP/IP是所有网络设备上既定的协议事实，一般分析问题使用OSI模型。 路由技术我们在这里说的路由技术一般是指，路由转发。主要涉及设备为路由器或者三层交换机。这些设备上会维护一张路由表，其中的信息可以是通过动态路由协议（例如OSPF，EIGRP，ISIS，RIP，BGP，静态路由，默认路由等）获取组成路由表的内容是：出口接口和对应网段 路由设备接收到一个数据包之后，会解封装，获取其中的目的IP地址信息（网段信息），然后查找路由表，选择最优路由去转发。路由设备上也会有一张ARP表，根据广播域 交换技术在一个局域网内，也就是一个广播域内使用的技术。通常会涉及到的设备就是交换机。交换机上会维护一张MAC地址转发表，其中的信息是MAC地址和端口的映射关系。交换机根据数据帧中的目的MAC地址进行数据包的转发 注意：在一个广播域内的数据流动是依靠二层MAC来实现的，因为在第一次会涉及到ARP，有了记录之后，交换机会记录他的MAC地址表，后续的速度就会较快 这个时候可以在白板上进行讲解，大致的讲解百度页面打开的整个过程。 3. 高可用+负载均衡keepalived 2种角色：master和backup； 4种状态：stop,master,backup,fault 检测脚本2种触发机制 当VRRP检测脚本检测到自身所承载应用的返回值不为0的时候，就会触发角色变化，这个时候，VRRP脚本中如果没有设置weight权重值，那么直接进入fault状态，在vrrp组中发送组播通告，宣告自己进入异常状态，让出master角色并且不参与竞选如果脚本中设置了weight权重值，这个时候又会分为两种情况。 当weight权重值大于0时，master的优先级不变，backup的优先级为weight+现在优先级在wight权重值小于0时，master的优先级为目前的优先级减去weight的绝对值，backup的优先级保持不变。经过我多次的实验，目前保证最佳切换效果的配置是Vrrp检测脚本组中不配置weight，并且所有主机都设置为backup，设置不抢占参数，这种情况下，能有效避免优先级设置不当导致的切换不成功。 Nginx Nginx工作在应用层（使用location，通过正则表达表达式进行相关匹配），负载均衡是基于upstream模块实现的，因此配置比较简单。但是对后端服务器的健康检测只能支持端口Nginx的负载均衡算法可以分为两类：内置策略和扩展策略，内置的有轮询，ip_hash等。扩展的有fair，通用hash，一致性hash等。 Nginx的负载均衡目前支持5种调度算法： rr轮询【默认算法】；接受到请求之后，按照时间顺序逐一分配到后端不同的服务器上 wrr加权轮询；权重值越大，被分配访问的概率就越大，主要用于后端服务器性能不一致的情况 ip_hash；每个请求按访问IP的哈希结果分配，计算之后，nginx内部会维护一张哈希表，这样每个访客固定访问一个后端服务器，可以有效的解决动态网页存在的session共享问题。 fair;【第三方算法，需要通过额外安装upstream_fair模块实现】。更智能的一个负载均衡算法，此算法可以根据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。这种策略具有很强的自适应性，但是实际的网络环境往往不是那么简单，因此要慎用。 url_hash；【第三方算法，需要通过额外安装hash模块实现】。也是哈希算法，只不过不是基于源IP，而是基于访问的URL来生成这张哈希表。每个URL定向到同一台后端服务器，可以进一步提高后端缓存服务器的效率。 注意：当算法是ip_hash的时候，后端服务器不能被添加weight和backup 我们在location中配置nginx负载均衡的时候，还需要添加proxy_next_upstream http_500 http_502 error timeout invalid_header; 这一行参数。用于定义故障转移策略。当后端服务器节点返回500、502和执行超时等错误时，自动将请求转发到upstream负载均衡器中的另一台服务器，实现故障转移。 Nginx负载均衡工作流： 当客户端访问 xxx 域名时请求会最先到达负载均衡器,负载均衡器就会去读取自己server标签段中的配置 到location里面一看,原来这是一个要往后端web节点抛的请求 而后,nginx通过 proxy_pass的配置项 在自己的主配置文件找到了事先定义好的后端web节点 最后,按照事先设置好的调度算法,把请求带上主机头和客户端原始ip一起抛给后端准备好的web服务器 nginx负载均衡较适合用于日pv 2000W以下的站点 HAProxy Haproxy能实现基于4层和7层的负载均衡， HAproxy的8中负载均衡算法1、roundrobin表示简单的轮询，每个服务器根据权重轮流使用，在服务器的处理时间平均分配的情况下这是最流畅和公平的算法。该算法是动态的，对于实例启动慢的服务器权重会在运行中调整。 2、leastconn连接数最少的服务器优先接收连接。leastconn建议用于长会话服务，例如LDAP、SQL、TSE等，而不适合短会话协议。如HTTP.该算法是动态的，对于实例启动慢的服务器权重会在运行中调整。 3、static-rr每个服务器根据权重轮流使用，类似roundrobin，但它是静态的，意味着运行时修改权限是无效的。另外，它对服务器的数量没有限制。 该算法一般不用； 4、source对请求源IP地址进行哈希，用可用服务器的权重总数除以哈希值，根据结果进行分配。只要服务器正常，同一个客户端IP地址总是访问同一个服务器。如果哈希的结果随可用服务器数量而变化，那么客户端会定向到不同的服务器； 该算法一般用于不能插入cookie的Tcp模式。它还可以用于广域网上为拒绝使用会话cookie的客户端提供最有效的粘连； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 5、uri表示根据请求的URI左端（问号之前）进行哈希，用可用服务器的权重总数除以哈希值，根据结果进行分配。只要服务器正常，同一个URI地址总是访问同一个服务器。一般用于代理缓存和反病毒代理，以最大限度的提高缓存的命中率。该算法只能用于HTTP后端； 该算法一般用于后端是缓存服务器； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 6、url_param在HTTP GET请求的查询串中查找中指定的URL参数，基本上可以锁定使用特制的URL到特定的负载均衡器节点的要求； 该算法一般用于将同一个用户的信息发送到同一个后端服务器； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 7、hdr(name)在每个HTTP请求中查找HTTP头，HTTP头将被看作在每个HTTP请求，并针对特定的节点； 如果缺少头或者头没有任何值，则用roundrobin代替； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 8、rdp-cookie（name）为每个进来的TCP请求查询并哈希RDP cookie； 该机制用于退化的持久模式，可以使同一个用户或者同一个会话ID总是发送给同一台服务器。如果没有cookie，则使用roundrobin算法代替； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 2种配置方式指的是在1.3版本之前，ha的负载均衡配置主要是在listen部分中进行配置在1.3版本之后，为了更好的维护和管理，将负载均衡的配置拆分成为了frotend和backend这两部分，为了保证兼容性，listen部分依然保留，目前主要使用listen部分配置HA的监控页面 HA通过ACL实现一些7层的功能例如通过path_end的ACl方法实现动静资源分离 通过hdr_dom(host)和hdr_reg(host)和hdr_beg(host)的方法实现虚拟主机 LVS关于LVS，它本身只是支持负载均衡，没有检测机制，因此要结合keepalived来使用【keepalived的诞生原因就是为了给LVS提供后端节点检测功能，到后面才添加了高可用的功能】。在这里需要明确一点，它只能转发4层数据包【IP+port】但是检测是能通过7层url进行监测的。LVS的8种算法：1.轮叫调度（Round Robin）调度器通过“轮叫”调度算法将外部请求按顺序轮流分配到集群中的真实服务器上，它均等地对待每一台服务器，而不管服务器上实际的连接数和系统负载。大锅饭调度：rr - 纯轮询方式，比较垃圾。把每项请求按顺序在真正服务器中分派 2.加权轮叫（Weighted Round Robin）调度器通过“加权轮叫”调度算法根据真实服务器的不同处理能力来调度访问请求。这样可以保证处理能力强的服务器能处理更多的访问流量。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。带权重的大锅饭调度：wrr -带权重轮询方式。把每项请求按顺序在真正服务器中循环分派，但是给能力较大的服务器分派较多的作业。 3.最少链接（Least Connections）调度器通过“最少连接”调度算法动态地将网络请求调度到已建立的链接数最少的服务器上。如果集群系统的真实服务器具有相近的系统性能，采用“最小连接”调度算法可以较好地均衡负载。谁不干活就给谁分配：lc - 根据最小连接数分派 4.加权最少链接（Weighted Least Connections）在集群系统中的服务器性能差异较大的情况下，调度器采用“加权最少链接”调度算法优化负载均衡性能，具有较高权值的服务器将承受较大比例的活动连接负载。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。带权重的谁不干活就给谁分配：wlc - 带权重的。机器配置好的权重高 5.基于局部性的最少链接（Locality-Based Least Connections）“基于局部性的最少链接”调度算法是针对目标IP地址的负载均衡，目前主要用于Cache集群系统。该算法根据请求的目标IP地址找出该目标IP地址最近使用的服务器，若该服务器是可用的且没有超载，将请求发送到该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的工作负载，则用“最少链接” 的原则选出一个可用的服务器，将请求发送到该服务器。基于地区的最少连接调度：lblc - 缓存服务器集群。基于本地的最小连接。把请求传递到负载小的服务器上 6.带复制的基于局部性最少链接（Locality-Based Least Connections with Replication）“带复制的基于局部性最少链接”调度算法也是针对目标IP地址的负载均衡，目前主要用于Cache集群系统。它与LBLC算法的不同之处是它要维护从一个目标 IP地址到一组服务器的映射，而LBLC算法维护从一个目标IP地址到一台服务器的映射。该算法根据请求的目标IP地址找出该目标IP地址对应的服务器组，按“最小连接”原则从服务器组中选出一台服务器，若服务器没有超载，将请求发送到该服务器；若服务器超载，则按“最小连接”原则从这个集群中选出一台服务器，将该服务器加入到服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的程度。带有复制调度的基于地区的最少连接调度：lblcr - 带复制调度的缓存服务器集群。某页面缓存在服务器A上，被访问次数极高，而其他缓存服务器负载较低，监视是否访问同一页面，如果是访问同一页面则把请求分到其他服务器。 7.目标地址散列（Destination Hashing）“目标地址散列”调度算法根据请求的目标IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。目标散列调度：realserver中绑定两个ip。ld判断来者的ISP商，将其转到相应的IP。 8.源地址散列（Source Hashing）“源地址散列”调度算法根据请求的源IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。了解这些算法原理能够在特定的应用场合选择最适合的调度算法，从而尽可能地保持Real Server的最佳利用性。当然也可以自行开发算法，不过这已超出本文范围，请参考有关算法原理的资料。源散列调度：源地址散列。基于client地址的来源区分。（用的很少） 补充：为什么4层性能比7层更好？在7层，因为负载均衡器要获取报文内部的内容，因此要先和客户端建立连接，才能收到客户发过来的报文内容，然后获取报文内容之后，再根据调度算法进行负载。也就是说7层负载会和客户端和后端服务器分别建立一个TCP连接，而4层负载均衡只需要一次，因此性能肯定比4层差。 三种负载均衡产品之间的对比HAProxy和LVS的4层负载对比因为LVS是基于Linux内核的，但是HAProxy是属于第三方应用，因此在性能上，LVS占据绝对优势。因此，如果只是做纯4层转发，则使用LVS HAProxy对比NginxHAProxy支持更为丰富的后端节点检测机制，并且性能比Nginx好，因此在并发量较大的情况下，使用HAproxy，日PV并发量较小的情况下可以使用Nginx，配置也较为简单。 4. Redis持久化策略数据持久化策略主要分为RDB和AOF两种 RDB方式：数据文件内记录的是实际的数据。因此在进行数据恢复的时候，速度较快。适合全量备份。在进行RDB持久化时，会fork出一个单独的进行，因此会CPU的开销较大。 AOF方式：数据文件内记录的是产生数据变化的命令。因此在进行数据恢复的时候，速度较慢，并且其中的内容可以编辑，因此适合在执行了一些类似flushall或者flushdb等命令时进行数据恢复 混合持久化：Redis4.0版本之后的持久化，结合了RDB和AOF的有点，当进行AOF重写的时候，将会把当前的数据转变成为RDB形式进行保存，重写之后的数据继续以AOF的格式保存 主从复制在Redis2.6版本之前，主从复制时，每次传输的都是全量数据，因此会非常占用网络带宽和相关资源。在这之后，在Redis master节点上可以设置复制缓存区，来实现差异的增量复制。但是当缓冲区满了之后，还是会执行全量复制。 淘汰策略淘汰策略是指当Redis进行即将使用到设置的最大内存量，执行的一个策略，避免出现内存溢出的问题，也就是一种内存回收机制。一般在使用到maxmemory的90%时触发，默认策略是不回收。 在redis中可以配置的策略主要有以下几种： noeviction policy 【默认策略，永不过期策略。】不会删除任何数据，拒绝任何写入操作并返回客户端错误信息（error）OOM command not allowed when used memory，此时Redis只响应读操作volatile-lru 根据LRU算法删除设置了超时属性（expire）的键，直到腾出足够空间为止，如果没有可以删除的键对象，则回退到noeviction策略 allkeys-lru 根据LRU算法删除键，不管数据有没有设置超时属性，直到腾出足够空间为止。 volatile-lfu 根据LFU算法删除设置了超时属性（expire）的键，直到腾出足够空间为止，如果没有可以删除的键对象，则回退到noeviction策略 allkeys-lfu 根据LFU算法删除键，不管数据有没有设置超时属性，直到腾出足够空间为止。 volatile-random 随机删除设置了超时属性（expire）的键，直到腾出足够的空间 allkeys-random 随机删除所有键，知道腾出足够空间为止 volatile-ttl 根据键值对象的ttl属性，删除最近将要过期的数据，如果没有，则回退到noeviction策略 常见性能问题 常见性能问题主要为： 内存设置不合理 大量的慢查询 key值（名称）设置过大 单个key的value过大 没有使用Redis的流水线功能 命令使用不合理，例如可以使用mset等或者禁止使用monitor等命令 客户端最大连接数设置【需要设置最大描述符，Redis默认会占用32个fd，因此可用的是1024-32】 TCP积压队列 定义AOF重写大小 客户端输出缓冲区 复制积压缓冲区 swap优化等等 哨兵模式 哨兵模式也就是Redis的高可用模式。一般的配置模式为一对主从，然后配置3个哨兵实例哨兵实例的设置原则：当有(n/2)+1个哨兵宣告需要进行切换时，才进行切换，这一点同样适用于zk等集群选举。因此最好3个以上的奇数个实例，偶数个会浪费一个。【这在5个以上节点时能看出明显的效果】 分布式集群 集群采用哈希槽的分配方式，一共有0-16383个槽最小建议配置为3主3从。Redis集群使用的是gossip协议。 cachecloud云平台 这是我从github上引入的Redis运维项目 5. Mysql+OracleMysql基础知识 Mysql主从复制原理 整体上来说，复制有3个步骤： A.master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events）； B.slave将master的binary log events拷贝到它的中继日志(relay log)； C.slave重做中继日志中的事件，将改变反映它自己的数据。 Mysql读写分离 Mysql高可用和集群有几种高可用方案：Mysql双主+keepalived【优点：架构简单，】 Mysql备份与恢复 逻辑备份：备份的是产生数据变化的sql语句。mysqldump能直接进行这个操作，但是因为它在备份过程中会锁表，并且备份的速度也非常的慢因此我们需要选择第三方工具。 物理备份：备份的是实际的数据，直接拷贝mysql的数据目录。 直接拷贝只适用于myisam类型的表。这种类型的表是与机器独立的。【这种备份的粒度较粗，不能实现更细粒度的数据恢复，特别是对于更新较为频繁的系统。】 实际生产环境中一般使用完整备份+增量备份每周日凌晨2点进行一次全量备份，之后的每天凌晨2点进行一次增量备份 然后再每天备份binlog日志【为了粒度更细致的数据恢复】 Mysql优化 mysql优化包括其他所有的网络服务优化，思路都是一致的。 分层次的来进行。【普遍规律+应用需求】 普遍规律： 首先是硬件层面 再次是操作系统层面的基础优化，例如文件描述符的数量，swap使用限制，文件系统（目前主流xfs） 再次是c/s架构方面的优化，例如TCP的连接队列大小，队列的缓存大小。tcp连接超时时间，tcp滑动窗口（发送和接受） 应用需求：数据库通用：最大连接数，索引（优先在where,order group等涉及的列上创建索引），sort，group等排序结果的缓冲区大小，慢查询，sql语句优化（减少使用like等开销大的语句），命令规范等Mysql：存储引擎 JAVA类：JVM设置，是否设置锁内存策略，堆内还是堆外内存，线程数量等。web类：压缩，静态文件缓存，CDN加速等 Mysql特殊：存储引擎等 Mysql常见问题 慢查询，sql写的有问题 mha的时候VIP漂移有问题 连接数问题 版本不一致问题 Oracle oracle没什么数据库概念上，oracle是只有一个数据库，然后里面有多用户，每个用户多表mysql是多个数据库，多个用户，采取授权的形式来访问 6. Ansible等自动化工具Ad-Hoc Ad-Hoc指的是一般性的临时操作 日常运维中主要使用的模块有： shell模块 yum模块 copy模块 service模块 PlaybookAnsible使用YAML语法描述配置文件，这个配置文件就被成为playbook，剧本 Ansible的核心理念是：极致的简单高效并且Ansbile是使用python编写的，因此在后续的二次开发上更占据优势。另一个趋势是python的运行方式，它和区块链一致，采用的是去中心化的部署方式，不需要安装客户端即可，通过SSH来实现，并且目前还提供了SSH的加速模式，适用于大规模的环境中，可以说，Ansible绝对是未来的趋势主流。 puppet、chef、slatstackpuppet和chef都是使用ruby编写的，并且配置繁琐，都需要配置客户端目前不适合 slatstack也是通过python编写，但是slatstack适用于更大的规模，因为ansible使用ssh来传输命令，而它使用zeroMQ来传输数据在1000台主机的情况下，MQ用时2秒左右，而ansible的SSH则用时85秒。 对比ansible默认情况下适用于200台以内的主机，适合中小型企业，如果数量再多可以使用Ansible的加速模式去实现 选型标准：选择最合适，如果当前的运维环境主机在百台，则ansible是最好的选择，如果上千台，那么无疑使用slatstack。 cobbler和kickstartkickstart是传统的批量装机方式，配置比较繁琐 cobbler是较早前的kickstart的升级版本，有点是容易配置 并且cobbler具有高级功能，可以根据不同机器的MAC地址来进行设置装机 关闭自动装机这里之前还发生过一个问题，就是有一次在装机的时候使用的是百兆交换机，导致老是有几台装不上，后来都换成千兆之后，就解决了这个问题。 关闭这个批量装机，因此centos的网卡名称不再是ethx的形式，因此在安装的时候，我们需要再ks文件中添加命令，来调整网卡的命令规则 7. Nginx，Httpd，tomcat，weblogic，php，gitlab，Jenkins这部分和web相关，主要是和电商，互联网公司等核心为web的紧密相关，也就是主要是LNMP这一套 Nginx基础知识基础知识 Nginx主要分为几个模块 全局配置【worker数量，worker的最大打开数量，CPU指定等】 Event模块配置【worker的最大连接数等，网络IO处理模型等】 Http模块【其中包括upstream段,server段,server中的location段等】 主要配置的地方就是HTTP模块中的upstream，server中的location段【动静分离等都是在这里进行配置】 注意：nginx的模块是静态的，在编译时就已经完全编译进去，而不是像Httpd是动态链接的形式 Nginx常见问题日志文件将磁盘存储空间占满了。 Nginx常见应用场景web服务器【一般会做动静分离，rewrite功能（重定向302是临时，301是永久，地址栏都改变，主要看爬虫变不变），防盗链】 负载均衡服务器 Nginx优化全局优化 工作进程数量（worker_processes数量）一般等于CPU的核数，因为每个进程是单线程的模式，使用epoll网络IO模型来进行处理。 worker_rlimit_nofile 60000；每个work进程最大打开文件数量。【这里需要跟操作系统的文件描述符相对应】 Event模块优化 worker进程最大连接优化，官方数据是能支持到5W【那么所有的连接数=5W*几个worker】 网络模型【通常使用epoll模型】 HTTP模块优化 不显示版本 关闭TCP延迟发送数据 keepalive的超时时间等 压缩传输的设置【压缩级别，压缩的触发大小】 Nginx和Httpd在这里主要说web，不说nginx的负载均衡，这部分已经在第3条说了。 tomcat常见问题**数据库连接问题，后端数据库异常，没有连接到tomcat乱码tomcat日志大小问题，权限问题JAVA_HOME没有设置正确 tomcat优化**主要分为2块，tomcat的JVM内存优化和tomcat的并发优化 内存优化：Tomcat内存优化主要是对 tomcat 启动参数优化，我们可以在 tomcat 的启动脚本 catalina.sh 中设置 java_OPTS 参数 JAVA_OPTS参数说明 -server 启用jdk 的 server 版； -Xms Java虚拟机初始化时的最小堆内存； -Xmx java虚拟机可使用的最大堆内存； 【堆内存建议设置一致，避免GC回收后再次动态分配，增大系统的开销】 -XX: PermSize 内存永久保留区域 -XX:MaxPermSize 内存最大永久保留区域 【这部分，默认64位的是256M】 JAVA_OPTS=’-Xms1024m -Xmx2048m -XX: PermSize=256M -XX:MaxNewSize=256m -XX:MaxPermSize=256m’ 并发优化/线程优化+缓存优化： 在Tomcat 配置文件 server.xml 中的 参数说明 1234567891011121314 maxThreads 客户请求最大线程数 表示最多同时处理多少个连接 minSpareThreads **Tomcat初始化时创建的 socket 线程数** maxSpareThreads **Tomcat连接器的最大空闲 socket 线程数 ** enableLookups 若设为true, 则支持域名解析，可把 ip 地址解析为主机名 redirectPort 在需要基于安全通道的场合，把客户请求转发到基于SSL 的 redirectPort 端口 acceptAccount 监听端口队列最大数，满了之后客户请求会被拒绝（不能小于maxSpareThreads ） connectionTimeout 连接超时 minProcessors 服务器创建时的最小处理线程数 maxProcessors 服务器同时最大处理线程数 URIEncoding URL统一编码 compression 打开压缩功能 compressionMinSize 启用压缩的输出内容大小，这里面默认为2KB compressableMimeType 压缩类型 connectionTimeout 定义建立客户连接超时的时间. 如果为 -1, 表示不限制建立客户连接的时间 参考配置： tomcat多实例部署http://blog.51cto.com/watchmen/1955972 传统方式复制目录的话，会造成资源浪费，因为lib和bin等公共资源会被多次加载，造成在内存中不必要的重复 思路：将bin下的文件和lib文件单独拆分出来 weblogicweblogic最开始bea公司的一个JAVA中间件产品，现在归属于oracle功能非常的强大，支持EJB比如在配置程序连接数据库时，不需要再代码中通过jdbc的方式去人工手动指定，而是通过后台管理页面的数据源配置中，进行配置。所以说，在一般的环境中，使用tomcat即可，如果涉及到大型的java应用开发，就要使用weblogic PHPPHP主要对接Nginx，处理php文件【通过php-fpm来处理】PHP-CGI 解释器每进程消耗 7 至 25 兆内存所以它的优化是进程数量的设置【包括启动时分配的，最小空闲的，最大空闲的，最大值】一般启动时分配5个，最小空闲为5个，最大空闲为32个，最大值为32个 GitlabJenkinsJDK支持tomcat支持maven支持Jenkins支持 Jenkins的安装一共有3个步骤 首先是下载war包到tomcat的webapps目录并将其重命名为ROOT.war，之后就是对其环境变量进行配置。 设定jenkins的目录及管理用户及编码修改tomcat目录下./conf/context.xml：增加jenkins环境变量 修改tomcat目录下的./conf/server.xml,是编码符合jenkins 步骤四：在第一次登陆jenkins页面时，需要输入一串加密数据这串数据位于其家目录下的./secrets/initialAdminPassword之中。 流程：JDK+tomcat部署Jenkins添加git 源码仓库使用maven进行构建【需要编写触发脚本，当有源码发生变化时，在2分钟后进行构建部署等操作】 8. 消息队列MQ产品使用MQ产品的原因 程序异步解耦 数据冗余 扩展性，不需要改变程序的代码，就可以扩展性能。 灵活性，峰值处理能力。 消息的顺序保证 异步通信，允许用户把消息放入队列中，但是并不立即处理它 ActiveMQ；老牌的MQ产品，完全遵守JMS规范。是apache开源的一个MQ产品，比较重量级，没有什么特殊的亮点 ActiveMQ的高可用集群模式通过ZK来实现，为了保证数据的一致性，因此会严重影响性能。从ActiveMQ 5.9开始，它实现了通过ZooKeeper + LevelDB实现高可用集群的部署方式。这种方式，对外只有Master提供服务这种方式实现了可以称之为半事务特性的机制，Master 将会存储并更新然后等待 (2-1)=1 个Slave存储和更新完成，才汇报 success RabbitMQ；遵循AMQP协议，借助erlang的特性在可靠性、稳定性和实时性上比别的MQ做得更好，非常重量级，性能比较好，适合企业级的开发。但是不利于做二次开发和维护 由于 rabbitmq 是使用 erlang 开发的，而 erlang 就是为分布式而生的。所以 rabbitmq 便于集群。rabbitmq 集群有两种模式：普通模式、镜像模式。 普通模式：也是默认模式，对于 queue 来说，消息实体只存在与其中的一个节点，A、B 两个节点只有相同的元数据，即队列的结构。当消息在A时，消费中从B中取消息时，消息会从A中传递到B中。所以consumer应尽量连接每一个节点，从中取消息。即对于同一个逻辑队列，要在多个节点建立物理Queue。否则无论consumer连rabbit01或rabbit02，出口总在rabbit01，会产生瓶颈。 镜像模式：镜像模式是 rabbitmq 的 HA 方案。其与普通模式的唯一不同之处在于消息会在 A，B 两个节点中同步。当然这种模式带来的副作用也是显而易见的。除了降低系统性能以外，如果队列数量过多，网络带宽将会受到影响。所以这种情况只运用到对高可靠性要求的场合上。 集群配置方式：安装erlang,然后同步三台机器上的.erlang.cookie文件内容因为RabbitMQ的集群是依赖erlang集群，而erlang集群是通过这个cookie进行通信认证的，因此我们做集群的第一步就是干cookie。注意：erlang.cookie文件中cookie值一致，且权限为owner只读。因此需要设置为600 注意： RabbitMQ单节点环境只允许是磁盘节点，防止重启RabbitMQ时丢失系统的配置信息。RabbitMQ集群环境至少要有一个磁盘节点，因为当节点加入或者离开集群时，必须要将该变更通知到至少一个磁盘节点。 kafka；也是apache基金会的一个MQ产品。高吞吐量，消息的接受和消费都是落地到磁盘，因此适用于大数据环境流处理，对实时性要求不是太高的环境，可以积压非常庞大的数据量（瓶颈在磁盘） kafka是一种分布式的，基于发布/订阅的消息系统。有主分区和副本分区的概念。并且kafka中的数据是追加的形式，保证了消息的有序性 rocketmq；阿里开发并开发的一个MQ产品，纯JAVA开发。具有高吞吐量、高可用性、适合大规模分布式系统应用的特点。RocketMQ思路起源于Kafka，但并不是Kafka的一个Copy，它对消息的可靠传输及事务性做了优化，目前在阿里集团被广泛应用于交易、充值、流计算、消息推送、日志流式处理、binglog分发等场景。 9. flume，zk，es，logstash，kibana系列flume flume：是一个日志收集软件。flume的agent设计实现这一系列的操作，一个agent就是一个java进程，运行在日志收集节点-也就是日志收集服务器节点。 agent里面包含3个核心的组件：source—-&gt;channel—–&gt;sink,类似生产者、仓库、消费者的架构。source:收集数据，可以处理各种类型sink：该组件是用于把数据发送到目的地的组件，目的地包括有：hdfs，kafka等等文件系统 工作流：flume的核心是把数据从数据源(source)收集过来，在将收集到的数据送到指定的目的地(sink)。为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel),待数据真正到达目的地(sink)后，flume在删除自己缓存的数据。 也就是说flume提供了一种类似事务机制。 flume的2种工作模式：主动模式和被动模式【主要是针对客户端来说】。这两种模式和zabbix的两种模式一样 在进行配置的时候，每个agent实例是通过别名来进行区分的。 kafka流式消息队列产品，接受flume发送过来的消息，或者日常产生端直接将JSON格式的数据发送到Kafka中。详见上方MQ产品 zookeeperzk:是一个分布式应用程序协调组件，用于为哪些原生没有提供集群功能的服务实现分布式集群。提供的功能包括：配置维护、域名服务、分布式同步、组服务等。zk的工作流：1、选举Leader。（选举zk集群中的leader）2、同步数据。3、选举Leader过程中算法有很多，但要达到的选举标准是一致的。4、Leader要具有最高的执行ID，类似root权限。5、集群中大多数的机器得到响应并接受选出的Leader。 注意：zk在3.5.0以上的版本会有一个内嵌的web服务，通过访问http://localhost:8080/commands来访问以上的命令列表。 一旦Zk集群启动之后，它将等待客户端的连接 esEs的主要功能是将收集的数据建立索引，方便日后数据的存储于检索。ES不止是一个全文本引擎，他还是一个分布式实时文档存储系统。这里，KCE的数据目的地和ES的数据来源设置成了一个分区，因此避免了磁盘IO的二次开销 logstash日志收集，需要在日志产生端配置，收集日志，再进行发送，目前使用flume来代替了。 kibanakibana不多说了，主要是提供了一个连接ES的入口 10. dockerdocker的核心三大组件是 镜像 容器 仓库 镜像主要分为几种，一个是官方的或者别人已经写好的镜像文件另一个可以自己产生镜像文件。 自己产生的镜像文件可以分为两种 在现有镜像的基础之上commit出来一个新的镜像 编写dockerfile文件，然后build出来一个镜像 建议通过dockerfile的形式产生镜像，因为使用commit出来的镜像会存在很多的缓存文件等。 容器是镜像的运行态，和程序及进程的概念比较像。 仓库主要分为两种，一个是存储镜像的仓库【里面的 镜像通过tag标签来尽心区分，默认是latest】，另一个是存储仓库名称的注册仓库 公网上的仓库可以是docker hub，也可以通过官方提供的registry镜像来简单搭建一套本地私有仓库环境: dockerfile编写dockerfile主要分为4个部分 基础镜像信息 from字段，也就是这个应用是以那个镜像为基础的 维护者信息，maintainer，也就是作者信息 镜像的操作指令，也就是在制作镜像是要执行的一系列操作，add加入一系列的文件，例如JDK，war包等 容器启动时执行指令-CMD，在启动时要执行的操作，例如启动项目等 在cachecloud中，基础镜像是使用的centos7.4-内核基础3.0-1811系统维护者是我，镜像的操作指令是JDK环境等等；容器启动时执行的命令是启动cachecloud项目 k8s k8s是谷歌开源的一个容器集群管理项目k8s对集群中的资源进行了不同级别的抽象，每个资源都是一个rest对象，通过API进行操作，通过JSON/YAML格式的模板文件进行定义要注意的是，k8s并不是直接对容器操作，它的操作最小单位是容器组。容器组由一个或多个容器组成。k8s围绕着容器组进行创建，调度，停止等生命周期管理。 ESXI,vsphere,xen,kvm 这些是第一家公司所使用的产品exsi和vsphere是vmware公司的企业虚拟化产品，相比于kvm，它有更好的性能，因此它是直接在物理上安装虚拟化操作系统，不需要第三方软件的实现。esxi是单机版本，vsphere是集中管理版本，支持在线迁移等高级功能。xenserver是思杰公司的一个虚拟化产品，单机的操作比vmware的esxi好，但是在涉及到多机环境时不是太好kvm需要linux系统的支持，然后还要安装一系列的组件，相对来说，更方便，但是不够专业，一般企业使用的相对较少。 11. 监控软件及JMX，JVMzabbix 我们的生产是怎么监控的 首先是监控模板，监控一些基础指标，例如CPU，内存，磁盘等 一些类似HAproxy，activemq等有web页面的应用我们通过web监控来实现【创建web场景，60秒内，尝试连接3次，如果3次都失败，则报警，这里还会涉及到一些有认证的页面，也是可以实现的。】 更高级一点的例如redis等应用，需要监控一些特定的指标，我们通过自定义监控项来实现。 JAVA类的应用，在后期慢慢的开放了JMX端口的情况下，陆续加入了JMX的监控。 自定义监控项为了简单高效，我们自己编写的脚本，判断引用的状态，将采用所能想到的一切来判断，然后再最后只输出一个0,如果服务不正常的话，则输出为1。 zabbix的一些优化操作采取zabbix的主动模式来进行监控使用自动发现的功能。 自动发现等操作 各监控产品的区别zabbix是一款商业的开源软件，涉及到的东西非常之多，因此官方能够靠咨询，技术服务等来收费运作。而cacti，nagios等是普通的开源软件，自然没有zabbix这么强大。 nagios的可视化功能非常弱，zabbix是有自己的可视化界面的【一般我们都是通过最新数据哪里查看，为了给zabbix减负，不是非必要的情况下，一般不会给监控项添加图形】它不支持自动发现，并且缺少图形展示工具，也没有历史数据，追查起来非常困难。 cacti是一个PHP程序它通过使用SNMP 协议获取远端网络设备和相关信息，（其实就是使用Net-SNMP 软件包的snmpget 和snmpwalk 命令获取）并通过RRDTOOL 工具绘图， 通过SNMP采集数据，并且自定义监控项等非常繁琐，报警方式需要添加插件等。 JMX监控 前提条件：需要JAVA类程序开放JMX端口【也就是开放API接口】 工作流：（1）zabbix_server需要知道一台主机上的特定端口的JMX值时，它会向Zabbix-Java-gateway进程去询问。这个连接进程叫做StartJavaPollers （2）Zabbix-Java-gateway使用JMXmanagementAPI这个API去查询特定的应用程序 注意：在配置的时候，StartJavaPollers线程数量要小于等于START_POLLERS设置的线程数量 这些操作操作完毕之后，在web页面上进行操作，添加JMX监控模板即可。 JVM调优 提到虚拟机的内存结构，可能首先想起来的就是堆栈。对象分配到堆上，栈上用来分配对象的引用以及一些基本数据类型相关的值。 JAVA虚拟机的内存结构是分了好几个区域的。分区域的好处是： 便于查找 便于内存回收【如果不分，回收内存就要全部内存扫描】 JVM内存分区（5部分）： 方法区 线程共享【这部分常被成为永久代，除了编译后的字节码之外，方法区中还会存放常量，静态变量以及及时编译器编译后的代码等数据。】 堆 线程共享【这部分一般是Java虚拟机中最大的一块内存区域，这块存储对象的实例。堆内存是垃圾收集器主要光顾的区域，一般来讲根据使用的垃圾收集器的不同，堆中还会划分为一些区域，比如新生代和老年代。新生代还可以再划分为Eden，Survivor等区域。另外为了性能和安全性的角度，在堆中还会为线程划分单独的区域，称之为线程分配缓冲区。更细致的划分是为了让垃圾收集器能够更高效的工作，提高垃圾收集的效率。】 Java栈 线程独享【每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。在Java虚拟机规范中，对于此区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展，当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。】 本地方法栈 线程独享【本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。】 程序计数器 线程独享【这部分内存不会内存溢出，字节码行号提示器】 堆（新生代和老年代等）： Xms256m 代表堆内存初始值为256MB Xmx1024m 代表堆内存最大值为1024MB如果-Xmx不指定或者指定偏小，应用可能会导致java.lang.OutOfMemory错误 方法区（永久代） PermSize和MaxPermSize指明虚拟机为java永久生成对象（Permanate generation）例如：class对象、方法对象这些可反射（reflective）对象分配内存限制，这些内存不包括在Heap（堆内存）区之中。-XX:PermSize=64MB 最小尺寸，初始分配XX:MaxPermSize=256MB 最大允许分配尺寸，按需分配这部分设置过小会导致：java.lang.OutOfMemoryError: PermGen space MaxPermSize缺省值和-server -client选项相关。-server选项下默认MaxPermSize为64m。 -client选项下默认MaxPermSize为32m 设置-Xms、-Xmx 相等以避免在每次GC 后调整堆的大小 在jdk1.8之前之前我们将储存类信息、常量、静态变量的方法区称为持久代(Permanent Generation)，PermSize和MaxPermSize是设置持久代大小的参数，在jdk1.8中持久代被完全移除了，所以这两个参数也被移除了，多了一个元数据区(Metadata Space)，所以设置元数据区大小的参数也变成对应的MetaspaceSize和MaxMetaspaceSize了 -XX:PermSize和-XX:MaxPermSize在jdk1.8中使用-XX:MetaspaceSize和-XX:MaxMetaspaceSize替代了现在能兼容正常启动，没有产生影响，知悉。 云产品说起阿里云，这期间还发生了一个人为事故。当初京东金融本来是通过我们的平台发送的，但是它要我们给他拉专线直接连接运营商。但是这边没有给他拉，而是买了一台阿里云服务器，暴露出一个公网IP地址让它连接，在这台服务器上面部署HAproxy，还是调整到我们的平台。【上边领导们的决定，我就不评论是非对错了 哈哈】 然后有一天，突然HAProxy的web监测报警，页面打不开。马上上服务器看，CPU爆了【买的服务器配置一般】检查进程。内存正常，磁盘正常，CPU爆了，然后再查看网络连接，发现有大量的CLOSE_WAIT（400个close_wait;100多个establish） 在TCP关闭时，主动关闭的一方发出 FIN 包，被动关闭的一方响应 ACK 包，此时，被动关闭的一方就进入了 CLOSE_WAIT 状态。如果一切正常，稍后被动关闭的一方也会发出 FIN 包，然后迁移到 LAST_ACK 状态。 导致产生大量close_wait的原因是突然遭遇大量的请求，即便响应速度不慢，但是也来不及消费，导致多余的请求还在队列里就被对方关闭了。（因为对方设置了超时时间）。但是linux没有对close_wait做类似超时控制的设置，如果不重启进程，这个状态很可能会永远的持续下去， AWS主要是当初想搭VPN，但是一大堆的限制，最终没成功，所以现在是直接买的商业的，稳定，速度也有保证。七牛云，产品主要是数据存储和CDN加速，我自己的博客目前也是在用七牛云。瑞江云，是公司在做什么业务时和人家合作时，人家送的，具体什么我就不知道了 13 自身素质关于这三个人的管理经验，是在第一家公司公司的时候。亿阳分为很多个部门，其中就有一个对外产品部门，当时是准备和人保合作，进入金融行业。因此拿下了一个标，但是招运维主管的时候的比较难招，差不不行，好的知道是外包驻场的形式一般也不愿意来，到最后实在没办法只能从公司内部要人了，然后就把我派过去了。在那边呆了有7个月左右。当时工作非常艰辛【上一家被换掉是因为政治原因，具体是谁就不知道了】，因此过去需要接受上一家的工作，然后开发二代新产品，中间不能停，也就是起承上启下的作用。当时1个月直接就瘦了10斤，天天加班。 高效办公系列软件TC:资源管理器Autohotey：热键管理器Listary：文件搜索浏览增强工具evernote:云笔记Fences:桌面管理工具Ditto：剪切板增强工具Snipaste：截图工具Everything:文件搜索工具 运维职业规划-如何通过运维思想做好运维工作运维思想-运维核心稳定性-网站/平台不宕机【这是运维的核心】一般通过以下方式来实现 架构使用集群+负载均衡+高可用+应用解耦，微服务等部署方式来保证性能 安全【】 运营推广不能在白天高峰期推广，需要和运维打招呼 前端图片的优化，不能使用大图等，尽量使用缩略图 数据库优化【加入Redis数据缓存层，sql语句优化等】 避免随时上线的操作【减少次数】 测试生产等环境保持一致【系统，软件版本，路径等等】 流程操作【运维标准和流程】 等等等等 数据不丢失 应用配置数据管理-【考虑使用CMDB等平台】 数据库数据 避免人为问题避免人为错误，主要分为两个方面， 一个是他人(主要是开发不严谨)产生的错误，这部分通过运维流程并结合工具控制。【比如测试不严谨，或者开发人员的代码有问题，直接把服务器资源跑没了】 一个是自己操作产生的问题，这部分通过一些智能化的自动化工具来尽量避免【避免在执行命令的时候误操作等等，常见的有】 解决：建立完善的流程制度，对运维来说，包括标准化，对开发测试来说，包括上线的流程化【通过运维制度和一些工具来实现】 提升运维效率这一点是放在最后的，是在上面都做好的前提下，然后再有这么一层，什么自动化，CICD，devops，不是说招几个运维开发就能解决的。一定是需要一个过程的。运维效率很重要，但是不能盲目的只盯住这个上面 个人如何做好运维工作主动性很多东西如果不主动去找系统负责人去推进，进度根本没法完成。 划重点的能力写文档，研究技术，培训讲解等，需要将其中最重要的东西给讲述出来。就比如在看书的时候，有时候一些大部头的书，可能一句话非常长，你要从中快速挑出这句话的重点。然后建立知识体系。【这就需要能快速的找出重点，快速浏览说明性的内容，因为有可能这些说明性内容对你目前的水平来说完全来说可以忽略】 全局观 比如像我当初对接那么多的系统，在出问题的时候，可能是后面某一个系统出问题，但是导致你直接无法使用，所以你需要根据症状， 态度某一项技术不会非常正常，要摆正心态，虚心向人学习，比如像开发学习，像DBA学习，等等。构建完善的知识体系。一个技术不会到会其实有时候就是一个月的事，根本没有大家想的那么恐怖，不要怕，大胆的去问。 换位思考，自身作则 上面的任务怎么说话去分派下去。怎么安排任务， 流程制度 流程化，制度化为了便于管理，减少出错的概率。因此要有流程和制度 新员工刚进来，可以适当的较少压力，因为有 分配任务的时候，要求下面的人去重复描述下，确保正确无误 优秀的思维去分享给团队，让团队一起成长 比如烧开水理论， 优秀的团队应该是一列高铁 个人职业规划个人现阶段的努力方向是能够快速解决问题这个要求就非常高，需要具备一定的开发能力。比如开发开发出来的程序，在测试上正常，但是一到生产上，服务器的负载就持续飙升，CPU资源被消耗殆尽，这个时候要能够快速的定位到进程。然后要能分析进程内部的资源消耗情况，比如调用内核的哪些系统调用的情况引起的异常等等，找到之后能不能定位到相应的程序代码，这样才能解决问题，而不是找到进程之后，简单的重启。【这一阶段基本上就是资深运维开发工程师级别，预计3年时间】 在这之后下一个阶段目标是未雨绸缪，在源头将问题遏制住因此需要具备开发能力，在软件需求评审和软件设计阶段就要参与进来。【在这一阶段基本上就达到了架构师的水平】 补充HTTP协议POST与GET的区别 GET是从服务器上获取数据，POST是向服务器传送数据 GET是通过发送HTTP协议通过URl参数传递进行接收，而POST是实体数据，通过表单提交 GET传送的数据量较小，不能大于2KB。POST传送的数据量较大，一般被默认为不受限制。 GET安全性非常低，POST安全性较高]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>职场</category>
        <category>Linux运维面试问题</category>
      </categories>
      <tags>
        <tag>Linux运维面试问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix的历史数据与趋势数据]]></title>
    <url>%2F2018%2F02%2F06%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2F%E8%BF%90%E7%BB%B4%E7%9B%91%E6%8E%A7%E4%BD%93%E7%B3%BB%2Fzabbix%2Fzabbix%E7%9A%84%E5%8E%86%E5%8F%B2%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%B6%8B%E5%8A%BF%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[参考链接官方文档 zabbix history trends历史与趋势数据详解 zabbix配置操作详解（三） Zabbix系统中的历史数据和趋势数据 正文历史与趋势历史数据和趋势数据是Zabbix系统中对采集到的监控项数据进行存储的两种方式。 历史根据设定的时间间隔保持每个收集的值， 而趋势是每个小时产生一个值（一条信息），内容为历史数据在一个小时内的平均值、最大值、最小值以及这一个小时内该监控项目所采集到的数据的个数。 在zabbix中的配置在监控项配置页面进行定义，在这里，我的配置是历史数据保留15天，趋势数据保留90天。如下图所示： 区别联系详解历史和趋势数据它们既有区别又有联系。 历史数据： Zabbix系统针对每个监控项目在每次采集时所收集到的数据，这个数据保存Zabbix系统数据库的历史表中，这就是所谓的历史数据。 因为每次所采集到的数据都保存在历史表中，所以如果监控项目的更新间隔越小，则在固定时间内所保存到历史表中的数据就越多。如果每个监控项目的更新间隔是30秒的话，则两个小时，该监控项目在Zabbix数据库的历史表中就会产生240条记录，一天就会产生2880条记录。 如果我们的Zabbix系统只监控一台被监控主机，且这台被监控主机只有一个被监控项目，那么每天产生2880条记录确实不值得一提的。但是，当我们监控系统所监控的项目比较多时，则这个数据量是非常大的。 比如说，如果我们监控系统监控1000个监控项目，且每个监控项目的更新间隔都是30秒，则每天历史表中就会产生2880*1000=2880000条记录，也即近300万条记录。而1000个监控项目可以监控多少主机呢？我们以48口的交换机为例，单监控每台交换机的每个端口的流量，则一台48口的交换机就有96个监控项目。所以，如果我们仅监控这样的48口的交换机，1000个监控项目只差不多只够监控10台这样交换机。由此可见，如果我们所监控主机的数量稍微多一点，或者更确切的来说，我们所监控的项目稍微多点，则Zabbix系统每天在其数据库中所产生的记录是非常大的。 因此，我们建议，如非必须的，我们在配置监控项目时，应尽量减小历史数据的保留天数，以免给数据库系统带来很大的压力。 趋势数据： 而趋势数据则不同，对于相同的更新间隔，系统所产生的趋势数据的数量远远没有历史数据那么庞大。对同一个监控项目，之所以趋势数据的数据量要远远小于历史数据的数据量，是由趋势数据的取值方式决定的。 趋势数据取值方式是，它取对应监控项目的历史数据在一个小时内的平均值、最大值、最小值以及这一个小时内该监控项目所采集到的数据的个数。 因此，不管一个监控项目的更新间隔是多少，它所对应的趋势数据在数据库中的记录都只有一条。更新间隔越小，仅可能导致数据个数增大，而不会影响该监控项目在趋势表里的记录条数的。 由此，或许你觉得趋势数据很不准确，你还是愿意保留更长时间的历史数据，以便查看较长时间的数据图。其实不是这样的，因为在Zabbix系统数据库的趋势表里不但保留一个小时内历史数据的最大值、最小值和平均值，而还保存这一个小时内所采集到的数据个数。因此，在要求并不是很高的场合，使用趋势数据绘出的监控项目的数据图的走势与用历史数据绘出的数据图的走势差别不会很大的。 不管是历史数据还是趋势数据，都会周期性被Zabbix服务器端一种称之为“主妇（housekeeper）”进程进行清理，它会周期性的删除过期的历史数据和趋势数据。 也正是因为这个进程的存在，才会使Zabbix系统数据的数据量不会一直的彭胀下去。而实际上，如果我们在保持Zabbix系统的被监控主机和被监控项目不变，且不更改监控项目的更新间隔的情况下，Zabbix系统的数据库的数据量会在增长到一定的数据量后不再增长，而是基本维持在这个数据量上不变。 “主妇”进程清理历史数据和趋势数据的频率可以在Zabbix服务器端组件(或服务器代理组件)的配置文件zabbix_server.conf中进行配置，它的配置项是HousekeepingFrequency。 特别注意： 1、 如果监控项目的“保留历史数据(天)”配置项被设置成0时，则数据库历史表中仅保留该监控项目所采集的最后一条数据，其它历史数据将不会被会保留。而且，引用该监控项目的触发器也只能使用该项目所采集的最后数据。因此，此时如果在触发器里引用该项目时使用max、avg、min等函数，其将没有意义。 2、 如果监控项目的“保留趋势数据(天)”配置项被设置成0时，则该项目在系统数据库的趋势表里将不保留任何数据。 配置建议具体该配置成什么样的周期，需要根据监控项以及数据库的配置以及对数据查看的要求程度来决定。这里只给出相关建议。 历史数据配置首先我们需要知道当前mysql的存储情况。在zabbix的前端页面上，我们可以看到如下图所示信息： 这个数值就是NVPS，也就是每秒处理平均数量（Number of processed values per second) 计算公式如下： 历史数据大小=NVPSx3600x24x365(天数)x50B 每个监控项大小约为50B，每秒条数为NVPS，一小时3600秒，一天24小时，一年365年。 具体单个监控项大小取决于数据库引擎，通常为50B 例如： 假设有6W个监控项，刷新周期都为60秒（我这里为30秒），那么每秒将会产生1000条数据，也就是每秒会向数据库写入1000条数据。如果我的历史数据保留天数为90天，那么需要的空间大小如下： 1000x3600x24x90x50=388 800 000 000(B) (约为362G，如果保存一年则为：362x4=1448G) 趋势数据配置因为趋势数据是每小时每个监控项一条记录，因此可以计算出大致所占的空间，其计算公式如下： 趋势数据大小=监控项个数x24x365(天数)x128B 每一个监控项的大小约为128B，每小时产生一条记录，一天24小时，一年365天 具体单个监控项大小取决于数据库引擎，通常为128B 例如： 假设有6W个监控项，保存一年的趋势数据，那么需要的空间如下： 60000x24x265x128=67 276 800 000(B) （约为67GB） 总结通过上面的计算对比，相信可以很直观的看到差别，在同样一年的情况下，历史与趋势所占存储空间的比例为：1448/67。 所以，具体选择什么周期需要根据公司的业务及实际情况（硬件配置等）来决定，并没有一个统一的标准，遵循这个公式，都可以很明确的计算预估出数据量情况。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维监控体系</category>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[autohotkeye常用操作]]></title>
    <url>%2F2018%2F02%2F06%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FIT%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7%2Fautohotkeye%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言AutoHotkey是一个windows下的开源、免费、自动化软件工具。它由最初旨在提供键盘快捷键的脚本语言驱动(称为：热键)，随着时间的推移演变成一个完整的脚本语言。但你不需要把它想得太深，你只需要知道它可以简化你的重复性工作，一键自动化启动或运行程序等等；以此提高我们的工作效率，改善生活品质；通过按键映射，鼠标模拟，定义宏等。 参考资料官方https://autohotkey.com/docs/AutoHotkey.htm 民间https://jeffjade.com/2016/03/11/2016-03-11-autohotkey/https://ahkcn.github.io/docs/AutoHotkey.htm 下载安装下载地址autohotkey下载地址 使用说明 AutoHotkey doesn’t do anything on its own; it needs a script to tell it what to do. A script is simply a plain text file with the .ahk filename extension containing instructions for the program, like a configuration file, but much more powerful. A script can do as little as performing a single action and then exiting, but most scripts define a number of hotkeys, with each hotkey followed by one or more actions to take when the hotkey is pressed. 也就是说，在实际使用的时候，是通过autohotkey去调用脚本，然后再去执行一系列的操作 脚本是自己定义个一个后缀为.ahk的文件 然后双击启动Ahk2Exe.exe，选择自己编写的这个ahk文件，执行convert，之后会生成一个ahk.exe的可执行文件。启动这个ahk.exe文件，就将配置加载，之后就可以使用这些热键进行一系列的操作 一个脚本中对应一系列热键 脚本符号这里简单说明下脚本中常用符号代表的含义： # 号 代表 Win 键； ! 号 代表 Alt 键； ^ 号 代表 Ctrl 键； + 号 代表 shift 键； :: 号(两个英文冒号)起分隔作用； run， 非常常用 的 AHK 命令之一; ; 号 代表注释后面一行内容； *通配符 即使附加的修饰键被按住也能激发热键. 这常与 重映射 按键或按钮组合使用. 例如: *#c::Run Calc.exe 表示：Win+C、Shift+Win+C、Ctrl+Win+C 等都会触发此热键。 run它的后面是要运行的程序完整路径（比如我的Sublime的完整路径是：D:\Program Files (x86)\Sublime Text 3\sublime_text.exe）或网址。为什么第一行代码只是写着“notepad”，没有写上完整路径？因为“notepad”是“运行”对话框中的命令之一。 如果你想按下“Ctrl + Alt + Shift + Win + Q”（这个快捷键真拉风啊。(￣▽￣)）来启动 QQ 的话，可以这样写： ^!+#q::run QQ所在完整路径地址。 AutoHotKey的强大，有类似Mac下的Alfred2之风，可以自我定制(当然啦，后者还是强大太多)。所以可以说，它强大与否，在于使用者的你爱或者不爱折腾。学以致用，如果简单的折腾下，可以使得我们工作效率大幅提升，何乐不为？况且，在见识的增长中，这可以给我们思维带来极大的营养。以下是笔者常用功能的脚本配置： 温馨提示： 以下几个系统默认的 Win 快捷键，请自行确认是否覆盖 Win + E：打开资源管理器； Win + D：显示桌面； Win + F：打开查找对话框； Win + R：打开运行对话框； Win + L：锁定电脑； Win + PauseBreak：打开系统属性对话框; Win + Q: 本地文件/网页等搜索; Win + U: 打开控制面板－轻松使用设置中心; 配置使用这是我自行编写的脚本的内容 #q::Run https://wx.qq.com/ #w::Run http://watchmen.xin/ #e::Run E:\software\tcmd\totalcmd\TOTALCMD64.EXE #r::Run, E:\software\ss\Shadowsocks.exe #t::Run, E:\software\Snipaste\Snipaste.exe #y::Run, E:\software\TIMqq\Bin\QQScLauncher.exe #u::Run, E:\software\foxmail\Foxmail.exe #i::Run, E:\software\xmanager\Xshell.exe 进阶单热键多命令类似下面的这种设置被称为单行热键, 因为它们只包含单个命令.​ #n::Run Notepad​ ^!c::Run calc.exe 要在一个热键中执行多个命令，请把首行放在热键定义的下面，且在最后行命令的下一行添加 return。例如： #n:: Run http://www.google.com Run Notepad.exe return 如果要运行的程序或文档没有在环境变量中, 那么需要指定它的完整路径才能运行: Run %A_ProgramFiles%\Winamp\Winamp.exe 在上面的例子中, %A_ProgramFiles% 是 内置变量. 使用它而不使用像 C:\Program Files 这样的, 脚本可以有更好的移植性, 这表示它在其他电脑上能执行的可能性更大. 注意: 命令和变量的名称是不区分大小写的. 例如, “Run” 等同于 “run”, 而 “A_ProgramFiles” 等同于 “a_programfiles”. 要让脚本等到程序或文档关闭后才继续执行, 请使用 RunWait 代替 Run. 在下面的例子中, 一直到用户关闭记事本后 MsgBox 命令才会继续执行. RunWait Notepad MsgBox The user has finished (Notepad has been closed).]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>IT基础知识</category>
        <category>常用软件工具</category>
      </categories>
      <tags>
        <tag>autohotkey</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[七牛云-qshell工具常用命令]]></title>
    <url>%2F2018%2F02%2F05%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FIT%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7%2F%E4%B8%83%E7%89%9B%E4%BA%91-qshell%E5%B7%A5%E5%85%B7%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[前言/简介qshell是利用七牛文档上公开的API实现的一个方便开发者测试和使用七牛API服务的命令行工具。 该工具设计和开发的主要目的就是帮助开发者快速解决问题。 目前该工具融合了七牛存储，CDN，以及其他的一些七牛服务中经常使用到的方法对应的便捷命令，比如b64decode，就是用来解码七牛的URL安全的Base64编码用的，所以这是一个面向开发者的工具。 官方资料文档https://developer.qiniu.com/kodo/tools/1302/qshell 视频教程http://notdelete.echohu.top/spjc/qshell-win.mp4 安装/环境准备目前在windows上使用qshell需要执行以下几个步骤添加命令到系统 下载qshell，存储到指定文件夹，例如我这里是：E:\software\qshell 重命名，将qshell_windows_x64.exe重命名为qshell.exe 添加系统环境变量，将E:\software\qshell追加到环境变量中 命令选项参数 描述 -d 设置是否输出DEBUG日志，如果指定这个选项，则输出DEBUG级别的日志 -m 切换到多用户模式，这样所有的临时文件写入都在命令运行的目录下 -h 打印命令列表帮助信息，遇到参数忘记的情况下，可以使用该命令 -v 打印工具版本，反馈问题的时候，请提前告知工具对应版本号 命令列表 实际操作我们使用qupload来进行文件的管理 官方文档 命令参数展示 命令语法： 1qshell qupload [&lt;ThreadCount&gt;] &lt;LocalUploadConfig&gt; 命令参数： 配置参数展示qupload 功能需要配置文件的支持，配置文件支持的全部参数如下： { &quot;src_dir&quot; : &quot;&lt;LocalPath&gt;&quot;, &quot;bucket&quot; : &quot;&lt;Bucket&gt;&quot;, &quot;file_list&quot; : &quot;&lt;FileList&gt;&quot;, &quot;key_prefix&quot; : &quot;&lt;Key Prefix&gt;&quot;, &quot;up_host&quot; : &quot;&lt;Upload Host&gt;&quot;, &quot;ignore_dir&quot; : false, &quot;overwrite&quot; : false, &quot;check_exists&quot; : false, &quot;check_hash&quot; : false, &quot;check_size&quot; : false, &quot;rescan_local&quot; : true, &quot;skip_file_prefixes&quot; : &quot;test,demo,&quot;, &quot;skip_path_prefixes&quot; : &quot;hello/,temp/&quot;, &quot;skip_fixed_strings&quot; : &quot;.svn,.git&quot;, &quot;skip_suffixes&quot; : &quot;.DS_Store,.exe&quot;, &quot;log_file&quot; : &quot;upload.log&quot;, &quot;log_level&quot; : &quot;info&quot;, &quot;log_rotate&quot; : 1, &quot;log_stdout&quot; : false, &quot;file_type&quot; : 0 } 参数具体含义如下： 密钥设置单用户 1qshell account ak sk 多用户 1qshell -m account ak sk 这里的ak、sk在个人面板中的密钥管理中查看，点击显示，然后进行复制粘贴 如下图所示： 上传图片这里我们选择qupload方式来进行图片的上传，在windows本地创建一个文件夹用户放置图片数据，每次同步该文件夹即可，不用再单独每张上传 步骤1：创建本地图片文件夹如下图所示，在指定位置下创建一个文件夹用于存放图片，在这里，我把它和我的博客文件夹放在同级 步骤2：创建配置文件如下图所示，在指定目录下创建配置文件，注意，这里需要使用编辑打开，不要用notpad++这些编辑器 步骤3：执行命令进行上传准备工作都做好后，执行如下命令直接上传： qshell qupload 1 c:\Users\56810\blog\config.txt qshell qupload 1 C:\Users\Administrator\blog\config.txt 如下图所示 其他配置刷新缓存官方资料 使用七牛云提供的 qshell 命令行工具，参考使用文档，先设置密钥，然后执行 cdnrefresh 命令来刷新缓存。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>IT基础知识</category>
        <category>常用软件工具</category>
      </categories>
      <tags>
        <tag>七牛云-qshell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RSS介绍及使用]]></title>
    <url>%2F2018%2F02%2F04%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FIT%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2FIT%E7%A7%91%E6%99%AE%E7%9F%A5%E8%AF%86%2FRSS%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[转载来源：http://www.ruanyifeng.com/blog/2006/01/rss.html RSS定义在解释RSS是什么之前，让我先来打一个比方。 读大学的时候，我有个习惯，就是每天要去看食堂后面的海报栏。在那里，会贴出各种各样最新的消息，比如哪个系要开讲座了、星期二晚上的电影放什么、二手货转让等等。只要看一下海报栏，就会对学校的各种活动心中有数。 如果没有海报栏的话，要想知道这些消息就会很麻烦。讲座消息会贴在各个系自己的公告栏里，电影排片表是贴在电影院里的，二手货消息则会贴在各幢宿舍的楼道里。我所在的大学有20几个系，一万多人，要想知道所有这些消息的话，即使是可能的话，也会相当的麻烦。 从这个例子出发，让我们来考虑一下互联网。 互联网是什么？最直观的说，就是一个杂乱无章的巨大信息源，其丰富和杂乱的程度，不仅是巨大的，而且几乎是无限的。 一个使用者，要想及时掌握的互联网上出现的最新信息，有办法吗？ 答案是没有办法，他只有一个网站一个网站的打开，去看有什么最新内容，就好比每天都必须去每一个系里走一遍，看有什么最新讲座。如果是几个网站，哪倒也不难，都去看一遍也花不了多少时间。但是随着你关注的网站数量上升，这项工作会迅速的变为”Mission Impossible”。想象一下，如果你每天关注几十个、甚至几百个网站，会是怎样的情景。光是打开它们的首页，就要花费多少时间啊，更别说浏览花去的时间了。 也许有人会说，普通人的话，谁会关心那么多网站啊？ 我要说，哪怕你只是一个网络的初级或最单纯的使用者，与你发生关系的网站数量也在急剧增加，因为Blog出现了。越来越多的人开始写作网络日志（Blog），把自己的想法和生活在网上展示，其中也必然包括你的朋友，或者其他你感兴趣的人。你想知道他/她的最新动向，就势必要留心他/她的Blog。所以，你的网站浏览清单总有一天会和你的电话本、MSN Message好友列表一样多，甚至更多。 那时，你会发现浏览网站会变成一种困难和低效率的行为。 有没有办法找到互联网上的”海报栏”，只去一个地方就知道你所想知道的所有最新内容？ 有，那就是RSS。 RSS内容和阅读器准确的说，RSS就像一个网站的海报，里面包括这个网站的最新内容，会自动更新。所以，我们只要订阅了RSS，就不会错过自己喜欢的网站的更新了。 但是光有海报还不行，还必须有海报栏，也就是说必须有RSS阅读器才行。因为RSS只是数据源，它本身是写给程序看的，必须经过阅读器转换，才能成为可以浏览的格式。 RSS阅读器多种多样，大致分为两种，一种是桌面型的，需要安装；另一种是在线型，直接使用浏览器进行阅读。 使用/订阅RSS在浏览器中订阅RSS，就必须先知道RSS的地址。一般来说，各个网站的首页都会用显著位置标明。名称可能会有些不同，比如RSS、XML、FEED，大家知道它们指的都是同样的东西就可以了。有时RSS后面还会带有版本号，比如2.0、1.0，甚至0.92，这个不必理会，它们只是内部格式不同，内容都是一样。 将RSS地址复制下来以后，你就可以在在线阅读器中添加。 以后，只用打开这一个网页，就可以看到所有你喜欢的网站的最新内容了。 推荐RSS阅读器个人目前在使用的RSS阅读器为：inoreader]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>IT基础知识</category>
        <category>IT科普知识</category>
      </categories>
      <tags>
        <tag>RSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown从入门到实践]]></title>
    <url>%2F2018%2F01%2F25%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FIT%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7%2FMarkdown%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Markdown介绍Markdown 是一种轻量级标记语言，它以纯文本形式(易读、易写、易更改)编写文档，常用的标记符号也不超过十个，并最终以HTML格式发布,让写作者专注于写作而不用关注样式。 划重点： 轻量级 标记语言 纯文本，所以兼容性极强，可以用所有文本编辑器打开。 让你专注于文字而不是排版。 格式式转换方便，Markdown 的文本你可以轻松转换为 html、电子书等。 Markdown 的标记语法有极好的可读性，常用的标记符号不过十来个 参考资料看完我这篇文章，再看完我下面推荐的这些内容，然后对比归纳总结，认真实践后，可以说在平常工作学习中完全够用。 官方资料 Markdown 语法说明 (简体中文版) Markdown 语法介绍 易读易写!-MarkDown语法说明 个人文章 献给写作者的 Markdown 新手指南 Markdown——入门指南 Markdown 基本语法 编辑器 个人在用的编辑器是MarkdownPad 2。各个工具之间相差不会很大，熟练掌握快捷键是提高效率的好方法 核心理念Markdown 的目标是实现「易读易写」，成为一种适用于网络的书写语言。。不管从任何角度来说，可读性，都是最重要的。Markdown 的语法全由一些符号所组成，这些符号经过精挑细选，其作用一目了然。比如：在文字两旁加上星号，看起来就像强调。 划重点： 语法是非常简单的符号 即写即读 兼容HTMLMarkdown 的构想不是要使得 HTML文档更容易书写。HTML 已经很容易写了。Markdown 的理念是，能让文档更容易读、写和随意改。 HTML是一种发布的格式，而Markdown 是一种书写的格式。也因此，Markdown 的格式语法只涵盖纯文本可以涵盖的范围。 常用操作标题（MarkdownPad中快捷键为Ctrl+1/2/3/4）：Markdown 支持两种标题的语法，类 Setext 和类 atx 形式。类 Setext 形式是用底线的形式，利用 = （最高阶标题）和 - （第二阶标题），例如： This is an H1 ============= This is an H2 ------------- 任何数量的 = 和 - 都可以有效果。但是这种形式只支持2层标题。 类 Atx 形式则是在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶，例如： # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 强调在Markdown中，可以使用 和 _ 来表示斜体和加粗。*单个为斜体，2个为加粗 加粗（MarkdownPad中快捷键为Ctrl+b）：加粗部分使用方式如下： **Coding，让开发更简单** __Coding，让开发更简单__ 实际展示效果如下： Coding，让开发更简单 Coding，让开发更简单 斜体（MarkdownPad中快捷键为Ctrl+l）：斜体部分的使用如下： *Coding，让开发更简单* _Coding，让开发更简单_ 实际展示效果展示如下： Coding，让开发更简单 Coding，让开发更简单 列表无序列表（MarkdownPad中快捷键为Ctrl+u）：* list1 前面使用*号 - list2 前面使用-号 + list3 前面使用+号 效果如下： list1 list2 list3 有序列表(MarkdownPad中快捷键为Ctrl+shift+o）：1. list1 使用数字+英文的点号，空格后接数据 2. list2 效果如下： list1 list2 区块引用（MarkdownPad中快捷键为Ctrl+q）在我们写作的时候经常需要引用他人的文字，这个时候引用这个格式就很有必要了，在 Markdown 中，你只需要在你希望引用的文字前面加上 &gt; 就好了。注意&gt;和文本之间要保留一个字符的空格。 &gt; 数据1 使用&gt;号 &gt; 数据2 &gt; &gt; 二级引用 注意区块引用可以包含多级引用 &gt; 实际效果展示： 数据1 数据2 这是二级引用 三级引用 代码区块（MarkdownPad中快捷键为Ctrl+k）：代码区块包括3种，文字内和单独一行以及指定代码格式的区块行 文字内加区块，不会加空白处底纹使用``（数字1左边，ESC下面的按键） 实际效果展示：在文件中含有代码区块是什么样子 整行的代码区块行，会加空白处底纹（快捷操作：全部选中然后敲Tab）缩进4个空格或者一个制表符（tab键）或者将代码块包裹在代码块包裹在 “/` 之间（避免无休止的缩进）。 实际效果展示 123require 'redcarpet'markdown = Redcarpet.new("Hello World!")puts markdown.to_html 实际效果展示：​ 现在的效果就是整整一个的区块行，如果这段代码比较长的话，那么markdown就会在下面生成一个查看条，供用户左右拉取调整，就是如现在所示。 指定代码格式的区块行 实际效果展示： 12$ line1-test1$ line2-test2 分割线/分隔线（MarkdownPad中快捷键为Ctrl+r）：一行中用三个以上的星号、减号、底线来建立一个分隔线，可以在字符之间加入空格，也可以不加空格 * * * *** ***** --- - - - 实际效果展示如下： 网页链接网页链接有2种方式，一种是直接显示链接，一种是通过文字进行跳转 直接显示&lt;https://www.baidu.com&gt; 用&lt;&gt;尖括号将内容包起来，markdown就会自动把它转成链接。网页链接、邮箱链接等都采用这种方式 实际效果展示如下：这段话中将要插入百度https://www.baidu.com的链接 文字跳转More info: [Server](https://hexo.io/docs/server.html) 前面是解释性说明，[]内是可以跳转的文字，()内是真正访问的地址。 实际效果展示如下： 请点击百度调整到百度页面 图片链接图片链接分为2部分，一种是在文字中，通过文字来链接到图片位置，用户需要点击这个文字链接去查看图片，优点是使文字更简约，缺点是无法直观的看到图。因此，第二种方式是直接在文章中显示图片。 我们把这两种方式分别称之为：行内式和参考式 行内式行内式的图片语法看起来像是： ![Alt text](/path/to/img.jpg) 参考案例：![Alt text](/path/to/img.jpg &quot;Optional title&quot;) 详细叙述如下： 一个惊叹号 ! 接着一个方括号，里面放上图片的替代文字 接着一个普通括号，里面放上图片的网址， 最后还可以用引号包住并加上 选择性的 ‘title’ 文字。 参考式参考式的图片语法则长得像这样：​ ![Alt text][id] 「id」是图片参考的名称，图片参考的定义方式则和连结参考一样： 参考案例：[id]: url/to/image &quot;Optional title attribute&quot; 参考式超链接一般用在学术论文上面，或者另一种情况，如果某一个链接在文章中多处使用，那么使用引用 的方式创建链接将非常好，它可以让你对链接进行统一的管理。 参考式同样适用于网页链接的使用 表格普通表格： First Header | Second Header | Third Header ------------ | ------------- | ------------ Content Cell | Content Cell | Content Cell Content Cell | Content Cell | Content Cell 设置表格两边内容对齐，中间内容居中，例如： First Header | Second Header | Third Header :----------- | :-----------: | -----------: Left | Center | Right Left | Center | Right 实际效果展示： First Header Second Header Third Header Left Center Right Left Center Right 文本居中居中使用html方式添加，格式如下： 1&lt;center&gt;这一行需要居中&lt;/center&gt; 文本居中的引用先看下实际效果： 主要用于主页等显示，和上面的文本场景有点不一样。 具体实现： &lt;!-- HTML方式: 直接在 Markdown 文件中编写 HTML 来调用 --&gt; &lt;!-- 其中 class=&quot;blockquote-center&quot; 是必须的 --&gt; &lt;blockquote class=&quot;blockquote-center&quot;&gt;blah blah blah&lt;/blockquote&gt; &lt;!-- 标签 方式，要求NexT版本在0.4.5或以上 --&gt; {% centerquote %} content {% endcenterquote %} &lt;!-- 标签别名 --&gt; {% cq %} content {% endcq %} 添加空行&lt;br /&gt; 使用该方法进行插入 反斜杠转义\*literal asterisks\* 使用这种方式来输出*号 实际效果展示： *literal asterisks* 字体与字号字体，字号和颜色编辑如下代码 &lt;font face=&quot;黑体&quot;&gt;我是黑体字&lt;/font&gt; &lt;font face=&quot;微软雅黑&quot;&gt;我是微软雅黑&lt;/font&gt; &lt;font face=&quot;STCAIYUN&quot;&gt;我是华文彩云&lt;/font&gt; &lt;font color=#0099ff size=7 face=&quot;黑体&quot;&gt;color=#0099ff size=72 face=&quot;黑体&quot;&lt;/font&gt; &lt;font color=#00ffff size=72&gt;color=#00ffff&lt;/font&gt; &lt;font color=gray size=72&gt;color=gray&lt;/font&gt; Size：规定文本的尺寸大小。可能的值：从 1 到 7 的数字。浏览器默认值是 3 效果如下： 我是黑体字我是微软雅黑我是华文彩云color=#0099ff size=72 face=”黑体”color=#00ffffcolor=gray 字体颜色语法格式：&lt;font color=指定颜色的英文单词&gt;内容&lt;/font&gt;，例如 例如将字体颜色修改为红色： 代码为：&lt;font color=red&gt;内容&lt;/font&gt; 内容 背景色&lt;table&gt;&lt;tr&gt;&lt;td bgcolor=orange&gt;背景色是：orange&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt; 实际效果如下： 背景色是：orange 删除线文本两端加上两个~~即可 删除我 复选框列表在列表符号后面加上[]或者[x]代表选中或者未选中情况 - [x] C - [x] C++ - [x] Java - [x] Qt - [x] Android - [ ] C# - [ ] .NET 实际效果为 C C++ Java Qt Android C# .NET 生成目录-TOC插件首先下载和安装 Visual Studio Code 锚点网页中，锚点其实就是页内超链接，也就是链接本文档内部的某些元素，实现当前页面中的跳转。比如我这里写下一个锚点，点击回到目录，就能跳转到目录。 在目录中点击这一节，就能跳过来。还有下一节的注脚。这些根本上都是用锚点来实现的。 语法描述： 代码： 这里使用截图的方式展示，因为直接编写的话，hexo会检测报错(因为%没有对应的%结尾) emoji表情Github的Markdown语法支持添加emoji表情，输入不同的符号码（两个冒号包围的字符）可以显示出不同的表情。 比如:blush:，可以显示: :blush: 注释注释是写作者自己的标注记录，不被浏览器解析渲染。HTML 以 结尾的闭包定义注释（支持跨行），不在正文中显示。 Markdown 沿用 HTML Comment 注释格式： &lt;!-- This text will not appear in the browser window. --&gt; 折叠块代码如下： &lt;details&gt; &lt;summary&gt;点击展开答案&lt;/summary&gt; &lt;p&gt; 象&lt;/p&gt; &lt;/details&gt; 效果如下： 你和猪，打一种动物 点击展开答案 象 代码高亮与原来使用缩进来添加代码块的语法不同，这里使用 来包含多行代码： 三个 ``` 要独占一行。 指定图片大小Markdown 不支持指定图片的显示大小，不过可以通过直接插入&lt;img /&gt;标签来指定相关属性： &lt;img src=&quot;https://avatars2.githubusercontent.com/u/3265208?v=3&amp;s=100&quot; alt=&quot;GitHub&quot; title=&quot;GitHub,Social Coding&quot; width=&quot;50&quot; height=&quot;50&quot; /&gt; 效果如下： 在单元格里换行借助于 HTML 里的 实现。 示例代码： | Header1 | Header2 | |---------|----------------------------------| | item 1 | 1. one&lt;br /&gt;2. two&lt;br /&gt;3. three | 示例效果： Header1 Header2 item 1 1. one2. two3. three]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>IT基础知识</category>
        <category>常用软件工具</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TotalCommander常用快捷键]]></title>
    <url>%2F2018%2F01%2F25%2FIT%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-Linux%E8%BF%90%E7%BB%B4%E6%96%B9%E5%90%91%2FIT%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7%2FTotalCommander%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[基础知识参考资料可以先看下相关资料，这些资料对概念介绍等做的非常详细也写的非常好，因此这里不再赘述，本文主要是针对实际的运用。 官方资料 https://www.ghisler.com/官网上没有相对应的文档，需要额外去搜寻 优秀个人文章 TC学堂——最易读的Total Commander教程-强烈推荐通过该网站进行学习 Total Commander快捷键 实际操作常用目录这部分设置可以说是TC操作的精华，效率直接甩开windows资源管理器几条街。 快速添加ctrl+d，添加，然后a直接添加 常用目录高级配置通过自定义配置，可以自定义调整常用目录的名称、顺序等，后续的增删改查也在此页面进行。 ctrl+d，添加，进去之后按c进入常用目录配置对话框。在里面配置的时候，需要再最前面人为添加&amp;。 名称设置： &amp;1 test $b blog 命令参考设置： cd C:\Users\56810\blog\blog 直达组合键通过直达组合键，可以直接切换到指定目录下。 设置：alt+s 调出窗口，再按s进行配置。一共可以使用的个数是一般都是类似ctrl+alt+F1/F2..F11这么11个组合键 名称设置： &amp;1 desktop $b blog 命令参考设置： cd C:\Users\56810\blog\blog 配置完成之后，切换到桌面只需要：alt+s+1 切换磁盘分区Alt+F1调出分区选项之后，按D则进入D盘，E则进入E盘。 目录内容查看Alt+1 详细的列表信息 Alt+2 图形信息显示 Alt+3 目录树显示 多Tab标签操作ctrl+t 新建tab ctrl+上箭头 新建父目录tab ctrl+w 关闭标签 ctrl+shift+w 关闭所有非活动标签 ctrl+tab, ctrl+shift+tab 在同侧的tab间切换 改变tab排列顺序（包括在两个窗口间移动）：鼠标左键拖动。 自定义快捷键，直接切换到第N个标签可以在 wincmd.ini 中 [Shortcuts] 段，增加如下内容， C+1=cm_SrcActivateTab1 C+2=cm_SrcActivateTab2 C+3=cm_SrcActivateTab3 效果： ctrl+1～3 激活第 1～3 个标签，依次类推 压缩操作压缩： 选中文件之后，执行Alt+F5 查看压缩文件内容（不解压缩）： ctrl+右箭头或者直接回车 解压缩：Alt+F9 文件搜索Alt+F7 创建操作F7/Shift+F7 新建一个或多层文件夹。可以像DOS那样新建多层的目录，比如c:\file\a\b\c Shift+F4 新建文本文件，调用记事本编辑（自定义编辑器） 其他快捷键ctrl +e 进入资源管理器 alt + f1 选择第一个窗口的磁盘 +f2就是选择第二个窗口的磁盘 alt+下箭头 历史记录 alt+左箭头 返回上一个操作目录（历史目录） alt+右箭头 返回下一个操作目录（历史目录） ctrl+\ 返回到当前目录的根目录 Ctrl+Shift+Enter 查看当前的路径 shift+F10 右键 F3 文件内容预览 ctrl+M 批量重命名 Shift+F4 新建文本文件，调用记事本编辑（自定义编辑器） Ctrl+加号 全部选择同一类型的文件（例如压缩文件，目录文件） Ctrl+减号 全部取消同一类型的文件（例如压缩文件，目录文件）]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>IT基础知识</category>
        <category>常用软件工具</category>
      </categories>
      <tags>
        <tag>TC操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F23%2F%E4%B8%AA%E4%BA%BA%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2FHexo%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is my first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask author on GitHub. 前言之前使用阿里云结合wordpress的方式搭博客，但是自己维护起来不是太方便，云服务器一旦攻击，数据是个问题。之后在51cto和csdn上写，但是要受到平台的限制。最近发现github有博客功能(几年前就推出了，竟然现在才发现)，完美解决这些问题。github提供空间，用户自行选择博客框架，专注于内容，大部分人应该还是喜欢这种简约风主题。目前这个博客使用github-pages+Hexo来实现。 参考资料搭建 https://zhuanlan.zhihu.com/p/26625249 http://eleveneat.com/2015/04/24/Hexo-a-blog/ 进阶 主要参考官方资料 Hexo文档 https://hexo.io/zh-cn/docs/ Next主题使用手册 http://theme-next.iissnan.com/ 根据官方资料，按图索骥，基本上都能很好的把所有功能实现出来。使用问题可以随时沟通交流 markdown语法 关于markdown的使用，可以看我的这篇博文 Markdown语法 Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy Generate static files + Deploy to remote sites1$ hexo g -d More info: Deployment]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>个人博客</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
