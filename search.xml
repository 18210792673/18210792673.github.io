<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[网络专线]]></title>
    <url>%2F2018%2F06%2F06%2F%E7%BD%91%E7%BB%9C%E4%B8%93%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[参考文献： 百度百科 概述什么是网络专线？笼统来说，网络专线就是为某个机构拉一条独立的网线，也就是一个独立的局域网，例如军事，银行等，让用户的数据传输变得可靠可信，专线的优点就是安全性好，QoS（ quality of service 服务质量）可以得到保证。不过，专线租用价格也相对比较高，而且管理也需要专业人员。 网络专线主要有两种信道： 物理专用信道。物理专用信道就是在服务商到用户之间铺设有一条专用的线路，线路只给用户独立使用，其他的数据不能进入此线路，而一般的线路就允许多用户共享信道； 【独享物理线路的形式】 虚拟专用信道；虚拟专用信道就是在一般的信道上为用户保留一定的带宽，使用户可以独享这部分带宽，就像在公用信道上又开了一个通道，只让相应用户使用，而且用户的数据是加密的，以此来保证可靠性与安全性；【在共享物理上创建逻辑独享线路】 这里连接的通道是用户端的出口网关设备（一般是路由器）到ISP的接入端这一段的线路。后续的上网还是通过ISP去实现 目前市面上的信道有： 帧中继（Frame Relay） 数字数据网（DDN Digital Data Network） 异步传输模式（ATM Asynchronous Transfer Mode） X.25（分组交换业务网） 第三代ADSL（非对称用户数字链路） 虚拟专用网络（VPN Virtual Private Network）以及E1等。 什么是互联网专线？互联网专线接入业务是指为客户提供各种速率的专用链路（主要提供传输速率为2M及以上速率），直接连接IP骨干网络，实现方便快捷的高速互联网上网服务。互联网专线接入业务按照客户需求可提供更高速率的专线接入，主要有2Mb/s、10Mb/s、100Mb/s、1000Mb/s等等。 和网络专线的区别 互联网专线跳过了ISP的环节，直接连接Internet骨干网络 主要特点 1.与普通互联网接入相比，其特点是客户通过相对永久的通信线路接入Internet。 2.与拨号上网的最大区别是专线与Internet之间保持着永久、高速、稳定的连接，客户可以实现24小时对Internet的访问，随时获取全球信息资源，提高商务交易的效率。 3.专线客户拥有固定的真实IP地址，可以相对方便地向Internet上的其他客户提供信息服务。 4.专线具有误码率低，时延小的特点。 5.专有带宽的整条电路资源仅为一个客户服务，全程带宽完全独享。 什么是裸光纤？裸光纤就是指专线光纤。通俗又权威的说法：裸光纤就是中间没有连接/经过任何传输设备的光纤，也就是直通光缆。 一般来讲，用户向电信或其他公司租用裸光纤，就是指电信或其他公司只提供光纤物理通道，不提供数据处理等服务，整条光纤干线也不经过任何数据处理设备，由用户自行配置两地的收发设备。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识</category>
        <category>网络专线</category>
      </categories>
      <tags>
        <tag>网络专线</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[磁盘性能测试]]></title>
    <url>%2F2018%2F06%2F03%2F%E7%A3%81%E7%9B%98%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[文献资料： 阿里云-ECS之-块存储性能 性能衡量指标衡量存储产品的性能指标主要包括： IOPS 吞吐量 访问时延。 IOPS和Throughput吞吐量两个参数是衡量存储性能的主要指标，两者在不同的情况下都能表示存储的性能状况，但应用的场景不尽相同。同时，两者之间也存在着相互的联系 IOPSIOPS是Input/Output Operations per Second，即每秒能处理的I/O个数（也就是在一秒内，磁盘进行多少次 I/O 读写），用于表示块存储处理读写（输出/输入）的能力。如果要部署事务密集型应用，需要关注IOPS性能。 ps:一次完整的读写（有读有写），才算一次 最普遍的IOPS性能指标是顺序操作和随机操作，如下表所示。 IOPS又可细分为如下几个指标： Toatal IOPS，混合读写和顺序随机I/O负载情况下的磁盘IOPS，这个与实际I/O情况最为相符，大多数应用关注此指标。 Random Read IOPS，100%随机读负载情况下的IOPS。 Random Write IOPS，100%随机写负载情况下的IOPS。 Sequential Read IOPS，100%顺序读负载情况下的IOPS。 Sequential Write IOPS，100%顺序写负载情况下的IOPS。 吞吐量吞吐量是指单位时间内可以成功传输的数据数量。 如果要部署大量顺序读写的应用，需要关注吞吐量。 磁盘的吞吐量，也就是每秒磁盘 I/O 的流量，即磁盘写入加上读出的数据的大小。 IOPS 与吞吐量的关系 每秒 I/O 吞吐量＝ IOPS* 平均 I/O SIZE。 从公式可以看出： I/O SIZE 越大，IOPS 越高，那么每秒 I/O 的吞吐量就越高。因此，我们会认为 IOPS 和吞吐量的数值越高越好。实际上，对于一个磁盘来讲，这两个参数均有其最大值，而且这两个参数也存在着一定的关系。 访问延迟访问时延是指块存储处理一个I/O需要的时间。 如果您的应用对时延比较敏感，比如数据库（过高的时延会导致应用报错），建议您使用固态硬盘介质的SSD云盘、SSD共享块存储或本地SSD盘类产品。 如果您的应用更偏重存储吞吐能力，对时延不太敏感，比如Hadoop离线计算等吞吐密集型应用，建议您使用本地HDD盘类产品，如d1或d1ne大数据型实例。 云盘性能这里以目前主流的阿里云云盘系列产品进行说明 云盘性能三种云盘的性能对比如下表所示。 注意： SSD云盘的性能因数据块大小而异，数据块越小，吞吐量越小，IOPS越高，如下表所示。只有挂载到I/O优化的实例时，SSD云盘才能获得期望的IOPS性能。挂载到非I/O优化的实例时，SSD云盘无法获得期望的IOPS性能。 单盘性能计算公式说明： 以单块SSD云盘最大IOPS计算公式为例说明：起步1200 IOPS，每GiB增加30 IOPS，最高20000 IOPS。 以单块SSD云盘最大吞吐量计算公式为例说明：起步80 MBps，每GiB增加0.5 MBps，上限为 300 MBps的吞吐量。 不同云盘的单路访问时延如下： SSD云盘：0.5−2 ms 高效云盘：1−3 ms 普通云盘：5−10 ms 性能测试根据ECS实例的操作系统不同，您可以使用不同的工具测试块存储性能： Linux实例：可以使用DD、fio或sysbench等工具测试块存储性能。 Windows实例：可以使用fio、Iometer等工具测试块存储性能。 本文以Linux实例和fio为例，说明如何使用fio测试块存储性能。在进行测试前，请确保块存储已经4 KiB对齐。 警告： 测试裸盘可以获得真实的块存储盘性能，但直接测试裸盘会破坏文件系统结构，请在测试前提前做好数据备份。建议您只在新购无数据的ECS实例上使用工具测试块存储性能，避免造成数据丢失。 测试随机写IOPS，运行以下命令： fio -direct=1 -iodepth=128 -rw=randwrite -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=iotest -name=Rand_Write_Testing 测试随机读IOPS，运行以下命令： fio -direct=1 -iodepth=128 -rw=randread -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=iotest -name=Rand_Read_Testing 测试顺序写吞吐量，运行以下命令： fio -direct=1 -iodepth=64 -rw=write -ioengine=libaio -bs=1024k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=iotest -name=Write_PPS_Testing 测试顺序读吞吐量，运行以下命令： fio -direct=1 -iodepth=64 -rw=read -ioengine=libaio -bs=1024k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=iotest -name=Read_PPS_Testing 下表以测试随机写IOPS的命令为例，说明命令中各种参数的含义。 参数 说明 direct=1 表示测试时忽略I/O缓存，数据直写。 -iodepth=128 表示使用AIO时，同时发出I/O数的上限为128。 -rw=randwrite 表示测试时的读写策略为随机写（random writes）。作其它测试时可以设置为： - randread（随机读random reads）- read（顺序读sequential reads）- write（顺序写sequential writes）- randrw（混合随机读写mixed random reads and writes）。 -ioengine=libaio 表示测试方式为libaio（Linux AIO，异步I/O）。应用使用I/O通常有二种方式：同步和异步。同步的I/O一次只能发出一个I/O请求，等待内核完成才返回。这样对于单个线程iodepth总是小于1，但是可以透过多个线程并发执行来解决。通常会用 16−32 根线程同时工作将iodepth塞满。异步则通常使用libaio这样的方式一次提交一批I/O请求，然后等待一批的完成，减少交互的次数，会更有效率。 -bs=4k 表示单次I/O的块文件大小为4k。未指定该参数时的默认大小也是4k。测试IOPS时，建议将bs设置为一个比较小的值，如本示例中的4k。测试吞吐量时，建议将bs设置为一个较大的值，如本示例中的1024k。 -size=1G 表示测试文件大小为1G。 -numjobs=1 表示测试线程数为1。 -runtime=1000 表示测试时间为1000秒。如果未配置，则持续将前述-size指定大小的文件，以每次-bs值为分块大小写完 -group_reporting 表示测试结果里汇总每个进程的统计信息，而非以不同job汇总展示信息。 -filename=iotest 指定测试文件的名称，比如iotest。测试裸盘可以获得真实的磁盘性能，但直接测试裸盘会破坏文件系统结构，请在测试前提前做好数据备份。 -name=Rand_Write_Testing 表示测试任务名称为Rand_Write_Testing，可以随意设定。 以下以一块800 GiB SSD云盘随机读IOPS性能的测试结果为例，说明如何理解fio测试结果。 Rand_Read_Testing: (g=0): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=128 fio-2.2.8 Starting 1 process Jobs: 1 (f=1): [r(1)] [21.4% done] [80000KB/0KB/0KB /s] [20.0K/0/0 iops] [eta 00Jobs: 1 (f=1): [r(1)] [28.6% done] [80000KB/0KB/0KB /s] [20.0K/0/0 iops] [eta 00Jobs: 1 (f=1): [r(1)] [35.7% done] [80000KB/0KB/0KB /s] [20.0K/0/0 iops] [eta 00Jobs: 1 (f=1): [r(1)] [42.9% done] [80004KB/0KB/0KB /s] [20.1K/0/0 iops] [eta 00Jobs: 1 (f=1): [r(1)] [50.0% done] [80004KB/0KB/0KB /s] [20.1K/0/0 iops] [eta 00Jobs: 1 (f=1): [r(1)] [57.1% done] [80000KB/0KB/0KB /s] [20.0K/0/0 iops] [eta 00Jobs: 1 (f=1): [r(1)] [64.3% done] [80144KB/0KB/0KB /s] [20.4K/0/0 iops] [eta 00Jobs: 1 (f=1): [r(1)] [71.4% done] [80388KB/0KB/0KB /s] [20.1K/0/0 iops] [eta 00Jobs: 1 (f=1): [r(1)] [78.6% done] [80232KB/0KB/0KB /s] [20.6K/0/0 iops] [eta 00Jobs: 1 (f=1): [r(1)] [85.7% done] [80260KB/0KB/0KB /s] [20.7K/0/0 iops] [eta 00Jobs: 1 (f=1): [r(1)] [92.9% done] [80016KB/0KB/0KB /s] [20.4K/0/0 iops] [eta 00Jobs: 1 (f=1): [r(1)] [100.0% done] [80576KB/0KB/0KB /s] [20.2K/0/0 iops] [eta 00m:00s] Rand_Read_Testing: (groupid=0, jobs=1): err= 0: pid=9845: Tue Sep 26 20:21:01 2017 read : io=1024.0MB, bw=80505KB/s, iops=20126, runt= 13025msec slat (usec): min=1, max=674, avg= 4.09, stdev= 6.11 clat (usec): min=172, max=82992, avg=6353.90, stdev=19137.18 lat (usec): min=175, max=82994, avg=6358.28, stdev=19137.16 clat percentiles (usec): | 1.00th=[ 454], 5.00th=[ 668], 10.00th=[ 812], 20.00th=[ 996], | 30.00th=[ 1128], 40.00th=[ 1256], 50.00th=[ 1368], 60.00th=[ 1480], | 70.00th=[ 1624], 80.00th=[ 1816], 90.00th=[ 2192], 95.00th=[79360], | 99.00th=[81408], 99.50th=[81408], 99.90th=[82432], 99.95th=[82432], | 99.99th=[82432] bw (KB /s): min=79530, max=81840, per=99.45%, avg=80064.69, stdev=463.90 lat (usec) : 250=0.04%, 500=1.49%, 750=6.08%, 1000=12.81% lat (msec) : 2=65.86%, 4=6.84%, 10=0.49%, 20=0.04%, 100=6.35% cpu : usr=3.19%, sys=10.95%, ctx=23746, majf=0, minf=160 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.1% issued : total=r=262144/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0 latency : target=0, window=0, percentile=100.00%, depth=128 Run status group 0 (all jobs): READ: io=1024.0MB, aggrb=80504KB/s, minb=80504KB/s, maxb=80504KB/s, mint=13025msec, maxt=13025msec Disk stats (read/write): vdb: ios=258422/0, merge=0/0, ticks=1625844/0, in_queue=1625990, util=99.30% 输出结果中，主要关注以下这行内容： read : io=1024.0MB, bw=80505KB/s, iops=20126, runt= 13025msec 这表示fio做了1 GiB I/O，速率约为80 MiB/s，总IOPS为20126，运行时间为13秒。由IOPS值可知，该SSD云盘的IOPS性能为 20126，而根据公式计算的数值为： min{1200+30 * 容量, 20000} = min{1200+30 * 800, 20000} = 20000 测试结果与公式计算结果相近。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>服务器硬件</category>
        <category>磁盘</category>
      </categories>
      <tags>
        <tag>磁盘性能测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ECS从入门到实践]]></title>
    <url>%2F2018%2F06%2F03%2FECS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[参考文献： 阿里云官方文档 ECS基础知识ECS 概述云服务器Elastic Compute Service（ECS）是阿里云提供的一种基础云计算服务。使用云服务器ECS就像使用水、电、煤气等资源一样便捷、高效。您无需提前采购硬件设备，而是根据业务需要，随时创建所需数量的云服务器ECS实例。在使用过程中，随着业务的扩展，您可以随时扩容磁盘、增加带宽。如果不再需要云服务器，也能随时释放资源，节省费用。 名词解释在使用ECS之前，需要了解以下概念 地域和可用区：是指ECS资源所在的物理位置。 实例：等同于一台虚拟机，包含CPU、内存、操作系统、网络、磁盘等最基础的计算组件。 实例规格：是指实例的不同配置，包括vCPU核数、内存、网络性能等。实例规格决定了ECS实例的计算和存储能力。 镜像：是指ECS实例运行环境的模板，一般包括操作系统和预装的软件。操作系统支持多种Linux发行版本和不同的Windows版本。 块存储：包括基于分布式存储架构的 弹性块存储，以及基于物理机本地硬盘的 本地存储。 快照：是指某一个时间点上一块弹性块存储的数据备份。 网络类型：包括 - 专有网络：基于阿里云构建的一个隔离的网络环境，专有网络之间逻辑上彻底隔离。更多信息，请参考专有网络VPC。 - 经典网络：统一部署在阿里云公共基础内，规划和管理由阿里云负责。 安全组：由同一地域内具有相同保护需求并相互信任的实例组成，是一种虚拟防火墙，用于设置不同实例的网络访问控制。 SSH密钥对：远程登录Linux ECS实例的验证方式，阿里云存储公钥，您需要自己妥善保管私钥。您也可以选择使用 用户名密码 验证登录Linux ECS实例。 IP地址：包括用于 内网通信 的内网IP或私有IP，以及用于访问Internet的公网IP。 弹性公网IP：可以与实例反复绑定或解绑的静态公网IP地址。 云服务器管理控制台：是指ECS的Web操作界面。 ECS的优势与普通的IDC机房或服务器厂商相比，阿里云提供的云服务器ECS具有以下优势： 高可用性 安全 弹性 高可用性 相较于普通的IDC机房以及服务器厂商，阿里云会使用更严格的IDC标准、服务器准入标准以及运维标准，以保证云计算整个基础框架的高可用性、数据的可靠性以及云服务器的高可用性。 在此基础之上，阿里云所提供的每个地域都存在多可用区。当您需要更高的可用性时，可以利用阿里云的多可用区搭建自己的主备服务或者双活服务。对于面向金融领域的两地三中心的解决方案，您也可以通过多地域和多可用区搭建出更高的可用性服务。其中包括容灾、备份等服务，阿里云都有非常成熟的解决方案。 在阿里云的整个框架下，这些服务可以非常平滑地进行切换，相关的信息可以在阿里云行业解决方案中找到。无论是两地三中心，还是电子商务以及视频服务等，都可以在阿里云找到对应的行业解决方案。 此外，阿里云提供了如下三项支持： 提升可用性的产品和服务，包括云服务器、负载均衡、多备份数据库服务以及数据迁移服务DTS等。 行业合作伙伴以及生态合作伙伴，帮助您完成更高、更稳定的架构，并且保证服务的永续性。 多种多样的培训服务，让您从业务端到底层的基础服务端，在整条链路上实现高可用。 安全性 选择了云计算，最关心的问题就是云计算的安全与稳定。阿里云近期通过了很多的国际安全标准认证，包括ISO27001、MTCS等，这些所有的安全合规都要求对于用户数据的私密性、用户信息的私密性以及用户隐私的保护都有非常严格的要求。对于云计算，推荐您使用阿里云专有网络。 在阿里云专有网络之上，可以产生更多的业务可能性。您只需进行简单配置，就可在自己的业务环境下，与全球所有机房进行串接，从而提高了业务的灵活性、稳定性以及业务的可发展性。 对于原来拥有自建的IDC机房，也不会产生问题。阿里云专有网络可以拉专线到原有的IDC机房，形成混合云的架构。阿里云可以提供各种混合云的解决方案和非常多的网络产品，形成强大的网络功能，让您的业务更加灵活。结合阿里云的生态，您可以在云上发展出意想不到的业务生态。 阿里云专有网络更加稳定和安全。 稳定性：业务搭建在专有网络上，而网络的基础设施将会不停进化，使您每天都拥有更新的网络架构以及更新的网络功能，使得您的业务永远保持在一个稳定的状态。专有网络允许您自由地分割、配置和管理自己的网络。 安全性：面对互联网上不断的攻击流量，专有网络天然就具备流量隔离以及攻击隔离的功能。业务搭建在专有网络上后，专有网络会为业务筑起第一道防线。 总之，专有网络提供了稳定、安全、快速交付、自主可控的网络环境。对于传统行业以及未接触到云计算的行业和企业而言，借助专有网络混合云的能力和混合云的架构，它们将享受云计算所带来的技术红利。 弹性 云计算最大的优势就在于弹性。目前，阿里云已拥有在数分钟内开出一家中型互联网公司所需要的IT资源的能力，这就能够保证大部分企业在云上所构建的业务都能够承受巨大的业务量压力。 计算弹性 纵向弹性，即单个服务器的配置变更。传统IDC模式下，很难做到对单个服务器进行变更配置。而对于阿里云，当您购买了云服务器或者存储的容量后，可以根据业务量的增长或者减少自由变更自己的配置。关于纵向弹性的具体应用，详情请参考 升降配。 横向弹性。对于游戏应用或直播平台出现的高峰期，若在传统的IDC模式下，您根本无法立即准备资源；而云计算却可以使用弹性的方式帮助客户度过这样的高峰。当业务高峰消失时，您可以将多余的资源释放掉，以减少业务成本的开支。利用横向的扩展和缩减，配合阿里云的弹性伸缩，完全可以做到定时定量的伸缩，或者按照业务的负载进行伸缩。关于横向弹性的具体应用，详情请参考 弹性伸缩。 存储弹性。阿里云拥有很强的存储弹性。当存储量增多时，对于传统的IDC方案，您只能不断去增加服务器，而这样扩展的服务器数量是有限的。在云计算模式下，将为您提供海量的存储，当您需要时可以直接购买，为存储提供最大保障。关于存储弹性的具体应用，详情请参考磁盘扩容。 网络弹性。云上的网络也具有非常大的灵活性。只要您购买了阿里云的专有网络，那么所有的网络配置与线下IDC机房配置可以是完全相同的，并且可以拥有更多的可能性。可以实现各个机房之间的互联互通，各个机房之间的安全域隔离，对于专有网络内所有的网络配置和规划都会非常灵活。关于网络弹性的具体应用，详情请参考专有网络。 总之，对于阿里云的弹性而言，是计算的弹性、存储的弹性、网络的弹性以及您对于业务架构重新规划的弹性。您可以使用任意方式去组合自己的业务，阿里云都能够满足您的需求。 块存储概念阿里云为您的云服务器ECS提供了丰富的块存储产品类型，包括基于分布式存储架构的弹性块存储产品，以及基于物理机本地硬盘的本地存储产品。其中： 弹性块存储，是阿里云为云服务器ECS提供的数据块级别的随机存储，具有低时延、持久性、高可靠等性能，采用 三副本的分布式机制，为ECS实例提供99.9999999%的数据可靠性保证。可以随时创建或释放，也可以随时扩容。 本地存储，也称为本地盘，是指挂载在ECS云服务器所在物理机（宿主机）上的本地硬盘，是一种临时块存储。是专为对存储I/O性能有极高要求的业务场景而设计的存储产品。该类存储为实例提供块级别的数据访问能力，具有低时延、高随机IOPS、高吞吐量的I/O能力。 块存储、对象存储、文件存储的区别阿里云目前主要提供三种数据存储产品，分别是块存储、文件存储（NAS）和 对象存储（OSS）。 三者区别如下： 块存储：是阿里云为ECS云服务器提供的块设备，高性能、低时延，满足随机读写，可以像使用物理硬盘一样格式化建文件系统使用。可用于大部分通用业务场景下的数据存储。 对象存储（OSS，Object Storage Service）：可以理解是一个海量的存储空间，最适合存储互联网上产生的图片、短视频、音频等海量非结构化数据，您可以通过API在任何时间、任何地点访问对象存储里的数据。常用于互联网业务网站搭建、动静资源分离、CDN加速等业务场景。 文件存储（NAS，Network Attached Storage）：类似于对象存储，适合存储非结构化的海量数据。但是您需要通过标准的文件访问协议访问这些数据，比如 Linux 系统需要使用Network File System (NFS)协议，Windows系统需要使用Common Internet File System (CIFS)协议。您可以通过设置权限让不同的客户端同时访问同一份文件。文件存储适合企业部门间文件共享、广电非线编、高性能计算、Docker等业务场景。 块存储性能衡量块存储产品的性能指标主要包括：IOPS、吞吐量和访问时延。 IOPS IOPS是Input/Output Operations per Second，即每秒能处理的I/O个数，用于表示块存储处理读写（输出/输入）的能力。如果要部署事务密集型应用，需要关注IOPS性能。 最普遍的IOPS性能指标是顺序操作和随机操作，如下表所示。 吞吐量 吞吐量是指单位时间内可以成功传输的数据数量。 如果要部署大量顺序读写的应用，需要关注吞吐量。 访问延迟 访问时延是指块存储处理一个I/O需要的时间。 如果您的应用对时延比较敏感，比如数据库（过高的时延会导致应用报错），建议您使用固态硬盘介质的SSD云盘、SSD共享块存储或本地SSD盘类产品。 如果您的应用更偏重存储吞吐能力，对时延不太敏感，比如Hadoop离线计算等吞吐密集型应用，建议您使用本地HDD盘类产品，如d1或d1ne大数据型实例。 不同云盘之间的性能测试对比请看文档：云盘性能对比部分 弹性块存储弹性块存储，是阿里云为云服务器ECS提供的数据块级别的随机存储，具有低时延、持久性、高可靠等性能，采用 分布式三副本机制，为ECS实例提供99.9999999%的数据可靠性保证。弹性块存储支持在可用区内自动复制您的数据，防止意外硬件故障导致的数据不可用，保护您的业务免于组件故障的威胁。就像硬盘一样，您可以对挂载到ECS实例上的弹性块存储做分区、创建文件系统等操作，并持久存储数据。 您可以根据业务需要随时扩容弹性块存储。具体操作，请参见 扩容数据盘 和 扩容系统盘。您也可以为弹性块存储创建快照，备份数据。关于快照的更多信息，参见 快照。 根据是否可挂载到多台ECS实例，弹性块存储可以分为： 云盘：一块云盘只能挂载到同一地域、同一可用区的一台ECS实例。 共享块存储：一块共享块存储可以同时挂载到同一地域、同一可用区的16台ECS实例。 说明：共享块存储目前仍处于公测阶段，公测期间支持最多同时挂载到4台ECS实例上。 总结： 也就是说弹性块存储在使用的时候，可以被当做是本地的磁盘，也可以是当做网络存储，类似NFS等挂载到多台ECS主机上使用区分云盘和共享块存储的方式是能否被多台ECS同时挂载 云盘根据性能分类 根据性能不同，云盘可以分为： ESSD云盘：又称增强型SSD云盘，是阿里云全新推出的超高性能的云盘产品。基于新一代分布式块存储架构，结合25GE网络和RDMA技术，为您提供单盘高达100万的随机读写能力和低至100μs的单路时延能力。ESSD云盘处于邀测阶段，更多信息，请参见 ESSD云盘FAQ。 SSD云盘：采用固态硬盘作为存储介质，能够提供稳定的高随机I/O、高数据可靠性的高性能存储。 高效云盘：采用固态硬盘与机械硬盘的混合介质作为存储介质。 普通云盘：采用机械磁盘作为存储介质 根据用途分类 根据用途不同，云盘可以作： 系统盘：生命周期与系统盘所挂载的ECS实例相同，随实例一起创建和释放。不可共享访问。系统盘可选的容量范围与实例所选的镜像有关： Linux（不包括CoreOS）+ FreeBSD：20 GiB ~ 500 GiB CoreOS：30 GiB ~ 500 GiB Windows：40 GiB ~ 500 GiB 数据盘：可以与ECS实例同时创建，也可以 单独创建，不可共享访问。与ECS实例同时创建的数据盘，生命同期与实例相同，随实例一起创建和释放。单独创建的数据盘，可以 单独释放，也可以 设置为随ECS实例一起释放。数据盘的容量由云盘类型决定，详细信息，请参见 块存储性能。作数据盘用时，云盘与共享块存储共享数据盘配额，即一台实例最多挂载16块数据盘。 共享块存储共享块存储是一种支持多台ECS实例并发读写访问的数据块级存储设备，具备多并发、高性能、高可靠等特性，数据可靠性可以达到 99.9999999%。单块共享块存储最多可以同时挂载到16台ECS实例。目前尚处于公测阶段（申请公测资格），最多同时挂载到4台ECS实例。 共享块存储只能作数据盘用，只能单独创建，可以共享访问。您可以 设置共享块存储与挂载的ECS实例一起释放。 根据性能不同，共享块存储可以分为： SSD共享块存储：采用固态硬盘作为存储介质，能够提供稳定的高随机I/O、高数据可靠性的高性能存储。 高效共享块存储：采用固态硬盘与机械硬盘的混合介质作为存储介质。 挂载到实例上时，共享块存储与云盘共享数据盘配额，即一台实例最多挂载16块数据盘。 更多共享块存储的信息，请参见 共享块存储FAQ。 网络和安全性内网目前阿里云的云服务器ECS内网间，非I/O优化的实例为千兆共享的带宽，I/O优化的实例为万兆共享的带宽，没有特殊限制。由于是共享网络，因此无法保证带宽速度是不变的。 如果两台同地域的ECS实例之间需要传输数据，一般建议使用内网连接。同时，云数据库RDS、负载均衡（SLB） 以及 对象存储（OSS） 相关的内网速度也都是千兆共享的环境。这些产品间也都可以使用内网相互连接使用。 目前只要是相同地域下，SLB、云数据库RDS、OSS与ECS之间都可以直接内网互通连接使用。 弹性网卡弹性网卡（ENI）是一种可以附加到专有网络VPC类型ECS实例上的虚拟网卡，通过弹性网卡，您可以实现高可用集群搭建、低成本故障转移和精细化的网络管理。所有地域均支持弹性网卡。 使用场景弹性网卡适用于以下几种场景： 搭建高可用集群 满足系统高可用架构对于单实例多网卡的需求。 低成本故障迁移 通过将弹性网卡从ECS实例分离后再附加到另外一台ECS实例，将故障实例上的业务流量快速迁移到备用实例，实现服务快速恢复。 精细化网络管理 可以为实例配置多个弹性网卡，例如用于内部管理的弹性网卡及用于面向公网业务访问的弹性网卡等，完成管理数据和业务数据间的隔离。可以根据源IP、协议、端口等对每张弹性网卡配置精准的安全组规则，从而对每张弹性网卡的流量进行安全访问控制。 弹性网卡类型 弹性网卡分为两种类型： 主网卡 在创建专有网络实例时随实例默认创建的弹性网卡称作主网卡。主网卡的生命周期和实例保持一致，您无法分离主网卡与实例。 辅助网卡 您可以创建辅助网卡，并将其附加到实例上或从实例上分离。每个实例能附加的网卡上限与实例规格相关，详细信息，请参考 实例规格族。 弹性网卡属性 属性 数量 主私有IP地址 1个 MAC地址 1个 安全组 至少1个，最多5个 描述信息 1个 网卡名称 1个 限制约束 使用弹性网卡有如下限制： 一个账号在一个地域内默认最多可创建100个弹性网卡。如果需要更多，请 提交工单 申请。 ECS实例与弹性网卡必须在同一VPC的同一可用区中，可以分属于不同交换机。 每台实例允许附加的弹性网卡数量由实例规格决定。详细信息，请参见 实例规格族。 非I/O优化实例规格不支持弹性网卡。 您不能在一个实例上附加多个弹性网卡来提高实例带宽。 说明：实例的带宽能力由实例规格决定。 安全组安全组是一个逻辑上的分组，这个分组是由同一个地域（Region）内具有相同安全保护需求并相互信任的实例组成。每个实例至少属于一个安全组，在创建的时候就需要指定。同一安全组内的实例之间网络互通，不同安全组的实例之间默认内网不通。可以授权两个安全组之间互访。 安全组是一种虚拟防火墙，具备状态检测包过滤功能。安全组用于设置单台或多台云服务器的网络访问控制，它是重要的网络安全隔离手段，用于在云端划分安全域。 安全组限制 单个安全组内的实例个数不能超过 1000。如果您有超过 1000 个实例需要内网互访，可以将他们分配到多个安全组内，并通过互相授权的方式允许互访。 每个实例最多可以加入 5 个安全组。 每个用户的安全组最多 100 个。 对安全组的调整操作，对用户的服务连续性没有影响。 安全组是有状态的。如果数据包在 Outbound 方向是被允许的，那么对应的此连接在 Inbound 方向也是允许的。 安全组的网络类型分为经典网络和专有网络。 - 经典网络类型的实例可以加入同一地域（Region）下经典网络类型的安全组。 - 专有网络类型的实例可以加入同一专有网络（VPC）下的安全组。 安全组规则 安全组规则可以允许或者禁止与安全组相关联的云服务器 ECS 实例的公网和内网的入出方向的访问。 您可以随时授权和取消安全组规则。您的变更安全组规则会自动应用于与安全组相关联的ECS实例上。 在设置安全组规则的时候，安全组的规则务必简洁。如果您给一个实例分配多个安全组，则该实例可能会应用多达数百条规则。访问该实例时，可能会出现网络不通的问题。 安全组规则限制 每个安全组最多有 100 条安全组规则。 DDOS基础防护阿里云云盾默认为ECS实例免费提供5 Gbit/s恶意流量攻击，即 DDoS基础防护能力。这一功能可以有效防止云服务器ECS实例受到恶意攻击，从而保证ECS系统的稳定，即当流入ECS实例的流量超出实例规格对应的限制时，云盾就会帮助ECS实例限流，避免ECS系统出现问题。 企业版入门企业级用户在购买和使用云服务器ECS实例时，通常需考虑如下几点： 配置选型 估算成本 网络规划 配置安全组 制定快照策略 镜像迁移 用负载均衡实现ECS的高可用性 配置选型参考资料：阿里云官方资料 用户指南]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>公有云产品</category>
        <category>阿里云</category>
        <category>ECS</category>
      </categories>
      <tags>
        <tag>ECS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[带宽计算方法及B与b说明]]></title>
    <url>%2F2018%2F05%2F30%2F%E5%B8%A6%E5%AE%BD%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E5%8F%8AB%E4%B8%8Eb%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[参考文献： 带宽计算方法及大B与小b说明 bit与Byte的关系源信息 在计算机科学中，bit（比特）是表示信息的最小单位，叫做二进制位；一般用0和1表示。 Byte叫做字节，由8个比特位（8bit）组成一个字节(1Byte)，用于表示计算机中的一个字符。 bit与Byte之间可以进行换算，其换算关系为：1Byte=8bit（或简写为：1B=8b） 在实际应用中一般用简称， 即1bit简写为1b(注意是小写英文字母b)，1Byte简写为1B（注意是大写英文字母B）。 ISP的表示 在计算机网络或者是网络运营商(Internet service provider)中，一般宽带速率的表示单位用bps(或b/s，小b)表示； bps表示比特每秒即表示每秒钟传输多少位信息，是bit per second的缩写。在实际所说的1M带宽的意思是1Mbps（是兆比特每秒Mbps不是兆字节每秒MBps） 换算公式: 1Byte = 8bit 1B = 8b---------- 1B/s=8b/s(或1Bps=8bps) 1KB = 1024B---------- 1KB/s=1024B/s 1MB = 1024KB ---------- 1MB/s=1024KB/s 最终： 1Mbps = 1024*1024 bps = 1024 Kbps = 1024/8 KBps = 128KBps = 128KB/s 规范提示： 在实际书写中，B应表示Byte(字节)，b应表示bit(比特)，但是我们在实际书写中很容易把bit和Byte都混写为b ，如把Mb/s和MB/s都混写为Mb/s，导致人们在实际计算中因单位的混淆而出错。 实际应用在实际上网应用中，下载软件时常常看到诸如下载速度显示为128KB（KB/s），103KB/s等等宽带速率大小字样，因为ISP提供的线路带宽使用的单位是比特（bit，即小b），而一般下载软件显示的是字节（byte，1byte＝8bits），所以要通过换算，才能得实际值。 所以，我们可以按照公式换算一下： 128KB/s=128×8(Kb/s)=1024Kb/s=1Mb/s即：128KB/s=1Mb/s。 也就是说1Mb的带宽，下载速度为128KB/s秒 在一些软件的带宽的显示页面，通常的显示页面也是以bps的方式来显示，这个时候，我们就需要进行一下换算，例如下面的页面截图（阿里云带宽使用情况） 图中所选的这个值是：13272120 bps(bits/s)，我们下面进行换算： 13272120 bps = 13272120/1024 Kbps = 13272120/1024/1024 Mbps = 12.65723 Mbps 换算之后，我们可以看到这里显示的带宽是12.6M 补充：ADSL宽带知识ADSL（Asymmetric Digital Subscriber Loop）技术是一种不对称数字用户线实现宽带接入互连网的技术，ADSL作为一种传输层的技术，充分利用现有的铜线资源，在一对双绞线上提供上行640kbps（理论上行1Mbps）下行8Mbps的带宽，从而克服了传统用户在”最后一公里”的”瓶颈”，实现了真正意义上的宽带接入。 上行速率：是指用户电脑向网络发送信息时的数据传输速率。 下行速率： 是指网络向用户电脑发送信息时的传输速率。比如用 FTP上传文件到网上去，影响上传速度的就是“上行速率”；而从网上下载文件，影响下载速度的就是“下行速率”。 当然，在实际上传下载过程中，线路、设备 (含计算机及其他设备)等的质量也会对速度造成或多或少的影响。 上行速率对上行速率的影响 TCP/IP规定，每一个封包，都需要有acknowledge信息的回传，也就是说，传输的资料，需要有一个收到资料的信息回复，才能决定后面的传输速度，并决定是否重新传输遗失的资料。 行的带宽一部分就是用来传输这些acknowledge(确认)资料的，当上行负载过大的时候，就会影响acknowledge资料的传送速度，并进而影响到下载速度。这对非对称数字环路也就是ADSL这种上行带宽远小于下载带宽的连接来说影响尤为明显。 有试验证明，当上传满载时，下载速度讲变为理想速度的40%，这就可以解释为什么很多朋友用BT下载的时候稍微限速反而能够获得更大的下载速度。 总结在网络运营商提供的宽带速率单位中，”bps”是指”bit per second” 而我们在日常生活中，使用的一般是”Byte persecond”(Bps) 我们说的带宽几M几M指的是 2Mbps、8Mbps这种格式，为了便于更加直观的查看，我们会转回成为KB的形式，也就是说，我们拿到这个数字之后，需要先*1024，将M变成K，然后再/8，最后的单位就是我们最常使用的单位了 举个栗子： 1M的带宽，理论的下载速度为：1*1024/8= 128KB/s 8M的带宽，理论的下载速度为：8*1024/8 = 1024KB/s = 1MB/s 在8M带宽之后，我们的换算，可以直接除以8来得到结果 100M的带宽，理论的下载速度为： 100/8 = 12.5MB/s]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识</category>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>带宽计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础知识]]></title>
    <url>%2F2018%2F05%2F24%2FPython%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Python简介Python擅长领域Web开发 django pyramid tornado bottle flask webpy 网络编程 twisted request scrapy paramiko scipy pandas ipython GUI图形开发 wxpython pyqt kivy 运维自动化 openstack saltstack ansible 腾讯蓝鲸 编程语言概述编程语言主要从以下几个角度为进行分类： 低级语言与高级语言 编译型和解释型 静态语言和动态语言 强类型定义语言和弱类型定义语言 每个分类代表什么意思呢，我们一起来看一下。 低级语言与高级语言低级语言 最初的计算机程序都是使用0和1的序列表示的，程序员直接使用的是机器指令，无需翻译，从纸带打孔输入即可得到结果。后来为了记忆方便，就将用0、1序列表示的机器指令都用符号助记，这些与机器指令一一对应的助记符号就成为了汇编指令，从而诞生了汇编语言。 无论是机器指令还是汇编指令，都是面向机器的，这些统称为低级语言。因此是针对特定机器的机器指令的助记符，所以汇编语言是无法独立于机器（特定的CPU体系结构）的 但是汇编语言也是需要经过翻译才能转变为机器指令，所以也就有了将运行在一种机器上的汇编语言翻译成为另一类机器上的机器指令的方法，这被称之为：交叉汇编技术 高级语言 高级语言是从人类的逻辑思维角度出发的计算机语言，因此，抽象程度大大提高，需要经过编译成特定机器上的目标代码才能执行，一条高级语言的语句往往需要若干条机器指令来完成。 高级语言独立于机器的特性是靠编译器为不同机器生成不同的目标代码（或者机器指令）来实现的。具体来说，要将高级语言编译到什么程度，这跟编译的技术相挂钩，可以编译成为直接可以执行的目标代码，也可以变成成为一种中间表示，然后拿到不同的机器和系统上面去执行，这种情况通常是又需要支撑环境，比如解释器或者虚拟机的支持，Java程序编译成为bytecode，再由不同平台上的虚拟机执行就是很好的例子。 所以，说高级语言不依赖于机器，指的是在不同的机器或者平台上高级语言的程序本身不变，而通过编译器编译得到的目标代码去使用不同的机器。从这个意义来说，通过交叉编译，一些汇编程序也可以获得不同机器之间的可移植性，但是这种途径获得的移植性远远不如高级语言来的方便和实用。 总结 我们说的低级语言和高级语言，主要的区别点在于主要针对的对象，低级语言主要针对的对象是特定的机器，而高级语言，主要针对的对象是人类的逻辑。 编译型和解释型概念 编译器是把源程序的每一条语句都编译成机器语言,并保存成二进制文件,这样运行时计算机可以直接以机器语言来运行此程序,速度很快; 解释器则是只在执行程序时,才一条一条的解释成机器语言给计算机来执行,所以运行速度是不如编译后的程序运行的快的. 这是因为计算机不能直接认识并执行我们写的语句,它只能认识机器语言(是二进制的形式) 以下是各语言的分类 编译型and解释型 编译型 优点：编译器一般会有预编译的过程对代码进行优化。因为编译只做一次，运行时不需要编译，所以编译型语言的程序执行效率高。可以脱离语言环境独立运行。 缺点：编译之后如果需要修改就需要整个模块重新编译。编译的时候根据对应的运行环境生成机器码，不同的操作系统之间移植就会有问题，需要根据运行的操作系统环境编译不同的可执行文件。 也就是说，使用编译器将源代码文件进行编译之后，生成文件之后，计算机就可以直接运行，不再需要借助其他的东西，因此运行效率是最高的，但是这要求前期的准备工作必须做的非常完善，因为如果有代码需要修改的话，需要重新编译是生成执行文件。 解释型 优点：有良好的平台兼容性，在任何环境中都可以运行，前提是安装了解释器（一般我们也称之为虚拟机）。灵活，修改代码的时候可以直接修改，可以快速部署，不用停机维护 缺点：每次运行的时候都需要使用解释器解释一遍，因此在性能上是不如编译型语言的。 解释型语言可以无视运行的系统平台，只要有解释器的存在，能将源代码解释翻译成为计算机能够识别的机器语言即可。 两者对比 编译是将源程序翻译成可执行的目标代码，翻译与执行是分开的；而解释是对源程序的翻译与执行一次性完成，不生成可存储的目标代码。这只是表象，二者背后的最大区别是：对解释执行而言，程序运行时的控制权在解释器而不在用户程序；对编译执行而言，运行时的控制权在用户程序。 解释具有良好的动态特性和可移植性，比如在解释执行时可以动态改变变量的类型、对程序进行修改以及在程序中插入良好的调试诊断信息等，而将解释器移植到不同的系统上，则程序不用改动就可以在移植了解释器的系统上运行。同时解释器也有很大的缺点，比如执行效率低，占用空间大，因为不仅要给用户程序分配空间，解释器本身也占用了宝贵的系统资源。【这就是为什么JAVA虚拟机优化知识相当重要】 编译器是把源程序的每一条语句都编译成机器语言,并保存成二进制文件,这样运行时计算机可以直接以机器语言来运行此程序,速度很快;而解释器则是只在执行程序时,才一条一条的解释成机器语言给计算机来执行,所以运行速度是不如编译后的程序运行的快的。 深度扩展 我们先看看编译型，其实它和汇编语言是一样的：也是有一个负责翻译的程序来对我们的源代码进行转换，生成相对应的可执行代码。这个过程说得专业一点，就称为编译（Compile），而负责编译的程序自然就称为编译器（Compiler）。如果我们写的程序代码都包含在一个源文件中，那么通常编译之后就会直接生成一个可执行文件，我们就可以直接运行了。但对于一个比较复杂的项目，为了方便管理，我们通常把代码分散在各个源文件中，作为不同的模块来组织。这时编译各个文件时就会生成目标文件（Object file）而不是前面说的可执行文件。一般一个源文件的编译都会对应一个目标文件。这些目标文件里的内容基本上已经是可执行代码了，但由于只是整个项目的一部分，所以我们还不能直接运行。待所有的源文件的编译都大功告成，我们就可以最后把这些半成品的目标文件“打包”成一个可执行文件了，这个工作由另一个程序负责完成，由于此过程好像是把包含可执行代码的目标文件连接装配起来，所以又称为链接（Link），而负责链接的程序就叫……就叫链接程序（Linker）。链接程序除了链接目标文件外，可能还有各种资源，像图标文件啊、声音文件啊什么的，还要负责去除目标文件之间的冗余重复代码，等等，所以……也是挺累的。链接完成之后，一般就可以得到我们想要的可执行文件了。 上面我们大概地介绍了编译型语言的特点，现在再看看解释型。噢，从字面上看，“编译”和“解释”的确都有“翻译”的意思，它们的区别则在于翻译的时机安排不大一样。打个比方：假如你打算阅读一本外文书，而你不知道这门外语，那么你可以找一名翻译，给他足够的时间让他从头到尾把整本书翻译好，然后把书的母语版交给你阅读；或者，你也立刻让这名翻译辅助你阅读，让他一句一句给你翻译，如果你想往回看某个章节，他也得重新给你翻译。 两种方式，前者就相当于我们刚才所说的编译型：一次把所有的代码转换成机器语言，然后写成可执行文件；而后者就相当于我们要说的解释型：在程序运行的前一刻，还只有源程序而没有可执行程序；而程序每执行到源程序的某一条指令，则会有一个称之为解释程序的外壳程序将源代码转换成二进制代码以供执行，总言之，就是不断地解释、执行、解释、执行……所以，解释型程序是离不开解释程序的。像早期的BASIC就是一门经典的解释型语言，要执行BASIC程序，就得进入BASIC环境，然后才能加载程序源文件、运行。解释型程序中，由于程序总是以源代码的形式出现，因此只要有相应的解释器，移植几乎不成问题。编译型程序虽然源代码也可以移植，但前提是必须针对不同的系统分别进行编译，对于复杂的工程来说，的确是一件不小的时间消耗，况且很可能一些细节的地方还是要修改源代码。而且，解释型程序省却了编译的步骤，修改调试也非常方便，编辑完毕之后即可立即运行，不必像编译型程序一样每次进行小小改动都要耐心等待漫长的Compiling…Linking…这样的编译链接过程。不过凡事有利有弊，由于解释型程序是将编译的过程放到执行过程中，这就决定了解释型程序注定要比编译型慢上一大截，像几百倍的速度差距也是不足为奇的。 编译型与解释型，两者各有利弊。前者由于程序执行速度快，同等条件下对系统要求较低，因此像开发操作系统、大型应用程序、数据库系统等时都采用它，像C/C++、Pascal/Object Pascal（Delphi）、VB等基本都可视为编译语言，而一些网页脚本、服务器脚本及辅助开发接口这样的对速度要求不高、对不同系统平台间的兼容性有一定要求的程序则通常使用解释性语言，如Java、JavaScript、VBScript、Perl、Python等等。 但既然编译型与解释型各有优缺点又相互对立，所以一批新兴的语言都有把两者折衷起来的趋势，例如Java语言虽然比较接近解释型语言的特征，但在执行之前已经预先进行一次预编译，生成的代码是介于机器码和Java源代码之间的中介代码，运行的时候则由JVM（Java的虚拟机平台，可视为解释器）解释执行。它既保留了源代码的高抽象、可移植的特点，又已经完成了对源代码的大部分预编译工作，所以执行起来比“纯解释型”程序要快许多。而像VB6（或者以前版本）、C#这样的语言，虽然表面上看生成的是.exe可执行程序文件，但VB6编译之后实际生成的也是一种中介码，只不过编译器在前面安插了一段自动调用某个外部解释器的代码（该解释程序独立于用户编写的程序，存放于系统的某个DLL文件中，所有以VB6编译生成的可执行程序都要用到它），以解释执行实际的程序体。C#（以及其它.net的语言编译器）则是生成.net目标代码，实际执行时则由.net解释系统（就像JVM一样，也是一个虚拟机平台）进行执行。当然.net目标代码已经相当“低级”，比较接近机器语言了，所以仍将其视为编译语言，而且其可移植程度也没有Java号称的这么强大，Java号称是“一次编译，到处执行”，而.net则是“一次编码，到处编译”。呵呵，当然这些都是题外话了。总之，随着设计技术与硬件的不断发展，编译型与解释型两种方式的界限正在不断变得模糊。 静态语言和动态语言通常我们所说的动态语言、静态语言是指动态类型语言和静态类型语言。【主要指的是数据类型】 动态类型语言：动态类型语言是指在运行期间才去做数据类型检查的语言，也就是说，在用动态类型的语言编程时，永远也不用给任何变量指定数据类型，该语言会在你第一次赋值给变量时，在内部将数据类型记录下来。Python和Ruby就是一种典型的动态类型语言，其他的各种脚本语言如VBScript也多少属于动态类型语言。 静态类型语言：静态类型语言与动态类型语言刚好相反，它的数据类型是在编译其间检查的，也就是说在写程序时要声明所有变量的数据类型，C/C++是静态类型语言的典型代表，其他的静态类型语言还有C#、JAVA等。 强类型定义语言和弱类型定义语言 强类型定义语言：强制数据类型定义的语言。也就是说，一旦一个变量被指定了某个数据类型，如果不经过强制转换，那么它就永远是这个数据类型了。举个例子：如果你定义了一个整型变量a,那么程序根本不可能将a当作字符串类型处理。强类型定义语言是类型安全的语言。 弱类型定义语言：数据类型可以被忽略的语言。它与强类型定义语言相反, 一个变量可以赋不同数据类型的值。 强类型定义语言在速度上可能略逊色于弱类型定义语言，但是强类型定义语言带来的严谨性能够有效的避免许多错误。另外，“这门语言是不是动态语言”与“这门语言是否类型安全”之间是完全没有联系的！ 例如：Python是动态语言，是强类型定义语言（类型安全的语言）; VBScript是动态语言，是弱类型定义语言（类型不安全的语言）; JAVA是静态语言，是强类型定义语言（类型安全的语言）。 注意： 固定了数据类型，并不是说该变量就不能再被赋值了，如果变量被重新赋值了，那么相应的，它的数据类型也可能会发生变化。 这里说的是，赋值这个，数据类型就固定了，在后续使用这个变量的时候，数据类型不会发生改变。 通过上面这些介绍，我们可以得出，python是一门动态解释性的强类型定义高级语言。那这些基因成就了Python的哪些优缺点呢？我们继续往下看。 Python的优缺点优点 Python的定位是“优雅”、“明确”、“简单”，所以Python程序看上去总是简单易懂，初学者学Python，不但入门容易，而且将来深入下去，可以编写那些非常非常复杂的程序。 开发效率非常高，Python有非常强大的第三方库，基本上你想通过计算机实现任何功能，Python官方库里都有相应的模块进行支持，直接下载调用后，在基础库的基础上再进行开发，大大降低开发周期，避免重复造轮子。 高级语言————当你用Python语言编写程序的时候，你无需考虑诸如如何管理你的程序使用的内存一类的底层细节 可移植性————由于它的开源本质，Python已经被移植在许多平台上（经过改动使它能够工 作在不同平台上）。如果你小心地避免使用依赖于系统的特性，那么你的所有Python程序无需修改就几乎可以在市场上所有的系统平台上运行 比如某些程序，必须要调用windows的dll，那么就会依赖操作系统，因此我们要做的就是尽量避免依赖这些 可扩展性————如果你需要你的一段关键代码运行得更快或者希望某些算法不公开，你可以把你的部分程序用C或C++编写，然后在你的Python程序中使用它们。 可嵌入性————你可以把Python嵌入你的C/C++程序，从而向你的程序用户提供脚本功能。 缺点 速度慢，Python 的运行速度相比C语言确实慢很多，跟JAVA相比也要慢一些，因此这也是很多所谓的大牛不屑于使用Python的主要原因，但其实这里所指的运行速度慢在大多数情况下用户是无法直接感知到的，必须借助测试工具才能体现出来，比如你用C运一个程序花了0.01s,用Python是0.1s,这样C语言直接比Python快了10倍,算是非常夸张了，但是你是无法直接通过肉眼感知的，因为一个正常人所能感知的时间最小单位是0.15-0.4s左右，哈哈。其实在大多数情况下Python已经完全可以满足你对程序速度的要求，除非你要写对速度要求极高的搜索引擎等，这种情况下，当然还是建议你用C去实现的。 代码不能加密，因为PYTHON是解释性语言，它的源码都是以名文形式存放的，不过我不认为这算是一个缺点，如果你的项目要求源代码必须是加密的，那你一开始就不应该用Python来去实现。 线程不能利用多CPU问题，这是Python被人诟病最多的一个缺点，GIL即全局解释器锁（Global Interpreter Lock），是计算机程序设计语言解释器用于同步线程的工具，使得任何时刻仅有一个线程在执行，Python的线程是操作系统的原生线程。在Linux上为pthread，在Windows上为Win thread，完全由操作系统调度线程的执行。一个python解释器进程内有一条主线程，以及多条用户程序的执行线程。即使在多核CPU平台上，由于GIL的存在，所以禁止多线程的并行执行。关于这个问题的折衷解决方法，我们在以后线程和进程章节里再进行详细探讨。 任何一门语言都不是完美的，都有擅长和不擅长做的事情，建议各位不要拿一个语言的劣势去跟另一个语言的优势来去比较，语言只是一个工具，是实现程序设计师思想的工具，就像我们之前中学学几何时，有的时候需要要圆规，有的时候需要用三角尺一样，拿相应的工具去做它最擅长的事才是正确的选择。 Python解释器当我们编写Python代码时，我们得到的是一个包含Python代码的以.py为扩展名的文本文件。要运行代码，就需要Python解释器去执行.py文件。 由于整个Python语言从规范到解释器都是开源的，所以理论上，只要水平够高，任何人都可以编写Python解释器来执行Python代码（当然难度很大）。事实上，确实存在多种Python解释器。 CPython 当我们从Python官方网站下载并安装好Python 2.7后，我们就直接获得了一个官方版本的解释器：CPython。这个解释器是用C语言开发的，所以叫CPython。在命令行下运行python就是启动CPython解释器。 CPython是使用最广的Python解释器。教程的所有代码也都在CPython下执行。 IPython IPython是基于CPython之上的一个交互式解释器，也就是说，IPython只是在交互方式上有所增强，但是执行Python代码的功能和CPython是完全一样的。好比很多国产浏览器虽然外观不同，但内核其实都是调用了IE。 CPython用&gt;&gt;&gt;作为提示符，而IPython用In [序号]:作为提示符。 PyPy PyPy是另一个Python解释器，它的目标是执行速度。PyPy采用JIT技术(即时编译技术)，对Python代码进行动态编译（注意不是解释），所以可以显著提高Python代码的执行速度。 绝大部分Python代码都可以在PyPy下运行，但是PyPy和CPython有一些是不同的，这就导致相同的Python代码在两种解释器下执行可能会有不同的结果。如果你的代码要放到PyPy下执行，就需要了解PyPy和CPython的不同点。 Python创始人说：如果想代码跑的快，那就使用pypy Jython Jython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行。 IronPython IronPython和Jython类似，只不过IronPython是运行在微软.Net平台上的Python解释器，可以直接把Python代码编译成.Net的字节码。 总结： Python的解释器很多，但使用最广泛的还是CPython。如果要和Java或.Net平台交互，最好的办法不是用Jython或IronPython，而是通过网络调用来交互，确保各程序之间的独立性。 变量和字符编码变量的作用：存储数据，为了后面调用 变量存储在内存当中，每一个拥有独立的内存空间。 Python变量的定义规则 变量名只能是 字母、数字或下划线的任意组合 变量名的第一个字符不能是数字 以下关键字不能声明为变量名 [‘and’, ‘as’, ‘assert’, ‘break’, ‘class’, ‘continue’, ‘def’, ‘del’, ‘elif’, ‘else’, ‘except’, ‘exec’, ‘finally’, ‘for’, ‘from’, ‘global’, ‘if’, ‘import’, ‘in’, ‘is’, ‘lambda’, ‘not’, ‘or’, ‘pass’, ‘print’, ‘raise’, ‘return’, ‘try’, ‘while’, ‘with’, ‘yield’] 变量和常量注意，在C++等语言中可以定义常量，但是在python中没有常量的概念 那么，如果我们在python中想要定义一个常量，我们就将这个变量名大写。这是一种自发遵守的代码规范，并不是说不能再次修改，只是便于人类识别。 例如： PIE = &quot;watchmen&quot; 字符编码计算机的底层就是电路，电路到最底层就只有两种状态，一种是通电，一种是不通电，那么也就是只能表示两种状态。 128 64 32 16 8 4 2 1 1 1 1 1 1 1 1 1 进位：后面的数字表示有上限之后，才进位到前面，例如：8+4+2+1=15，在表示16的时候，需要在前面表示进一位表示 第n位数表示的值：2^n-1,例如16=2^5-1=2^4 = 8+4+2+1+1 第n位数之前的值的总和是：2^n-1 -1。例如第5位数的前4位数的总和为：2^4-1=16-1 也就是说，我们可以通过这种方式来计算，n位数的总和：2^n-1 上面这些是二进制和数字的对应关系 现在可以将数字和字符进行对应，指定某个数字对应某个字母 根据这种对应关系，人们就创建了ASCII编码方式 ASCII（American Standard Code for Information Interchange，美国标准信息交换代码）是基于拉丁字母的一套电脑编码系统，主要用于显示现代英语和其他西欧语言，其最多只能用 8 位来表示（一个字节），即：2**8 = 256-1，所以，ASCII码最多只能表示 255 个符号。 图中显示的是ASCII的上半部分，一共127个，剩余的128-255个是预留的。 预留的这128个空间，无法存储下中文，因此重新创造了编码方式（扩展编码） 将指定的空间，用来存储索引信息，只要是定位到这个空间，那么就将指向另外的一张中文表（大约7000+个汉字），这就是GB2312编码（1980年创建） 为了处理汉字，程序员设计了用于简体中文的GB2312和用于繁体中文的big5。 GB2312(1980年)一共收录了7445个字符，包括6763个汉字和682个其它符号。汉字区的内码范围高字节从B0-F7，低字节从A1-FE，占用的码位是72*94=6768。其中有5个空位是D7FA-D7FE。 GB2312 支持的汉字太少。1995年的汉字扩展规范GBK1.0收录了21886个符号，它分为汉字区和图形符号区。汉字区包括21003个字符。2000年的 GB18030是取代GBK1.0的正式国家标准。该标准收录了27484个汉字，同时还收录了藏文、蒙文、维吾尔文等主要的少数民族文字。 现在的PC平台必须支持GB18030，对嵌入式产品暂不作要求。所以手机、MP3一般只支持GB2312。 从ASCII、GB2312、GBK 到GB18030，这些编码方法是向下兼容的，即同一个字符在这些方案中总是有相同的编码，后面的标准支持更多的字符。在这些编码中，英文和中文可以统一地处理。区分中文编码的方法是高字节的最高位不为0。按照程序员的称呼，GB2312、GBK到GB18030都属于双字节字符集 (DBCS)。 有的中文Windows的缺省内码还是GBK，可以通过GB18030升级包升级到GB18030。不过GB18030相对GBK增加的字符，普通人是很难用到的，通常我们还是用GBK指代中文Windows内码。 Unicode 显然ASCII码无法将世界上的各种文字和符号全部表示，所以，就需要新出一种可以代表所有字符和符号的编码，即：Unicode Unicode（统一码、万国码、单一码）是一种在计算机上使用的字符编码。Unicode 是为了解决传统的字符编码方案的局限而产生的，它为每种语言中的每个字符设定了统一并且唯一的二进制编码，规定虽有的字符和符号最少由 16 位来表示（2个字节），即：2 **16 = 65536，注：此处说的的是最少2个字节，可能更多 UTF-8，是对Unicode编码的压缩和优化，他不再使用最少使用2个字节，而是将所有的字符和符号进行分类： ascii码中的内容用1个字节保存、欧洲的字符用2个字节保存，东亚的字符用3个字节保存.. UTF-8，可以动态改变长度，可以动态变化的编码集]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
        <category>老男孩视频学习笔记</category>
        <category>Python基础知识</category>
      </categories>
      <tags>
        <tag>老男孩视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IT类书籍资料]]></title>
    <url>%2F2018%2F05%2F20%2FIT%E7%B1%BB%E4%B9%A6%E7%B1%8D%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[从今天开始（2018年5月20日）记录个人运维职业成长过程中所看过的书籍资料情况 科普读物 名称 类型 作者 出版年份 阅读状态 评级 备注 浪潮之巅 书籍 吴军 未读 5星 数学之美 书籍 吴军 未读 数据库Mysql： 名称 类型 作者 出版年份 阅读状态 评级 备注 MySQL技术内幕_InnoDB存储引擎.第2版 书籍 高性能mysql 书籍 MySQL 5.5从零开始学 书籍 Redis 名称 类型 作者 出版年份 阅读状态 评级 备注 Redis运维与开发 书籍 付磊、张益军 2017年4月 精读，已看完 5星 搜狐视频团队出品，其还开源了cachecloud云平台 自动化 名称 类型 作者 出版年份 阅读状态 评级 备注 SaltStack技术入门与实践 书籍 刘继伟、沈灿、赵舜东 2016年1月 编程Python 名称 类型 作者 出版年份 阅读状态 评级 备注 Python编程从入门到实践 书籍]]></content>
      <categories>
        <category>IT基础知识</category>
        <category>IT类书籍资料</category>
      </categories>
      <tags>
        <tag>IT书籍资料</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Salt常用命令]]></title>
    <url>%2F2018%2F05%2F20%2FSalt%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[有关SaltStack相关知识，可以看另一篇文章，本文只是记录常用命令，便于日常使用检索 SaltStack技术入门与实践 Salt-master端命令 查看证书签证情况 [root@master ~]# salt-key -L 同意签证所有没有接受的签证请求 [root@master ~]# salt-key -A -y 在这里，A代表–accept-all的含义 Salt-minion端命令]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维自动化</category>
        <category>SaltStack</category>
      </categories>
      <tags>
        <tag>SaltStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见应用程序/命令-协议/端口号对照表]]></title>
    <url>%2F2018%2F05%2F20%2F%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E7%AB%AF%E5%8F%A3%E5%8F%B7%E5%AF%B9%E7%85%A7%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[网络服务-端口对照关系表 应用程序API（命令） 主流程序 程序类别 3层协议 4层协议 5层协议 6层协议 7层协议 默认服务端口 程序说明 其他信息 FTP数据端口 vsftpd 网络程序 IP TCP FTP 20 文件传输 FTP连接控制认证 vsftpd 网络程序 IP TCP FTP 21 文件传输 ssh openssh 22 加固的远程连接 sallt-key salt-master 网络程序 IP TCP 4505 SaltStack-master端监听端口 sallt-key salt-master 网络程序 IP TCP 4506 SaltStack-master端监听端口 系统服务-协议对照关系表 应用程序/命令 主流程序 程序类别 3层协议 4层协议 5层协议 6层协议 7层协议 默认服务端口 程序说明 其他信息 ping命令 iputils 系统程序 icmp 探测主机到主机之间是否可通信]]></content>
      <categories>
        <category>IT基础知识</category>
        <category>应用协议端口对照表</category>
      </categories>
      <tags>
        <tag>常见应用程序/命令-协议/端口号对照表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7安装Python3]]></title>
    <url>%2F2018%2F05%2F20%2FCentos7%E5%AE%89%E8%A3%85Python3%2F</url>
    <content type="text"><![CDATA[在centos 7中，默认安装的python版本为2.7,一般情况下，我们都需要对python进行升级 [root@master ~]# cat /etc/redhat-release CentOS Linux release 7.3.1611 (Core) [root@master ~]# python --version Python 2.7.5 环境说明[root@master ~]# which python /usr/bin/python [root@master ~]# ll /usr/bin/python lrwxrwxrwx 1 root root 7 Apr 13 16:50 /usr/bin/python -&gt; python2 [root@master ~]# ll /usr/bin/python2 lrwxrwxrwx 1 root root 9 Apr 13 16:50 /usr/bin/python2 -&gt; python2.7 [root@master ~]# ll /usr/bin/python2.7 -rwxr-xr-x 1 root root 7136 Aug 4 2017 /usr/bin/python2.7 我们知道我们的python命令是在/usr/bin目录下 可以看到，python指向的是python2，python2指向的是python2.7 因此我们可以装个python3，然后将python指向python3，然后python2指向python2.7，那么两个版本的python就能共存了。 正式安装下载python3的源码包 wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz 解压编译安装 [root@master software]# tar -zxvf Python-3.6.5.tgz [root@master software]# cd Python-3.6.5 [root@master Python-3.6.5]# ./configure --prefix=/usr/local/python3 [root@master Python-3.6.5]# make &amp;&amp; make install 添加软链接 [root@master Python-3.6.5]# mv /usr/bin/python /usr/bin/python.bak [root@master Python-3.6.5]# ln -s /usr/local/python3/bin/python3.6 /usr/bin/python [root@master Python-3.6.5]# python --version Python 3.6.5 补充操作更改yum配置安装完毕之后，我们需要修改yum的配置，因为其要使用python2执行，此时我们修改了python的指向路径，不修改则会导致yum无法正常使用。 vim /usr/bin/yum 把#! /usr/bin/python修改为#! /usr/bin/python2 vim /usr/libexec/urlgrabber-ext-down 把#! /usr/bin/python 修改为#! /usr/bin/python2]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>基础环境配置</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SaltStack技术入门与实践]]></title>
    <url>%2F2018%2F05%2F20%2FSaltStack%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[参考文件： 书籍：《SaltStack技术入门与实践》 SaltStack基础入门SlatStack是基于python开发的一套C/S架构配置管理工具，底层使用zeroMQ消息队列pub/sub的方式通信，使用SSL证书签发的方式进行认证管理，采用RSA Key方式确认身份，传输采用AES加密，这使得它的安全性有了一定的保障。 并且，SaltStack不止是一个配置管理工具，它还是一个做云计算与数据中心架构编排的利器- 重点： Python C/S架构 Server和Client之间的通信采用ZeroMQ pub/sub通信方式 SSL证书提供加密传输 服务架构在slatstack架构中，服务端叫做Master，客户端叫做Minion（英文翻译为：奴才；仆从；宠臣） 在我们理解的C/S架构中，工作流程为： 客户端发送请求给服务器端 服务器端接受请求并处理 处理完成之后，返回客户端 在slatstack中，不仅有这种传统的C/S架构服务模式，而且还有消息队列中的发布/订阅(pub/sub)服务模式，这就使得saltstack的应用场景更加丰富。 目前在实际的环境中，一般使用C/S架构进行配置管理 工作机制 Master和Minion端都是以守护进程的模式运行的，一直监听配置文件中定义的ret_port(接受minion请求)和publish_port（发布消息）的端口 当Minion运行时会自动连接到配置文件里面定义的Master地址和ret_port端口进行连接认证。 saltstack除了传统的的C/S架构，还有Masterless架构，如果采用该架构，我们就不需要单独安装一台salt-master机器，只需要在每台机器上安装Minion。然后采用本机只负责本机的配置管理工作机制服务模式。 Saltstack架构安装当前salt的安装一共有4四种方式： yum方式安装 pip方式安装 源码包方式安装 salt-bootstrap方式安装 在这里，我们使用yum方式进行安装,另外3种方式可以参见书籍：《SaltStack技术入门与实践》 安装环境为：centos7+python3，如果不知道如何升级python，可以看我的另一篇文章：Centos7安装Python3 参考文献： 官方安装指南 yum源安装# yum -y install https://repo.saltstack.com/py3/redhat/salt-py3-repo-latest-2.el7.noarch.rpm master端安装安装 # yum -y install salt-master 启动服务 # systemctl start salt-master 额外操作 如果是阿里云服务器，我们还要在安全组中开放4505和4506端口 minion端安装配置安装 # yum -y install salt-minion 注意：minion端在安装之后还需要进行配置（在配置文件中添加Master端的相关信息） # sed -i &apos;s/#master: salt/master: 47.93.54.101/g&apos; /etc/salt/minion 启动服务 # systemctl start salt-minion 补充是否将服务设置为开机自动启动，请根据实际情况决定： 查看： # systemctl is-enabled salt-master/salt-minion disabled 设置： # systemctl enable salt-master/salt-minion 开启SaltStack之旅证书管理在前面的介绍中我们知道了saltstack使用SSL签证的方式进行安全认证，接下来我们就开始进行证书的管理 minion端服务启动之后，我们在master端就能看到Minion的证书签证请求，下面我们需要对这台Minion进行签售证书 [root@master ~]# salt-key -L Accepted Keys: Denied Keys: Unaccepted Keys: slave.kisspuppet.com Rejected Keys: [root@master ~]# salt-key -A -y #同意签证所有没有接受的请求 The following keys are going to be accepted: Unaccepted Keys: slave.kisspuppet.com Key for minion slave.kisspuppet.com accepted. [root@master ~]# salt-key -L Accepted Keys: slave.kisspuppet.com Denied Keys: Unaccepted Keys: Rejected Keys: 证书签售成功之后，我们可以运行命令检测我们master与minion之间的通信是否正常 [root@master ~]# salt slave.kisspuppet.com test.ping slave.kisspuppet.com: True 日常命令参数首先我们需要知道，我们在安装salt-master和salt-minion的时候都安装了哪些文件，这样有利于我们去了解SaltStack日后的一些日常操作。 master端[root@master ~]# rpm -ql salt-master /etc/salt/master #salt master配置文件 /etc/salt/master.d /etc/salt/pki/master /usr/bin/salt #salt master核心操作命令 /usr/bin/salt-cp #salt 文件传输命令 /usr/bin/salt-key #salt 证书管理命令 /usr/bin/salt-master /usr/bin/salt-run /usr/bin/salt-unity /usr/lib/systemd/system/salt-master.service /usr/share/man/man1/salt-cp.1.gz /usr/share/man/man1/salt-key.1.gz /usr/share/man/man1/salt-master.1.gz /usr/share/man/man1/salt-run.1.gz /usr/share/man/man1/salt-unity.1.gz /usr/share/man/man1/salt.1.gz /usr/share/man/man7/salt.7.gz salt命令语法 Usage: salt [options] &apos;&lt;target&gt;&apos; &lt;function&gt; [arguments] salt命令相关重要参数 -c CONFIG_DIR, –config-dir=CONFIG_DIR 指定配置文件目录（默认为/etc/salt） –async Run the salt command but don’t wait for a reply 异步执行 -d, –doc, –documentation 查看指定模块或者所有模块文档 –username=USERNAME 指定外部认证用户名 –password=PASSWORD 指定外部认证用户密码 具体命令请看下面的输出： [root@master ~]# salt -h Usage: salt [options] &apos;&lt;target&gt;&apos; &lt;function&gt; [arguments] Salt allows for commands to be executed across a swath of remote systems in parallel, so they can be both controlled and queried with ease. Options: --version show program&apos;s version number and exit -V, --versions-report Show program&apos;s dependencies version number and exit. -h, --help show this help message and exit --saltfile=SALTFILE Specify the path to a Saltfile. If not passed, one will be searched for in the current working directory. -c CONFIG_DIR, --config-dir=CONFIG_DIR Pass in an alternative configuration directory. Default: &apos;/etc/salt&apos;. -t TIMEOUT, --timeout=TIMEOUT Change the timeout, if applicable, for the running command (in seconds). Default: 5. --args-stdin Read additional options and/or arguments from stdin. Each entry is newline separated. --hard-crash Raise any original exception rather than exiting gracefully. Default: False. --no-parse=argname1,argname2,... Comma-separated list of named CLI arguments (i.e. argname=value) which should not be parsed as Python data types -s, --static Return the data from minions as a group after they all return. -p, --progress Display a progress graph. Requires &quot;progressbar&quot; python package. --failhard Stop batch execution upon first &quot;bad&quot; return. --async Run the salt command but don&apos;t wait for a reply. --subset=SUBSET Execute the routine on a random subset of the targeted minions. The minions will be verified that they have the named function before executing. -v, --verbose Turn on command verbosity, display jid and active job queries. --hide-timeout Hide minions that timeout. --show-jid Display jid without the additional output of --verbose. -b BATCH, --batch=BATCH, --batch-size=BATCH Execute the salt job in batch mode, pass either the number of minions to batch at a time, or the percentage of minions to have running. --batch-wait=BATCH_WAIT Wait the specified time in seconds after each job is done before freeing the slot in the batch for the next one. --batch-safe-limit=BATCH_SAFE_LIMIT Execute the salt job in batch mode if the job would have executed on more than this many minions. --batch-safe-size=BATCH_SAFE_SIZE Batch size to use for batch jobs created by batch- safe-limit. --return=RETURNER Set an alternative return method. By default salt will send the return data from the command back to the master, but the return data can be redirected into any number of systems, databases or applications. --return_config=RETURNER_CONF Set an alternative return method. By default salt will send the return data from the command back to the master, but the return data can be redirected into any number of systems, databases or applications. --return_kwargs=RETURNER_KWARGS Set any returner options at the command line. --module-executors=EXECUTOR_LIST Set an alternative list of executors to override the one set in minion config. --executor-opts=EXECUTOR_OPTS Set alternate executor options if supported by executor. Options set by minion config are used by default. -d, --doc, --documentation Return the documentation for the specified module or for all modules if none are specified. --args-separator=ARGS_SEPARATOR Set the special argument used as a delimiter between command arguments of compound commands. This is useful when one wants to pass commas as arguments to some of the commands in a compound command. --summary Display summary information about a salt command. --metadata=METADATA Pass metadata into Salt, used to search jobs. --output-diff Report only those states that have changed. --config-dump Dump the master configuration values --preview-target Show the minions expected to match a target. Does not issue any command. Logging Options: Logging options which override any settings defined on the configuration files. -l LOG_LEVEL, --log-level=LOG_LEVEL Console logging log level. One of &apos;all&apos;, &apos;garbage&apos;, &apos;trace&apos;, &apos;debug&apos;, &apos;profile&apos;, &apos;info&apos;, &apos;warning&apos;, &apos;error&apos;, &apos;critical&apos;, &apos;quiet&apos;. Default: &apos;warning&apos;. --log-file=LOG_FILE Log file path. Default: &apos;/var/log/salt/master&apos;. --log-file-level=LOG_LEVEL_LOGFILE Logfile logging log level. One of &apos;all&apos;, &apos;garbage&apos;, &apos;trace&apos;, &apos;debug&apos;, &apos;profile&apos;, &apos;info&apos;, &apos;warning&apos;, &apos;error&apos;, &apos;critical&apos;, &apos;quiet&apos;. Default: &apos;warning&apos;. Target Options: Target selection options. -H, --hosts List all known hosts to currently visible or other specified rosters -E, --pcre Instead of using shell globs to evaluate the target servers, use pcre regular expressions. -L, --list Instead of using shell globs to evaluate the target servers, take a comma or space delimited list of servers. -G, --grain Instead of using shell globs to evaluate the target use a grain value to identify targets, the syntax for the target is the grain key followed by a globexpression: &quot;os:Arch*&quot;. -P, --grain-pcre Instead of using shell globs to evaluate the target use a grain value to identify targets, the syntax for the target is the grain key followed by a pcre regular expression: &quot;os:Arch.*&quot;. -N, --nodegroup Instead of using shell globs to evaluate the target use one of the predefined nodegroups to identify a list of targets. -R, --range Instead of using shell globs to evaluate the target use a range expression to identify targets. Range expressions look like %cluster. -C, --compound The compound target option allows for multiple target types to be evaluated, allowing for greater granularity in target matching. The compound target is space delimited, targets other than globs are preceded with an identifier matching the specific targets argument type: salt &apos;G@os:RedHat and webser* or E@database.*&apos;. -I, --pillar Instead of using shell globs to evaluate the target use a pillar value to identify targets, the syntax for the target is the pillar key followed by a glob expression: &quot;role:production*&quot;. -J, --pillar-pcre Instead of using shell globs to evaluate the target use a pillar value to identify targets, the syntax for the target is the pillar key followed by a pcre regular expression: &quot;role:prod.*&quot;. -S, --ipcidr Match based on Subnet (CIDR notation) or IP address. Additional Target Options: Additional options for minion targeting. --delimiter=DELIMITER Change the default delimiter for matching in multi- level data structures. Default: &apos;:&apos;. External Authentication: -a EAUTH, --auth=EAUTH, --eauth=EAUTH, --external-auth=EAUTH Specify an external authentication system to use. -T, --make-token Generate and save an authentication token for re-use. The token is generated and made available for the period defined in the Salt Master. --username=USERNAME Username for external authentication. --password=PASSWORD Password for external authentication. Output Options: Configure your preferred output format. --out=OUTPUT, --output=OUTPUT Print the output from the &apos;salt&apos; command using the specified outputter. --out-indent=OUTPUT_INDENT, --output-indent=OUTPUT_INDENT Print the output indented by the provided value in spaces. Negative values disables indentation. Only applicable in outputters that support indentation. --out-file=OUTPUT_FILE, --output-file=OUTPUT_FILE Write the output to the specified file. --out-file-append, --output-file-append Append the output to the specified file. --no-color, --no-colour Disable all colored output. --force-color, --force-colour Force colored output. --state-output=STATE_OUTPUT, --state_output=STATE_OUTPUT Override the configured state_output value for minion output. One of &apos;full&apos;, &apos;terse&apos;, &apos;mixed&apos;, &apos;changes&apos; or &apos;filter&apos;. Default: &apos;none&apos;. --state-verbose=STATE_VERBOSE, --state_verbose=STATE_VERBOSE Override the configured state_verbose value for minion output. Set to True or False. Default: none. You can find additional help about salt issuing &quot;man salt&quot; or on http://docs.saltstack.com [root@master ~]# minion端[root@slave ~]# rpm -ql salt-minion /etc/salt/minion /etc/salt/minion.d /etc/salt/pki/minion /etc/salt/proxy /usr/bin/salt-call /usr/bin/salt-minion /usr/bin/salt-proxy /usr/lib/systemd/system/salt-minion.service /usr/lib/systemd/system/salt-proxy@.service /usr/share/man/man1/salt-call.1.gz /usr/share/man/man1/salt-minion.1.gz /usr/share/man/man1/salt-proxy.1.gz Minio端主要介绍salt-call命令，因为salt-call命令的output和log相关参数与salt命令一样，这里就不对salt-call这两个参数进行讲解了，大家可以参照salt命令的output与log的相关参数 salt-call命令语法 Usage: salt-call [options] &lt;function&gt; [arguments] option:选项 target salt-call命令相关重要参数 -c CONFIG_DIR, –config-dir=CONFIG_DIR 指定配置文件目录（默认为/etc/salt） –master=MASTER Specify the master to use 指定master信息 -d, –doc, –documentation 查看指定模块或者所有模块文档 命令输出如下： [root@slave ~]# salt-call -h Usage: salt-call [options] &lt;function&gt; [arguments] salt-call is used to execute module functions locally on a Salt Minion Options: --version show program&apos;s version number and exit -V, --versions-report Show program&apos;s dependencies version number and exit. -h, --help show this help message and exit --saltfile=SALTFILE Specify the path to a Saltfile. If not passed, one will be searched for in the current working directory. -c CONFIG_DIR, --config-dir=CONFIG_DIR Pass in an alternative configuration directory. Default: &apos;/etc/salt&apos;. --cachedir=CACHEDIR Cache Directory --args-stdin Read additional options and/or arguments from stdin. Each entry is newline separated. --hard-crash Raise any original exception rather than exiting gracefully. Default: False. --no-parse=argname1,argname2,... Comma-separated list of named CLI arguments (i.e. argname=value) which should not be parsed as Python data types -g, --grains Return the information generated by the salt grains. -m MODULE_DIRS, --module-dirs=MODULE_DIRS Specify an additional directory to pull modules from. Multiple directories can be provided by passing `-m /--module-dirs` multiple times. -d, --doc, --documentation Return the documentation for the specified module or for all modules if none are specified. --master=MASTER Specify the master to use. The minion must be authenticated with the master. If this option is omitted, the master options from the minion config will be used. If multi masters are set up the first listed master that responds will be used. --return=RETURNER Set salt-call to pass the return data to one or many returner interfaces. --local Run salt-call locally, as if there was no master running. --file-root=FILE_ROOT Set this directory as the base file root. --pillar-root=PILLAR_ROOT Set this directory as the base pillar root. --states-dir=STATES_DIR Set this directory to search for additional states. --retcode-passthrough Exit with the salt call retcode and not the salt binary retcode. --metadata Print out the execution metadata as well as the return. This will print out the outputter data, the return code, etc. --set-metadata=METADATA Pass metadata into Salt, used to search jobs. --id=ID Specify the minion id to use. If this option is omitted, the id option from the minion config will be used. --skip-grains Do not load grains. --refresh-grains-cache Force a refresh of the grains cache. -t AUTH_TIMEOUT, --timeout=AUTH_TIMEOUT Change the timeout, if applicable, for the running command. Default: 60. --output-diff Report only those states that have changed. Logging Options: Logging options which override any settings defined on the configuration files. -l LOG_LEVEL, --log-level=LOG_LEVEL Console logging log level. One of &apos;all&apos;, &apos;garbage&apos;, &apos;trace&apos;, &apos;debug&apos;, &apos;profile&apos;, &apos;info&apos;, &apos;warning&apos;, &apos;error&apos;, &apos;critical&apos;, &apos;quiet&apos;. Default: &apos;warning&apos;. --log-file=LOG_FILE Log file path. Default: &apos;/var/log/salt/minion&apos;. --log-file-level=LOG_LEVEL_LOGFILE Logfile logging log level. One of &apos;all&apos;, &apos;garbage&apos;, &apos;trace&apos;, &apos;debug&apos;, &apos;profile&apos;, &apos;info&apos;, &apos;warning&apos;, &apos;error&apos;, &apos;critical&apos;, &apos;quiet&apos;. Default: &apos;warning&apos;. Output Options: Configure your preferred output format. --out=OUTPUT, --output=OUTPUT Print the output from the &apos;salt-call&apos; command using the specified outputter. --out-indent=OUTPUT_INDENT, --output-indent=OUTPUT_INDENT Print the output indented by the provided value in spaces. Negative values disables indentation. Only applicable in outputters that support indentation. --out-file=OUTPUT_FILE, --output-file=OUTPUT_FILE Write the output to the specified file. --out-file-append, --output-file-append Append the output to the specified file. --no-color, --no-colour Disable all colored output. --force-color, --force-colour Force colored output. --state-output=STATE_OUTPUT, --state_output=STATE_OUTPUT Override the configured state_output value for minion output. One of &apos;full&apos;, &apos;terse&apos;, &apos;mixed&apos;, &apos;changes&apos; or &apos;filter&apos;. Default: &apos;none&apos;. --state-verbose=STATE_VERBOSE, --state_verbose=STATE_VERBOSE Override the configured state_verbose value for minion output. Set to True or False. Default: none. Profiling support: --profiling-path=PROFILING_PATH Folder that will hold all stats generations path. Default: &apos;/tmp/stats&apos;. --enable-profiling Enable generating profiling stats. See also: --profiling-path. You can find additional help about salt-call issuing &quot;man salt-call&quot; or on http://docs.saltstack.com [root@slave ~]# SaltStack配置文件saltstack的配置文件分别为： Master： /etc/salt/master Minion： /etc/salt/minion 这两部分的内容详细可以看书 master配置文件这里只列出几个比较重要的参数 #max_open_files: 100000 默认注释，根据master和minion的数量进行适当的调整 #timeout: 5 默认注释，根据master和minion的网络状况进行适当的调整 #auto_accept: False #autosign_file: /etc/salt/autosign.conf 默认注释，这两个auto参数在大规模部署minion的时候可以设置自动签证（配置其中一个就可以生效） master_tops和所有以external开头的参数-这些参数是saltstack与外部系统进行整合的相关配置参数 minion配置文件 SaltStack组件]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维自动化</category>
        <category>SaltStack</category>
      </categories>
      <tags>
        <tag>SaltStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix清理历史数据]]></title>
    <url>%2F2018%2F05%2F17%2Fzabbix%E6%B8%85%E7%90%86%E5%8E%86%E5%8F%B2%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[参考文献： 概述zabbix一般都是这几个表太大， history， history_uint，history_log zabbix里面的时间是用的时间戳方式记录，我们可以转换一下，然后根据时间戳来删除； 比如要删除2014年的1月1号以前的数据，先将标准时间转换为时间戳 # date +%s -d &quot;2018-05-15 00:00:01&quot; 1526313601 实际操作停止应用停止zabbix_server [root@dwb-dev1 ~]$ /etc/init.d/zabbix_server stop 清理数据启动mysql /bin/sh /usr/bin/mysqld_safe --datadir=/var/lib/mysql --socket=/var/lib/mysql/mysql.sock --pid-file=/var/run/mysqld/mysqld.pid --basedir=/usr --user=mysql /usr/libexec/mysqld --basedir=/usr --datadir=/var/lib/mysql --user=mysql --log-error=/var/log/mysqld.log --pid-file=/var/run/mysqld/mysqld.pid --socket=/var/lib/mysql/mysql.sock mysql&gt; DELETE FROM history_uint WHERE clock &lt; 1526313601; mysql&gt; delete from history where clock &lt; 1526313601; mysql&gt; optimize table history; mysql&gt; optimize table history_uint; 注：执行过优化命令之后可能会需要很长的一段时间，中间不要中断，否则容易丢失数据。 启动应用在数据库中给pt用户进行授权 删除3个月之前的数据 pt-archiver --source h=192.168.1.202,P=3306,D=zabbix,t=history,u=pt,p=123456 --no-check-charset --where &apos;clock &lt; unix_timestamp(date_sub(curdate(),interval 3 month))&apos; --limit=500 --commit-each --sleep-coef=0.1 --nosafe-auto-increment --noversion-check --why-quit --purge --statistics --bulk-delete --progress=100000 &gt;&gt; ./tmp.log [root@master004-qa ~]# cat tmp.log TIME ELAPSED COUNT 2018-05-21T10:02:53 0 0 2018-05-21T11:37:33 5679 100000 2018-05-21T12:31:14 8900 200000 2018-05-21T12:38:00 9307 300000 2018-05-21T13:03:57 10863 313744 Started at 2018-05-21T10:02:53, ended at 2018-05-21T13:40:20 Source: D=zabbix,P=3306,h=192.168.1.202,p=...,t=history,u=pt SELECT 313744 INSERT 0 DELETE 313675 Action Count Time Pct select 629 11701.8659 89.69 bulk_deleting 628 1310.4642 10.04 commit 629 30.3126 0.23 other 0 4.7184 0.04 Exiting because there are no more rows. [root@master004-qa ~]# 释放空间 pt-online-schema-change --alter &quot;engine=innodb&quot; --print --charset utf8 --chunk-time 1.000000 --max-load Threads_running=50 --recurse=1 --check-interval 5.000000 --alter-foreign-keys-method=none --force --execute --statistics --max-lag 3.000000 --noversion-check --recursion-method=processlist --progress percentage,1 --user=pt --password=123456 --host=192.168.1.202 --port=3306 D=zabbix,t=history 创建主键 mysql -A zabbix -e &quot;alter table history add column id bigint auto_increment primary key first&quot;]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维监控体系</category>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDS]]></title>
    <url>%2F2018%2F05%2F14%2FRDS%2F</url>
    <content type="text"><![CDATA[官方文档 基础知识阿里云关系型数据库（Relational Database Service，简称 RDS）是一种稳定可靠、可弹性伸缩的在线数据库服务。基于阿里云分布式文件系统和SSD盘高性能存储，RDS 支持 MySQL、SQL Server、PostgreSQL 和 PPAS（Postgre Plus Advanced Server，一种高度兼容 Oracle 的数据库）引擎，并且提供了容灾、备份、恢复、监控、迁移等方面的全套解决方案，彻底解决数据库运维的烦恼。 RDS的特点云数据库RDS作为一个公共的关系型数据库，高可用和高安全是其首要优势，其次才是高性能，因为没人会使用既不稳定又不安全的服务。RDS的优势主要体现在如下几点： RDS提供了主备双节点的实例，双节点可以在同一地域的不同可用区，MySQL实例的双节点还可以在不同地域，当主实例出现故障时可快速切换到备实例，保障了RDS的稳定性。 RDS在数据的存取上加入了中间层，所有请求都会经过中间层，而且有SQL注入的请求都会被中间层拦截掉。在底层数据写入上，RDS采用了最高安全级别的写入，保证在主机异常掉电的情况下数据不会出现丢失。以此来保障数据库的高安全性。 RDS源码团队持续对MySQL进行源码优化，在标准的基准测试中性能和稳定性上都是高于社区版本的。 关于这部分内容可以查看：对比ECS自建数据库与RDS性能时的注意事项 访问控制数据库账号 当用户创建实例后，RDS并不会为用户创建任何初始的数据库账号。 有如下两种方式来创建数据库帐号： 用户可以通过控制台或者API来创建普通数据库账号，并设置数据库级别的读写权限。 如果用户需要更细粒度的权限控制，比如表、视图，字段级别的权限，也可以通过控制台或者API先创建高权限数据库账号，并使用数据库客户端和高权限数据库账号来创建普通数据库账号。高权限数据库账号可以为普通数据库账号设置表级别的读写权限。 说明：通过高权限数据库账号创建的普通数据库账号，无法通过控制台或者API进行管理。 IP白名单 虽然RDS不支持ECS的安全组功能，但是RDS提供了IP白名单来实现网络安全访问控制。 默认情况下，RDS实例被设置为不允许任何IP访问，即127.0.0.1。 用户可以通过控制台的数据安全性模块或者API来添加IP白名单规则。IP白名单的更新无需重启RDS实例，因此不会影响用户的使用。 IP白名单可以设置多个分组，每个分组可配置1000个IP或IP段。 设置白名单后，只有以下服务器才能访问RDS实例： 白名单中 IP 地址所属的服务器 白名单中 ECS 安全组内的 ECS 实例 注意事项： 系统会给每个实例创建一个默认的default白名单分组，该白名单分组只能被修改或清空，但不能被删除。 对于新建的RDS实例，系统默认会将回送地址127.0.0.1添加到default白名单分组中，IP地址127.0.0.1代表禁止所有IP地址或IP段访问该RDS实例。所以，在您设置白名单时，需要先将127.0.0.1删除，然后再添加您允许访问该RDS实例的IP地址或IP段。 若将白名单设置为%或者0.0.0.0/0，代表允许任何IP访问RDS实例。该设置将极大降低数据库的安全性，如非必要请勿使用。 安全组 目前仅杭州、青岛、香港地域支持 ECS 安全组。 目前仅支持添加一个安全组。 对白名单中的 ECS 安全组的更新将实时应用到白名单中。 系统安全 RDS 处于多层防火墙的保护之下，可以有力地抗击各种恶意攻击，保证数据的安全。 RDS 服务器不允许直接登录，只开放特定的数据库服务所需要的端口。 RDS 服务器不允许主动向外发起连接，只能接受被动访问。 数据链路服务阿里云数据库提供全数据链路服务，包括 DNS、负载均衡、Proxy 等。因为 RDS 使用原生的 DB Engine，对数据库的操作高度类似，基本没有学习成本。 DNS DNS 模块提供域名到 IP 的动态解析功能，以便规避 RDS 实例 IP 地址改变带来的影响。在连接池中设置域名后，即使对应的IP地址发生了变化，仍然可以正常访问 RDS 实例。 例如，某 RDS 实例的域名为 test.rds.aliyun.com，对应的 IP 地址为 10.10.10.1。某程序连接池中设置为 test.rds.aliyun.com 或 10.10.10.1 都可以正常访问 RDS 实例。 一旦该 RDS 实例发生了可用区迁移或者版本升级后，IP 地址可能变为 10.10.10.2。如果程序连接池中设置的是域名 test.rds.aliyun.com，则仍然可以正常访问 RDS 实例。但是如果程序连接池中设置的是IP地址 10.10.10.1，就无法访问 RDS 实例了。 负载均衡 负载均衡 模块提供实例 IP 地址（包括内网 IP 和外网 IP），以便屏蔽物理服务器变化带来的影响。 例如，某 RDS 实例的内网 IP 地址为 10.1.1.1，对应的 Proxy 或者 DB Engine 运行在 192.168.0.1 上。在正常情况下，负载均衡 模块会将访问 10.1.1.1 的流量重定向到 192.168.0.1 上。当 192.168.0.1 发生了故障，处于热备状态的 192.168.0.2 接替了 192.168.0.1 的工作。此时 负载均衡 模块会将访问 10.1.1.1 的流量重定向到 192.168.0.2 上，RDS 实例仍旧正常提供服务。 Proxy Proxy 模块提供数据路由、流量探测和会话保持等功能。 数据路由功能：支持大数据场景下的分布式复杂查询聚合和相应的容量管理。 流量探测功能：降低 SQL 注入的风险，在必要情况下支持 SQL 日志的回溯。 会话保持功能：解决故障场景下的数据库连接中断问题。 高可用服务高可用服务由 Detection、Repair、Notice 等模块组成，主要保障数据链路服务的可用性，除此之外还负责处理数据库内部的异常。 另外，RDS 还通过迁移到支持多可用区的地域和采用适当的高可用策略，提升 RDS 的高可用服务。 Detection Detection 模块负责检测 DB Engine 的主节点和备节点是否提供了正常的服务。通过间隔为 8~10 秒的心跳信息，HA 节点可以轻易获得主节点的健康情况，结合备节点的健康情况和其它 HA 节点的心跳信息，Detection 模块可以排除网络抖动等异常引入的误判风险，在 30 秒内完成异常切换操作。 Repair Repair 模块负责维护 DB Engine 的主节点和备节点之间的复制关系，还会修复主节点或者备节点在日常运行中出现的错误。 例如： 主备复制异常断开的自动修复 主备节点表级别损坏的自动修复 主备节点 Crash 的现场保存和自动修复 Notice Notice 模块负责将主备节点的状态变动通知到 负载均衡 或者 Proxy，保证用户访问正确的节点。 例如：Detection 模块发现主节点异常，并通知 Repair 模块进行修复。Repair 模块进行了尝试后无法修复主节点，通知 Notice 进行流量切换。Notice 模块将切换请求转发至 负载均衡 或者Proxy，此时用户流量全部指向备节点。与此同时，Repair 在别的物理服务器上重建了新的备节点，并将变动同步给 Detection 模块。Detection 模块开始重新检测实例的健康状态。 多可用区 RDS在特定地域提供了多可用区部署的能力，也就是将RDS的主备实例分别部署于同一地域的不同可用区。相对于单可用区 RDS 实例，多可用区 RDS 实例可以承受更高级别的灾难。 目前多可用区 RDS 不额外收取任何费用，用户可以直接在已开通多可用区的地域购买多可用区 RDS 实例，也可以通过跨可用区迁移将单可用区 RDS 实例转化成多可用区 RDS 实例。 注意： 因为多可用区之间存在一定的网络延迟，因此多可用区 RDS 实例在采用半同步数据复制方案的时候，对于单个更新的响应时间会比单可用区实例长。这种情况最好通过提高并发量的方式来实现整体吞吐量的提高。 高可用策略 高可用策略是根据用户自身业务的特点，采用服务优先级和数据复制方式之间的不同组合，以组合出适合自身业务特点的高可用策略。 服务优先级有以下两个级别： RTO（Recovery Time Objective）优先：数据库应该尽快恢复服务，即可用时间最长。对于数据库在线时间要求比较高的用户应该使用 RTO 优先策略。 RPO（Recovery Point Objective）优先：数据库应该尽可能保障数据的可靠性，即数据丢失量最少。对于数据一致性要求比较高的用户应该使用 RPO 优先策略。 数据复制方式有以下三种方式： 异步复制（Async）：应用发起更新（含增加、删除、修改操作）请求，Master 完成相应操作后立即响应应用，Master 向 Slave 异步复制数据。因此异步复制方式下，Slave 不可用不影响主库上的操作，而 Master 不可用有较小概率会引起数据不一致。 强同步复制（Sync）：应用发起更新（含增加、删除、修改操作）请求，Master 完成操作后向 Slave 复制数据，Slave 接收到数据后向 Master 返回成功信息，Master 接到 Slave 的反馈后再响应应用。Master 向 Slave 复制数据是同步进行的，因此 Slave 不可用会影响 Master 上的操作，而 Master 不可用不会引起数据不一致。 半同步复制（Semi-Sync）：正常情况下数据复制方式采用强同步复制方式，当 Master 向 Slave 复制数据出现异常的时候（Slave 不可用或者双节点间的网络异常），Master 会暂停对应用的响应，直到复制方式超时退化成异步复制。如果允许应用在此时更新数据，则 Master 不可用会引起数据不一致。当双节点间的数据复制恢复正常（Slave 恢复或者网络恢复），异步复制会恢复成强同步复制。恢复成强同步复制的时间取决于半同步复制的实现方式，阿里云数据库 MySQL 5.5 版和 MySQL 5.6 版有所不同。 实际操作MySQL数据库版本阿里云上的MySQL提供基础版、高可用版和金融版三种版本 基础版一般就是用于个人学习、或开发测试时使用。目前基础版只提供MySQL 5.7版本，并且只提供单节点部署，性价比非常高。基础版采用计算节点与存储分离的实现方式，也就是说假如计算节点宕机，MySQL就不可用啦，但数据都存在云盘里面不会丢，数据一致性还是可以得到保证，不用担心数据丢失。可用性不高这是基础版的最大问题，反正只是用于不重要的场景，生产环境大家是不会选用基础版的。 高可用版顾名思义，为应用提供了数据库的高可用保障，也就是说至少要用双节点。RDS MySQL高可用版采用一主一备的经典高可用架构，采用基于binlog的数据复制技术维护数据库的可用性和数据一致性。同时，高可用版从性能上也可以保障业务生产环境的需求，配置上采用物理服务器部署，本地SSD硬盘，提供最佳性能，各方面表现均衡。 最高级的是金融版，针对像金融、证券、保险等行业的核心数据库，他们对数据安全性、可用性要求非常高。金融版采用三节点，实现一主两备的部署架构，通过binlog日志多副本多级别复制，确保数据的强一致性，可提供金融级的数据可靠性和跨机房容灾能力。 规格阿里云上MySQL有三种规格类型：通用型、独享型和独占型。 其中通用型和独享型都是在一台物理服务器上划分多个资源隔离的区域，为不同用户提供MySQL数据库实例。他们的不同点在于，通用型对于CPU和存储空间采用了复用的技术。当部署在同一台服务器上的所有MySQL 实例都很繁忙的情况下，有可能会出现实例间的CPU争抢，或存储的争抢；而独享型虽然也是多个数据库实例共享一台物理服务器，但资源隔离策略上确保每个用户都可以独享所分配到的CPU、内存、I/O、存储，不会出现多个实例发生资源争抢的情况。 最高级别的一种是独占型，是指一个MySQL实例独占一台服务器，会获得最好的性能，当然价格也最贵。最求极致性能但对价格不敏感的客户一般会在重要业务系统采用独占型实例。 使用流程通常，从新购实例到可以开始使用实例，需要完成如下操作： 使用限制高权限账号数据库连接注意：目前只支持同一个可用区的连接，不同可用区无法连接，如果需要跨越可用区，需要进行设置 目前RDS连接可以使用DMS连接或者第三方工具连接 跨可用区访问管理工具-DMSDMS 是一款访问管理云端数据库的Web服务，支持Redis、 MySQL、SQL Server、PostgreSQL和MongoDB等数据源。DMS提供了数据管理、对象管理、数据流转和实例管理四部分功能。DMS使用也非常简单： 数据迁移-DTS相关资料： 文档中心 帮助中心 ECS自建数据库迁移到RDS DTS概述数据传输(Data Transmission)服务DTS是阿里云提供的一种支持RDBMS(关系型数据库)、NoSQL、OLAP等多种数据源之间数据交互的数据服务。它提供了数据迁移、实时数据订阅及数据实时同步等多种数据传输能力。通过数据传输可实现不停服数据迁移、数据异地灾备、跨境数据同步、缓存更新策略等多种业务应用场景，助您构建安全、可扩展、高可用的数据架构。 数据传输服务DTS的目标是帮用户将复杂的数据交互工作承担下来，让用户可以专注于上层的业务开发，数据传输服务承诺99.95%的链路稳定性。 数据传输服务DTS支持多种数据源类型，例如： 关系型数据库：Oracle、MySQL、SQLServer、PostgreSQL NoSQL: Redis OLAP: 分析型数据库AnalyticDB 迁移服务主要帮助用户把数据从本地数据库迁移到阿里云数据库，或者把阿里云数据库的一个实例迁移到另一实例中。阿里云数据库提供了数据传输服务DTS（Data Transfer Service）工具，方便用户快速的迁移数据库。 DTS是一个云上的数据传输服务，能快速的将本地数据库或者RDS中的实例迁移到另一个RDS实例中。关于DTS简介，请参见DTS产品概述。 DTS提供了三种迁移模式，分别为结构迁移、全量迁移和增量迁移： 结构迁移：DTS会将迁移对象的结构定义迁移到目标实例，目前支持结构迁移的对象有表、视图、触发器、存储过程和存储函数。 全量迁移：DTS会将源数据库迁移对象已有数据全部迁移到目标实例中。 注意：在全量迁移过程中，为了保证数据一致性，无主键的非事务表会被锁定。锁定期间这些表无法写入，锁定时长依赖于这些表的数据量大小。在这些无主键非事务表迁移完成后，锁才会释放。 增量迁移：DTS会将迁移过程中数据变更同步到目标实例。 , 注意：如果迁移期间进行了DDL操作，这些结构变更不会同步到目标实例。 源及目标数据迁移支持的源实例类型包括: (1) RDS实例 (2) 本地自建数据库 (3) ECS自建数据库 数据迁移支持的目标实例包括： (1) RDS实例 (2) ECS自建数据库 (3) Redis实例 Mysql迁移限制对于本地 MySQL-&gt;RDS for MySQL 的数据迁移，DTS 支持结构迁移、全量数据迁移及增量数据迁移，各迁移类型的功能及限制如下： 迁移过程中，不支持 DDL 操作。 结构迁移不支持 event 的迁移。 如果使用了对象名映射功能后，依赖这个对象的其他对象可能迁移失败。 当选择增量迁移时，源端的本地 MySQL 实例需要按照要求开启 binlog。 当选择增量迁移时，源库的 binlog_format 需要设置为 row。 当选择增量迁移且源 MySQL 实例如果为 5.6 或以上版本时，它的 binlog_row_image 必须为 full。 数据同步数据实时同步功能旨在帮助用户实现两个数据源之间的数据实时同步。通过数据实时同步功能可实现数据异地灾备、本地数据灾备、跨境数据同步及在线离线数据打通(OLTP-&gt;OLAP数据同步)等多种业务场景。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>公有云产品</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>RDS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP请求]]></title>
    <url>%2F2018%2F05%2F13%2FHTTP%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[本文转载自：HTTP深入浅出 http请求 概述HTTP(HyperText Transfer Protocol)是一套计算机通过网络进行通信的规则。 计算机专家设计出HTTP，使HTTP客户（如Web浏览器）能够从HTTP服务器(Web服务器)请求信息和服务，HTTP目前协议的版本是1.1。 HTTP是一种无状态的协议，无状态是指Web浏览器和Web服务器之间不需要建立持久的连接，这意味着当一个客户端向服务器端发出请求，然后Web服务器返回响应(response)，连接就被关闭了，在服务器端不保留连接的有关信息.HTTP遵循请求(Request)/应答(Response)模型。Web浏览器向Web服务器发送请求，Web服务器处理请求并返回适当的应答。所有HTTP连接都被构造成一套请求和应答。 HTTP使用内容类型，是指Web服务器向Web浏览器返回的文件都有与之相关的类型。所有这些类型在MIME Internet邮件协议上模型化，即Web服务器告诉Web浏览器该文件所具有的种类，是HTML文档、GIF格式图像、声音文件还是独立的应用程序。大多数Web浏览器都拥有一系列的可配置的辅助应用程序，它们告诉浏览器应该如何处理Web服务器发送过来的各种内容类型。 通信过程在一次完整的HTTP通信过程中，Web浏览器与Web服务器之间将完成下列7个步骤： 1. 建立TCP连接 在HTTP工作开始之前，Web浏览器首先要通过网络与Web服务器建立连接，该连接是通过TCP来完成的，该协议与IP协议共同构建Internet，即著名的TCP/IP协议族，因此Internet又被称作是TCP/IP网络。HTTP是比TCP更高层次的应用层协议，根据规则，只有低层协议建立之后才能，才能进行更层协议的连接，因此，首先要建立TCP连接，一般TCP连接的端口号是80 2. Web浏览器向Web服务器发送请求命令 一旦建立了TCP连接，Web浏览器就会向Web服务器发送请求命令 例如：GET/sample/hello.jsp HTTP/1.1 3. Web浏览器发送请求头信息 浏览器发送其请求命令之后，还要以头信息的形式向Web服务器发送一些别的信息，之后浏览器发送了一空白行来通知服务器，它已经结束了该头信息的发送。 4. Web服务器应答 客户机向服务器发出请求后，服务器会客户机回送应答， HTTP/1.1 200 OK 应答的第一部分是协议的版本号和应答状态码 5. Web服务器发送应答头信息 正如客户端会随同请求发送关于自身的信息一样，服务器也会随同应答向用户发送关于它自己的数据及被请求的文档。 6. Web服务器向浏览器发送数据 Web服务器向浏览器发送头信息后，它会发送一个空白行来表示头信息的发送到此为结束，接着，它就以Content-Type应答头信息所描述的格式发送用户所请求的实际数据 7. Web服务器关闭TCP连接 一般情况下，一旦Web服务器向浏览器发送了请求数据，它就要关闭TCP连接，然后如果浏览器或者服务器在其头信息加入了这行代码Connection:keep-alive 添加之后，TCP连接在发送后将仍然保持打开状态，于是，浏览器可以继续通过相同的连接发送请求。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带宽。 HTTP请求HTTP请求格式当浏览器向Web服务器发出请求时，它向服务器传递了一个数据块，也就是请求信息，HTTP请求信息由3部分组成： 请求方法URI协议/版本 请求头(Request Header) 请求正文 下面是一个HTTP请求的例子： GET/sample.jspHTTP/1.1 Accept:image/gif.image/jpeg,*/* Accept-Language:zh-cn Connection:Keep-Alive Host:localhost User-Agent:Mozila/4.0(compatible;MSIE5.01;Window NT5.0) Accept-Encoding:gzip,deflate username=jinqiao&amp;password=1234 说明： （1）请求方法URI协议/版本 请求的第一行是“方法URL议/版本”：GET/sample.jsp HTTP/1.1 以上代码中“GET”代表请求方法，“/sample.jsp”表示URI，“HTTP/1.1代表协议和协议的版本。 根据HTTP标准，HTTP请求可以使用多种请求方法。例如：HTTP1.1支持7种请求方法：GET、POST、HEAD、OPTIONS、PUT、DELETE和TARCE。在Internet应用中，最常用的方法是GET和POST。 URL完整地指定了要访问的网络资源，通常只要给出相对于服务器的根目录的相对目录即可，因此总是以“/”开头，最后，协议版本声明了通信过程中使用HTTP的版本。 （2）请求头(Request Header) 请求头包含许多有关的客户端环境和请求正文的有用信息。例如，请求头可以声明浏览器所用的语言，请求正文的长度等。 Accept:image/gif.image/jpeg.*/* Accept-Language:zh-cn Connection:Keep-Alive Host:localhost User-Agent:Mozila/4.0(compatible:MSIE5.01:Windows NT5.0) Accept-Encoding:gzip,deflate. （3）请求正文 请求头和请求正文之间是一个空行，这个行非常重要，它表示请求头已经结束，接下来的是请求正文。 请求正文中可以包含客户提交的查询字符串信息：username=jinqiao&amp;password=1234 在以上的例子的HTTP请求中，请求的正文只有一行内容。当然，在实际应用中，HTTP请求正文可以包含更多的内容。 HTTP请求方法我这里只讨论GET方法与POST方法 GET方法 GET方法是默认的HTTP请求方法，我们日常用GET方法来提交表单数据，然而用GET方法提交的表单数据只经过了简单的编码，同时它将作为URL的一部分向Web服务器发送，因此，如果使用GET方法来提交表单数据就存在着安全隐患上。例如Http://127.0.0.1/login.jsp?Name=zhangshi&amp;Age=30&amp;Submit=%cc%E+%BD%BB从上面的URL请求中，很容易就可以辩认出表单提交的内容。（？之后的内容）另外由于GET方法提交的数据是作为URL请求的一部分所以提交的数据量不能太大 POST方法 POST方法是GET方法的一个替代方法，它主要是向Web服务器提交表单数据，尤其是大批量的数据。POST方法克服了GET方法的一些缺点。通过POST方法提交表单数据时，数据不是作为URL请求的一部分而是作为标准数据传送给Web服务器，这就克服了GET方法中的信息无法保密和数据量太小的缺点。因此，出于安全的考虑以及对用户隐私的尊重，通常表单提交时采用POST方法。 从编程的角度来讲，如果用户通过GET方法提交数据，则数据存放在QUERY＿STRING环境变量中，而POST方法提交的数据则可以从标准输入流中获取。 HTTP响应HTTP应答与HTTP请求相似，HTTP响应也由3个部分构成，分别是： 协议状态版本代码描述 响应头(Response Header) 响应正文 下面是一个HTTP响应的例子： HTTP/1.1 200 OK Server:Apache Tomcat/5.0.12 Date:Mon,6Oct2003 13:23:42 GMT Content-Length:112 &lt;html&gt; &lt;head&gt; &lt;title&gt;HTTP响应示例&lt;title&gt; &lt;/head&gt; &lt;body&gt; Hello HTTP! &lt;/body&gt; &lt;/html&gt; （1）协议状态版本代码描述 协议状态代码描述HTTP响应的第一行类似于HTTP请求的第一行，它表示通信所用的协议是HTTP1.1服务器已经成功的处理了客户端发出的请求（200表示成功）: HTTP/1.1 200 OK （2）响应头 响应头(Response Header)响应头也和请求头一样包含许多有用的信息，例如服务器类型、日期时间、内容类型和长度等： Server:Apache Tomcat/5.0.12 Date:Mon,6Oct2003 13:13:33 GMT Content-Type:text/html Last-Moified:Mon,6 Oct 2003 13:23:42 GMT Content-Length:112 （3）响应正文 响应正文响应正文就是服务器返回的HTML页面： &lt;html&gt; &lt;head&gt; &lt;title&gt;HTTP响应示例&lt;title&gt; &lt;/head&gt; &lt;body&gt; Hello HTTP! &lt;/body&gt; &lt;/html&gt; 响应头和正文之间也必须用空行分隔。 HTTP应答码 HTTP应答码也称为状态码，它反映了Web服务器处理HTTP请求状态。HTTP应答码由3位数字构成，其中首位数字定义了应答码的类型： 1XX－信息类(Information),表示收到Web浏览器请求，正在进一步的处理中 2XX－成功类（Successful）,表示用户请求被正确接收，理解和处理例如：200 OK 3XX-重定向类(Redirection),表示请求没有成功，客户必须采取进一步的动作。 4XX-客户端错误(Client Error)，表示客户端提交的请求有错误 例如：404 NOT Found，意味着请求中所引用的文档不存在。 5XX-服务器错误(Server Error)表示服务器不能完成对请求的处理：如 500 常见请求方法 GET 通过请求URI得到资源 POST用于添加新的内容 PUT用于修改某个内容 DELETE删除某个内容 CONNECT,用于代理进行传输，如使用SSL OPTIONS询问可以执行哪些方法 PATCH,部分文档更改 PROPFIND, (wedav)查看属性 PROPPATCH, (wedav)设置属性 MKCOL, (wedav)创建集合（文件夹） COPY, (wedav)拷贝 MOVE, (wedav)移动 LOCK, (wedav)加锁 UNLOCK (wedav)解锁 TRACE用于远程诊断服务器 HEAD类似于GET, 但是不返回body信息，用于检查对象是否存在，以及得到对象的元数据]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识</category>
        <category>HTTP请求</category>
      </categories>
      <tags>
        <tag>HTTP请求</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper常用命令]]></title>
    <url>%2F2018%2F05%2F02%2FZookeeper%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[zk客户端命令zookeeper命令行工具类似于Linux的shell环境，使用它我们可以简单的对zookeeper进行访问、数据创建、数据修改等操作 语法： $ sh zkCli.sh -server 127.0.0.1:2181 一些简单操作： 显示根目录下、文件： ls / 使用 ls 命令来查看当前 ZooKeeper 中所包含的内容 显示根目录下、文件： ls2 / 查看当前节点数据并能看到更新次数等数据 创建文件，并设置初始内容： create /zk “test” 创建一个新的 znode节点“ zk ”以及与它关联的字符串 获取文件内容： get /zk 确认 znode 是否包含我们所创建的字符串 修改文件内容： set /zk “zkbak” 对 zk 所关联的字符串进行设置 删除文件： delete /zk 将刚才创建的 znode 删除 退出客户端： quit 帮助命令： help 四字命令： ZooKeeper 支持某些特定的四字命令字母与其的交互。它们大多是查询命令，用来获取 ZooKeeper 服务的当前状态及相关信息。用户在客户端可以通过 telnet 或 nc 向 ZooKeeper 提交相应的命令 echo stat|nc 127.0.0.1 2181 来查看哪个节点被选择作为follower或者leader echo ruok|nc 127.0.0.1 2181 测试是否启动了该Server，若回复imok表示已经启动。 echo dump| nc 127.0.0.1 2181 ,列出未经处理的会话和临时节点。 echo kill | nc 127.0.0.1 2181 ,关掉server echo conf | nc 127.0.0.1 2181 ,输出相关服务配置的详细信息。 echo cons | nc 127.0.0.1 2181 ,列出所有连接到服务器的客户端的完全的连接 / 会话的详细信息。 echo envi |nc 127.0.0.1 2181 ,输出关于服务环境的详细信息（区别于 conf 命令）。 echo reqs | nc 127.0.0.1 2181 ,列出未经处理的请求。 echo wchs | nc 127.0.0.1 2181 ,列出服务器 watch 的详细信息。 echo wchc | nc 127.0.0.1 2181 ,通过 session 列出服务器 watch 的详细信息，它的输出是一个与 watch 相关的会话的列表。 echo wchp | nc 127.0.0.1 2181 ,通过路径列出服务器 watch 的详细信息。它输出一个与 session 相关的路径。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL普通用户无法本地登录的解决方法及MySQL的用户认证算法]]></title>
    <url>%2F2018%2F05%2F02%2FMySQL%E6%99%AE%E9%80%9A%E7%94%A8%E6%88%B7%E6%97%A0%E6%B3%95%E6%9C%AC%E5%9C%B0%E7%99%BB%E5%BD%95%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%E5%8F%8AMySQL%E7%9A%84%E7%94%A8%E6%88%B7%E8%AE%A4%E8%AF%81%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[参考文献： MySQL普通用户无法本地登录的解决方法及MySQL的用户认证算法 问题在启动cachecloud项目的时候，发现日志中出现大量的连接数据库报错 我的授权命令为： mysql&gt; grant all privileges on cache_cloud.* to &apos;cachecloud&apos;@&apos;%&apos; identified by &apos;Cache_cloud123&apos;; Query OK, 0 rows affected (0.00 sec) 这样的配置，按道理来说，是不应该出现连不上的（%代表任意的主机来源，并且已经排查了防火墙等因素） 在本地登录发现发现存在如下问题： 当输入之前设置的密码时，将会一直提示：“ERROR 1045 (28000): Access denied”，而当我们不输入密码，也就是说输入密码为空时则能正常进入数据库。 我们使用USER()和CURRENT_USER()两个函数查看所使用的用户。 mysql&gt; SELECT USER(), CURRENT_USER(); +----------------------+----------------+ | USER() | CURRENT_USER() | +----------------------+----------------+ | cachecloud@localhost | @localhost | +----------------------+----------------+ 1 row in set (0.00 sec) mysql&gt; USER()函数返回你在客户端登陆时指定的用户名和主机名。 CURRENT_USER()函数返回的是MySQL使用授权表中的哪个用户来认证你的登录请求。 这里发现，之前设置的授权规则并没有生效，是数据库使用的是’’@’localhost’这个来源信息来进行登录认证，而’’@’localhost’这个匿名用户是没有密码的，因此我输入空密码登录成功了。但是登录后，所对应的用户的匿名用户。 一般在MySQL在安装完毕后，我们使用mysql_install_db这个脚本生成授权表，会默认创建’’@’localhost’这个匿名用户。正是因为这个匿名用户，影响了其他用户从本地登录的认证。 原因那么MySQL是如何进行用户身份认证呢？ MySQL的简要认证算法如下： 当用户从客户端请求登陆时，MySQL将授权表中的条目与客户端所提供的条目进行比较，包括用户的用户名，密码和主机。 授权表中的Host字段是可以使用通配符作为模式进行匹配的，如test.example.com, %.example.com, %.com和%都可以匹配test.example.com这个主机。 授权表中的User字段不允许使用模式匹配，但是可以有一个空字符的用户名代表匿名用户，并且空字符串可以匹配所有的用户名，就像通配符一样。 当user表中的Host和User有多个值可以匹配客户端提供的主机和用户名时，MySQL将user表读入内存，并且按照一定规则排序，按照排序规则读取到的第一个匹配客户端用户名和主机名的条目对客户端进行身份验证。 排序规则： 对于Host字段，按照匹配的精确程度进行排序，越精确的排序越前，例如当匹配test.example.com这个主机时, %.example.com比%.com更精确，而test.example.com比%.example.com更精确。 对于User字段，非空的字符串用户名比空字符串匹配的用户名排序更靠前。 User和Host字段都有多个匹配值，MySQL使用主机名排序最前的条目，在主机名字段相同时再选取用户名排序更前的条目。 因此，如果User和Host字段都有多个匹配值，主机名最精确匹配的条目被用户对用户进行认证。 了解了这个规则之后，我们就知道为什么cachecloud登录失败了。 在使用该用户进行本机登录的时候，mysql中有2个匹配条目 ‘cachecloud’@’%’ ‘’@’localhost’ 匿名用户能够匹配的原因上面说过，空字符串可以匹配所有的用户名，就像通配符一样。 根据MySQL认证时的排序规则，第一个条目的用户名排序更前，第二个条目的主机名更精确，排序更前。 而MySQL会优先使用主机名排序第一的条目进行身份认证，因此’’@’localhost’被用户对客户端进行认证。因此，只有使用匿名用户的空密码才能登录进数据库。就会出现刚才上面的情况了。 解决删除匿名用户【仅仅是为了安全也有这个必要】 为什么root用户不会受影响，而只有普通用户不能从本地登录？ 因为mysql_install_db脚本会在授权表中生成’root’@’localhost’这个账户。同样的，使用root登录MySQL时，’root’@’localhost’和’’@’localhost’都能匹配登录的账户，但是根据排序规则，主机名相同，而用户名非空字符串优先，因此’roo’@’localhost’这个条目的排序更靠前。使用root本地登录是不会被匿名用户遮盖。 [root@qa1-common004 ~]# mysql -u root -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 942 Server version: 5.6.40 MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. mysql&gt; mysql&gt; select user,host from mysql.user; +------------+---------------------------+ | user | host | +------------+---------------------------+ | cachecloud | % | | root | 127.0.0.1 | | root | ::1 | | | localhost | | root | localhost | | | qa1-common004.ecs.east1-b | | root | qa1-common004.ecs.east1-b | +------------+---------------------------+ 7 rows in set (0.00 sec) mysql&gt; delete from mysql.user where user=&apos;&apos;; Query OK, 2 rows affected (0.00 sec) mysql&gt; select user,host from mysql.user; +------------+---------------------------+ | user | host | +------------+---------------------------+ | cachecloud | % | | root | 127.0.0.1 | | root | ::1 | | root | localhost | | root | qa1-common004.ecs.east1-b | +------------+---------------------------+ 5 rows in set (0.00 sec) mysql&gt; flush privileges; Query OK, 0 rows affected (0.00 sec) mysql&gt; exit 退出之后再次登录，问题得到解决。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>数据库</category>
        <category>MySQL</category>
        <category>MySQL问题</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql密码设置修改及恢复]]></title>
    <url>%2F2018%2F05%2F02%2FMysql%E5%AF%86%E7%A0%81%E8%AE%BE%E7%BD%AE%E4%BF%AE%E6%94%B9%E5%8F%8A%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[mysql 5.6版本mysql 5.6版本中，安装完毕之后不会设置初始密码，服务启动之后，直接输入mysql即可进入数据库 安装之后的第一次密码设置 # mysql 在终端直接输入mysql进入数据库 mysql&gt; set password = password(&apos;Mysql_password123&apos;); mysql&gt; flush privileges; 修改密码 忘记密码之后的恢复]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>数据库</category>
        <category>MySQL</category>
        <category>Mysql密码</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下JDK的安装配置]]></title>
    <url>%2F2018%2F04%2F28%2FLinux%E4%B8%8BJDK%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在Linux中JDK的配置主要分为以下几个步骤： 下载 解压 软链接 配置系统/用户环境变量 下载：官方下载链接：下载 JAVA环境的配置主要分为两种，一种是由root用户操作，针对所有用户全局生效的配置，一种是由具体普通用户操作，仅针对该用户生效的配置 因此，以下的配置根据实际需求。 全局生效-管理员权限操作解压+软链接 # tar -zxvf jdk-7u75-linux-x64.tar.gz -C /usr/local/ # cd /usr/local/ # ln -s /usr/local/jdk1.7.0_75/ /usr/local/JDK 创建软链接目的：灵活的版本升级切换 配置系统环境变量： # vim /etc/profile 在文件末尾添加以下内容 export JAVA_HOME=/usr/local/JDK export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib export PATH=$JAVA_HOME/bin:$JRE_HOME/bin::$CLASSPATH:$PATH # source /etc/profile 用户局部生效-用户环境变量**注意提示符的变化，这里以appdev用户为例 解压+软链接 $ tar -zxvf jdk-7u75-linux-x64.tar.gz $ ln -s /home/appdev/jdk1.7.0_75/ /home/appdev/JDK 创建软链接目的：灵活的版本升级切换 配置系统环境变量： $ vim .bash_profile 在文件末尾添加以下内容 export JAVA_HOME=/home/appdev/JDK export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib export PATH=$JAVA_HOME/bin:$JRE_HOME/bin::$CLASSPATH:$PATH $ source .bash_profile]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>基础环境配置</category>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>JDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cachecloud-Redis云平台]]></title>
    <url>%2F2018%2F04%2F28%2FCachecloud%2F</url>
    <content type="text"><![CDATA[Cachecloud介绍有关cachecloud的一些基础知识，官方都有非常详细的文档，这里不再花费篇幅进行复述，下面是相关的资料链接，请自行查看。 github官网： https://github.com/sohutv/cachecloud Wiki: https://github.com/sohutv/cachecloud/wiki 博客： https://cachecloud.github.io/ 官方视频： http://my.tv.sohu.com/pl/9100280/index.shtml 简介： CacheCloud提供一个Redis云管理平台：实现多种类型(Redis Standalone、Redis Sentinel、Redis Cluster)自动部署、解决Redis实例碎片化现象、提供完善统计、监控、运维功能、减少开发人员的运维成本和误操作，提高机器的利用率，提供灵活的伸缩性，提供方便的接入客户端。 提供的功能： 监控统计： 提供了机器、应用、实例下各个维度数据的监控和统计界面。 一键开启： Redis Standalone、Redis Sentinel、Redis Cluster三种类型的应用，无需手动配置初始化。 Failover： 支持哨兵,集群的高可用模式。 伸缩： 提供完善的垂直和水平在线伸缩功能。 完善运维： 提供自动运维和简化运维操作功能，避免纯手工运维出错。 方便的客户端 方便快捷的客户端接入。 元数据管理： 提供机器、应用、实例、用户信息管理。 流程化： 提供申请，运维，伸缩，修改等完善的处理流程 一键导入： 一键导入已经存在Redis 须知： Redis集群、redis哨兵集群、Redis单实例等在CacheCloud中都是以应用的形式存在，一个应用对应一个appid 一个redis集群是一个应用，分配一个appid（不管其中有几个节点） 一个哨兵集群是一个应用，分配一个appid（不管其中有几个主从节点和哨兵节点） 一个单实例是一个应用，分配一个appid 如何使用： 我们在平台上的执行任何操作都需要**账号**，创建的单节点、哨兵、集群等都是以用户申请的应用形式存在的。普通用户的主要工单有 注册用户申请 应用申请 应用扩容 应用配置修改 管理员的界面可操作的选项较多，此处不做详细说明。 客户端如何连接： 客户端在第一次启动的时候去CacheCloud通过appId拿到Redis的节点信息，之后不会与CacheCloud打交道了。 流程图如下所示： 安装部署这里只说单机环境，高可用环境将在下面章节说明：CacheCloud高可用架构 环境要求： JDK 7+ Maven 3+ MySQL 5.5+ Redis 3+ 基础环境JDK+MavenJDK： 步骤： 下载 解压 软链接 配置系统环境变量 操作如下： [root@qa1-common004 local]# java -version java version &quot;1.8.0_77&quot; Java(TM) SE Runtime Environment (build 1.8.0_77-b03) Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode) [root@qa1-common004 local]# which java /usr/local/jdk/bin/java [root@qa1-common004 local]# ll /usr/local/jdk lrwxrwxrwx 1 root root 11 Apr 17 17:01 /usr/local/jdk -&gt; jdk1.8.0_77 这里我使用的是1.8版本。 详细操作请看文章：Linux下JDK的安装配置 Maven 步骤： 下载 下载链接 解压 软链接 配置系统环境变量 操作如下： # wget http://www-eu.apache.org/dist/maven/maven-3/3.5.3/binaries/apache-maven-3.5.3-bin.tar.gz # tar -zxvf apache-maven-3.5.3-bin.tar.gz -C /usr/local/ # cd /usr/local/ # ln -s /usr/local/apache-maven-3.5.3/ /usr/local/maven # vim /etc/profile 在文件末尾添加以下内容，保存退出 M3_HOME=/usr/local/maven export PATH=$M3_HOME/bin:$PATH [root@host-192-168-8-37 ~]# source /etc/profile 下载CacheCloud项目# yum -y install git # git clone https://github.com/sohutv/cachecloud.git # ls cachecloud/ cachecloud-open-client cachecloud-open-common cachecloud-open-web LICENSE pom.xml README.md script MySQL这里安装mysql5.7版本 配置yum源并安装 centos6.8 【6.8安装5.6版本，安装5.7时涉及依赖关系过多】 # wget http://repo.mysql.com/mysql-community-release-el6-5.noarch.rpm # rpm -ivh mysql-community-release-el6-5.noarch.rpm # yum -y install mysql-community-server centos 7.x 【5.7版本】 # wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm # rpm -ivh mysql57-community-release-el7-11.noarch.rpm # yum -y install mysql-server 修改mysql配置文件 # vim /etc/my.cnf [mysqld] character-set-server=utf8 启动 # /etc/init.d/mysqld start 数据库配置创建数据库 mysql&gt; create database cache_cloud default charset utf8; Query OK, 1 row affected (0.00 sec) 创建cachecloud用户 mysql&gt; grant all privileges on cache_cloud.* to &apos;cachecloud&apos;@&apos;%&apos; identified by &apos;Cache_cloud123&apos;; Query OK, 0 rows affected (0.00 sec) 导入初始化数据注意，这里已经不是在数据库中了 [root@qa1-common004 script]# pwd /root/software/cachecloud/script [root@qa1-common004 script]# mysql -u root -p cache_cloud &lt; cachecloud.sql Enter password: 修改cachecloud配置数据库设置 [root@qa1-common004 swap]# pwd /root/software/cachecloud/cachecloud-open-web/src/main/swap [root@qa1-common004 swap]# cat online.properties cachecloud.db.url = jdbc:mysql://172.24.64.132:3306/cache_cloud?useUnicode=true&amp;amp;characterEncoding=UTF-8 cachecloud.db.user = cachecloud cachecloud.db.password = Cache_cloud123 cachecloud.maxPoolSize = 20 isClustered = true isDebug = false spring-file=classpath:spring/spring-online.xml log_base=/opt/cachecloud-web/logs web.port=8585 log.level=WARN 注意这里需要提前在数据库中删除匿名用户 开启机器监控功能 # pwd /root/software/cachecloud/cachecloud-open-web/src/main/java/com/sohu/cache/schedule/jobs # vim ServerJob.java 将稳中的注释去掉，修改之后的文件如下所示： 如果公司已经有完善的监控，那么不建议开启机器监控，能够一定程度上减小数据库的压力。 cachecloud构建及启动项目构建 在cachecloud的根目录下执行以下maven命令，该命令会进行项目的构建 [root@qa1-common004 cachecloud]# pwd /root/software/cachecloud [root@qa1-common004 cachecloud]# [root@host-192-168-8-37 cachecloud]# mvn clean compile install -Ponline [root@host-192-168-8-37 cachecloud]# cd script/ [root@host-192-168-8-37 script]# sh deploy.sh /root/software/ 启动 # sh /opt/cachecloud-web/start.sh 启动成功之后的web页面如下图所示： 实际使用redis数据节点初始化执行初始化脚本 sh cachecloud-init.sh cachecloud 添加主机redis应用模板配置注意：在部署redis相关应用之前，一定要先进行模板的配置，因为默认配置下，redis的守护进程模式为关系，保护模式也是开启的 修改配置： 配置名称：daemonize；配置值：yes;配置说明：是否守护进程 新增配置： 配置名称：protected-mode；配置值：no;配置说明：保护模式 配置名称：bind；配置值：0.0.0.0;配置说明：绑定ip 注意：哨兵的配置模板中只需要新增protected-mode参数即可。 部署哨兵应用导入已经存在的redis实例redis哨兵cachecloud使用优化哨兵复用问题： 使用cachecloud部署哨兵集群时，每次生成的哨兵节点都是不一样的，这种情况，会造成一定的资源浪费（每一对主从都需要至少3个哨兵节点，对服务器的端口资源、内存资源等都会造成一定的浪费） 因此，我们采取复用哨兵节点的方式来实现redis的主从高可用 实现步骤： 哨兵模板中设置端口，将端口固定，为了后续的配置方便 手动创建主从节点 哨兵中添加新建的主从节点 在cachecloud平台上导入这个应用 相当于其实是导入redis哨兵的方式 哨兵配置： redis-cli -p 6388 sentinel monitor master-test-qa1 172.24.64.134 6385 2&amp;&amp;redis-cli -p 6388 SENTINEL set master-test-qa1 auth-pass redis123&amp;&amp;redis-cli -p 6388 sentinel set master-test-qa1 down-after-milliseconds 20000&amp;&amp;redis-cli -p 6388 sentinel set master-test-qa1 failover-timeout 60000 关闭master节点的持久化 AOF持久化：appendonly 配置为no cachecloud坑- CacheCloud安装部署及使用常见问题及注意事项 数据库版本问题： 如果使用mysql5.7，则需要进行针对sql文件做一些设置（only_full_group_by模式设置等） cachecloud平台乱码问题： 需要修改online.properties配置文件中的连接串（使用这种方式：jdbc:mysql://127.0.0.1:3306/cache_cloud?useUnicode=true&amp;characterEncoding=UTF-8） cachecloud后台配置模板：默认配置下，redis没有开启守护进程运行方式、开启了保护模式等，需要做一些配置修改之后才可以正常启动 机器监控数据无法展示问题：除了在程序文件中去掉相应的代码注释，还需要将cachecloud-open-web/nmon下指定系统版本的nmon文件放到/opt/cachecloud/soft/目录下 密码配置问题：密码配置栏中，输入密码之后，还需要点击更新才可以生效 Jedis支持redis版本问题：Jedis暂时无法稳定支持redis4.x版本，因此涉及到的集群水平扩容等功能是无法实现的（集群创建等还是可以支持的），因此我们建议使用3版本，后续关注Jedis的版本发布情况。 应用导入时提示：节点不是存活的 cachecloud节点上需要安装redis，因为他会使用redis-cli 去ping指定的节点，没有返回pong时，则会报错 哨兵导入问题： 如果哨兵是复用的，也就是说一组哨兵节点监听了多对主从节点，那么在导入的时候回出现问题，目前导入功能只支持一个mastername 主机名设置问题： 注意hosts文件需要进行配置]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>数据库</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>cachecloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile编写]]></title>
    <url>%2F2018%2F04%2F28%2FDockerfile%E7%BC%96%E5%86%99%2F</url>
    <content type="text"><![CDATA[在docker中，创建镜像的方法主要有三种： 基于已有镜像的容器创建 基于本地模板导入 基于dockerfile创建 dockerfile是一个文本格式的配置文件，用户可以使用dockerfile来快速创建自定义镜像。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>虚拟化</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker基础知识]]></title>
    <url>%2F2018%2F04%2F28%2FDocker%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[基础知识]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>虚拟化</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo]]></title>
    <url>%2F2018%2F04%2F26%2FDubbo%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>分布式</category>
        <category>Dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RPC框架]]></title>
    <url>%2F2018%2F04%2F26%2FRPC%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[参考文献： 知乎 基础知识远程过程调用（英语：Remote Procedure Call，缩写为 RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用，例：Java RMI。 RPC就是要像调用本地的函数一样去调远程函数。 传统的本地调用方式： 代码文件中定义和调用 或者import的方式导入模块之后再进行调用 而远程调用，指的是，我们要调用执行的函数是在远程的机器上的，]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>分布式</category>
        <category>RPC</category>
      </categories>
      <tags>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML]]></title>
    <url>%2F2018%2F04%2F25%2FHTML%2F</url>
    <content type="text"><![CDATA[基础知识常用指令段落标识标签&lt;p&gt;&lt;/p&gt;标识一整个段落； 其中标签&lt;p&gt;指出了段落的开头位置， 而标签&lt;/p&gt;指出了段落的结束位置。 代码为： &lt;p&gt;&lt;/p&gt;标识段落；标签 &lt;p&gt;指出了段落的开头位置，而标签&lt;/p&gt;指出了段落的结束位置。 效果： 标识段落；标签指出了段落的开头位置，而标签指出了段落的结束位置。### 颜色标识 ###代码为： 内容效果：内容### 链接 ###在HTML页面中，链接是使用锚（mao）标签来定义了： 格式为： link text 这里也就是类似markdown的链接使用方式，也可以说是类似Linux中软链接的方式，可以隐藏后端真实的URL串，并且让对外的链接保持最新也要容易得多。实际案例： Learning Log]]></content>
      <categories>
        <category>编程语言</category>
        <category>HTML</category>
      </categories>
      <tags>
        <tag>HTML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[disconf]]></title>
    <url>%2F2018%2F04%2F25%2Fdisconf%2F</url>
    <content type="text"><![CDATA[官方资料： github主页 简介disconf专注于各种「分布式系统配置管理」的「通用组件」和「通用平台」, 提供统一的「配置管理服务」 工作流程如下图所示： 实现目标： 部署极其简单：同一个上线包，无须改动配置，即可在 多个环境中(RD/QA/PRODUCTION) 上线 【每个应用中只需要配置相关的参数即可。】 部署动态化：更改配置，无需重新打包或重启，即可 实时生效 统一管理：提供web平台，统一管理 多个环境(RD/QA/PRODUCTION)、多个产品 的所有配置 核心目标：一个jar包，到处运行,涉及到配置不会存储在项目文件中 部署及使用]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>分布式</category>
        <category>disconf</category>
      </categories>
      <tags>
        <tag>disconf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python零碎知识及版本变化记录]]></title>
    <url>%2F2018%2F04%2F23%2Fpython%E9%9B%B6%E7%A2%8E%E7%9F%A5%E8%AF%86%E5%8F%8A%E7%89%88%E6%9C%AC%E5%8F%98%E5%8C%96%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[零碎知识 CharField类型必须添加max_length来指定长度上限，如果要想没有限制，则考虑使用TextField类型 版本变化##django## 外键： Django2.0版本之后，创建外键时需要在后面加上on_delete topic = models.ForeignKey(Topic) 应该修改为： topic = models.ForeignKey(Topic,on_delete=models.CASCADE) django.core.urlresolvers变化 Django 2.0 removes the django.core.urlresolvers module, which was moved to django.urls in version 1.10. You should change any import to use django.urls instead.]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python版本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[运维工作日常问题记录]]></title>
    <url>%2F2018%2F04%2F23%2F%E8%BF%90%E7%BB%B4%E5%B7%A5%E4%BD%9C%E6%97%A5%E5%B8%B8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[问题目录： su: cannot set user id: Resource temporarily unavailable ignoring option PermSize=512m; support was removed in 8.0 DTS迁移问题 #su: cannot set user id: Resource temporarily unavailable# 问题描述： 切换用户时提示资源不可用问题解决 切换用户时出现如下提示符： [root@qa3-app018 ~]# su - appqa su: cannot set user id: Resource temporarily unavailable 常见的能够控制用户资源的文件一般有两个，一是/etc/profile,二是/etc/security/limits.conf。 除此之外，在Centos6.x版本后，还有一个配置文件对ulimit设置生效，就是/etc/security/limits.d/90-nproc.conf /etc/security/limits.conf文件如下图所示： domain: 是指限制的对象，可以是个人，也可以是组，组前面要加@符号，也可以设置为除root用户外的 任何人，用*号表示； type: 是指类型，soft是当前系统生效的值，hard是系统可以设置的最大值； item: 项目，是可以对什么项目做限制，如最大进程数，文件最大值； value: 值，所设置的值的大小。 这里我的设置是： root soft nofile 65535 root hard nofile 65535 * soft nofile 65535 * hard nofile 65535 可以看到是没有问题的 /etc/security/limits.d/90-nproc.conf[appqa@qa3-app018 ~]$ cat /etc/security/limits.d/90-nproc.conf # Default limit for number of user&apos;s processes to prevent # accidental fork bombs. # See rhbz #432903 for reasoning. * soft nproc 1024 root soft nproc unlimited 这这里发现默认最大进程数只有1024 将1024修改为10240之后，再次执行su即可恢复正常，问题得到解决。 ignoring option PermSize=512m; support was removed in 8.0问题描述： java1.8之后持久代被废弃 使用jdk1.8的时候设置了vm参数：-Xmx2048m -XX:PermSize=512m -XX:MaxPermSize=768m -Xss2m此时运行java程序时VM提示如下警告： Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=512m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=768m; support was removed in 8.0 程序能正常兼容启动，不会产生影响，但这个提示还是引起了注意 问题原因： -XX:PermSize和-XX:MaxPermSize在jdk1.8中被弃用了，使用-XX:MetaspaceSize和-XX:MaxMetaspaceSize替代。 所以此时VM参数正确应为：-Xmx2048m -XX:MetaspaceSize=512m -XX:MaxMetaspaceSize=768m -Xss2m 根本原因： 在jdk1.8之前之前我们将储存类信息、常量、静态变量的方法区称为持久代(Permanent Generation)，PermSize和MaxPermSize是设置持久代大小的参数。 但是在jdk1.8中，持久代被完全移除了，所以这两个参数也被移除了，多了一个元数据区(Metadata Space)，所以设置元数据区大小的参数也变成对应的MetaspaceSize和MaxMetaspaceSize了。 DTS迁移问题问题描述： 使用DTS进行迁移的时候，页面不出现数据库等信息。 问题解决： 前端一个控件缓存的问题。 显示迁移列表的树形控件是单独引用的，连接上没有添加版本号，浏览器缓存未更新。 后面我们会再url带上版本信息，强制浏览器重新获取。 把这个js加上版本号 强制刷新 不用清缓存 会自动获取新的。这样就不会有问题了]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维工作日常问题</category>
      </categories>
      <tags>
        <tag>运维问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中方法与函数的区别]]></title>
    <url>%2F2018%2F04%2F22%2Fpython%E4%B8%AD%E5%B1%9E%E6%80%A7%E4%B8%8E%E6%96%B9%E6%B3%95%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[定义: function(函数) —— A series of statements which returns some value toa caller. It can also be passed zero or more arguments which may beused in the execution of the body. method(方法) —— A function which is defined inside a class body. Ifcalled as an attribute of an instance of that class, the methodwill get the instance object as its first argument (which isusually called self). Function包含一个函数头和一个函数体, 支持0到n个形参 而Method则是在function的基础上, 多了一层类的关系, 正因为这一层类, 所以区分了 function 和 method.而这个过程是通过 PyMethod_New实现的 也就是说，函数可以脱离于类单独存在，在使用的时候，需要往函数中传入参数（实参） 而方法是与某个对象紧密联系的，不能脱离于类而存在方法的作用域只是在一个类中，只能在该类实例化后被该类使用 方法的绑定, 肯定是伴随着class的实例化而发生,我们都知道, 在class里定义方法, 需要显示传入self参数, 因为这个self是代表即将被实例化的对象。 定义角度： 从定义的角度上看，我们知道函数(function)就相当于一个数学公式，它理论上不与其它东西关系，它只需要相关的参数就可以。所以普通的在module中定义的称谓函数是很有道理的。 那么方法的意思就很明确了，它是与某个对象相互关联的，也就是说它的实现与某个对象有关联关系。这就是方法。虽然它的定义方式和函数是一样的。也就是说，在Class定义的函数就是方法。 总结： 函数是一段代码，通过名字来进行调用。它能将一些数据（参数）传递进去进行处理，然后返回一些数据（返回值），也可以没有返回值。所有传递给函数的数据都是显式传递的。 方法也是一段代码，也通过名字来进行调用，但它跟一个对象相关联。方法和函数大致上是相同的，但有两个主要的不同之处： 方法中的数据是隐式传递的； 方法可以操作类内部的数据（请记住，对象是类的实例化–类定义了一个数据类型，而对象是该数据类型的一个实例化）]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python方法与函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[湿气的产生及预防治疗]]></title>
    <url>%2F2018%2F04%2F22%2F%E6%B9%BF%E6%B0%94%E7%9A%84%E4%BA%A7%E7%94%9F%E5%8F%8A%E9%A2%84%E9%98%B2%E6%B2%BB%E7%96%97%2F</url>
    <content type="text"><![CDATA[文章结构： 第一部分：是什么-湿气的概念 &amp;&amp; 为什么-湿气的产生原因 第二部分：怎么做-如何预防及治疗 参考文献： 知乎-湿气是怎么回事，人为什么会有湿气？ 湿气（中医理论概念） 1. 湿气概念及产生原因 1.1 概念1.1.1 湿 我们在日常生活中，感受到湿的时候一般是物体含水量超出一定范围，这个水分可以依附到很多物体上，比如湿巾、湿木头、湿衣服等等（无法正常排出的水）。 但含水量也不能超过一定限度，依附不住的水就不叫做湿，而是自由的水，比如湿衣服滴下来的水，这就不称为湿。 1.1.2 湿气 湿气是一种中医理论中的概念。通俗的来说，就是人体内有多余的水份无法正常代谢排出，堆积在身体之内，从而影响身体健康。（具体原因可能是个人体质、疾病或生活习惯不良，造成体内水分调控系统失衡） 就像我们日常生活中所看见的：食物放在潮湿的地方相比干燥的地方，很快就会发霉，类比到人上，当人的湿气较重之后，会产生一系列疾病。 一般也把湿气称之为：湿邪 在致病的风、寒、暑、湿、燥、火这“六淫邪气” 中，中医最怕湿邪。 湿是最容易渗透的。湿邪从来不孤军奋战，总是要与别的邪气狼狈为奸。 湿气遇寒则成为寒湿，这就好比冬天的时候，如果气候干燥，不管怎么冷，人都还是能接受的，但如果湿气重，人就很难受了。南方的冬天比北方的冬天更令人难受，就是因为南方湿气比较重，寒湿袭人。 湿气遇热则成为湿热，这就好比夏天的桑拿天，又热又湿，让人喘不过气来，明显不如烈日当空、气候干燥的时候来得痛快。 湿气遇风则成为风湿，驱风很容易，但一旦成了风湿，就往往是慢性疾病，一时半会儿治不好了。 湿气在皮下，就形成肥胖，也是不好处理的健康问题…… 为什么现代人的病那么复杂，那么难治？因为他们体内有湿，体外的邪气总是和体内的湿气里应外合，纠缠不清！以前仅仅盛行于我国西南的川菜，风行全国，就是因为川味是辛辣的，以前只有生活在湿邪比较重的西南一带人需要用它来化解体内的湿气；全国人体内都有湿气了，这就需要辛辣来化解。 主导湿气的人体器官是：脾 因此，湿气问题的根本原因是各种原因导致的脾功能下降（也有可能是其他器官导致，因为脾在工作时要需要借助胃肝肾等器官），具体见下文。 1.2 产生原因-湿气是怎么来的？1.2.1 外在原因 一个是因为外在的环境，也就是湿邪进入到了身体。 比如长期居住在湿气重的地方，比如淋了雨还不及时擦干，比如晚上洗头没吹干就睡觉，让外界的湿气进入到体内。 湿气进入身体后常常奔着脾胃去，导致脾的运化能力下降，而这又会容易导致体内生湿。 1.2.2 内因 另外一个就是饮食习惯差，导致脾运化能力下降而生湿。【饮食不当，伤害脾胃，这是产生湿气的罪归祸首】 此外夏天的时候狂开空调，狂吃冷饮，硬生生的把要出来的水份给逼回去了。还有缺乏运动，没有及时的增强脾的工作能力。 脾主运化，吃进来的食物通过它来运化出精微物质，剩下的糟粕排出体外。当因为各种原因导致脾虚、运化能力下降的时候，精微物质就没法完全提炼出来。 1.2.3 原因解析 从微观的角度讲，物质没有完全被消化时，就成了携带营养物质的“垃圾”，成分复杂且分子比较大，没法被人体吸收，但又不像糟粕那么大块头好分辨，那么容易把它们驱逐。 它们的分子量和体积远大于水分子，潜伏着，聚集起来，极其容易把周围的水分子吸附住、束缚住，使含水量超出正常的生理水平，于是形成了湿。 脾被湿气困住，更加影响它的运化工作，导致湿气加重。湿一直凝聚不化，时间长了就成为痰，身体出于自保自救，把其中一部分水、二氧化碳和营养垃圾打包成了脂肪。所以中医常说胖人多痰湿，就是这个道理。【So，减肥先去湿气】 1.3 湿气的特点1.3.1 笨重并且混浊 湿气依附在身体某些地方，和身边的物体紧紧结合，难舍难离。物体湿的状态时会比干燥的时候重很多,所以体内有湿气的时候，我们往往觉得身体或头部沉重；湿气浊会导致身体气血流通不畅，长期聚集身体又没法整治它,导致有湿气的地方脏乱差，滋生各种毒害。 1.3.2 难缠粘人 什么东西被湿邪盯上，就好像被缠上了粘液，各种不爽，比如小便不畅，大便黏腻不爽等。此外它还很难去除，经常和你缠缠绵绵，病程较长，比如风湿病、温湿病。 1.3.3 阻遏气机、损伤阳气 湿气本质上属于阴邪，靠着它黏腻难缠的劲头，赖在脏腑经络上不走，导致气机升降无能，于是阳气就没法正常生发了。所以一般被湿邪困住的人，阳气都不太旺，会有脸色淡白，精力不济的现象。 1.4 湿气重的表现 头发爱出油、面部油亮, 小肚子大(常有胀气)，身体浮肿。 身体发沉、发重，浑身无力。 皮肤上会有湿疹，胃口不好，嘴里发黏。 常感到疲倦，精力不集中睡觉打呼噜，痰多，咳嗽,睡觉留口水、口臭、身体有异味，耳内湿（耳禅湿）毛发粗糙，易脱落。 舌质很胖，颜色偏淡。症状严重的，舌头边上会有齿痕，这叫“裙边舌”。 眼袋下垂，黑圆圈严重，肥胖，减肥后反弹，机能衰退，对房事不感兴趣质量不高等。 大便溏稀不成型，正常的大便是光滑的呈圆柱体，每次大便之后，不会粘在光滑的马桶壁上，如果你每次上完厕所，大便冲不干净，那么一定是体内湿气在作怪。而且，湿气会让便秘如影随形。下一次，当你大便的时候，很可能就会出现便秘。 等等等等 当湿气演变成为顽固性湿气的时候，身体会出现数十种不适： 以上症状，如果你占了2种以上，要引起注意了，这说明体内有湿气。湿气不除，是引发及恶化疾病的关键。 并且现代人由于工作强度、压力等都更大，因此运动量也原来越小，体内阴盛阳虚从而湿邪内郁。这也是当前越来越多的年轻人有湿气相关疾病的原因。 2. 如何预防及治疗 在这里，我们将预防和治疗两者结合在一起说明，因为光靠预防不能完全杜绝，或多或少肯定都还是会产生湿气。 湿气很重，不要只会傻傻拔罐。 2.1 药物目前没有什么比较好的药物，一般采用饮食结合运动的方式来预防和治疗湿气。 2.2 饮食这里只说该吃什么，至于不该吃什么，请看日常生活章节 薏米赤小豆桂圆粥 薏米：性寒。因此要用赤小豆来中和，并且每次的量不宜太多 赤小豆：性，。注意赤小豆是扁的，红豆是圆的 桂圆/枣：桂圆甘温。有的人体质偏寒，里面可以加一点温补的食物，像桂圆、大枣都可以 如果着凉感冒了，或是体内有寒，胃中寒痛，食欲不佳，可在薏米赤小豆汤中加几片生姜。生姜性温，能温中祛寒，健脾和胃。 肾虚的人，可在薏米赤小豆汤中加一些黑豆。因为黑色入肾，豆的形状也跟肾十分相似，以形补形，是补肾的佳品。 人们常说的脚气病，是典型的湿热下注。可在薏米赤小豆汤中加点碎黄豆，用熬出来的汤泡脚，这是治脚气的一个小秘方。 学会薏米赤小豆汤的加减变化，使用得当可以对生活中大部分常见病起到很好的治疗效果。 如下图所示： 2.3 运动现代人动脑多、体力消耗少，加上长期待在密闭空调内，很少流汗，身体调控湿度的能力变差。因此这也为产生湿气创造了条件。 运动出汗是很好的去湿气方式 2.4 日常生活2.4.1 不宜 不过食生冷肥甘厚腻甜辛辣， 避开生冷食物。这里说的生冷食物指的是冷饮、凉拌菜等，而不是水果。【这一点在夏天的时候最为明显，一些人在夏天时喝冷饮、和冰镇啤酒、吃冰镇西瓜、吃凉菜等毫无节制】 夏天尽量不吹空调 睡前务必吹干头发 饮食口味重，日常饮食口味经常过重的话，由于细胞渗透压的作用，浓度低的会向浓度高的一方渗透，力求平衡，从而会使身体处于不正常状态 不宜久坐，一小时不动两小时不动三小时不动，身体以为你不会动了，它的运行也会慢下来慢下来 不宜大量吃水果。 2.4.2 宜 晚上用热水泡脚。 每天晚上坚持用热水泡脚半小时（注意：时间是半小时），泡到微微出汗。 泡脚的同时敲打肘窝、腘和腋窝各5分钟。这三个地方是排湿气的重要部位。腋窝都知道，肘窝就是手肘后面弯曲部位，腘就是膝盖后面弯曲部位。 天气好的日子，勤晒衣物和被子，减少病菌，降低生病的可能。 夏天时家中易闷热潮湿，每天要适度开窗换气，新鲜的空气可以减少细菌病毒的滋生，以傍晚最适宜。 清淡饮食 保持衣物干爽,不要穿潮湿未干的衣服、盖潮湿的被子，被子(垫絮)要经常晒。 夏天不要贪凉睡地板 2.5 总结食疗、运动最多只能暂时缓解症状，找到自身湿气产生的原因，才能从根上断绝它。 我们不能怪罪脾胃太虚弱，吃那么多它累死也消化不完啊；不要怪它懒罢工不干活，湿气困着它，它也很无奈；别说它工作不到位，身体消耗少，营养物质只能不断堆积。 不形成良好的生活习惯，喝再多薏米粥、吃再多健脾祛湿的方药都是白搭！所以与其总是寻医问药寻找除湿气的方法，不如老老实实先好好吃饭、合理饮食、不贪凉不贪酒、加强体育锻炼.多动少吃清淡平衡饮食,这才是正确的姿势。]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>极简身体之道</category>
        <category>湿气</category>
      </categories>
      <tags>
        <tag>湿气</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[X-Pack]]></title>
    <url>%2F2018%2F04%2F19%2FX-Pack%2F</url>
    <content type="text"><![CDATA[参考资料： 官网 概述x-pack是Elastic Stack的扩展包，实现了如下的一系列功能： Security Monitoring Alerting and Notification Reporting Graph Machine Learning 安装]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>X-Pack</category>
      </categories>
      <tags>
        <tag>X-Pack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[磁盘基础知识]]></title>
    <url>%2F2018%2F04%2F18%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%A1%AC%E4%BB%B6%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>服务器硬件</category>
        <category>磁盘</category>
      </categories>
      <tags>
        <tag>磁盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程管理-进程性能分析]]></title>
    <url>%2F2018%2F04%2F18%2F%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86-%E8%BF%9B%E7%A8%8B%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[分为两部分 服务器即时数据查看 监控趋势数据查看 主要分为以下4个部分 - CPU 系统整体CPU情况，具体进程占用CPU情况，需要精确进程或者线程调用的系统调用，过去5次/5秒等时间内的进程占用情况 - 内存 系统整体占用情况，具体进程占用内存情况，需要精确进程或者线程调用的系统调用 - 磁盘 整体磁盘IO情况，具体进程占用磁盘IO情况，需要精确进程或者线程调用的系统调用 - 网络 系统整体网络IO情况，带宽使用情况；具体进程占用的网络IO情况，需要精确进程或者线程调用的系统调用 - 系统 - 系统负载 系统整体占用情况 本文主要讲述在服务器端的即时数据查看。 #CPU性能分析# 即时数据查看磁盘磁盘IO 网络端口 系统应用进程]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>进程管理</category>
        <category>进程性能分析</category>
      </categories>
      <tags>
        <tag>进程管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVS从入门到精通]]></title>
    <url>%2F2018%2F04%2F18%2FLVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>负载均衡</category>
        <category>LVS</category>
      </categories>
      <tags>
        <tag>LVS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-django项目]]></title>
    <url>%2F2018%2F04%2F17%2FPython-django%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[基础环境： Python3系列（针对环境变量做好软链接，将python3链接为python） sudo apt-get -y install python3 ln -s /usr/bin/python3 /usr/bin/python pip3(pip2不支持Python3.x，因此我们要安装pip来支持python3) sudo apt-get install python3-pip ln -s /usr/bin/pip3 /usr/bin/pip pip的升级：pip install –upgrade pip 依赖关系（python3-venv） sudo apt-get -y install python3-venv 环境准备创建激活虚拟环境要使用django，首先需要建立一个虚拟工作环境。虚拟环境是系统的一个位置，你可以在其中安装包，并将其与其他python包隔离。将项目的库与其他项目分离是有益的。 创建虚拟环境： wxh@wxh-virtual-machine:/opt$ python -m venv ll_env 激活虚拟环境 (ll_env) wxh@wxh-virtual-machine:/opt$ source ll_env/bin/activate 停止虚拟环境 (ll_env) wxh@wxh-virtual-machine:/opt$ deactivate 安装django安装django(python3之后对应的django版本为1.8.1，因此在安装的时候需要额外注意)： (ll_env) wxh@wxh-virtual-machine:/opt$ pip install Django django项目创建项目：(ll_env) wxh@wxh-virtual-machine:/opt$ sudo django-admin.py startproject learning_log . 创建完毕之后的目录结构如下所示： 注意事项： 命令末尾有有一个句点的存在，如果遗忘，可能出现一些问题。 manage.py文件是一个简单的程序，它接受命令并将其交给django的相关部分去运行，我们将会使用这些命令来管理诸如使用数据库和运行服务器等任务 目录learning_log有4个文件，其中最重要的的是setting.py、urls.py、wsgi.py。 setting.py指定django如何与系统交互以及如何管理项目。在开发项目的过程中，我们将修改其中的一些设置，并添加一些设置。 urls.py告诉django应该创建哪些网页来响应浏览器请求。 wsgi.py帮助django提供它创建的文件。（web server gateway interface）web服务器网关接口的缩写 在一个目录下，只能创建一个django项目，因为一个目录下不允许存在2个manage.py 创建数据库django将大部分与项目有关的信息都存储在数据库中，因此我们需要创建一个供django使用的数据库。为给项目“学习笔记”创建数据库，在处于活动虚拟环境中的情况下执行下面的命令： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py migrate 启动/停止项目核实django项目是否正确的创建，执行下面的命令启动： python manage.py runserver [port] 创建应用程序django由一系列应用程序组成（也可以看成是一系列的功能组件），他们协同工作，让项目成为一个整体。现在我们暂时只创建一个应用程序，它将完成项目的大部分工作。在后面，我们还将再添加一个管理用户账户的应用程序 (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py startapp learning_logs 命令startapp让django创建应用程序所需的基础设施。如果现在查看项目目录，将看到其中新增了一个文件夹learning_logs 其中最重要的文件是models、admin和views，我们将使用models来定义我们要在应用程序中管理的数据 注意：learning_logs是learning_log项目中的一个应用程序，这个概念要分清。 定义模型每位用户都需要在学习笔记中创建很多的主题（例如：数学、英语、语文等等）。用户输入的每个条目（文章，每篇笔记）都要与特定的主题相关联。这些条目将以文本的形式显示。我们还需要存储每个目录的时间戳，以便能够告诉用户每个条目都是什么时候创建的。 (ll_env) wxh@wxh-virtual-machine:/opt$ cat learning_logs/models.py from django.db import models # Create your models here. 我们可以看到这个文件的内容。django事先导入了模块models，让我们自己创建模型。 模型告诉django如何处理应用程序中存储的数据。在代码层面，模型就是一个类，就像前面讨论的每个类一样，包含属性和方法。 我们编写完毕之后的代码如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim models.py from django.db import models class Topic(models.Model): &quot;&quot;&quot;用户学习的主题&quot;&quot;&quot; text = models.CharField(max_length=200) date_added = models.DateTimeField(auto_now_add=True) def __str__(self): &quot;&quot;&quot;返回模型的字符串表示&quot;&quot;&quot; return self.text 我们创建了一个名为Topic的类，它继承了models模块中的类Model-django中一个定义了模型基本功能的类。新建的这个Topic类中，我们只定义了两个属性：text和data_added（也就是拿来保存主题和时间戳） 属性text是一个CharField——由字符或文本组成的数据（见）。需要存储少量的文本，如名称、标题或城市时，可使用CharField。定义CharField属性时，必须告诉Django该在数据库中预留多少空间。在这里，我们将max_length设置成了200（即200个字符），这对存储大多数主题名来说足够了。 属性date_added是一个DateTimeField——记录日期和时间的数据（见）。我们传递了实参auto_add_now=True，每当用户创建新主题时，这都让Django将这个属性自动设置成当前日期和时间 我们需要告诉django，默认应该使用哪个属性来显示有关主题的信息。django调用方法str()来显示模型的简单表示，在这里，我们编写了方法str(),它返回存储在属性text中的字符串。 激活模型要使用模型，必须让Django将应用程序包含到项目中。为此，打开settings.py（它位于项目learning_log目录下），你将看到一个这样的片段，这一段的配置告诉Django使用哪些应用程序安装在项目中： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo vim learning_log/settings.py 33 INSTALLED_APPS = [ 34 &apos;django.contrib.admin&apos;, 35 &apos;django.contrib.auth&apos;, 36 &apos;django.contrib.contenttypes&apos;, 37 &apos;django.contrib.sessions&apos;, 38 &apos;django.contrib.messages&apos;, 39 &apos;django.contrib.staticfiles&apos;, 40 ] 我们将新建的应用程序加载进项目之中，修改之后的配置如下所示： 33 INSTALLED_APPS = [ 34 &apos;django.contrib.admin&apos;, 35 &apos;django.contrib.auth&apos;, 36 &apos;django.contrib.contenttypes&apos;, 37 &apos;django.contrib.sessions&apos;, 38 &apos;django.contrib.messages&apos;, 39 &apos;django.contrib.staticfiles&apos;, 40 # my appalication 41 &apos;learning_logs&apos; 42 ] 通过这种将应用程序编组的方式，在项目不断增大，包含更多的应用程序时，可以有效的对应用程序进行跟踪。 数据库配置： 接下来，需要让django修改数据库，使其能够存储与模型Topic相关的信息，执行以下命令： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py makemigrations learning_logs Migrations for &apos;learning_logs&apos;: learning_logs/migrations/0001_initial.py - Create model Topic (ll_env) wxh@wxh-virtual-machine:/opt$ 命令makemigrations让Django确定该如何修改数据库，使其能够存储与我们定义的新模型相关联的数据。 输出表明Django创建了一个名为0001_initial.py的迁移文件，这个文件将在数据库中为模型Topic创建一个表。 下面来应用这种迁移，让Django替我们修改数据库： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py migrate Operations to perform: Apply all migrations: admin, auth, contenttypes, learning_logs, sessions Running migrations: Applying learning_logs.0001_initial... OK 注意：每当需要修改该应用程序管理的数据时（在这里是learning_logs应用程序），都采取如下三个步骤： 修改models.py； 对learning_logs调用makemigrations; 让django迁移项目** django管理网站一个网站，需要有管理员来管理网站，django提供的管理网站（admin site）能够轻松的实现。 接下来，我们将建立管理网站，并通过它使用模型Topic来添加一些主题 创建超级用户创建具备所有权限的用户—超级用户，执行以下命令： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py createsuperuser Username (leave blank to use &apos;root&apos;): ll_admin Email address: Password: Password (again): Superuser created successfully. 注意：django并不存储实际输入的明文密码，而是存储该密码的散列值，每当你输入密码的时候，django都将计算其散列值，并将结果与存储的散列值进行比较。 向管理网站注册模型Django自动在管理网站中添加了一些模型，如User和Group，但对于我们创建的模型，必须手工进行注册。 我们创建应用程序learning_logs时， Django在models.py所在的目录中创建了一个名为admin.py的文件： (ll_env) wxh@wxh-virtual-machine:/opt$ cat learning_logs/admin.py from django.contrib import admin # Register your models here. 修改之后的文件为： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim admin.py from django.contrib import admin from learning_logs.models import Topic admin.site.register(Topic) 导入我们要注册的模型Topic，再使用admin.site.register方法让django通过管理网站管理我们的模型 接下来访问：http://127.0.0.1:8000/admin可以直接使用我们刚才创建的超级管理员用户的用户名和密码进行登录。 登录之后的页面如下所示： 这个网页能够让你添加和修改用户和用户组，还可以管理刚才定义的模型Topic相关的数据。 我们能够看到刚才定义的模型Topic及其相关的数据（当前没有数据） 添加主题向管理网站注册了topic之后，我们需要添加主题，这里添加Chess和Rock Climbing主题。添加完毕之后，如下图所示： 定义模型Entry当前的模型只是定义了2个属性（主题和时间戳），并不能实际的保存数据，因此我们需要添加模型Entry 关系：每个条目都会与特定的主题相关联，即多对一的关系。 修改之后的代码如下图所示： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim models.py from django.db import models class Topic(models.Model): &quot;&quot;&quot;用户学习的主题&quot;&quot;&quot; text = models.CharField(max_length=200) date_added = models.DateTimeField(auto_now_add=True) def __str__(self): &quot;&quot;&quot;返回模型的字符串表示&quot;&quot;&quot; return self.text class Entry(models.Model): &quot;&quot;&quot;学到的有关某个主题的具体知识&quot;&quot;&quot; topic = models.ForeignKey(Topic,on_delete=models.CASCADE) text = models.TextField() date_added = models.DateTimeField(auto_now_add=True) class Meta: verbose_name_plural = &apos;entries&apos; def __str__(self): &quot;&quot;&quot;返回模型的字符串表示&quot;&quot;&quot; return self.text[:50] + &quot;...&quot; 像Topic一样， Entry也继承了Django基类Model。 第一个属性topic是一个ForeignKey实例。外键是一个数据库术语，它引用了数据库中的另一条记录；这些代码将每个条目关联到特定的主题。每个主题创建时，都给它分配了一个键（或ID）。需要在两项数据之间建立联系时，Django使用与每项信息相关联的键。稍后我们将根据这些联系获取与特定主题相关联的所有条目。 接下来是属性text，它是一个TextField实例（见）。这种字段不需要长度限制，因为我们不想限制条目的长度。 属性date_added让我们能够按创建顺序呈现条目，并在每个条目旁边放置时间戳。 我们在Entry类中嵌套了Meta类。 Meta存储用于管理模型的额外信息，在这里，它让我们能够设置一个特殊属性，让Django在需要时使用Entries来表示多个条目。如果没有这个类，Django将使用Entrys来表示多个条目。 最后，方法str()告诉Django，呈现条目时应显示哪些信息。由于条目包含的文本可能很长，我们让Django只显示text的前50个字符（见）。我们还添加了一个省略号，指出显示的并非整个条目。 迁移模型Entry由于我们添加了一个新模型，因此需要再次迁移数据库 步骤： 修改models.py makemigrations参数 mkigrate参数 命令及输出如下所示： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py makemigrations learning_logs Migrations for &apos;learning_logs&apos;: learning_logs/migrations/0002_entry.py - Create model Entry (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py migrate Operations to perform: Apply all migrations: admin, auth, contenttypes, learning_logs, sessions Running migrations: Applying learning_logs.0002_entry... OK 生成了一个新的迁移文件0002_entry.py，它告诉django如何修改数据库，使其能够存储与模型Entry相关的信息。 执行命令migrate，我们发现django应用了这种迁移且一切顺利。 向管理网站注册Entry首先需要修改admin.py (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim admin.py from django.contrib import admin from learning_logs.models import Topic,Entry admin.site.register(Topic) admin.site.register(Entry) 然后刷新页面，可以看到新的内容 接下来，我们添加条目 为当前两个主题都添加相应的条目： django shell输入一些数据后，就可通过交互式终端会话以编程方式查看这些数据了。这种交互式环境称。为Django shell，是测试项目和排除其故障的理想之地。下面是一个交互式shell会话示例： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py shell [sudo] password for wxh: Python 3.5.2 (default, Nov 23 2017, 16:37:01) [GCC 5.4.0 20160609] on linux Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. (InteractiveConsole) &gt;&gt;&gt; from learning_logs.models import Topic &gt; &gt;&gt;&gt; Topic.objects.all() &lt;QuerySet [&lt;Topic: Chess&gt;, &lt;Topic: Rock Climbing&gt;]&gt; &gt;&gt;&gt; topics = Topic.objects.all() &gt;&gt;&gt; for topic in topics: ... print (topic.id,topic) ... 8 Chess 9 Rock Climbing &gt;&gt;&gt; t = Topic.objects.get(id=8) &gt;&gt;&gt; t.text &apos;Chess&apos; &gt;&gt;&gt; t.date_added datetime.datetime(2018, 4, 23, 12, 51, 16, 430242, tzinfo=&lt;UTC&gt;) 命令python manage.py shell启动一个Python解释器，可使用它来探索存储在项目数据库中的数据 在这里，我们导入了模块learning_logs.models中的模型Topic，然后使用方法Topic.objects.all()来获取模型Topic的所有实例；它返回的是一个列表，称为查询集（queryset）。 我们可以像遍历列表一样遍历查询集。 我们将返回的查询集存储在topics中，然后打印每个主题的id属性和字符串表示。从输出可知，主题Chess的ID为2，而Rock Climbing的ID为9。 知道对象的ID后，就可获取该对象并查看其任何属性。 我们还可以查看与主题相关联的条目。前面我们给模型Entry定义了属性topic，这是一个ForeignKey，将条目与主题关联起来。利用这种关联， Django能够获取与特定主题相关联的所有条目，如下所示： &gt;&gt;&gt; t.entry_set.all() &lt;QuerySet [&lt;Entry: The opening is the first part of the game, roughly...&gt;]&gt; 为通过外键关系获取数据，可使用相关模型的小写名称、下划线和单词set。例如，假设你有模型Pizza和Topping，而Topping通过一个外键关联到Pizza；如果你有一个名为my_pizza的对象，表示一张比萨，就可使用代码my_pizza.topping_set.all()来获取这张比萨的所有配料。 编写用户可请求的网页时，我们将使用这种语法。确认代码能获取所需的数据时， shell很有帮助。如果代码在shell中的行为符合预期，那么它们在项目文件中也能正确地工作。如果代码引发了错误或获取的数据不符合预期，那么在简单的shell环境中排除 故障要比在生成网页的文件中排除故障容易得多。我们不会太多地使用shell，但应继续使用它来熟悉对存储在项目中的数据进行访问的Django语法。 注意：每次修改模型后，都需要重启shell,执行Ctr + D django APIOnce you’ve created your data models, Django automatically gives you a database-abstraction API that lets you create, retrieve, update and deleteobjects. Creating objects &gt;&gt;&gt; from learning_logs.models import Topic &gt;&gt;&gt; b = Topic(text=&apos;badminton&apos;) &gt;&gt;&gt; b.save() 注意： This performs an INSERT SQL statement behind the scenes. Django doesn’t hit the database until you explicitly call save() The save() method has no return value 其他操作，查看：官方资料 创建网页：学习笔记主页使用django创建网页的过程通常分为四个阶段： 定义模型 这里我们有Topic模型和Entry模型，模型我们可以理解为就是一个数据库，其中定义了几个字段以及其对应的数据类型，后续我们都是根据这些数据进行操作。 定义URL URL模式描述了URL是如何设计的，让Django知道如何将浏览器请求与网站URL匹配，以确定返回哪个网页 编写视图 每个URL都被映射到特定的视图——视图函数获取并处理网页所需的数据。【也就是URL与内容的映射关系】 编写模板 视图函数通常调用一个模板，后者生成浏览器能够理解的网页。【也就是内容数据的显示方式，视图前端显示模板】 接下来，我们来创建学习笔记的主页。我们将定义该主页的URL、编写其视图函数并创建一个简单的模板 逻辑关系： 项目中定义调用的应用程序的模块，然后在具体应用程序中配置URL和视图 映射URL用户通过在浏览器中输入URL以及单击链接来请求网页，因此我们需要确定项目需要哪些URL主 页 的 URL 最 重 要 ， 它 是 用 户 用 来 访 问 项 目 的 基 础 URL 。 当 前 ， 基 础 URL（http://localhost:8000）返回默认的Django网站，让我们知道正确地建立了项目。我们将修改这一点，将这个基础URL映射到“学习笔记”的主页。 打开项目主目录中的urls.py文件（该文件针对整个项目的url配置），默认的内容为： 前两行导入了为项目和管理网站管理URL的函数和模块，在这个针对整个项目的urls.py文件中，变量urlpatterns包含项目中的应用程序的URL。 admin.site.urls模块定义了可在管理网站中请求的所有URL。 修改之后的配置文件如下图所示： 现在，我们需要在learning_logs目录下创建urls.py文件文件内容如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim urls.py (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ cat urls.py &quot;&quot;&quot;定义learning_logs的URL模式&quot;&quot;&quot; from django.conf.urls import url from . import views app_name = &apos;learning_logs&apos; urlpatterns = [ #主页 url(r&apos;^$&apos;,views.index,name=&apos;index&apos;) ] 讲解： 实际的URL模式其实是对函数url()的调用，这个函数接受3个实参 这里的正则表达式让python查找开头和末尾之间没有任何东西的url。 python忽略项目基础的URL（在这里是:http://localhost:8000/），因此这个正则表达式与基础URL相匹配，其他的非基础URL都不与这个正则表达式匹配，如果请求的是其他的URL页面，django将会返回一个错误页面。 url()的第2个实参指定了要调用的视图函数。请求的URL与前面的正则表达式匹配时，django将会调用views.index（这个index视图函数稍后编写） 第3个实参，将这个url模式的名称指定为index(相当于是alias别名的形式)，让我们在代码的其他地方引用它。每当我们需要提供这个主页的链接时，我们可以直接使用这个名称，而不用编写URL。 编写视图每个URL都被映射到特定的视图函数——视图函数获取并处理网页所需的数据。【也就是URL与内容的映射关系】 视图函数接受请求中的信息，准备好生成网页所需的数据，再讲这些数据发送给浏览器—这通常还涉及到网页的模板 learning_logs中的文件views.py是执行命令python manage.py startapp时自动生成的，当前的文件内容如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ cat views.py from django.shortcuts import render # Create your views here. 现在，这个文件只导入了函数render()，它根据视图提供的数据渲染响应。修改之后的文件内容如下如所示： from django.shortcuts import render def index(request): &quot;&quot;&quot;学习笔记的主页&quot;&quot;&quot; return render(request,&apos;learning_logs/index.html&apos;) 当URL被刚才定义的模式匹配之后，django会在views.py文件中查找函数index()，再将请求对象传递给这个视图函数（也就是这里的request）。 接下来向函数render进行传2个实参，原始的请求对象以及一个用于创建网页的模板（模板目录下的learning_logs目录下的index.html文件） 下面我们来编写这个模板。 编写模板模板定义了网页的结构，也就是说模板指定了网页是什么样子的。 每当网页被请求时，django将会填入相关的数据。模板让你能够访问视图提供的任何数据。我们主页视图没有提供任何数据，因此相应的模板非常简单。 创建目录在目录learning_logs下创建templates目录，用户保存网页模板文件 然后创建子目录learninig_logs，并在该子目录下新建文件index.html 文件内容为： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo mkdir -p templates/learning_logs (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ cd templates/learning_logs (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim index.html &lt;p&gt;Learning Log&lt;/p&gt; &lt;p&gt;Learning Log helps you keep track of your learning, for any topic you&apos;re learning about.&lt;/p&gt; 现在重新访问网站，会发现显示的是刚才自定义的这个网页，不再显示django网页。 如下图所示： 总结创建网页的过程看起来会有点复杂，但是将URL、视图和模板分离，效果实际上会更好，这让我们可以分别考虑项目的不同方面。 例如：数据库专家可以专注于模型，程序员可以专注于视图代码，而web涉及人员可以专注于模板。 关系： 访问请求–&gt;url（入：定义url匹配规则，出：视图函数。同时还定义这个url的调用别名） –&gt;视图（入：url请求对象，出：创建网页的模板，包含路径和名称信息。动态内容在这里生成） –&gt;模板（html网页，其中包含具体的网页内容，这里只是静态内容，接受视图中的动态数据然后组装显示） 创建其他网页接下来，我们创建两个显示数据的网页。 其中一个列出所有的主题 另一个显示特定主题的所有条目 对于每一个网页，我们都将指定URL模式，编写一个视图函数，并编写一个模板。 为了效率，我们可以先编写一个父模板 模板继承父模板：base.html 我们在/opt/learning_logs/templates/learning_logs目录下创建base.html【在项目中，每个网页都将继承base.html】 内容如下： &lt;p&gt; &lt;a href=&quot;{ % url &apos;learning_logs:index&apos;% }&quot;&gt;Learning Log&lt;/a&gt; &lt;/p&gt; { % block content % }{ % endblock content % } 实际效果为： Learning Log { % block content % }{ % endblock content % } 说明： 在当前，所有页面都包含的元素只有顶端的标题。我们将在每个页面中包含这个模板，因此我们将这个标题设置到主页的链接。 模板标签是用大括号和百分号{ %% }表示的。模板标签是一小段代码，生成要在网页中显示的信息。 在这里，模板标签{ % url ‘learning_logs:index’% }生成一个URL，该URL与learning_logs/urls.py中定义的名为index的URL模式匹配，也就是说learning_logs是一个命名空间，而index是该命名空间中一个名称独特的URL模式。 在HTML页面中，链接是使用锚（mao）标签来定义了： 格式为： &lt;a href=&quot;link_url&quot;&gt;link text &lt;/a&gt; 这里也就是类似markdown的链接使用方式，也可以说是类似Linux中软链接的方式，可以隐藏后端真实的URL串，并且让对外的链接保持最新也要容易得多。 子模板现在我们需要重新编写index.html，使其继承base.html，修改之后的代码如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim index.html { % extends &quot;learning_logs/base.html&quot; % } { % block content % } &lt;p&gt;Learning Log helps you keep track of your learning, for any topic you&apos;re learning about.&lt;/p&gt; { % endblock content % } 子模板的第一行必须包含标签{ % extends % }，让Django知道它继承了哪个父模板。 注意：这时候虽然两个文件的目录结构都是一样，但是还是需要制定目录learning_logs。 这行代码导入了base.html页面的所有内容，让index.html能够指定要在content块中预留的空间中添加的内容。 在子模板中，只需要包含当前网页特有的内容，这不仅简化了每个模板，还使得网站修改起来容易得多，要修改很多网页都包含的元素，只需要在父模板中修改该元素。 在大型项目中，我们可以定义多级父模板，有一个用于整个网站的父模板，并且网站的每个主要部分都有一个父模板，每个部分中又去继承这个模板。 接下来，我们专注于另外两个网页： 显示全部所有主题的网页 显示特定主题中条目的网页 显示所有主题的页面URL模式首先定义显示所有主题页面的URL。在这里我们使用topics来表示 完整的URL应该是：http://localhost:8000/topics 我们修改urls.py &quot;&quot;&quot;定义learning_logs的URL模式&quot;&quot;&quot; from django.conf.urls import url from . import views app_name = &apos;learning_logs&apos; urlpatterns = [ #主页 url(r&apos;^$&apos;,views.index,name=&apos;index&apos;), url(r&apos;^topics/$&apos;,views.topics,name=&apos;topics&apos;) ] 视图函数修改views.py，添加上面指定的topics视图函数 from django.shortcuts import render from .models import Topic def index(request): &quot;&quot;&quot;学习笔记的主页&quot;&quot;&quot; return render(request,&apos;learning_logs/index.html&apos;) def topics(request): &quot;&quot;&quot;显示所有的主题&quot;&quot;&quot; topics = Topic.object.order_by(&apos;date_added&apos;) context = {&apos;topics&apos;:topics} return render(request,&apos;learning_logs/topics.html&apos;,context) 说明： 需要导入与所需数据相关联的模型，也就是导入Topic类 视图函数中包含一个形参（django从服务器收到的访问request对象） 第一行中，我们查询数据库，请求提供Topic对象，并且按照属性date_added进行排序，然后将返回的查询集存储在topics中 context定义了一个将要发送给模板的上下文。上下文是一个字典类型，其中的键是我们将在模板中用来访问数据的名称，而值是我们要发送给模板的数据。在这里，我们暂时只是定义了一个键值对，包含我们将要在网页中显示的主题。 创建使用数据的网页时，除了对象request和模板的路径之外，我们还将变量context传递给render() 模板在这里，我们需要定义上面指定的topics网页。页面接受字典context，以便能使用topics()提供的数据。 创建的topics.html页面内容如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim topics.html { % extends &quot;learning_logs/base.html&quot; % } { % block content % } &lt;p&gt;Topics&lt;/p&gt; &lt;ul&gt; { % for topic in topics % } &lt;li&gt;{{ topic }}&lt;/li&gt; { % empty % } &lt;li&gt; No topics have been added yet.&lt;/li&gt; { % endfor % } &lt;/ul&gt; { % endblock content % } 说明： 就像index.html一样，我们首先使用标签{ % extends % }来继承base.html，再开始定义content块。这个网页的主体是一个项目列表，其中列出了用户输入的主题。在标准的HTML中，项目列表被称为无序列表，用标签&lt;ul&gt;&lt;/ul&gt;表示。 我们使用了一个for循环的模板标签【注意：pytho使用缩进来指出哪些代码是for循环的组成部分，而在模板中，每个for循环都必须使用{ % endfor %标签来显示地指出其结束位置}】 在这里topics这个变量是从视图函数中传递过来的，因此不需要我们再指定 在HTML中，for循环的格式为： { % for item in list % } do something with each item { % endfor % } 在循环中，我们要将每个主题转换为一个项目列表项。要在模板中打印变量，需要将变量名用双花括号括起来。每次循环时，代码都被替换为topic的当前值。这些花括号不会出现在网页中，它们只是用于告诉Django我们使用了一个模板变量。 HTML标签表示一个项目列表项，在标签对内部，位于标签和之间的内容都是一个项目列表项。 修改父模板 我们现在需要修改父模板(base.html)，使其包含到显示所有主题的页面的链接修改之后的内容如下所示： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ cat base.html &lt;p&gt; &lt;a href=&quot;{%url 'learning_logs:index'%}&quot;&gt;Learning Log&lt;/a&gt; - &lt;a href=&quot;{%url 'learning_logs:topics'%}&quot;&gt;Topics&lt;/a&gt; &lt;/p&gt; {% block content%}{% endblock content%} 这个时候刷新页面，显示如下： ![topics](http://picture.watchmen.xin/python-django/alltopics.png) ### 显示特定主题主所有条目的页面 ### 接下来，我们需要创建一个专注于显示特定主题的页面-**`显示该主题的名称以及该主题的所有条目`** 同样，我们的顺序还是： 1. 定义URL模式 2. 编写视图函数 3. 创建网页模板 此外，我们还需要修改上一个网页（显示所有主题的网页），让每个项目列表都是一个链接，点击之后可以显示相应主题的所有条目 #### URL模式 #### (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim urls.py """定义learning_logs的URL模式""" from django.conf.urls import url from . import views app_name = 'learning_logs' urlpatterns = [ #主页 url(r'^$',views.index,name='index'), url(r'^topics/$',views.topics,name='topics'), url(r'^topics/(?P\d+)/$',views.topic,name='topic') ] **说明：** r让django将这个字符串视为原始字符串，并指出正则表达式包含在引号内。这个表达式的第二部分**`/(?P\d+)/`**与包含在两个//内的整数内容进行匹配，并将这个整数存储在一个名为topic_id的实参中，这部分表达式捕获URL中的值； ?P将匹配到的值存储到topic_id中，而表达式\d+与包含在两个斜杆内的任何数字都匹配，不管这个数字为多少位 当发现URL与这个模式匹配的时候，django将会调用视图函数topiic()，并将存储在topic_id中的值作为实参传递给它。在这个函数中，我们使用topic_id的值来获取相应的主题 #### 视图 #### 视图topic()需要从数据库中获取指定的主题以及与之相关联的所有条目。如下所示： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim views.py from django.shortcuts import render from .models import Topic def index(request): """学习笔记的主页""" return render(request,'learning_logs/index.html') def topics(request): """显示所有的主题""" topics = Topic.objects.order_by('date_added') context = {'topics':topics} return render(request,'learning_logs/topics.html',context) def topic(request,topic_id): """显示单个主题及其所有的条目""" topic = Topic.objects.get(id=topic_id) entries = topic.entry_set.order_by('-date_added') context = {'topic':topic,'entries':entries} return render(request,'learning_logs/topic.html',context) 在这个视图函数中，我们有两个形参。这个函数接受正则表达式(?P\d+)捕获的值，并将其存储到topic_id这个形参当中 接下来，我们使用get()来获取指定的主题 下一行中，我们获取与该主题相关联的条目并将它们按照date_add进行排序：date_added前面的减号（-）指定按照降序进行排序，也就是先显示最近的条目，我们将主题和条目都存储在字典context当中，再将这个字典发送给模板topic.html。 注意： > 第一行和第二行（get和order_by）的代码都被称之为查询，因为它们会向数据中查询特定的信息，在自己的项目中编写这样的查询时，可以现在django shell中先进行试验 > 相比于直接编写视图和网页模板，再在浏览器中检查结果，在shell中执行代码可以更加快速的获得反馈 #### 模板 #### 这个模板需要显示每个主题的名称和其对应条目的内容，如果当前主题不包含任何条目，我们还需要向用户指出这一点。 代码如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim topic.html {% extends "learning_logs/base.html" %} {% block content %} Topic:{{ topic }} Entries: {% for entry in entries%} {{ entry.date_added|date:'M d, Y H:i'}} {{ entry.text|linebreaks }} { % empty % } There are no entries for this topic yet. {% endfor %} {% endblock content %} 说明： 是一个模板变量，在这里变量都是这种表示方式，这里的变量topic，是存储在字典context中的 在&lt;ul&gt;&lt;/ul&gt;中我们利用for循环定义一个显示每个条目的项目列表（无序列表） 每个列表项目都将列出两项信息：条目的时间戳和完整的文本。 第一行时间戳：在django中的模板中，竖线（|）表示模板过滤器–【对模板变量的值进行修改的函数】。后面的部分我们称之为：过滤器 过滤之后的时间戳显示格式将是：’M d, Y H:i’以这样的格式显示时间戳： January 1, 2015 23:00 第二行显示text的完整值，而不仅仅是entry的前50个字符。过滤器linebreaks将包含换行符的长条目转换为浏览器能够理解的格式，避免显示一个不间断的文本块。 在最后，我们使用模板标签{ % empty % }打印一条消息，告诉用户当前主题还没有条目 注意：代码和说明中的{和%中其实是没有空格的，在这里故意加上空格是因为hexo博客框架无法正常处理。 将显示所有主题的页面中的每个主题都设置为链接在浏览器中查看显示特定主题的页面前，我们需要修改模板topics.html，让每个主题都链接到相应的网页 如下所示： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim topics.html 这个时候我们再次刷新页面，再单击其中的一个主题，将会看到类似于下图的页面： 用户账户在这一章节，我们将创建对用户友好而直观的网页，让用户无需通过管理网站就能添加新的主题和条目，以及编辑既有的条目。 我们还将添加一个用户注册系统，让用户能够创建账户和自己的学习笔记。让任意数量的用户都能与之交互，是Web应用程序的核心所在。 让用户能够输入数据当前，只有超级用户能够通过管理网站输入数据。我们不想让用户与管理网站进行交互，因此我们将使用django的表单创建工具来创建工具让用户能够输入数据的页面 这部分，主要会涉及以下几种数据 添加新主题 添加新条目 编辑条目 添加新主题在添加新主题时，需要使用到一个输入框【在这里是一个基于表单的页面】 用户添加主题的表单我们需要让用户输入并提交信息的页面是表单 并且在用户输入数据的时候，我们还需要进行验证，确认提供的信息是否是正确的数据类型 而且信息不是恶意的信息，例如终端服务器的代码。 然后，我们再对这些有效信息进行处理，并将其保存到数据库的合适地方，这些工作都是由django自动完成。 在django中，创建表单最简单的方式是使用ModeForm，它根据我们在第18章定义的模型中的信息自动创建表单。 创建一个forms.py文件，并存储到models.py所在目录中 代码如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim forms.py from django import forms from .models import Topic class TopicForm(forms.ModelForm): class Meta: model = Topic fields = [&apos;text&apos;] labels = {&apos;text&apos;:&apos;&apos;} 最简单的ModelForm版本只包含一个内嵌的Meta类，它告诉django根据哪个模型创建表单，以及在表单中包含哪些字段。 在这里，我们根据模型Topic创建一个表单，该表单只包含字段text，下面的代码让django不要为字段text生产标签。 URL定义在这里我们创建的url是：http://localhost:8000/new_topic/ 代码如下： &quot;&quot;&quot;定义learning_logs的URL模式&quot;&quot;&quot; from django.conf.urls import url from . import views app_name = &apos;learning_logs&apos; urlpatterns = [ #主页 url(r&apos;^$&apos;,views.index,name=&apos;index&apos;), url(r&apos;^topics/$&apos;,views.topics,name=&apos;topics&apos;), url(r&apos;^topics/(?P&lt;topic_id&gt;\d+)/$&apos;,views.topic,name=&apos;topic&apos;), # 用户添加新主题的网页 url(r&apos;^new_topic/$&apos;,views.&apos;new_topic&apos;,name=&apos;new_topic&apos;) ] 这个url模式将请求交给视图函数new_topic(),接下来我们将编写这个函数 视图函数new_topic()函数new_topic()需要处理两种情形 第一种情况是刚进入new_topic网页（在这种情况下，它应该显示一个空表单） 第二种情况是对提交的表单数据进行处理，并将用户重定向到网页topics 代码如下： from django.shortcuts import render from django.http import HttpResponseRedirect from django.urls import reverse from .models import Topic from .forms import TopicForm def index(request): &quot;&quot;&quot;学习笔记的主页&quot;&quot;&quot; return render(request,&apos;learning_logs/index.html&apos;) def topics(request): &quot;&quot;&quot;显示所有的主题&quot;&quot;&quot; topics = Topic.objects.order_by(&apos;date_added&apos;) context = {&apos;topics&apos;:topics} return render(request,&apos;learning_logs/topics.html&apos;,context) def topic(request,topic_id): &quot;&quot;&quot;显示单个主题及其所有的条目&quot;&quot;&quot; topic = Topic.objects.get(id=topic_id) entries = topic.entry_set.order_by(&apos;-date_added&apos;) context = {&apos;topic&apos;:topic,&apos;entries&apos;:entries} return render(request,&apos;learning_logs/topic.html&apos;,context) def new_topic(request): &quot;&quot;&quot;添加新主题&quot;&quot;&quot; if request.method != &apos;POST&apos;: # 未提交数据：创建一个新表单 form = TopicForm() else: # POST提交的数据，对数据进行处理 form = TopicForm(request.POST) if form.is_valid(): form.save() return HttpResponseRedirect(reverse(&apos;learning_logs:topics&apos;)) context = {&apos;form&apos;: form} return render(request,&apos;learning_logs/new_topic.html&apos;,context) 说明： 在这里，我们我们导入了HttpResponseRedirect类，用户提交主题后，我们会使用这个类将用户重定向到网页topics。 函数reverse()根据指定的URL模型确定URL，这意味着django将在页面被请求时生成URL。我们还导入了刚才创建的表单TopicFrom 补充信息：GET请求与POST创建Web应用程序时，将用到的两种主要请求类型是GET请求和POST请求。对于只是从服务器读取数据的页面，使用GET请求；在用户需要通过表单提交信息时，通常使用POST请求。 处理所有表单时，我们都将指定使用POST方法。还有一些其他类型的请求，但这个项目没有使用。 函数new_topic()将请求对象作为参数。 用户初次请求该网页时，其浏览器将发送GET请求； 用户填写并提交表单时，其浏览器将发送POST请求。 根据请求的类型，我们可以确定用户请求的是空表单（GET请求）还是要求对填写好的表单进行处理（POST请求）。 如果请求方法不是POST，请求就可能是GET，因此我们需要返回一个空表单（即便请求是其他类型的，返回一个空表单也不会有任何问题）。 我们创建一个TopicForm实例，将其存储在变量form中，再通过上下文字典将这个表单发送给模板。由于实例化TopicForm时我们没有指定任何实参， Django将创建一个可供用户填写的空表单。 如果请求方法为POST，将执行else代码块，对提交的表单数据进行处理。我们使用用户输入的数据（它们存储在request.POST中）创建一个TopicForm实例，这样对象form将包含用户提交的信息。 要将提交的信息保存到数据库，必须先通过检查确定它们是有效的。函数is_valid()核实用户填写了所有必不可少的字段（表单字段默认都是必不可少的），且输入的数据与要求的字段类型一致（例如，字段text少于200个字符，这是我们在第18章中的models.py中指定的）。 这种自动验证避免了我们去做大量的工作。如果所有字段都有效，我们就可调用save()，将表单中的数据写入数据库。保存数据后，就可离开这个页面了。 我们使用reverse()获取页面topics的URL，并将其传递给HttpResponseRedirect()，后者将用户的浏览器重定向到页面topics。在页面topics中，用户将在主题列表中看到他刚输入的主题。 模板new_topic下面来创建模板new_topic.html 代码如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim new_topic.html 链接到页面new_topic接下来，我们在页面topics中添加一个到页面new_topic的链接： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim topics.html 这个时候访问指定页面，可以看到有输入框出现 添加新条目现在用户可以添加新主题了，但是他们还想添加新的条目。 我们再次定义URL，编写视图函数和模板，并连接到添加新条目的网页，但在此之前，我们需要好在forms.py中再添加一个类 1. 用于添加新条目的表单 我们需要创建一个与模型Entry相关联的表单，但是这个表单的定制程度比TopicForm要高些 (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim forms.py from django import forms from .models import Topic,Entry class TopicForm(forms.ModelForm): class Meta: model = Topic fields = [&apos;text&apos;] labels = {&apos;text&apos;:&apos;&apos;} class EntryForm(forms.ModelForm): class Meta: model = Entry fields = [&apos;text&apos;] labels = {&apos;text&apos;:&apos;&apos;} widgets = {&apos;text&apos;:forms.Textarea(attrs={&apos;cols&apos;:80})} 说明： 在这里，再导入了Entry。我们定义了属性widgets。小部件（widgets）是一个HTML表单元素，例如单行文本框、多行文本区域或者下拉列表。 通过设置属性widgets，可覆盖Django选择的默认小部件。通过让Django使用forms.Textarea，我们定制了字段’text’的输入小部件，将文本区域的宽度设置为80列，而不是默认的40列。这给用户提供了足够的空间，可以编写有意义的条目。 2. URL模式new_entry 在用户添加新条目的页面的URL模式中，需要包含实参topic_id,因为条目必须与特定的主题相关联。该URL模式如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim urls.py &quot;&quot;&quot;定义learning_logs的URL模式&quot;&quot;&quot; from django.conf.urls import url from . import views app_name = &apos;learning_logs&apos; urlpatterns = [ #主页 url(r&apos;^$&apos;,views.index,name=&apos;index&apos;), url(r&apos;^topics/$&apos;,views.topics,name=&apos;topics&apos;), url(r&apos;^topics/(?P&lt;topic_id&gt;\d+)/$&apos;,views.topic,name=&apos;topic&apos;), # 用户添加新主题的网页 url(r&apos;^new_topic/$&apos;,views.new_topic,name=&apos;new_topic&apos;), # 用户添加新条目的页面 url(r&apos;^new_entry/(?P&lt;topic_id&gt;\d+)/$&apos;,views.new_entry,name=&apos;new_entry&apos;) ] 3. 视图函数new_entry]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VPC从入门到实践]]></title>
    <url>%2F2018%2F04%2F17%2FVPC%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[参考文献： 阿里云-VPC官方帮助文档 VPC基础知识VPC概述专有网络VPC（Virtual Private Cloud）是用户基于阿里云创建的自定义私有网络, 不同的专有网络之间二层逻辑隔离，用户可以在自己创建的专有网络内创建和管理云产品实例，比如ECS、负载均衡、RDS等。 特点 专有网络VPC（Virtual Private Cloud）是基于阿里云构建的一个隔离的网络环境。每个专有网络（也就是每个VPC）之间逻辑上彻底隔离。 专有网络是独有的的云上私有网络。用户可以完全掌控自己的专有网络，例如选择IP地址范围、配置路由表和网关等，可以在自己定义的专有网络中使用阿里云资源如ECS、RDS、SLB等。 可以将专有网络连接到其他专有网络，或本地网络（这里的本地网络包括IDC和公司内部机房），形成一个按需定制的网络环境，实现应用的平滑迁移上云和对数据中心的扩展。 【默认情况下，VPC内主机是无法与外网连接的。这里需要借助阿里云的公网介入产品，例如弹性公网IP、NAT网关、负载均衡等】 VPC重点内容小结： 不同用户的云服务器部署在不同的专有网络里。 不同专有网络之间通过隧道ID进行隔离。专有网络内部由于交换机和路由器的存在，所以可以像传统网络环境一样划分子网，每一个子网内部的不同云服务器使用同一个交换机互联，不同子网间使用路由器互联。 不同专有网络之间内部网络完全隔离，只能通过对外映射的IP（弹性公网IP和NAT IP）互联。 由于使用隧道封装技术对云服务器的IP报文进行封装，所以云服务器的数据链路层（二层MAC地址）信息不会进入物理网络，实现了不同云服务器间二层网络隔离，因此也实现了不同专有网络间二层网络隔离。 专有网络内的ECS使用安全组防火墙进行三层网络访问控制。 VPC中的路由器和交换机路由器（VRouter）是专有网络的枢纽。作为专有网络中重要的功能组件，它可以连接VPC内的各个交换机，同时也是连接VPC和其他网络的网关设备。每个专有网络创建成功后，系统会自动创建一个路由器。每个路由器关联一张路由表。更多信息，参见路由。 交换机（VSwitch）是组成专有网络的基础网络设备，用来连接不同的云产品实例。创建专有网络之后，您可以通过创建交换机为专有网络划分一个或多个子网。同一专有网络内的不同交换机之间内网互通。您可以将应用部署在不同可用区的交换机内，提高应用的可用性。 说明：交换机不支持组播和广播。您可以通过阿里云提供的组播代理工具实现组播代理。详情参见组播代理概述。 整体拓扑架构如下图所示： VPC可以使用的私网地址范围： 在创建专有网络和交换机时，您需要以CIDR地址块的形式指定专有网络使用的私网网段。关于CIDR的相关信息，参见维基百科上的Classless Inter-Domain Routing条目说明。 您可以使用下表中标准的私网网段及其子网作为VPC的私网地址。专有网络创建成功之后，无法修改网段。建议使用比较大的网段，尽量避免后续扩容。 网段 可用私网IP数量 （不包括系统保留） IP地址范围 192.168.0.0/16 65532 172.16.0.0/12 1048572 172.16.0.1-172.31.255.254 10.0.0.0/8 16777212 交换机的网段不能和所属的专有网络的网段重叠，可以是其子集或者相同，网段大小在16位网络掩码与29位网络掩码之间。 如果交换机的网段和专有网络的网段相同，您只能创建一个交换机。 更多网络规划的信息，参考VPC网络规划。 关系梳理： 一个VPC（专有网络）是一个大网段，例如：192.168.0.0/16 每一个可用区是其中的子网（VLAN），由交换机相连接，例如可用区A和B的网段：192.168.1.0/24、192.168.2.0/24 如果掩码是24位，那么这里一共可以划分出来2^8=256个可用区（可用的为256个，0-255，剔除了256） 每一个可用区的可用主机数量为：2^8=256个（可用的为254个，1-254，剔除了0和255) 一个交换机连接的是一个可用区（也就是一个子网），路由器连接的是每个可用区，对外的网段是VPC的网段。 基础架构及实现原理问题随着云计算的不断发展，对虚拟化网络的要求越来越高，比如弹性（scalability）、安全性（security）、可靠性（reliability）和私密性（privacy），并且还有极高的互联性能（performance）需求，因此催生了多种多样的网络虚拟化技术。 比较早的解决方案，是将虚拟机的网络和物理网络融合在一起，形成一个扁平的网络架构，例如大二层网络。随着虚拟化网络规模的扩大，这种方案中的ARP欺骗、广播风暴、主机扫描等问题会越来越严重。为了解决这些问题，出现了各种网络隔离技术，把物理网络和虚拟网络彻底隔开。其中一种技术是用户之间用VLAN进行隔离，但是VLAN的数量最大只能支持到4096个，无法支撑公共云的巨大用户量。 解决方案基于目前主流的隧道技术，专有网络（Virtual Private Cloud，简称VPC）隔离了虚拟网络。每个VPC都有一个独立的隧道号，一个隧道号对应着一个虚拟化网络。一个VPC内的ECS（Elastic Compute Service）实例之间的传输数据包都会加上隧道封装，带有唯一的隧道ID标识，然后送到物理网络上进行传输。不同VPC内的ECS实例因为所在的隧道ID不同，本身处于两个不同的路由平面，所以不同VPC内的ECS实例无法进行通信，天然地进行了隔离。 基于隧道技术和软件定义网络（Software Defined Network，简称SDN）技术，阿里云的研发在硬件网关和自研交换机设备的基础上实现了VPC产品。 逻辑架构如下图所示，VPC包含交换机、网关和控制器三个重要的组件。交换机和网关组成了数据通路的关键路径，控制器使用自研的协议下发转发表到网关和交换机，完成了配置通路的关键路径。整体架构里面，配置通路和数据通路互相分离。交换机是分布式的结点，网关和控制器都是集群部署并且是多机房互备的，并且所有链路上都有冗余容灾，提升了VPC产品的整体可用性。 交换机和网关性能在业界都是领先的。自研的SDN协议和控制器，能轻松管控公共云成千上万张虚拟网络。 VPC通信专有网络是完全隔离的网络环境。默认情况下，相同专有云网络内的ECS和云服务可以进行私网通信，但VPC与VPC之间、VPC与经典网络或公网不能互通。您可以使用弹性公网IP、高速通道、NAT、VPN网关或公网负载均衡等公网产品实现专有网络间的通信。 VPC与VPC通信 VPC与经典网络通信 VPC与Internet通信 VPC与本地IDC通信 VPC与VPC通信默认情况，不同专有网络间的ECS不能直接进行私网通信。 您可以使用高速通道的路由器接口，在两侧VPC的路由器上分别创建路由器接口，以及自有的骨干传输网络来搭建高速通道，轻松实现VPC之间安全可靠、方便快捷的通信。配置详情参考同账号下专有网络内网互通。 VPC与经典网络通信默认专有网络内的ECS不能访问经典网络的ECS或者云服务。 VPC访问经典网络 VPC与经典网络可以通过公网IP进行通信。只要VPC和经典网络中的ECS实例或云实例的公网IP符合下表中的任意一一条要求，专有网络就可以访问经典网络的云服务。 经典网络访问VPC 经典网络也可以通过公网IP访问VPC，只要VPC中的ECS实例或云服务也配置了公网IP。 VPC与Internel通信默认情况下，专有网络内的ECS不能与公网互通。您可以通过以下途径中的一种打通VPC与公网的通信： 为专有网络中的ECS实例分配公网IP，实现专有网络与公网的通信。详情参考分配公网IP。 您可以通过在ECS上绑定弹性公网IP（EIP），实现专有网络与公网的互通。详情参考弹性公网IP。 在专有网络的ECS上设置NAT网关，实现专有网络与公网的互通。详情参考端口映射和SNAT设置。 如果您有多台ECS需要和公网互通，您可以使用NAT网关的共享带宽包，一个带宽包内的所有ECS实例共享带宽，以节省您的费用。 专有网络的ECS实例添加到一个公网负载均衡实例中。详情参考配置公网负载均衡。 此情形下，专有网络中的ECS实例不能访问公网，只能接收负载均衡转发的公网请求。 VPC与本地IDC通信默认情况下，本地IDC网络中心和专有网络之间不能通信，您可以通过以下途径打通本地IDC与VPC之间的通信： 您可以使用高速通道的物理专线来连通本地IDC到阿里云的专线接入点，并建立虚拟边界路由器作为VPC到IDC的数据转发桥梁。详情参考专线接入。 您可以使用VPN网关来实现本地IDC网络中心与专有网络的互通，详情参考搭建VPN网关。 名词解释 注意： 路由条目包括系统路由和自定义路由两种类型。 使用限制 注意： 每个VPC只有一个路由器，也就只有一张路由表 每个VPC可以容纳的云产品数量最多为15000个 路由器不支持动态路由协议 交换机不支持二层的广播个组播 VPC应用场景此部分介绍了专有网络常用的使用场景和架构： 场景一：本地数据中心+云上业务的混合云模式 场景二：多租户的安全隔离 场景三：主动访问公网的抓取类业务 场景四：多个应用流量波动大——共享带宽包 场景一：本地数据中心+云上对外业务的混合云模式如果您有以下业务需求，建议您使用VPC+高速通道+ECS+RDS的配置架构。 将内部核心系统与核心数据放置在自建数据中心以确保核心数据的安全； 云上部署对外客户的应用系统，实时应对业务访问量激增。 架构解读： 使用VPC、RDS、ECS搭建云上业务系统，核心数据部署在云下自建数据中心，使用高速通道专线接入保证云上数据快速同步，实现云上云下数据互通，搭建一个混合云使用环境。 场景二：多租户的安全隔离如果您有以下业务需求，建议使用VPC+ECS+RDS+SLB的配置架构。 希望在云上构建一个完全隔离的业务环境，因为传统云架构的多租户共享机制不能保证数据安全； 自主定义私有网络配置。 架构解读：您可以在阿里云上创建一个专有网络，和其他租户的网络完全隔离。您可以完全掌控自己的虚拟网络，例如选择自己的IP地址范围、划分网段、配置路由表和网关等，从而实现安全而轻松的资源访问和应用程序访问。此外，您也可以通过专线或VPN等连接方式将您的专有网络与传统数据中心相连，形成一个按需定制的网络环境，实现应用的平滑迁移上云和对数据中心的扩展。 场景三：主动访问公网的抓取类业务如果您有以下业务需求，建议使用VPC+ECS+NAT网关的配置架构。 专有网络中的多个服务器可以主动访问互联网； 避免这些服务器的公网IP暴露在公网上。 架构解读： 您可以对专有网络中的同一虚拟交换机下的所有ECS做SNAT配置，多台ECS通过同一公网IP访问互联网，并可随时进行公网IP替换，避免被外界攻击。 场景四：多个应用流量波动大——共享带宽包如果您有以下需求，建设使用VPC+NAT网关+ECS的配置架构。 系统中同时存在多个面向互联网的应用； 各个应用都需要对外提供服务，并且其波峰时间点不一致。 架构解读：您可以购买多个专有网络类型的ECS，分别承载不同的应用业务，前端挂载NAT网关，通过配置DNAT IP转发规则实现多IP共享带宽功能，减轻波峰波谷效应，从而减少您的成本。 NAT网关SNAT：在NAT网关上操作，接受到后端ECS主机发送来的数据包时，将源IP地址，修改为自身的IP地址，堆外隐藏内部的信息，是内部主动对外的 DNAT：在NAT网关上操作，接受到internet发送过来的数据包时，将目的IP地址进行修改（这个时候是NAT网关的地址），修改成为内部ECS主机的IP地址。用于后端多套系统共享带宽。 S：源是NAT网关 D:目的地是NAT网关 入门实践搭建专有网络本教程将指引您搭建一个专有网络，并为专有网络中的ECS实例绑定一个弹性公网IP（EIP）进行公网访问。 参考文献: 官方帮助文档-搭建专有网络 步骤1 创建专有网络和交换机在专有网络中部署云资源，您必须至少创建一台交换机。完成以下操作步骤，创建专有网络和交换机： 登录专有网络管理控制台。 在顶部菜单栏，选择专有网络的地域。 注意：专有网络的地域和要部署的云资源的地域必须相同，本操作选择华北2（也就是北京地域）。 单击创建专有网络，根据以下信息配置专有网络和交换机，然后单击确定。 配置对照表： 创建如下图所示： 注意： 每个交换机的第一个和最后三个IP地址为系统保留地址。 例如/20的掩码，可以使用的IP范围应该是4094个（2^12-2=4096-2=4094），但是实际平台上显示可分配使用IP为4092个，因为有4个已经被系统占用（以172.16.0.0/20为例，172.176.0.0、172.31.255.253、172.31.255.254和172.31.255.255这些地址是系统保留地址。） 步骤二 创建ECS实例完成以下操作，在已创建的VPC中创建一个ECS实例： 在专有网络控制台的左侧导航栏，单击交换机。 选择交换机的地域，本操作选择华北1。 找到已创建的交换机，然后单击购买 &gt; ECS实例。 配置ECS实例后，单击立即购买。 本操作中ECS实例的网络配置如下： 网络：选择已创建的专有网络和交换机。 公网IP地址：选择不分配。 返回ECS管理控制台，查看已创建的ECS实例。 步骤三 创建EIP弹性公网IP（EIP）是可以独立购买和持有的公网IP地址资源。完成以下操作，创建EIP： 在专有网络控制台的左侧导航栏，单击弹性公网IP。 选择EIP的地域，然后单击申请弹性公网IP。本操作，选择华北1。 配置EIP，完成支付。 步骤四 绑定EIP完成以下操作，将EIP绑定到已创建的ECS实例上： 在专有网络控制台的左侧导航栏，单击弹性公网IP。 选择EIP的地域。 找到已创建的EIP，然后单击绑定。 在弹出的对话框中，实例类型选择ECS实例，然后选择已创建的ECS实例。 单击确定。 步骤五 公网访问测试以上4和步骤均省略，可以自行查看文档 ECS安全组配置这部分内容见官方文档： 官方文档1 官方文档2 交换机交换机（VSwitch）是组成专有网络的基础网络设备，用来连接不同的云产品实例。创建专有网络之后，您可以通过创建交换机为专有网络划分一个或多个子网，同一专有网络内的不同交换机之间内网互通。 您可以将应用部署在不同可用区的交换机内，提高应用的可用性。 交换机不支持组播和广播。您可以通过阿里云提供的组播代理工具实现组播代理。详情参见组播代理概述。 交换机的网段在创建交换机时，您需要以CIDR地址块的形式指定交换机的私网网段，交换机的网段限制如下： 交换机的网段可以和其所属的VPC网段相同或者是其VPC网段的子集。 例如，VPC的网段是192.168.0.0/16，那么该VPC内的交换机的网段可以是192.168.0.0/16，也可以是192.168.0.0/17，一直到192.168.0.0/29。 说明：如果您的交换机网段和所属VPC网段相同，您只能在该VPC下创建一台交换机。 交换机的网段的大小在16位网络掩码与29位网络掩码之间，可提供8-65536个地址。 每个交换机的第一个和最后三个IP地址为系统保留地址。 以192.168.1.0/24为例，192.168.1.0、 192.168.1.253、 192.168.1.254和192.168.1.255这些地址是系统保留地址。 交换机网段的确定还需要考虑该交换机下容纳的主机的数量。 如果该交换机有和其他专有网络的交换机，或本地数据中心通信的需求，确保交换机的网段和要通信的网段不冲突。 默认的专有网络和交换机当创建一个云产品实例时，如果您没有提前创建专有网络和交换机，您可以使用系统提供的默认专有网络配置。在实例创建后，一个默认的专有网络和交换机也会随之创建好。 说明：每个地域只有一个默认专有网络，但每个专有网络内的每个可用区都可创建一个默认交换机。 默认专有网和交换机的配置如下表所示。 路由表和路由条目创建专有网络时，系统会为该专有网络自动创建一个路由器和一张路由表。 路由表中的每一项是一条路由条目。路由条目指定了网络流量的导向目的地，由目标网段、下一跳类型、下一跳三部分组成。路由条目包括系统路由和自定义路由。 您不可以直接删除专有网络的路由器或路由表，但可以在路由表中添加自定义路由条目转发流量。删除VPC后，关联的路由器和路由表也会随之删除。 系统路由创建VPC时，系统会自动添加一条目标网段为100.64.0.0/10的系统路由用于VPC内的云产品通信。另外，阿里云也会为交换机自动添加一条以交换机网段为目标网段的系统路由。 我们不可以创建，也不能删除系统路由。 比如我创建了一个网段为192.168.0.0/16的专有网络，并在该专有网络下创建了两个网段为192.168.1.0/24和192.168.0.0/24的交换机，则该专有网络的路由表中会有如下三条系统路由： 目标网段 下一跳类型 下一跳 类型 100.64.0.0/10 - - 系统 192.168.1.0/24 - - 系统 192.168.0.0/24 - - 系统 自定义路由您可以根据需要，添加自定义路由。针对不同的功能，专有网络提供如下下一跳类型的路由： ECS实例：将指向目标网段的流量转发到专有网络内的一台ECS实例上。 当需要通过该ECS实例部署的应用访问互联网或其他应用时，配置此类型的路由。【一般是网关管理类应用服务器】 适用于将指定网络访问路由至ECS实例进行流量统一转发和管理的场景，例如将一台ECS实例配置为公网网关管理其他ECS实例访问公网。 VPN网关：将指向目标网段的流量转发到一个VPN网关上。 当需要通过VPN网关连接本地网络或者其他专有网络时，配置此类型的路由。 专有网络：将来指向标网段的流量转发到一个专有网络内。 当需要使用高速通道连接两个专有网络时，配置此类型的路由。 边界路由器：将指向目标网段的流量转发到一个边界路由器上。 当需要使用高速通道连接本地网络（物理专线接入）时，才需要配置此类型的路由。 选路规则路由表采用最长前缀匹配原则作为流量的路由选路规则。最长前缀匹配是指当路由表中有多条条目可以匹配目的IP时，采用掩码最长（最精确）的一条路由作为匹配项并确定下一跳。 路由实例VPC内网路由当您在VPC内的一台ECS实例（ECS01）自建了NAT网关或绑定了弹性公网IP，您需要专有网络内的云资源通过该ECS实例访问公网时（实际上是被外部访问），可以添加如下一条自定义路由： 目标网段 下一跳类型 下一跳 0.0.0.0/0 ECS实例 ECS01 VPC互连VPC之间的互连又可以分为两种情况，一种是直接使用高速通道进行连接，一种是使用VPN网关进行连接。 第一种：使用高速通道进行连接 当使用高速通道连接两个VPC（VPC1 172.16.0.0/12和VPC2 192.168.0.0/16）时，创建完两个互相连接的路由器接口后，您还需要在两个VPC中分别添加如下一条路由： VPC1的路由配置: 目标网段 下一跳类型 下一跳 192.168.0.0/16 路由器接口（专有网络方向） VPC2的路由配置: 目标网段 下一跳类型 下一跳 172.16.0.0/12 路由器接口（专有网络方向） 第二种：使用VPN网关进行连接 如下图所示，当使用VPN网关连接两个VPC（VPC1 172.16.0.0/12和VPC2 10.0.0.0/8）时，配置完VPN网关后，需要在VPC中分别添加如下路由： VPC1的路由配置 目标网段 下一跳类型 下一跳 10.0.0.0/8 VPN网关 VPN网关1 VPC2的路由配置 目标网段 下一跳类型 下一跳 172.16.0.0/12 VPN网关 VPN网关2 VPC连接本地网络-（自建机房或者IDC）VPC连接本地网络又可以分为两种情况，一种是直接使用高速通道进行连接，一种是使用VPN网关进行连接。 第一种：使用高速通道连接本地网络 如下图所示，当使用高速通道物理专线连接专有网络和本地网络时，配置完专线和边界路由器后，需要配置如下路由： VPC端的路由配置 目标网段 下一跳类型 下一跳 192.168.0.0/16 路由器接口（普通路由） RI1 边界路由器端的配置 目标网段 下一跳类型 下一跳 192.168.0.0/16 指向专线 RI3 172.16.0.0/12 指向VPC RI2 本地网络的路由配置 目标网段 下一跳类型 下一跳 172.16.0.0/12 — 本地网关设备 第二种：使用VPN网关连接本地网络 如下图所示，当使用VPN网关连接VPC（网段：172.16.0.0/12）和本地网络（网段：192.168.0.0/16）时，配置好VPN网关后，需要在VPC内添加如下一条路由： 目标网段 下一跳类型 下一跳 192.168.0.0/16 VPN网关 已创建的VPN网关 添加自定义路由步骤如下所示： 登录专有网络管理控制台。 在左侧导航栏，单击路由表。 选择路由表所属的VPC的地域，然后单击目标路由表的ID链接。 单击添加路由条目。 在弹出的对话框，配置路由条目： 最佳实践网络规划在进行网络规划的时候，有几个问题需要去解决： 问题1：应该使用几个VPC？ 问题2：应该使用几个交换机？ 问题3：应该选择什么网段？ 问题4：VPC与VPC互通或者与线下IDC互通时，如何规划网段？ 问题1：应该使用几个VPC？单个VPC： 如果没有多地域部署系统的要求且各系统之间也不需要通过VPC进行隔离，那么推荐使用一个VPC。 目前，单个VPC内运行的云产品实例可达15000个，这样的容量基本上可以满足需求。 ps：可用区是指在同一地域内，电力和网络互相独立的物理区域，在同一地域内可用区与可用区之间内网互通。 单个VPC的拓扑如下所示： 多个VPC： 有以下需求时，那么建议使用多个VPC 多地域部署系统 VPC是地域级别的资源，是不能跨地域部署的。当您有多地域部署系统的需求时，就必然需要使用多个VPC。基于阿里巴巴骨干网构建的高速通道产品能轻松实现跨地域，跨国VPC间的互通。详情参考高速通道VPC互通。 多业务系统隔离 如果在一个地域的多个业务系统需要通过VPC进行严格隔离，比如生产环境和测试环境，那么也需要使用多个VPC，如下图所示。 问题2：应该使用几个交换机？首先，即使只使用一个VPC，也尽量使用至少两个交换机，并且将两个交换机分布在不同可用区，这样可以实现跨可用区容灾。 同一地域不同可用区之间的网络通信延迟很小，但也需要经过业务系统的适配和验证。由于系统调用复杂加上系统处理时间、跨可用区调用等原因可能产生期望之外的网络延迟。建议您进行系统优化和适配，在高可用和低延迟之间找到平衡。 其次，使用多少个交换机还和系统规模、系统规划有关。如果前端系统可以被公网访问并且有主动访问公网的需求，考虑到容灾可以将不同的前端系统部署在不同的交换机下，将后端系统部署在另外的交换机下。 问题3：应该选择什么网段？在创建VPC和交换机时，您必须以无类域间路由块 (CIDR block) 的形式为您的专有网络划分私网网段。 VPC网段规划 网段范围 网段 可用IP地址数量 备注 192.168.0.0/16 65532 去除系统占用地址 172.16.0.0/12 1048572 去除系统占用地址 10.0.0.0/8 16777212 去除系统占用地址 注意：如果有除此之外的特殊网段要求，也可以提工单或者通过客户经理申请开通。 如果有多个VPC，或者有VPC和线下IDC构建混合云的需求，建议使用上面这些标准网段的子网作为VPC的网段，掩码建议不超过16位。 如果云上只有一个VPC并且不需要和本地IDC互通时，可以选择上表中的任何一个网段或其子网。 VPC网段的选择还需要考虑到是否使用了经典网络。如果您使用了经典网络，并且计划将经典网络的ECS实例和VPC网络连通，那么，建议您选择非10.0.0.0/8作为VPC的网段，因为经典网络的网段也是10.0.0.0/8。 交换机网段 可以根据以下建议规划交换机网段。同样，交换机创建成功后，网段无法再修改。 交换机的网段的大小在16位网络掩码与29位网络掩码之间，可提供8-65536个地址。16位掩码能支持65532个ECS实例，而小于29位掩码又太小，没有意义。 交换机的网段可以和其所属的VPC网段相同，或者是其VPC网段的子网。比如VPC的网段是192.168.0.0/16，那么该VPC下的虚拟交换机的网段可以是192.168.0.0/16，也可以是192.168.0.0/17一直到192.168.0.0/29。 如果交换机网段和所属VPC网段相同，您在该VPC下只能创建一台交换机。 每个交换机的第一个和最后三个IP地址为系统保留地址。以192.168.1.0/24为例，192.168.1.0、 192.168.1.253、192.168.1.254和192.168.1.255这些地址是系统保留地址。 ClassicLink功能允许经典网络的ECS和192.168.0.0/16，10.0.0.0/8，172.16.0.0/12这三个VPC网段的ECS通信。例如，如果要和经典网络通信的VPC网段是10.0.0.0/8，则要和经典网络ECS通信的交换机的网段必须是10.111.0.0/16。详情参考ClassicLink。 交换机网段的确定还需要考虑该交换机下容纳ECS的数量。 问题4：VPC与VPC互通或者与本地数据中心互通时，如何规划网段？如下图所示，比如您在华东1、华北2、华南1三个地域分别有VPC1、VPC2和VPC3三个VPC。VPC1和VPC2通过高速通道内网互通，VPC3目前没有和其他VPC通信的需求，将来可能需要和VPC2通信。另外，您在上海还有一个自建IDC，需要通过高速通道（专线功能）和华东1的VPC1私网互通。 此例中VPC1和VPC2使用了不同的网段，而VPC3暂时没有和其他VPC互通的需求，所以VPC3的网段和VPC2的网段相同。但考虑到将来VPC2和VPC3之间有私网互通的需求，所以两个VPC中的交换机的网段都不相同。VPC互通要求互通的交换机的网段不能一样，但VPC的网段可以一样。 在多VPC的情况下，建议遵循如下网段规划原则： 尽可能做到不同VPC的网段不同，不同VPC可以使用标准网段的子网来增加VPC可用的网段数。 如果不能做到不同VPC的网段不同，则尽量保证不同VPC的交换机网段不同。 如果也不能做到交换机网段不同，则保证要通信的交换机网段不同。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>公有云产品</category>
        <category>阿里云</category>
        <category>VPC</category>
      </categories>
      <tags>
        <tag>VPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper从入门到精通]]></title>
    <url>%2F2018%2F04%2F17%2FZookeeper%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logstash]]></title>
    <url>%2F2018%2F04%2F17%2FLogstash%2F</url>
    <content type="text"><![CDATA[概述logstash由以下几个部分组成 input filter output How Logstash Works每个input都拥有一个线程 input生成的event存储在内存或者磁盘中]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Logstash</category>
      </categories>
      <tags>
        <tag>Logstash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kibana]]></title>
    <url>%2F2018%2F04%2F17%2FKibana%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Kibana</category>
      </categories>
      <tags>
        <tag>Kibana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Filebeat]]></title>
    <url>%2F2018%2F04%2F17%2FFilebeat%2F</url>
    <content type="text"><![CDATA[本文基于Filebeat 6.2.3版本 参考文献： 官网 1. Filebeat基础知识 Filebeat consists of two main components: prospectors and harvesters. These components work together to tail files and send event data to the output that you specify. prospectors:勘探者；探矿者【也就是数据变化的探测者，也就是入向配置】 harvesters：收割机；收获者【也就是数据的下游接收端，也就是出向配置】 spooler：处理程序；【处理程序会集合这些事件，最后filebeat会发送集合的数据到你指定的地点。】 Filebeat工作流程： 当你开启filebeat程序的时候，它会启动一个或多个探测器（prospectors）去检测你指定的日志目录或文件，对于探测器找出的每一个日志文件，filebeat启动收割进程（harvester），每一个收割进程读取一个日志文件的新内容，并发送这些新的日志数据到处理程序（spooler），处理程序会集合这些事件，最后filebeat会发送集合的数据到你指定的地点。 流程图如下： 1.1 What is harvester? A harvester is responsible for reading the content of a single file. The harvester reads each file, line by line, and sends the content to the output. One harvester is started for each file. The harvester is responsible for opening and closing the file, which means that the file descriptor remains open while the harvester is running. If a file is removed or renamed while it’s being harvested, Filebeat continues to read the file. This has the side effect that the space on your disk is reserved until the harvester closes. By default, Filebeat keeps the file open until close_inactive is reached. Closing a harvester has the following consequences:【关闭时按以下顺序依`次执行，也就是关闭顺序】 The file handler is closed, freeing up the underlying resources if the file was deleted while the harvester was still reading the file.The harvesting of the file will only be started again after scan_frequency has elapsed. If the file is moved or removed while the harvester is closed, harvesting of the file will not continue. To control when a harvester is closed, use the close_* configuration options. harvester负责打开和关闭文件 当harvester捕获到一个文件之后，这个文件被删除或者重命名，它将继续读取这个文件。【副作用是占用磁盘空间直到harvester进程关闭】 1.2 What is prospector? A prospector is responsible for managing the harvesters and finding all sources to read from. If the input type is log, the prospector finds all files on the drive that match the defined glob paths and starts a harvester for each file. Each prospector runs in its own Go routine. The following example configures Filebeat to harvest lines from all log files that match the specified glob patterns: filebeat.prospectors: - type: log paths: - /var/log/*.log - /var/path2/*.log prospector负责管理harvesters进程以及寻找需要去读取的资源（input设置） Filebeat的input type当前有两种设置：log和stdin The log prospector checks each file to see whether a harvester needs to be started, whether one is already running, or whether the file can be ignored (see ignore_older).New lines are only picked up if the size of the file has changed since the harvester was closed. 注意，Filebeat只能读取本地的文件，也就是需要在每个日志产生端都安装：Filebeat prospectors can only read local files. There is no functionality to connect to remote hosts to read stored files or logs. 1.3 How does Filebeat keep the state of filesfilebeat通过定期去刷新，将状态落地到磁盘的注册文件中，以这种形式来保持文件检测状态 Filebeat keeps the state of each file and frequently flushes the state to disk in the registry file The state is used to remember the last offset a harvester was reading from and to ensure all log lines are sent filebeat的状态信息记录的是最新的读取偏移量，如果下游的接受者（ES、kafka、logstash等不可达），filebeat将会保持这种状态，当检测后可达之后将会重新发送 当filebeat重启时，将会重新构建这个注册文件 每个prospector针对每个文件都会保持一个状态，也就是一个注册文件，因为文件可能会被删除或者重命名 针对每个文件，filebeat存储一个唯一的标识符去发现该文件之前是否有被收集过 For each file, Filebeat stores unique identifiers to detect whether a file was harvested previously. 1.4 how does Filebeat ensure at-least-once delivery?Filebeat保证一个事件的完整及正确性，它将发送最少一次给设置的下游输出，以保证没有数据丢失。 Filebeat guarantees that events will be delivered to the configured output at least once and with no data loss. Filebeat is able to achieve this behavior because it stores the delivery state of each event in the registry file. Filebeat能够保证这种特性的原因是因为它将分发状态也存储在这个注册文件中。 当下游因为阻塞或者其他原因，没有对某一个事件进行确认的时候，filebeat将一直尝试去发送这个事件，直到收到ACK If Filebeat shuts down while it’s in the process of sending events, it does not wait for the output to acknowledge all events before shutting down. Any events that are sent to the output, but not acknowledged before Filebeat shuts down, are sent again when Filebeat is restarted. This ensures that each event is sent at least once, but you can end up with duplicate events being sent to the output. You can configure Filebeat to wait a specific amount of time before shutting down by setting theshutdown_timeout option. 当Filebeat异常关闭，再次启动的时候，它将会重新发送没有接受到ACk的事件。通过这种机制来保证每个事件至少发送一次。 因此，为了减少重新发送event事件的次数，可以通过设置shutdown_timeout参数来设置当filebeat关闭时，等待多少时间之后再关闭进程，以保证收到尽量多的ACK 注意：虽然拥有这种机制来保证数据的不丢失，但还是存在一些可能的情况导致数据的丢失（日志轮转和删除文件时） 例如： If log files are written to disk and rotated faster than they can be processed by Filebeat 【当日志刚好触发到日志轮转条件时，并且此时filebeat还没有来得及收集的时候，原因是inode节点发生变化】 if files are deleted while the output is unavailable 【当该文件被删除时，并且输出不可用时】 总结：filebeat会维护一个注册文件【是落地到磁盘中的】，该注册文件中包含2个信息 所发送事件的偏移量，精确记录当前的发送情况。 下游断开时，将保持直到连接后再发送 发送事件的ACk，记录发送事件的接受情况。 没有收到，将一直持续发送。 2. Filebeat安装部署配置启动2.1 安装curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.2.3-x86_64.rpm rpm -vih filebeat-6.2.3-x86_64.rpm 2.2 启动service filebeat start ./filebeat -e -c ./bigdata.yml 2.3 命令主要提供了8个命令API [root@master ~]# filebeat --help Usage: filebeat [flags] filebeat [command] Available Commands: export Export current config or index template #导出ES的索引模板 help Help about any command keystore Manage secrets keystore modules Manage configured modules #Filebeat的模块相关 run Run filebeat setup Setup index template, dashboards and ML jobs #设置初始化环境，包括索引模板，kibana的仪表盘等 test Test config version Show current version info Flags: -E, --E setting=value Configuration overwrite -M, --M setting=value Module configuration overwrite -N, --N Disable actual publishing for testing -c, --c string Configuration file, relative to path.config (default &quot;filebeat.yml&quot;) --cpuprofile string Write cpu profile to file -d, --d string Enable certain debug selectors -e, --e Log to stderr and disable syslog/file output -h, --help help for filebeat --httpprof string Start pprof http server --memprofile string Write memory profile to this file --modules string List of enabled modules (comma separated) --once Run filebeat only once until all harvesters reach EOF --path.config string Configuration path (default &quot;&quot;) --path.data string Data path (default &quot;&quot;) --path.home string Home path (default &quot;&quot;) --path.logs string Logs path (default &quot;&quot;) --plugin pluginList Load additional plugins --setup Load the sample Kibana dashboards --strict.perms Strict permission checking on config files (default true) -v, --v Log at INFO level Use &quot;filebeat [command] --help&quot; for more information about a command. 注意： filebeat有：Config File Ownership and Permissions On systems with POSIX file permissions, all Beats configuration files are subject to ownership and file permission checks. The purpose of these checks is to prevent unauthorized users from providing or modifying configurations that are run by the Beat. The owner of the configuration files must be either root or the user who is executing the Beat process. The permissions on each file must disallow writes by anyone other than the owner. 也就是说只有root用户或者文件的属主才有权限执行命令 2.4 编辑配置文件配置文件生成规则 安装完毕之后，配置文件路径：/etc/filebeat/filebeat.yml参考配置文件为：filebeat.reference.yml 这里采用的是rpm包方式安装，因此生成规则如下： filebeat.yml整个配置文件分为几个部分，分别是： - Modules configuration - Filebeat prospectors 【上游输入设置。这部分包含内容为：type设置|path路径设置，】 - Filebeat autodiscover - Filebeat global options - General - Elastic Cloud - Outputs 【下游输出设置。这部分内容为：ES|kafka|logstash|】 - Paths - Dashboards - Template 【ES模板配置】 - Kibana - Logging - X - - - Monitoring 【x - - - 监控】 - HTTP Endpoint 让我们来配置filebeat： modules模块设置filebeat的模块实现了快速部署的方式。该部分的设置是可选的，你可以选择在后面自己定义prospectors设置相关设置。可以通过以下3种方式开启modules Enable module configs in the modules.d directoryedit Enable modules when you run Filebeatedit Enable module configs in the filebeat.yml file 使用了模块之后，仍然可以指定变量设置来覆盖模块中的定义【最小局部生效】。并且，可以使用高级设置来覆盖prospector中的相关设置 模块命令： 第一种方式： ./filebeat modules enable apache2 mysql 开启modules.d下的指定模块 ./filebeat modules list 查看启动的模块情况 第二种方式： ./filebeat -e --modules nginx,mysql,system 在启动filebeat的时候启动模块 第三种方式： filebeat.modules: - module: nginx - module: mysql - module: system 模块的高级设置： 能够覆盖prospector中的设置 Behind the scenes, each module starts a Filebeat prospector. Advanced users can add or override any prospector settings. For example, you can set close_eof to true in the module configuration: - module: nginx access: prospector: close_eof: true Or at the command line like this: ./filebeat -M &quot;nginx.access.prospector.close_eof=true&quot; Here you see how to use the -M flag along with the –modules flag: ./filebeat --modules nginx -M &quot;nginx.access.prospector.close_eof=true&quot; You can use wildcards to change variables or settings for multiple modules/filesets at once. For example, the following command enables close_eof for all the filesets in the nginx module: ./filebeat -M &quot;nginx.*.prospector.close_eof=true&quot; The following command enables close_eof for all prospectors created by any of the modules: ./filebeat -M &quot;*.*.prospector.close_eof=true&quot; Filebeat global options 设置这部分可以设置filebeat自动去探测检测文件 filebeat.config.prospectors: enabled: true path: configs/*.yml reload.enabled: true reload.period: 10s prospectors设置对于大多数的基本filebeat配置，你可以定义一个单一探测器针对一个单一的路径，例如： filebeat.prospectors: - input_type: log paths: - /var/log/*.log 在这个例子中，探测器会收集/var/log/*.log的所有匹配文件，这意味这filebeat会手机所有的/var/log下以.log结尾的文件，此处还支持Golang Glob支持的所有模式。 在预定义级别的子目录中获取所有文件，可以使用这个配置：/var/log//.log，这会找到/var/log下所有子目录中所有的以.log结尾的文件。但它并不会找到/var/log文件夹下的以.log结尾的文件。现在它还不能递归的在所有子目录中获取所有的日志文件。 Outputs设置如果你设置输出到elasticsearch中，那么你需要在filebeat的配置文件中设置elasticsearch的IP地址与端口。 output.elasticsearch: hosts: [&quot;192.168.1.42:9200&quot;] 如果要使用Logstash对Filebeat收集的数据执行附加处理，则需要将Filebeat配置为使用Logstash。 ＃----------------------------- Logstash输出------------------ -------------- output.logstash： hosts：[“127.0.0.1:5044”] 如果您打算使用随Filebeat提供的示例Kibana仪表板，需要配置Kibana，这段是属于kibana设置，不输出output设置 #============================== Kibana ===================================== setup.kibana： host：“localhost：5601” 如果设置ES和kibana的安全性设置，使用以下的配置 output.elasticsearch: hosts: [&quot;myEShost:9200&quot;] username: &quot;elastic&quot; password: &quot;elastic&quot; setup.kibana: host: &quot;mykibanahost:5601&quot; username: &quot;elastic&quot; password: &quot;elastic&quot; Template设置这一部分主要设置ES的模板 在Elasticsearch中，索引模板用于定义设置和映射，以确定如何分析字段。通过使用ES模板，可以有效的减轻存储压力ES模板：通过对索引中的每个字段做事先的预定义数据类型（例如ID，name等分别使用存储空间最小的数据类型） 在安装完毕Filebeat之后，会生成fields.yml这个ES模板文件下游如果是ES的话，Filebeat在启动的时候会自动的加载这个模板文件如果要关闭自动加载功能，则将以下参数设置为false setup.template.enabled: false 注意：如果该模板已经存在，则不会覆盖它，除非您配置Filebeat来指定执行此操作。 如果下游连接的不是ES而是logstash，那么需要手动导入模板 filebeat setup --template -E output.logstash.enabled=false -E &apos;output.elasticsearch.hosts=[&quot;localhost:9200&quot;]&apos; 强制Kibana使用最新的Filebeat索引信息 如果当前ES中已经有了filebeat的索引信息，那么修改模板之后，因此模板不会被覆盖，因此需要强制刷新生效。 curl -XDELETE &apos;http://localhost:9200/filebeat-*&apos; 中转方式导入ES模板 如果Filebeat没有直接连接到ES，那么可以将模板文件先导出到可以连接到ES的主机上，再通过这台去导入 filebeat export template &gt; filebeat.template.json curl -XPUT -H &apos;Content-Type: application/json&apos; http://localhost:9200/_template/filebeat-6.2.3 -d@filebeat.template.json 相关命令 ./filebeat setup -e 导入ES的索引末班 ./filebeat -e --modules system 导入模块的命令，这里是导入system模块 ./filebeat -e --modules system,nginx,mysql 一次运行多个模块 Kibana设置再kibana上显示filebeat的索引信息之前，you need to create the index pattern, filebeat-*， and load the dashboards into Kibana.不过在filebeat的6.0.0版本之后，这部分操作通过配置文件中的kibana配置部署来实现也就是上面说到的这一段的配置： #============================== Kibana ===================================== setup.kibana： host：“localhost：5601” 在配置之前请确保kibana已经处于运行状态，然后执行如下命令 filebeat setup --dashboards 多行日志处理处理多行日志，主要包括JAVA的堆栈内存，程序语言的类似\换行功能，时间戳引导的一段日志，应用指定的start–end等日志段 默认情况下JAVA堆栈日志是有多行组成的，例如： Exception in thread &quot;main&quot; java.lang.NullPointerException at com.example.myproject.Book.getTitle(Book.java:16) at com.example.myproject.Author.getBookTitles(Author.java:25) at com.example.myproject.Bootstrap.main(Bootstrap.java:14) 因此在filebeat中要把这些多行的日志组合成为一个event。这就需要以下的配置： multiline.pattern: &apos;^[[:space:]]&apos; multiline.negate: false multiline.match: after This configuration merges any line that begins with whitespace up to the previous line. 如果还涉及到更复杂的JAVA堆栈日志格式，例如： Exception in thread &quot;main&quot; java.lang.IllegalStateException: A book has a null property at com.example.myproject.Author.getBookIds(Author.java:38) at com.example.myproject.Bootstrap.main(Bootstrap.java:14) Caused by: java.lang.NullPointerException at com.example.myproject.Book.getId(Book.java:22) at com.example.myproject.Author.getBookIds(Author.java:35) ... 1 more 那么需要如下的配置： multiline.pattern: &apos;^[[:space:]]+(at|\.{3})\b|^Caused by:&apos; multiline.negate: false multiline.match: after In this example, the pattern matches the following lines: a line that begins with spaces followed by the word at or … a line that begins with the words Caused by:]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Filebeat</category>
      </categories>
      <tags>
        <tag>Filebeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka]]></title>
    <url>%2F2018%2F04%2F17%2FKafka%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume]]></title>
    <url>%2F2018%2F04%2F17%2FFlume%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch]]></title>
    <url>%2F2018%2F04%2F17%2FElasticsearch%2F</url>
    <content type="text"><![CDATA[更多内容请看：官方文档 基础知识ES在整个ELk架构中提供的功能： 数据存储 数据搜索 数据分析 索引模板在Elasticsearch中，索引模板用于定义设置和映射，以确定如何分析字段。（例如ID，name等分别使用存储空间最小的数据类型） 索引模板允许您定义创建新索引时将自动应用的模板。 模板仅适用于索引创建时。更改模板将不会影响现有的索引。在使用创建索引的API时，如果设置了数据类型，那么将会覆盖模板的设置，也就是最小局部生效]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK架构之Filebeat+kafka+logstash+Elasticsearch]]></title>
    <url>%2F2018%2F04%2F17%2FELK%E6%9E%B6%E6%9E%84%E4%B9%8BFilebeat-kafka-logstash-Elasticsearch%2F</url>
    <content type="text"><![CDATA[整体架构 组件关系说明： logstash 和filebeat都具有日志收集功能，filebeat更轻量，占用资源更少，但logstash 具有filter功能，能过滤分析日志。一般结构都是filebeat采集日志，然后发送到消息队列，redis，kafaka。然后logstash去获取，利用filter功能过滤分析，然后存储到elasticsearch中 filebeat–&gt;kafka集群–&gt;logstash–&gt;(file server文件系统|kafka集群|ES集群) Filebeat 日志收集 kafka 日志接受消息队列 logstash 将日志进行过滤分析后存储到ES ES 存储，检索，分析 FilebeatFilebeat配置文件： filebeat.prospectors: - input_type: log paths: - /home/appdeploy/deploy/logs/pinpoint/*-pinpoint.log document_type: tools.pinpoint.data scan_frequency: 5s tail_files: true output.kafka: hosts: [&quot;10.10.10.92:9092&quot;, &quot;10.10.10.93:9092&quot;, &quot;10.10.10.94:9092&quot;] topic: &apos;%{[type]}&apos; partition.round_robin: reachable_only: false required_acks: 1 compression: gzip max_message_bytes: 1000000 codec.format: string: &apos;%{[message]}&apos; 说明 filebeat.prospectors: - input_type: log #类型选择log|stdin中的log，在filebeat最新版本中input_type写成type paths: - /home/appdeploy/deploy/logs/pinpoint/*-pinpoint.log #指定探测的文件 document_type: tools.pinpoint.data scan_frequency: 5s #prospector每隔5秒去检测日志的生成情况 tail_files: true #设置之后，filebeat读取新文件时从文件末尾开始读取，而不是开头，如果设置了日志轮转，那么新文件的第一行将会被忽略 output.kafka: #输出策略采用kafka hosts: [&quot;10.10.10.92:9092&quot;, &quot;10.10.10.93:9092&quot;, &quot;10.10.10.94:9092&quot;] #kafka集群的配置信息 topic: &apos;%{[type]}&apos; #设置topic使用文件类型 partition.round_robin: #Topic的分区算法 reachable_only: false # 如果设置为true，那么event将只会推送到leader上，默认设置就是false required_acks: 1 #ACk可靠性级别，0表示不响应，1表示本地commit，-1表示所有的副本commit。默认为1 compression: gzip # 设置输出压缩编解码器，可选snappy and gzip。默认就是gzip max_message_bytes: 1000000 # JSON格式的信息一个传输允许的最大大小上限 codec.format: string: &apos;%{[message]}&apos; Kafka在这里，kafka的作用是将日志接受到消息队列中，以备后续的logstash收取 kafka的配置【每台的配置除了id不同之外，其他均类似】 [root@qa-bigdata002 config]# egrep -v ‘^#|^$’ server.properties broker.id=0 port=9092 host.name=172.24.80.87 num.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 log.dirs=/home/kafka/kafka-logs num.partitions=2 num.recovery.threads.per.data.dir=1 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 log.cleaner.enable=false zookeeper.connect=172.24.80.87:2181,172.24.80.88:2181,172.24.80.89:2181 zookeeper.connection.timeout.ms=600000 说明 broker.id=0 # The id of the broker. This must be set to a unique integer for each broker，在一个kafka集群中，每个broker的ID必须不同 port=9092 host.name=172.24.80.87 num.network.threads=3 #The number of threads handling network requests num.io.threads=8 # The number of threads doing disk I/O，将数据落地到磁盘的线程数量，一般为CPU核数的倍数 socket.send.buffer.bytes=102400 #发送缓冲区的大小，单位是字节，这里是1M socket.receive.buffer.bytes=102400 #接受缓冲区大小，这里是1M socket.request.max.bytes=104857600 #最大接受的请求数量，防止OOM的出现，out of memory,这里设置为104M log.dirs=/home/kafka/kafka-logs # 落地到磁盘的文件的存储路径 num.partitions=2 # 每个Topic的分区数量 num.recovery.threads.per.data.dir=1 #在日志数据恢复时， log.retention.hours=168 #日志保留小时数，这里的168小时，也就是保留7天。 log.segment.bytes=1073741824 #单个日志的文件的最大大小，现在配置为1G log.retention.check.interval.ms=300000 #每隔5分钟检测日志文件是否可以被删除 log.cleaner.enable=false #设置flase之后，日志保留策略将会采用上面的分段及超时设置，如果为true， zookeeper.connect=172.24.80.87:2181,172.24.80.88:2181,172.24.80.89:2181 #ZK的配置 zookeeper.connection.timeout.ms=600000 #连接ZK的超时时间 consumer.properties # Zookeeper connection string # comma separated host:port pairs, each corresponding to a zk # server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot; zookeeper.connect=172.24.80.87:2181,172.24.80.88:2181,172.24.80.89:2181 # timeout in ms for connecting to zookeeper zookeeper.connection.timeout.ms=6000 #consumer group id group.id=test-consumer-group producer.properties # list of brokers used for bootstrapping knowledge about the rest of the cluster # format: host1:port1,host2:port2 ... metadata.broker.list=172.24.80.87:9092,172.24.80.88:9092,172.24.80.89:9092 zookeeper.properties # the directory where the snapshot is stored. dataDir=/tmp/zookeeper # the port at which the clients will connect clientPort=2181 # disable the per-ip limit on the number of connections since this is a non-production config maxClientCnxns=0 LogstashElasticsearchES模板设置通过使用ES模板，可以有效的减轻存储压力ES模板：通过对索引中的每个字段做事先的预定义数据类型（例如ID，name等分别使用存储空间最小的数据类型）]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>ELk日志处理平台</category>
        <category>Filebeat+kafka+logstash+Elasticsearch</category>
      </categories>
      <tags>
        <tag>ELK架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异地多活机房建设]]></title>
    <url>%2F2018%2F04%2F17%2F%E5%BC%82%E5%9C%B0%E5%A4%9A%E6%B4%BB%E6%9C%BA%E6%88%BF%E5%BB%BA%E8%AE%BE%2F</url>
    <content type="text"><![CDATA[异地多活参考文献 DTS使用场景 1. 前言概述随着业务的快速发展，对于很多公司来说，构建于单地域的技术体系架构，会面临诸如下面的多种问题： (1) 由于单地域底层基础设施的有限性限制了业务的可扩展性，例如城市供电能力，网络带宽建设能力等。 (2) 出现城市级别的故障灾害时，无法保证服务的可持续性，服务难以实现高可用。 (3) 用户分布比较广的业务，远距离访问延迟高，严重影响用户体验。 为解决企业遇到的这些问题，用户可以选择构建异地多活架构，在同城/异地构建多个单元(业务中心)。根据业务的某个维度将业务流量切分到各个单元(例如：电商的买家维度)。各个业务单元可以分布在不同的地域，从而有效解决了单地域部署带来的基础设施的扩展限制问题。 各个单元之间的数据层通过DTS的双向同步进行全局同步，保证全局数据一致。当任何一个单元出现故障时，只要将这个单元的流量切到其他单元即可在完成业务的秒级恢复，从而有效保证了服务的可持续性。 异地多活架构的单元可以根据用户分布选择部署区域，业务上可以按照用户所属区域划分单元流量，实现用户就近访问，避免远距离访问，降低访问延迟，提升用户访问体验。 当前公司的核心应用都是运行在阿里云上，这种方式存在以下几个缺点： 星型拓扑结构，所有压力都集中在一套系统之上，整体系统的高可用性还不够充分。新增IDC机房之后，可以将一部分流量引入到就近的云下机房。为核心系统减负，也就是说，同一个应用对应有多个生产环境。 的 1.1 思路整体思路： 为什么？ 是什么？ 怎么做？ 1.2 为什么？为什么需要异地机房？1.3 是什么？ 异地机房应该是什么样子，能实现什么功能，对当前架构有什么影响？1.4 怎么做？ 如何实现？这部分内容将会在下面的章节进行具体的阐述。 2. 具体实施]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维架构</category>
        <category>异地多活</category>
      </categories>
      <tags>
        <tag>异地多活机房建设</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用软件激活密钥]]></title>
    <url>%2F2018%2F04%2F16%2F%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E6%BF%80%E6%B4%BB%E5%AF%86%E9%92%A5%2F</url>
    <content type="text"><![CDATA[vmware workstations 14 pro CG54H-D8D0H-H8DHY-C6X7X-N2KG6 【亲测可用】 ZC3WK-AFXEK-488JP-A7MQX-XL8YF AC5XK-0ZD4H-088HP-9NQZV-ZG2R4 ZC5XK-A6E0M-080XQ-04ZZG-YF08D ZY5H0-D3Y8K-M89EZ-AYPEG-MYUA8]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>常用软件激活密钥</category>
      </categories>
      <tags>
        <tag>软件激活密钥</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令之curl命令]]></title>
    <url>%2F2018%2F04%2F16%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%B9%8Bcurl%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>Linux基础知识</category>
        <category>Linux常用命令</category>
      </categories>
      <tags>
        <tag>curl命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IT常用英语记录]]></title>
    <url>%2F2018%2F04%2F16%2FIT%E5%B8%B8%E7%94%A8%E8%8B%B1%E8%AF%AD%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[术语对照表 单词短语 释义 CURD CRUD (创建：Create， 读取：Read，更新：Update，删除： Delete) 是对于存储的信息可以进行操作的同义词。是一个对四种操作持久化信息的基本操作的助记符。CRUD 通常是指适用于存于数据库或数据存储器上的信息的操作，不过也可以应用在高层级的应用操作，例如通过在设置状态字段并标记删除的而并非移除数据的伪删除。 SDK RDS Remote Data Services。远程数据服务。云数据库 ECS 云服务器 ECS（Elastic Compute Service）是一种弹性可伸缩的计算服务 VPC 专有网络VPC（Virtual Private Cloud）是基于阿里云构建的一个隔离的网络环境，专有网络之间逻辑上彻底隔离。 PM Product Manager，产品经理 RD Research and Development 研究与开发 QA Qualtiy Assurance 品质保证。QA的主要职责就是质量保证工作。 OP Operator，操作员，管理员。 href hypertext reference 超文本连接 DML 数据变更是指DML(包含insert/delete/update) DDL 结构变更是指DDL(例如：create/drop/alter table) pki 公钥基础设施（Public Key Infrastructure） 单词对照表 英语 释义 side effect 副作用 properties 性能，属性，性质，特性，财产 involves 包含，牵涉 at-least-once 至少一次 deprecated 弃用，废弃，不赞成的 shipper 托运人；发货人；货主 prospectors 勘探者；探矿者 harvesters 收割机；收获者 layout 布局；设计；安排；陈列 keystore 密钥库;文件;密码;签名文件 permitted 被允许的；允许 individual 个人的；个别的；独特的；个体 seamlessly 无缝地 compatibility 兼容性 capabilities 能力；权限；功能；责任 migration 迁移；移民；移动 plural 复数 retrieve 检索，重新得到 Throughout 自始至终，到处；全部；贯穿，遍及 represent 代表；表现；描绘；回忆；再赠送 high-performance 高性能的；高效能的 objective 客观 Subjective 主观 roster 花名册；执勤人员表；逐项登记表 parse 解析；从语法上分析 encountered 遇到；曾遭遇 scalability 可扩展性]]></content>
      <categories>
        <category>IT基础知识</category>
        <category>IT英语</category>
      </categories>
      <tags>
        <tag>IT常用英语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jumpserver安装部署及使用]]></title>
    <url>%2F2018%2F04%2F13%2Fjumpserver%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. 安装部署【注意：务必保证版本及操作一致】 参考链接：一步一步安装文档 1.1 环境准备1.1.1 安装依赖关系yum -y install wget sqlite-devel xz gcc automake zlib-devel openssl-devel epel-release git libffi-devel python-devel 注意：python-deve需要安装对应的版本，我这里安装的是python36-devel 1.1.2 建立python虚拟环境使用原因：因为 CentOS 6/7 自带的是 Python2，而 Yum 等工具依赖原来的 Python，为了不扰乱原来的环境我们来使用 Python 虚拟环境 如果服务器上没有python3.6.1+环境，则需要手动安装 $ wget https://www.python.org/ftp/python/3.6.1/Python-3.6.1.tar.xz $ tar xvf Python-3.6.1.tar.xz &amp;&amp; cd Python-3.6.1 $ ./configure &amp;&amp; make &amp;&amp; make install $ cd /opt $ python3 -m venv py3 $ source /opt/py3/bin/activate 看到下面的提示符代表成功，以后运行 Jumpserver 都要先运行以上 source 命令，以下所有命令均在该虚拟环境中运行 (py3) [root@localhost py3] 在源码安装python3时可能会出现报错，关键字：“ake: * [Objects/unicodeobject.o] Error 4”。这个时候，修改Makefile文件，把‘-DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes’中的‘03’改为‘02’，然后再重新编译安装即可。 1.2 安装启动 jumpserver1.2.1 下载jumpservercd /opt git clone --depth=1 https://github.com/jumpserver/jumpserver.git &amp;&amp; cd jumpserver &amp;&amp; git checkout master 注意：不要安装在/root、/home 等目录下，以免权限问题 1.2.2 安装RPM依赖包cd requirements yum -y install $(cat rpm_requirements.txt) 1.2.3 安装python库依赖pip install -r requirements.txt # 不要指定-i参数，因为镜像上可能没有最新的包，如果没有任何报错请继续 1.2.4 安装 Redis, Jumpserver 使用 Redis 做 cache 和 celery broke$ yum -y install redis $ service redis start 1.2.5 安装配置MySQLcreate database jumpserver default charset &apos;utf8&apos;; grant all on jumpserver.* to &apos;jumpserver&apos;@&apos;127.0.0.1&apos; identified by &apos;Jumpserver_password_123&apos;; 1.2.6 修改jumpserver配置文件$ cd /opt/jumpserver $ cp config_example.py config.py $ vi config.py # 我们计划修改 DevelopmentConfig中的配置，因为默认jumpserver是使用该配置，它继承自Config 注意: 配置文件是 Python 格式，不要用 TAB，而要用空格 在该文件中新添加一个类 class DevelopmentConfig(Config): DEBUG = True DB_ENGINE = &apos;mysql&apos; DB_HOST = &apos;127.0.0.1&apos; DB_PORT = 3306 DB_USER = &apos;jumpserver&apos; DB_PASSWORD = &apos;somepassword&apos; DB_NAME = &apos;jumpserver&apos; config = DevelopmentConfig() # 确保使用的是刚才设置的配置文件，该行默认在文件末尾就存在。 1.2.7 生成数据库表结构和初始化结构$ cd /opt/jumpserver/utils $ bash make_migrations.sh 1.2.8 运行jumpserver$ cd /opt/jumpserver $ python3 run_server.py all 运行不报错，请浏览器访问 http://192.168.244.144:8080/ (这里只是 Jumpserver, 没有 Web Terminal，所以访问 Web Terminal 会报错) 账号: admin 密码: admin 2. jumpserver配置2.1 安装 SSH Server 和 WebSocket Server: Coco【此时还是在虚拟环境下】 $ cd /opt $ git clone https://github.com/jumpserver/coco.git &amp;&amp; cd coco &amp;&amp; git checkout master 2.2 安装 Web Terminal 前端: Luna3. 常用命令启动 jumpserver /opt/jumpserver/service.sh start 停止 jumpserver /opt/jumpserver/service.sh stop 重启 jumpserver /opt/jumpserver/service.sh restart 查看 jumpserver 状态 /opt/jumpserver/service.sh status 4. 配置优化/注意事项配置文件路径：/opt/jumpserver/ jumpserver.conf、 配置如下： \[base] url = access_url（安装时配置） key = o57ev5oc1nwe44r4 ip = 0.0.0.0 port = 8000 log = debug \[db] engine = mysql host = mysql_addr（安装时配置） port = 3306 user = jumpserver password = password（安装时配置） database = jumpserver \[mail] mail_enable = 1 email_host = smtp.163.com email_port = 25 email_host_user = name@163.com（安装时配置） email_host_password = password（安装时配置） email_use_tls = False email_use_ssl = False \[connect] nav_sort_by = ip 问题/总结日志压缩删除 访问服务器记录日志生成路径：/opt/jumpserver/logs/tty 需要定时压缩文件夹，并保留一段时间的历史日志 压缩文件夹命令： for i in `ls -d 201802*`; do tar czvf ${i}.tar.gz ${i}; done 5. 使用连接之后的终端页面如下所示： 添加用户并为之授权步骤1：web平台添加用户 步骤2：SSH登录跳板机，进入home目录下的对应用户名称目录下，编辑/home/wangping/.ssh/authorized_keys文件 步骤3：将事先生成好的公钥追加如该文件之中，授权完成。 添加备注信息控制台–&gt;资产管理–&gt;查看资产–&gt;输入主机名称点击搜索–&gt;编辑–&gt;页面末尾–&gt;备注（多个名称之间以/进行分割）–&gt;提交 jumpserver常见问题##用户推送失败## 问题 报错:”系统用户 ops 推送失败 [ redis037_cachecloud ], 推送成功 [ ] 进入系统用户详情，查看失败原因”如下图所示： 问题解决： 可能是在批量添加的时候，IP地址前面多加了一个空格]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维安全</category>
        <category>堡垒机</category>
      </categories>
      <tags>
        <tag>jumpserver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix结合Grafana实现炫酷动态可视化监控]]></title>
    <url>%2F2018%2F04%2F04%2FZabbix%E7%BB%93%E5%90%88Grafana%E5%AE%9E%E7%8E%B0%E7%82%AB%E9%85%B7%E5%8A%A8%E6%80%81%E5%8F%AF%E8%A7%86%E5%8C%96%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[来源：公众号-运维军团-《10分钟打造炫酷的监控大屏》 说明Grafana是一个开源的数据展示工具，是一个开箱即用的可视化工具，具有功能齐全的度量仪表盘和图形编辑器，有灵活丰富的图形化选项，可以混合多种风格，支持多个数据源，例如Graphite、InfluxDB、Mysql、Zabbix等等。虽然zabbix监控性能毋庸置疑，但zabbix图形显示过于简单、丑，因此利用zabbix作为数据源，结合Grafana作前端展示再好不过了。 重要的是Grafana的使用也超级简单，安装完成后登陆添加数据源即可，后面的事情就是添加图表等工作了。 实操未更新完，待续]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维监控体系</category>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>Grafana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ditto常用操作]]></title>
    <url>%2F2018%2F04%2F04%2Fditto%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[概述Ditto 是一款开源、免费、强大的剪贴板增强工具。可以把复制过的所有内容保存起来（可以设定保存日期或条目总数），快捷地供后续调用。还可以合并粘贴，纯文本粘贴，支持分组、置顶、快速搜索、热键粘贴功能。并且，还可以通过网络共享剪贴板内容。 主页：http://ditto-cp.sourceforge.net/ 教程：http://xbeta.info/ditto.htm 操作平常情况下，Ditto只是系统托盘中的图标。按下热键（默认` ctrl+``）后，会出现的粘贴主界面；再点击右键会弹出功能丰富的菜单。 详细请参看教程]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>Ditto</category>
      </categories>
      <tags>
        <tag>Ditto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fences桌面]]></title>
    <url>%2F2018%2F04%2F04%2Ffences%E6%A1%8C%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[下载安装目前的fences版本都是收费版本，可以下载免费版使用30天之后才酌情是否购买。官网 早期Fences免费版本连接：下载链接【该版本不是太好用】 使用fences的使用比较简单，这里介绍3点吧。 1、右键框选桌面空白处，即可出现桌面小区域。 这个应该大家都懂吧，基本功能。 2、桌面空白处双击鼠标左键，隐藏全部桌面图标 对桌面有洁癖的同学来说，这是个好福音。 3、设置好你的桌面区域后，可以锁定它们 当我们已经设置好桌面的区域后，可以将这些区域锁定，这样就无法再调整它们。 在桌面空白处右键鼠标，“查看”里选择“锁定fences”即可，这样你在桌面设置的这些fences小区域，就跟桌面“结合”在一起了，如果想要调整，取消勾选即可。]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>Fences</category>
      </categories>
      <tags>
        <tag>fences</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[listary常用操作]]></title>
    <url>%2F2018%2F04%2F04%2Flistary%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[参考资料官网 官网链接 网友文章 Listary——让文件在指尖流动 Listary Pro - 能极大幅度提高你 Windows 文件浏览与搜索速度效率的「超级神器」 实际操作常用快捷操作 ctrl敲2下 调出Listary窗口【可以在设置中设置第二种方式】 左键双击 在文件夹中双击会调出Listary的收藏，最近文档等。 在电脑页面输入e 即可定位到E盘，以此类推 目录页面输入名称一部分 定位到该子目录或者文件 ctrl+右键 针对选中内容进行动作【例如打开文件夹等】 智能匹配只要输入文件名的一部分就可以找到这个文件，支持中文与英文。 比如，我输入测试 md就可以搜索到测XX试OO.md这个文件。 自然，输入的越多，返回的结果越精确。随着使用记录的积累，常用的文件或程序会获得更高的优先级。 打开保存文件浏览对话框增强Ctrl+G 在打开框中切换到上一次打开的目录 Ctrl+O 直接打开上一次打开目录中的文件]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>Listary</category>
      </categories>
      <tags>
        <tag>Listary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简历内容应答]]></title>
    <url>%2F2018%2F04%2F04%2F%E7%AE%80%E5%8E%86%E5%86%85%E5%AE%B9%E5%BA%94%E7%AD%94%2F</url>
    <content type="text"><![CDATA[0. 自我介绍面试官 早上/下午好 我叫汪小华 大学就读于晋中学院 网络工程专业。目前一共有三年工作经验， 实习和第一份工作都是在亿阳信通在这期间从最基础的桌面运维干起，一直到最后独立接手负责了一个项目从0到1的这么一个整体过程，有很多的收获。这一期间对如何做好运维工作有了一些的感悟。 第二份工作是被内推到创世漫道，主要负责公司linux平台的调整，主要和我对接的是公司的架构师，因此在这个过程中对运维思想这方面有了很大的提升。 有关这两部分具体的内容我会在后面和您聊如何通过运维思想做好运维工作时谈及 我的优点是有一定的网络基础，平时喜欢问为什么，和同事交流谈论的时候喜欢拿纸笔写写画画。最强的技能部分应该是进程管理，这一点我觉得对运维工作来说，至关重要，关于这部分稍后我们可以一块交流探讨。 我的缺点目前是如何将所学知识有机的结合成为一个整体，构成一张知识之网这种能力还不够，这一点也是在后续的工作中需要去刻意修炼的。 以上是我的自我介绍，今天我要应聘的岗位是Linux运维工程师，谢谢！ 1. 发行版本区别及shell、python这些发行版本本质上没有什么区别，都是类unix系统。redhat的系统是免费的，但是他服务是收费的，并且有一些类似RHCS等服务只有收费版才支持。而centos是redhat社区版本，国内使用较多，社区相当活跃。SUSE也是分为两种，一种企业版本的SLES，一种是opensuse，sles主要是一些金融国企在使用，安全性较好，opensuse是社区版本，suse使用起来和centos相差不多，软件包形式使用源自redhat的rpm，但是管理工具使用的是zypper。ubuntu源自于debain，国外使用的较多，也是相当热门的一个发型版本，在桌面领域有绝对技术优势，适合开发人员使用。 **有关shell的问题，做面试题，看abs【每天看一点】。** python目前正在学习，目前基础部分已经学完了，正在学习django项目，学完之后，要花钱买一套马哥或者老男孩的python视频来补充，预计下半年能够做项目。 2. OSI模型、TCP/IP部分；路由交换基本原理OSI7层模型OSI 7层模式主要是由国际标准化组织（ISO）创建的，是一个国际通用的标准，它被开发出作为一个参照标准，用于指导如何设计网络通信系统。说的简单一点就是统一网络设备商的协议标准，实现不同网络设备和谐共存的环境（主机，路由器，交换机等等都是网络设置，都要遵循同一套的通信标准）它一共分为7层，每一层在网络通信数据传输过程中都定义了不同的功能。 OSI7层模型主要分为(从下到上)：物理层，数据链路层，网络层，传输层，会话层，表示层，应用层。每一层说明： layer function Application data flow；离用户最近的一层，它使一个网络应用能和另一个网络应用相互通信 Presentation 定义数据格式；数据压缩、加解密等。 Session 定义如何建立和终止连接，是告诉4层怎么做，4层只管被动的接受然后去做。 Transport 数据段；将上层的数据流进行分段；建立和终止网络连接；常用于流量控制和数据恢复 Network 数据包；使用IP地址在Internet上唯一确定一台设备；定义设备路由，寻址 Data link 数据帧；将上层数据封装成数据帧，其中包含源目MAC地址以及帧校验字段（用于检测传输错误）；它包含2个子层（LLC和MAC） Physical 比特流；定义了比特流如何在两台设备之间流通；主要涉及线缆，网卡， 下面是每一层常见的对应协议 layer protocol Application HTTP,FTP,Telnet,SMTP,SNMP Presentation MIME,TIFF,GIF,JPEG,PICT,ASCII,EBCDIC,encryption,MPEG,MIDI,HTML Session SSl/TLS,NetBIOS,RPC Transport TCP,UDP Network IP,ICMP,ARP,RARP Data link PPP,HDLC,IEEE 802.3/802.2,FDDI,ATM,IEEE 802.5/802.2 Physical Ethernet TCP/IP协议族TCP/IP模型类似OSI模型，作用也是描述一套指导标准，实现网络设备之间的通信，它被设计成4层 Application Transport Internet Network Access 其对应关系为： TCP/IP model OSI model Application Application Presentation Session Transport Transport Internet Network Network Access Data link Physical OSI和TCP/IP的区别除了层数的区别之外，它们之间最大的区别就是： OSI模型规定了在一个网络上传输数据所需要的步骤，并且它是非常具体的，定义了每一层使用什么协议以及如何使用；而TCP/IP模型是不特定的。另外一个区别就是，目前TCP/IP是所有网络设备上既定的协议事实，一般分析问题使用OSI模型。 路由技术我们在这里说的路由技术一般是指，路由转发。主要涉及设备为路由器或者三层交换机。这些设备上会维护一张路由表，其中的信息可以是通过动态路由协议（例如OSPF，EIGRP，ISIS，RIP，BGP，静态路由，默认路由等）获取组成路由表的内容是：出口接口和对应网段 路由设备接收到一个数据包之后，会解封装，获取其中的目的IP地址信息（网段信息），然后查找路由表，选择最优路由去转发。路由设备上也会有一张ARP表，根据广播域 交换技术在一个局域网内，也就是一个广播域内使用的技术。通常会涉及到的设备就是交换机。交换机上会维护一张MAC地址转发表，其中的信息是MAC地址和端口的映射关系。交换机根据数据帧中的目的MAC地址进行数据包的转发 注意：在一个广播域内的数据流动是依靠二层MAC来实现的，因为在第一次会涉及到ARP，有了记录之后，交换机会记录他的MAC地址表，后续的速度就会较快 这个时候可以在白板上进行讲解，大致的讲解百度页面打开的整个过程。 3. 高可用+负载均衡keepalived 2种角色：master和backup； 4种状态：stop,master,backup,fault 检测脚本2种触发机制 当VRRP检测脚本检测到自身所承载应用的返回值不为0的时候，就会触发角色变化，这个时候，VRRP脚本中如果没有设置weight权重值，那么直接进入fault状态，在vrrp组中发送组播通告，宣告自己进入异常状态，让出master角色并且不参与竞选如果脚本中设置了weight权重值，这个时候又会分为两种情况。 当weight权重值大于0时，master的优先级不变，backup的优先级为weight+现在优先级在wight权重值小于0时，master的优先级为目前的优先级减去weight的绝对值，backup的优先级保持不变。经过我多次的实验，目前保证最佳切换效果的配置是Vrrp检测脚本组中不配置weight，并且所有主机都设置为backup，设置不抢占参数，这种情况下，能有效避免优先级设置不当导致的切换不成功。 Nginx Nginx工作在应用层（使用location，通过正则表达表达式进行相关匹配），负载均衡是基于upstream模块实现的，因此配置比较简单。但是对后端服务器的健康检测只能支持端口Nginx的负载均衡算法可以分为两类：内置策略和扩展策略，内置的有轮询，ip_hash等。扩展的有fair，通用hash，一致性hash等。 Nginx的负载均衡目前支持5种调度算法： rr轮询【默认算法】；接受到请求之后，按照时间顺序逐一分配到后端不同的服务器上 wrr加权轮询；权重值越大，被分配访问的概率就越大，主要用于后端服务器性能不一致的情况 ip_hash；每个请求按访问IP的哈希结果分配，计算之后，nginx内部会维护一张哈希表，这样每个访客固定访问一个后端服务器，可以有效的解决动态网页存在的session共享问题。 fair;【第三方算法，需要通过额外安装upstream_fair模块实现】。更智能的一个负载均衡算法，此算法可以根据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。这种策略具有很强的自适应性，但是实际的网络环境往往不是那么简单，因此要慎用。 url_hash；【第三方算法，需要通过额外安装hash模块实现】。也是哈希算法，只不过不是基于源IP，而是基于访问的URL来生成这张哈希表。每个URL定向到同一台后端服务器，可以进一步提高后端缓存服务器的效率。 注意：当算法是ip_hash的时候，后端服务器不能被添加weight和backup 我们在location中配置nginx负载均衡的时候，还需要添加proxy_next_upstream http_500 http_502 error timeout invalid_header; 这一行参数。用于定义故障转移策略。当后端服务器节点返回500、502和执行超时等错误时，自动将请求转发到upstream负载均衡器中的另一台服务器，实现故障转移。 Nginx负载均衡工作流： 当客户端访问 xxx 域名时请求会最先到达负载均衡器,负载均衡器就会去读取自己server标签段中的配置 到location里面一看,原来这是一个要往后端web节点抛的请求 而后,nginx通过 proxy_pass的配置项 在自己的主配置文件找到了事先定义好的后端web节点 最后,按照事先设置好的调度算法,把请求带上主机头和客户端原始ip一起抛给后端准备好的web服务器 nginx负载均衡较适合用于日pv 2000W以下的站点 HAProxy Haproxy能实现基于4层和7层的负载均衡， HAproxy的8中负载均衡算法1、roundrobin表示简单的轮询，每个服务器根据权重轮流使用，在服务器的处理时间平均分配的情况下这是最流畅和公平的算法。该算法是动态的，对于实例启动慢的服务器权重会在运行中调整。 2、leastconn连接数最少的服务器优先接收连接。leastconn建议用于长会话服务，例如LDAP、SQL、TSE等，而不适合短会话协议。如HTTP.该算法是动态的，对于实例启动慢的服务器权重会在运行中调整。 3、static-rr每个服务器根据权重轮流使用，类似roundrobin，但它是静态的，意味着运行时修改权限是无效的。另外，它对服务器的数量没有限制。 该算法一般不用； 4、source对请求源IP地址进行哈希，用可用服务器的权重总数除以哈希值，根据结果进行分配。只要服务器正常，同一个客户端IP地址总是访问同一个服务器。如果哈希的结果随可用服务器数量而变化，那么客户端会定向到不同的服务器； 该算法一般用于不能插入cookie的Tcp模式。它还可以用于广域网上为拒绝使用会话cookie的客户端提供最有效的粘连； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 5、uri表示根据请求的URI左端（问号之前）进行哈希，用可用服务器的权重总数除以哈希值，根据结果进行分配。只要服务器正常，同一个URI地址总是访问同一个服务器。一般用于代理缓存和反病毒代理，以最大限度的提高缓存的命中率。该算法只能用于HTTP后端； 该算法一般用于后端是缓存服务器； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 6、url_param在HTTP GET请求的查询串中查找中指定的URL参数，基本上可以锁定使用特制的URL到特定的负载均衡器节点的要求； 该算法一般用于将同一个用户的信息发送到同一个后端服务器； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 7、hdr(name)在每个HTTP请求中查找HTTP头，HTTP头将被看作在每个HTTP请求，并针对特定的节点； 如果缺少头或者头没有任何值，则用roundrobin代替； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 8、rdp-cookie（name）为每个进来的TCP请求查询并哈希RDP cookie； 该机制用于退化的持久模式，可以使同一个用户或者同一个会话ID总是发送给同一台服务器。如果没有cookie，则使用roundrobin算法代替； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 2种配置方式指的是在1.3版本之前，ha的负载均衡配置主要是在listen部分中进行配置在1.3版本之后，为了更好的维护和管理，将负载均衡的配置拆分成为了frotend和backend这两部分，为了保证兼容性，listen部分依然保留，目前主要使用listen部分配置HA的监控页面 HA通过ACL实现一些7层的功能例如通过path_end的ACl方法实现动静资源分离 通过hdr_dom(host)和hdr_reg(host)和hdr_beg(host)的方法实现虚拟主机 LVS关于LVS，它本身只是支持负载均衡，没有检测机制，因此要结合keepalived来使用【keepalived的诞生原因就是为了给LVS提供后端节点检测功能，到后面才添加了高可用的功能】。在这里需要明确一点，它只能转发4层数据包【IP+port】但是检测是能通过7层url进行监测的。LVS的8种算法：1.轮叫调度（Round Robin）调度器通过“轮叫”调度算法将外部请求按顺序轮流分配到集群中的真实服务器上，它均等地对待每一台服务器，而不管服务器上实际的连接数和系统负载。大锅饭调度：rr - 纯轮询方式，比较垃圾。把每项请求按顺序在真正服务器中分派 2.加权轮叫（Weighted Round Robin）调度器通过“加权轮叫”调度算法根据真实服务器的不同处理能力来调度访问请求。这样可以保证处理能力强的服务器能处理更多的访问流量。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。带权重的大锅饭调度：wrr -带权重轮询方式。把每项请求按顺序在真正服务器中循环分派，但是给能力较大的服务器分派较多的作业。 3.最少链接（Least Connections）调度器通过“最少连接”调度算法动态地将网络请求调度到已建立的链接数最少的服务器上。如果集群系统的真实服务器具有相近的系统性能，采用“最小连接”调度算法可以较好地均衡负载。谁不干活就给谁分配：lc - 根据最小连接数分派 4.加权最少链接（Weighted Least Connections）在集群系统中的服务器性能差异较大的情况下，调度器采用“加权最少链接”调度算法优化负载均衡性能，具有较高权值的服务器将承受较大比例的活动连接负载。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。带权重的谁不干活就给谁分配：wlc - 带权重的。机器配置好的权重高 5.基于局部性的最少链接（Locality-Based Least Connections）“基于局部性的最少链接”调度算法是针对目标IP地址的负载均衡，目前主要用于Cache集群系统。该算法根据请求的目标IP地址找出该目标IP地址最近使用的服务器，若该服务器是可用的且没有超载，将请求发送到该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的工作负载，则用“最少链接” 的原则选出一个可用的服务器，将请求发送到该服务器。基于地区的最少连接调度：lblc - 缓存服务器集群。基于本地的最小连接。把请求传递到负载小的服务器上 6.带复制的基于局部性最少链接（Locality-Based Least Connections with Replication）“带复制的基于局部性最少链接”调度算法也是针对目标IP地址的负载均衡，目前主要用于Cache集群系统。它与LBLC算法的不同之处是它要维护从一个目标 IP地址到一组服务器的映射，而LBLC算法维护从一个目标IP地址到一台服务器的映射。该算法根据请求的目标IP地址找出该目标IP地址对应的服务器组，按“最小连接”原则从服务器组中选出一台服务器，若服务器没有超载，将请求发送到该服务器；若服务器超载，则按“最小连接”原则从这个集群中选出一台服务器，将该服务器加入到服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的程度。带有复制调度的基于地区的最少连接调度：lblcr - 带复制调度的缓存服务器集群。某页面缓存在服务器A上，被访问次数极高，而其他缓存服务器负载较低，监视是否访问同一页面，如果是访问同一页面则把请求分到其他服务器。 7.目标地址散列（Destination Hashing）“目标地址散列”调度算法根据请求的目标IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。目标散列调度：realserver中绑定两个ip。ld判断来者的ISP商，将其转到相应的IP。 8.源地址散列（Source Hashing）“源地址散列”调度算法根据请求的源IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。了解这些算法原理能够在特定的应用场合选择最适合的调度算法，从而尽可能地保持Real Server的最佳利用性。当然也可以自行开发算法，不过这已超出本文范围，请参考有关算法原理的资料。源散列调度：源地址散列。基于client地址的来源区分。（用的很少） 补充：为什么4层性能比7层更好？在7层，因为负载均衡器要获取报文内部的内容，因此要先和客户端建立连接，才能收到客户发过来的报文内容，然后获取报文内容之后，再根据调度算法进行负载。也就是说7层负载会和客户端和后端服务器分别建立一个TCP连接，而4层负载均衡只需要一次，因此性能肯定比4层差。 三种负载均衡产品之间的对比HAProxy和LVS的4层负载对比因为LVS是基于Linux内核的，但是HAProxy是属于第三方应用，因此在性能上，LVS占据绝对优势。因此，如果只是做纯4层转发，则使用LVS HAProxy对比NginxHAProxy支持更为丰富的后端节点检测机制，并且性能比Nginx好，因此在并发量较大的情况下，使用HAproxy，日PV并发量较小的情况下可以使用Nginx，配置也较为简单。 4. Redis持久化策略数据持久化策略主要分为RDB和AOF两种 RDB方式：数据文件内记录的是实际的数据。因此在进行数据恢复的时候，速度较快。适合全量备份。在进行RDB持久化时，会fork出一个单独的进行，因此会CPU的开销较大。 AOF方式：数据文件内记录的是产生数据变化的命令。因此在进行数据恢复的时候，速度较慢，并且其中的内容可以编辑，因此适合在执行了一些类似flushall或者flushdb等命令时进行数据恢复 混合持久化：Redis4.0版本之后的持久化，结合了RDB和AOF的有点，当进行AOF重写的时候，将会把当前的数据转变成为RDB形式进行保存，重写之后的数据继续以AOF的格式保存 主从复制在Redis2.6版本之前，主从复制时，每次传输的都是全量数据，因此会非常占用网络带宽和相关资源。在这之后，在Redis master节点上可以设置复制缓存区，来实现差异的增量复制。但是当缓冲区满了之后，还是会执行全量复制。 淘汰策略淘汰策略是指当Redis进行即将使用到设置的最大内存量，执行的一个策略，避免出现内存溢出的问题，也就是一种内存回收机制。一般在使用到maxmemory的90%时触发，默认策略是不回收。 在redis中可以配置的策略主要有以下几种： noeviction policy 【默认策略，永不过期策略。】不会删除任何数据，拒绝任何写入操作并返回客户端错误信息（error）OOM command not allowed when used memory，此时Redis只响应读操作 volatile-lru 根据LRU算法删除设置了超时属性（expire）的键，直到腾出足够空间为止，如果没有可以删除的键对象，则回退到noeviction策略 allkeys-lru 根据LRU算法删除键，不管数据有没有设置超时属性，直到腾出足够空间为止。 volatile-lfu 根据LFU算法删除设置了超时属性（expire）的键，直到腾出足够空间为止，如果没有可以删除的键对象，则回退到noeviction策略 allkeys-lfu 根据LFU算法删除键，不管数据有没有设置超时属性，直到腾出足够空间为止。 volatile-random 随机删除设置了超时属性（expire）的键，直到腾出足够的空间 allkeys-random 随机删除所有键，知道腾出足够空间为止 volatile-ttl 根据键值对象的ttl属性，删除最近将要过期的数据，如果没有，则回退到noeviction策略 常见性能问题 常见性能问题主要为： 内存设置不合理 大量的慢查询 key值（名称）设置过大 单个key的value过大 没有使用Redis的流水线功能 命令使用不合理，例如可以使用mset等或者禁止使用monitor等命令 客户端最大连接数设置【需要设置最大描述符，Redis默认会占用32个fd，因此可用的是1024-32】 TCP积压队列 定义AOF重写大小 客户端输出缓冲区 复制积压缓冲区 swap优化等等 哨兵模式 哨兵模式也就是Redis的高可用模式。一般的配置模式为一对主从，然后配置3个哨兵实例哨兵实例的设置原则：当有(n/2)+1个哨兵宣告需要进行切换时，才进行切换，这一点同样适用于zk等集群选举。因此最好3个以上的奇数个实例，偶数个会浪费一个。【这在5个以上节点时能看出明显的效果】 分布式集群 集群采用哈希槽的分配方式，一共有0-16383个槽最小建议配置为3主3从。Redis集群使用的是gossip协议。 cachecloud云平台 这是我从github上引入的Redis运维项目 5. Mysql+OracleMysql基础知识 Mysql主从复制原理 整体上来说，复制有3个步骤： A.master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events）； B.slave将master的binary log events拷贝到它的中继日志(relay log)； C.slave重做中继日志中的事件，将改变反映它自己的数据。 Mysql读写分离 Mysql高可用和集群有几种高可用方案：Mysql双主+keepalived【优点：架构简单，】 Mysql备份与恢复 逻辑备份：备份的是产生数据变化的sql语句。mysqldump能直接进行这个操作，但是因为它在备份过程中会锁表，并且备份的速度也非常的慢因此我们需要选择第三方工具。 物理备份：备份的是实际的数据，直接拷贝mysql的数据目录。 直接拷贝只适用于myisam类型的表。这种类型的表是与机器独立的。【这种备份的粒度较粗，不能实现更细粒度的数据恢复，特别是对于更新较为频繁的系统。】 实际生产环境中一般使用完整备份+增量备份每周日凌晨2点进行一次全量备份，之后的每天凌晨2点进行一次增量备份 然后再每天备份binlog日志【为了粒度更细致的数据恢复】 Mysql优化 mysql优化包括其他所有的网络服务优化，思路都是一致的。 分层次的来进行。【普遍规律+应用需求】 普遍规律： 首先是硬件层面 再次是操作系统层面的基础优化，例如文件描述符的数量，swap使用限制，文件系统（目前主流xfs） 再次是c/s架构方面的优化，例如TCP的连接队列大小，队列的缓存大小。tcp连接超时时间，tcp滑动窗口（发送和接受） 应用需求：数据库通用：最大连接数，索引（优先在where,order group等涉及的列上创建索引），sort，group等排序结果的缓冲区大小，慢查询，sql语句优化（减少使用like等开销大的语句），命令规范等Mysql：存储引擎 JAVA类：JVM设置，是否设置锁内存策略，堆内还是堆外内存，线程数量等。web类：压缩，静态文件缓存，CDN加速等 Mysql特殊：存储引擎等 Mysql常见问题 慢查询，sql写的有问题 mha的时候VIP漂移有问题 连接数问题 版本不一致问题 Oracle oracle没什么数据库概念上，oracle是只有一个数据库，然后里面有多用户，每个用户多表mysql是多个数据库，多个用户，采取授权的形式来访问 6. Ansible等自动化工具Ad-Hoc Ad-Hoc指的是一般性的临时操作 日常运维中主要使用的模块有： shell模块 yum模块 copy模块 service模块 PlaybookAnsible使用YAML语法描述配置文件，这个配置文件就被成为playbook，剧本 Ansible的核心理念是：极致的简单高效并且Ansbile是使用python编写的，因此在后续的二次开发上更占据优势。另一个趋势是python的运行方式，它和区块链一致，采用的是去中心化的部署方式，不需要安装客户端即可，通过SSH来实现，并且目前还提供了SSH的加速模式，适用于大规模的环境中，可以说，Ansible绝对是未来的趋势主流。 puppet、chef、slatstackpuppet和chef都是使用ruby编写的，并且配置繁琐，都需要配置客户端目前不适合 slatstack也是通过python编写，但是slatstack适用于更大的规模，因为ansible使用ssh来传输命令，而它使用zeroMQ来传输数据在1000台主机的情况下，MQ用时2秒左右，而ansible的SSH则用时85秒。 对比ansible默认情况下适用于200台以内的主机，适合中小型企业，如果数量再多可以使用Ansible的加速模式去实现 选型标准：选择最合适，如果当前的运维环境主机在百台，则ansible是最好的选择，如果上千台，那么无疑使用slatstack。 cobbler和kickstartkickstart是传统的批量装机方式，配置比较繁琐 cobbler是较早前的kickstart的升级版本，有点是容易配置 并且cobbler具有高级功能，可以根据不同机器的MAC地址来进行设置装机 关闭自动装机这里之前还发生过一个问题，就是有一次在装机的时候使用的是百兆交换机，导致老是有几台装不上，后来都换成千兆之后，就解决了这个问题。 关闭这个批量装机，因此centos的网卡名称不再是ethx的形式，因此在安装的时候，我们需要再ks文件中添加命令，来调整网卡的命令规则 7. Nginx，Httpd，tomcat，weblogic，php，gitlab，Jenkins这部分和web相关，主要是和电商，互联网公司等核心为web的紧密相关，也就是主要是LNMP这一套 Nginx基础知识基础知识 Nginx主要分为几个模块 全局配置【worker数量，worker的最大打开数量，CPU指定等】 Event模块配置【worker的最大连接数等，网络IO处理模型等】 Http模块【其中包括upstream段,server段,server中的location段等】 主要配置的地方就是HTTP模块中的upstream，server中的location段【动静分离等都是在这里进行配置】 注意：nginx的模块是静态的，在编译时就已经完全编译进去，而不是像Httpd是动态链接的形式 Nginx常见问题日志文件将磁盘存储空间占满了。 Nginx常见应用场景web服务器【一般会做动静分离，rewrite功能（重定向302是临时，301是永久，地址栏都改变，主要看爬虫变不变），防盗链】 负载均衡服务器 Nginx优化全局优化 工作进程数量（worker_processes数量）一般等于CPU的核数，因为每个进程是单线程的模式，使用epoll网络IO模型来进行处理。 worker_rlimit_nofile 60000；每个work进程最大打开文件数量。【这里需要跟操作系统的文件描述符相对应】 Event模块优化 worker进程最大连接优化，官方数据是能支持到5W【那么所有的连接数=5W*几个worker】 网络模型【通常使用epoll模型】 HTTP模块优化 不显示版本 关闭TCP延迟发送数据 keepalive的超时时间等 压缩传输的设置【压缩级别，压缩的触发大小】 Nginx和Httpd在这里主要说web，不说nginx的负载均衡，这部分已经在第3条说了。 tomcat常见问题**数据库连接问题，后端数据库异常，没有连接到tomcat乱码tomcat日志大小问题，权限问题JAVA_HOME没有设置正确 tomcat优化**主要分为2块，tomcat的JVM内存优化和tomcat的并发优化 内存优化：Tomcat内存优化主要是对 tomcat 启动参数优化，我们可以在 tomcat 的启动脚本 catalina.sh 中设置 java_OPTS 参数 JAVA_OPTS参数说明 -server 启用jdk 的 server 版； -Xms Java虚拟机初始化时的最小堆内存； -Xmx java虚拟机可使用的最大堆内存； 【堆内存建议设置一致，避免GC回收后再次动态分配，增大系统的开销】 -XX: PermSize 内存永久保留区域 -XX:MaxPermSize 内存最大永久保留区域 【这部分，默认64位的是256M】 JAVA_OPTS=’-Xms1024m -Xmx2048m -XX: PermSize=256M -XX:MaxNewSize=256m -XX:MaxPermSize=256m’ 并发优化/线程优化+缓存优化： 在Tomcat 配置文件 server.xml 中的 参数说明 maxThreads 客户请求最大线程数 表示最多同时处理多少个连接 minSpareThreads **Tomcat初始化时创建的 socket 线程数** maxSpareThreads **Tomcat连接器的最大空闲 socket 线程数 ** enableLookups 若设为true, 则支持域名解析，可把 ip 地址解析为主机名 redirectPort 在需要基于安全通道的场合，把客户请求转发到基于SSL 的 redirectPort 端口 acceptAccount 监听端口队列最大数，满了之后客户请求会被拒绝（不能小于maxSpareThreads ） connectionTimeout 连接超时 minProcessors 服务器创建时的最小处理线程数 maxProcessors 服务器同时最大处理线程数 URIEncoding URL统一编码 compression 打开压缩功能 compressionMinSize 启用压缩的输出内容大小，这里面默认为2KB compressableMimeType 压缩类型 connectionTimeout 定义建立客户连接超时的时间. 如果为 -1, 表示不限制建立客户连接的时间 参考配置： tomcat多实例部署http://blog.51cto.com/watchmen/1955972 传统方式复制目录的话，会造成资源浪费，因为lib和bin等公共资源会被多次加载，造成在内存中不必要的重复 思路：将bin下的文件和lib文件单独拆分出来 weblogicweblogic最开始bea公司的一个JAVA中间件产品，现在归属于oracle功能非常的强大，支持EJB比如在配置程序连接数据库时，不需要再代码中通过jdbc的方式去人工手动指定，而是通过后台管理页面的数据源配置中，进行配置。所以说，在一般的环境中，使用tomcat即可，如果涉及到大型的java应用开发，就要使用weblogic PHPPHP主要对接Nginx，处理php文件【通过php-fpm来处理】PHP-CGI 解释器每进程消耗 7 至 25 兆内存所以它的优化是进程数量的设置【包括启动时分配的，最小空闲的，最大空闲的，最大值】一般启动时分配5个，最小空闲为5个，最大空闲为32个，最大值为32个 GitlabJenkinsJDK支持tomcat支持maven支持Jenkins支持 Jenkins的安装一共有3个步骤 首先是下载war包到tomcat的webapps目录并将其重命名为ROOT.war，之后就是对其环境变量进行配置。 设定jenkins的目录及管理用户及编码修改tomcat目录下./conf/context.xml：增加jenkins环境变量 修改tomcat目录下的./conf/server.xml,是编码符合jenkins 步骤四：在第一次登陆jenkins页面时，需要输入一串加密数据这串数据位于其家目录下的./secrets/initialAdminPassword之中。 流程：JDK+tomcat部署Jenkins添加git 源码仓库使用maven进行构建【需要编写触发脚本，当有源码发生变化时，在2分钟后进行构建部署等操作】 8. 消息队列MQ产品使用MQ产品的原因 程序异步解耦 数据冗余 扩展性，不需要改变程序的代码，就可以扩展性能。 灵活性，峰值处理能力。 消息的顺序保证 异步通信，允许用户把消息放入队列中，但是并不立即处理它 ActiveMQ；老牌的MQ产品，完全遵守JMS规范。是apache开源的一个MQ产品，比较重量级，没有什么特殊的亮点 ActiveMQ的高可用集群模式通过ZK来实现，为了保证数据的一致性，因此会严重影响性能。从ActiveMQ 5.9开始，它实现了通过ZooKeeper + LevelDB实现高可用集群的部署方式。这种方式，对外只有Master提供服务这种方式实现了可以称之为半事务特性的机制，Master 将会存储并更新然后等待 (2-1)=1 个Slave存储和更新完成，才汇报 success RabbitMQ；遵循AMQP协议，借助erlang的特性在可靠性、稳定性和实时性上比别的MQ做得更好，非常重量级，性能比较好，适合企业级的开发。但是不利于做二次开发和维护 由于 rabbitmq 是使用 erlang 开发的，而 erlang 就是为分布式而生的。所以 rabbitmq 便于集群。rabbitmq 集群有两种模式：普通模式、镜像模式。 普通模式：也是默认模式，对于 queue 来说，消息实体只存在与其中的一个节点，A、B 两个节点只有相同的元数据，即队列的结构。当消息在A时，消费中从B中取消息时，消息会从A中传递到B中。所以consumer应尽量连接每一个节点，从中取消息。即对于同一个逻辑队列，要在多个节点建立物理Queue。否则无论consumer连rabbit01或rabbit02，出口总在rabbit01，会产生瓶颈。 镜像模式：镜像模式是 rabbitmq 的 HA 方案。其与普通模式的唯一不同之处在于消息会在 A，B 两个节点中同步。当然这种模式带来的副作用也是显而易见的。除了降低系统性能以外，如果队列数量过多，网络带宽将会受到影响。所以这种情况只运用到对高可靠性要求的场合上。 集群配置方式：安装erlang,然后同步三台机器上的.erlang.cookie文件内容因为RabbitMQ的集群是依赖erlang集群，而erlang集群是通过这个cookie进行通信认证的，因此我们做集群的第一步就是干cookie。注意：erlang.cookie文件中cookie值一致，且权限为owner只读。因此需要设置为600 注意： RabbitMQ单节点环境只允许是磁盘节点，防止重启RabbitMQ时丢失系统的配置信息。RabbitMQ集群环境至少要有一个磁盘节点，因为当节点加入或者离开集群时，必须要将该变更通知到至少一个磁盘节点。 kafka；也是apache基金会的一个MQ产品。高吞吐量，消息的接受和消费都是落地到磁盘，因此适用于大数据环境流处理，对实时性要求不是太高的环境，可以积压非常庞大的数据量（瓶颈在磁盘） kafka是一种分布式的，基于发布/订阅的消息系统。有主分区和副本分区的概念。并且kafka中的数据是追加的形式，保证了消息的有序性 rocketmq；阿里开发并开发的一个MQ产品，纯JAVA开发。具有高吞吐量、高可用性、适合大规模分布式系统应用的特点。RocketMQ思路起源于Kafka，但并不是Kafka的一个Copy，它对消息的可靠传输及事务性做了优化，目前在阿里集团被广泛应用于交易、充值、流计算、消息推送、日志流式处理、binglog分发等场景。 9. flume，zk，es，logstash，kibana系列flume flume：是一个日志收集软件。flume的agent设计实现这一系列的操作，一个agent就是一个java进程，运行在日志收集节点-也就是日志收集服务器节点。 agent里面包含3个核心的组件：source—-&gt;channel—–&gt;sink,类似生产者、仓库、消费者的架构。source:收集数据，可以处理各种类型sink：该组件是用于把数据发送到目的地的组件，目的地包括有：hdfs，kafka等等文件系统 工作流：flume的核心是把数据从数据源(source)收集过来，在将收集到的数据送到指定的目的地(sink)。为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel),待数据真正到达目的地(sink)后，flume在删除自己缓存的数据。 也就是说flume提供了一种类似事务机制。 flume的2种工作模式：主动模式和被动模式【主要是针对客户端来说】。这两种模式和zabbix的两种模式一样 在进行配置的时候，每个agent实例是通过别名来进行区分的。 kafka流式消息队列产品，接受flume发送过来的消息，或者日常产生端直接将JSON格式的数据发送到Kafka中。详见上方MQ产品 zookeeperzk:是一个分布式应用程序协调组件，用于为哪些原生没有提供集群功能的服务实现分布式集群。提供的功能包括：配置维护、域名服务、分布式同步、组服务等。zk的工作流：1、选举Leader。（选举zk集群中的leader）2、同步数据。3、选举Leader过程中算法有很多，但要达到的选举标准是一致的。4、Leader要具有最高的执行ID，类似root权限。5、集群中大多数的机器得到响应并接受选出的Leader。 注意：zk在3.5.0以上的版本会有一个内嵌的web服务，通过访问http://localhost:8080/commands来访问以上的命令列表。 一旦Zk集群启动之后，它将等待客户端的连接 esEs的主要功能是将收集的数据建立索引，方便日后数据的存储于检索。ES不止是一个全文本引擎，他还是一个分布式实时文档存储系统。这里，KCE的数据目的地和ES的数据来源设置成了一个分区，因此避免了磁盘IO的二次开销 logstash日志收集，需要在日志产生端配置，收集日志，再进行发送，目前使用flume来代替了。 kibanakibana不多说了，主要是提供了一个连接ES的入口 10. dockerdocker的核心三大组件是 镜像 容器 仓库 镜像主要分为几种，一个是官方的或者别人已经写好的镜像文件另一个可以自己产生镜像文件。 自己产生的镜像文件可以分为两种 在现有镜像的基础之上commit出来一个新的镜像 编写dockerfile文件，然后build出来一个镜像 建议通过dockerfile的形式产生镜像，因为使用commit出来的镜像会存在很多的缓存文件等。 容器是镜像的运行态，和程序及进程的概念比较像。 仓库主要分为两种，一个是存储镜像的仓库【里面的 镜像通过tag标签来尽心区分，默认是latest】，另一个是存储仓库名称的注册仓库 公网上的仓库可以是docker hub，也可以通过官方提供的registry镜像来简单搭建一套本地私有仓库环境: dockerfile编写dockerfile主要分为4个部分 基础镜像信息 from字段，也就是这个应用是以那个镜像为基础的 维护者信息，maintainer，也就是作者信息 镜像的操作指令，也就是在制作镜像是要执行的一系列操作，add加入一系列的文件，例如JDK，war包等 容器启动时执行指令-CMD，在启动时要执行的操作，例如启动项目等 在cachecloud中，基础镜像是使用的centos7.4-内核基础3.0-1811系统维护者是我，镜像的操作指令是JDK环境等等；容器启动时执行的命令是启动cachecloud项目 k8s k8s是谷歌开源的一个容器集群管理项目k8s对集群中的资源进行了不同级别的抽象，每个资源都是一个rest对象，通过API进行操作，通过JSON/YAML格式的模板文件进行定义要注意的是，k8s并不是直接对容器操作，它的操作最小单位是容器组。容器组由一个或多个容器组成。k8s围绕着容器组进行创建，调度，停止等生命周期管理。 ESXI,vsphere,xen,kvm 这些是第一家公司所使用的产品exsi和vsphere是vmware公司的企业虚拟化产品，相比于kvm，它有更好的性能，因此它是直接在物理上安装虚拟化操作系统，不需要第三方软件的实现。esxi是单机版本，vsphere是集中管理版本，支持在线迁移等高级功能。xenserver是思杰公司的一个虚拟化产品，单机的操作比vmware的esxi好，但是在涉及到多机环境时不是太好kvm需要linux系统的支持，然后还要安装一系列的组件，相对来说，更方便，但是不够专业，一般企业使用的相对较少。 11. 监控软件及JMX，JVMzabbix 我们的生产是怎么监控的 首先是监控模板，监控一些基础指标，例如CPU，内存，磁盘等 一些类似HAproxy，activemq等有web页面的应用我们通过web监控来实现【创建web场景，60秒内，尝试连接3次，如果3次都失败，则报警，这里还会涉及到一些有认证的页面，也是可以实现的。】 更高级一点的例如redis等应用，需要监控一些特定的指标，我们通过自定义监控项来实现。 JAVA类的应用，在后期慢慢的开放了JMX端口的情况下，陆续加入了JMX的监控。 自定义监控项为了简单高效，我们自己编写的脚本，判断引用的状态，将采用所能想到的一切来判断，然后再最后只输出一个0,如果服务不正常的话，则输出为1。 zabbix的一些优化操作采取zabbix的主动模式来进行监控使用自动发现的功能。 自动发现等操作 各监控产品的区别zabbix是一款商业的开源软件，涉及到的东西非常之多，因此官方能够靠咨询，技术服务等来收费运作。而cacti，nagios等是普通的开源软件，自然没有zabbix这么强大。 nagios的可视化功能非常弱，zabbix是有自己的可视化界面的【一般我们都是通过最新数据哪里查看，为了给zabbix减负，不是非必要的情况下，一般不会给监控项添加图形】它不支持自动发现，并且缺少图形展示工具，也没有历史数据，追查起来非常困难。 cacti是一个PHP程序它通过使用SNMP 协议获取远端网络设备和相关信息，（其实就是使用Net-SNMP 软件包的snmpget 和snmpwalk 命令获取）并通过RRDTOOL 工具绘图， 通过SNMP采集数据，并且自定义监控项等非常繁琐，报警方式需要添加插件等。 JMX监控 前提条件：需要JAVA类程序开放JMX端口【也就是开放API接口】 工作流：（1）zabbix_server需要知道一台主机上的特定端口的JMX值时，它会向Zabbix-Java-gateway进程去询问。这个连接进程叫做StartJavaPollers （2）Zabbix-Java-gateway使用JMXmanagementAPI这个API去查询特定的应用程序 注意：在配置的时候，StartJavaPollers线程数量要小于等于START_POLLERS设置的线程数量 这些操作操作完毕之后，在web页面上进行操作，添加JMX监控模板即可。 JVM调优 提到虚拟机的内存结构，可能首先想起来的就是堆栈。对象分配到堆上，栈上用来分配对象的引用以及一些基本数据类型相关的值。 JAVA虚拟机的内存结构是分了好几个区域的。分区域的好处是： 便于查找 便于内存回收【如果不分，回收内存就要全部内存扫描】 JVM内存分区（5部分）： 方法区 线程共享【这部分常被成为永久代，除了编译后的字节码之外，方法区中还会存放常量，静态变量以及及时编译器编译后的代码等数据。】 堆 线程共享【这部分一般是Java虚拟机中最大的一块内存区域，这块存储对象的实例。堆内存是垃圾收集器主要光顾的区域，一般来讲根据使用的垃圾收集器的不同，堆中还会划分为一些区域，比如新生代和老年代。新生代还可以再划分为Eden，Survivor等区域。另外为了性能和安全性的角度，在堆中还会为线程划分单独的区域，称之为线程分配缓冲区。更细致的划分是为了让垃圾收集器能够更高效的工作，提高垃圾收集的效率。】 Java栈 线程独享【每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。在Java虚拟机规范中，对于此区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展，当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。】 本地方法栈 线程独享【本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。】 程序计数器 线程独享【这部分内存不会内存溢出，字节码行号提示器】 堆（新生代和老年代等）： Xms256m 代表堆内存初始值为256MB Xmx1024m 代表堆内存最大值为1024MB如果-Xmx不指定或者指定偏小，应用可能会导致java.lang.OutOfMemory错误 方法区（永久代） PermSize和MaxPermSize指明虚拟机为java永久生成对象（Permanate generation）例如：class对象、方法对象这些可反射（reflective）对象分配内存限制，这些内存不包括在Heap（堆内存）区之中。-XX:PermSize=64MB 最小尺寸，初始分配XX:MaxPermSize=256MB 最大允许分配尺寸，按需分配这部分设置过小会导致：java.lang.OutOfMemoryError: PermGen space MaxPermSize缺省值和-server -client选项相关。-server选项下默认MaxPermSize为64m。 -client选项下默认MaxPermSize为32m 设置-Xms、-Xmx 相等以避免在每次GC 后调整堆的大小 在jdk1.8之前之前我们将储存类信息、常量、静态变量的方法区称为持久代(Permanent Generation)，PermSize和MaxPermSize是设置持久代大小的参数，在jdk1.8中持久代被完全移除了，所以这两个参数也被移除了，多了一个元数据区(Metadata Space)，所以设置元数据区大小的参数也变成对应的MetaspaceSize和MaxMetaspaceSize了 -XX:PermSize和-XX:MaxPermSize在jdk1.8中使用-XX:MetaspaceSize和-XX:MaxMetaspaceSize替代了现在能兼容正常启动，没有产生影响，知悉。 云产品说起阿里云，这期间还发生了一个人为事故。当初京东金融本来是通过我们的平台发送的，但是它要我们给他拉专线直接连接运营商。但是这边没有给他拉，而是买了一台阿里云服务器，暴露出一个公网IP地址让它连接，在这台服务器上面部署HAproxy，还是调整到我们的平台。【上边领导们的决定，我就不评论是非对错了 哈哈】 然后有一天，突然HAProxy的web监测报警，页面打不开。马上上服务器看，CPU爆了【买的服务器配置一般】检查进程。内存正常，磁盘正常，CPU爆了，然后再查看网络连接，发现有大量的CLOSE_WAIT（400个close_wait;100多个establish） 在TCP关闭时，主动关闭的一方发出 FIN 包，被动关闭的一方响应 ACK 包，此时，被动关闭的一方就进入了 CLOSE_WAIT 状态。如果一切正常，稍后被动关闭的一方也会发出 FIN 包，然后迁移到 LAST_ACK 状态。 导致产生大量close_wait的原因是突然遭遇大量的请求，即便响应速度不慢，但是也来不及消费，导致多余的请求还在队列里就被对方关闭了。（因为对方设置了超时时间）。但是linux没有对close_wait做类似超时控制的设置，如果不重启进程，这个状态很可能会永远的持续下去， AWS主要是当初想搭VPN，但是一大堆的限制，最终没成功，所以现在是直接买的商业的，稳定，速度也有保证。七牛云，产品主要是数据存储和CDN加速，我自己的博客目前也是在用七牛云。瑞江云，是公司在做什么业务时和人家合作时，人家送的，具体什么我就不知道了 13 自身素质关于这三个人的管理经验，是在第一家公司公司的时候。亿阳分为很多个部门，其中就有一个对外产品部门，当时是准备和人保合作，进入金融行业。因此拿下了一个标，但是招运维主管的时候的比较难招，差不不行，好的知道是外包驻场的形式一般也不愿意来，到最后实在没办法只能从公司内部要人了，然后就把我派过去了。在那边呆了有7个月左右。当时工作非常艰辛【上一家被换掉是因为政治原因，具体是谁就不知道了】，因此过去需要接受上一家的工作，然后开发二代新产品，中间不能停，也就是起承上启下的作用。当时1个月直接就瘦了10斤，天天加班。 高效办公系列软件TC:资源管理器Autohotey：热键管理器Listary：文件搜索浏览增强工具evernote:云笔记Fences:桌面管理工具Ditto：剪切板增强工具Snipaste：截图工具Everything:文件搜索工具 运维职业规划-如何通过运维思想做好运维工作运维思想-运维核心稳定性-网站/平台不宕机【这是运维的核心】一般通过以下方式来实现 架构使用集群+负载均衡+高可用+应用解耦，微服务等部署方式来保证性能 安全【】 运营推广不能在白天高峰期推广，需要和运维打招呼 前端图片的优化，不能使用大图等，尽量使用缩略图 数据库优化【加入Redis数据缓存层，sql语句优化等】 避免随时上线的操作【减少次数】 测试生产等环境保持一致【系统，软件版本，路径等等】 流程操作【运维标准和流程】 等等等等 数据不丢失 应用配置数据管理-【考虑使用CMDB等平台】 数据库数据 避免人为问题避免人为错误，主要分为两个方面， 一个是他人(主要是开发不严谨)产生的错误，这部分通过运维流程并结合工具控制。【比如测试不严谨，或者开发人员的代码有问题，直接把服务器资源跑没了】 一个是自己操作产生的问题，这部分通过一些智能化的自动化工具来尽量避免【避免在执行命令的时候误操作等等，常见的有】 解决：建立完善的流程制度，对运维来说，包括标准化，对开发测试来说，包括上线的流程化【通过运维制度和一些工具来实现】 提升运维效率这一点是放在最后的，是在上面都做好的前提下，然后再有这么一层，什么自动化，CICD，devops，不是说招几个运维开发就能解决的。一定是需要一个过程的。运维效率很重要，但是不能盲目的只盯住这个上面 个人如何做好运维工作主动性很多东西如果不主动去找系统负责人去推进，进度根本没法完成。 划重点的能力写文档，研究技术，培训讲解等，需要将其中最重要的东西给讲述出来。就比如在看书的时候，有时候一些大部头的书，可能一句话非常长，你要从中快速挑出这句话的重点。然后建立知识体系。【这就需要能快速的找出重点，快速浏览说明性的内容，因为有可能这些说明性内容对你目前的水平来说完全来说可以忽略】 全局观 比如像我当初对接那么多的系统，在出问题的时候，可能是后面某一个系统出问题，但是导致你直接无法使用，所以你需要根据症状， 态度某一项技术不会非常正常，要摆正心态，虚心向人学习，比如像开发学习，像DBA学习，等等。构建完善的知识体系。一个技术不会到会其实有时候就是一个月的事，根本没有大家想的那么恐怖，不要怕，大胆的去问。 换位思考，自身作则 上面的任务怎么说话去分派下去。怎么安排任务， 流程制度 流程化，制度化为了便于管理，减少出错的概率。因此要有流程和制度 新员工刚进来，可以适当的较少压力，因为有 分配任务的时候，要求下面的人去重复描述下，确保正确无误 优秀的思维去分享给团队，让团队一起成长 比如烧开水理论， 优秀的团队应该是一列高铁 个人职业规划个人现阶段的努力方向是能够快速解决问题这个要求就非常高，需要具备一定的开发能力。比如开发开发出来的程序，在测试上正常，但是一到生产上，服务器的负载就持续飙升，CPU资源被消耗殆尽，这个时候要能够快速的定位到进程。然后要能分析进程内部的资源消耗情况，比如调用内核的哪些系统调用的情况引起的异常等等，找到之后能不能定位到相应的程序代码，这样才能解决问题，而不是找到进程之后，简单的重启。【这一阶段基本上就是资深运维开发工程师级别，预计3年时间】 在这之后下一个阶段目标是未雨绸缪，在源头将问题遏制住因此需要具备开发能力，在软件需求评审和软件设计阶段就要参与进来。【在这一阶段基本上就达到了架构师的水平】 补充HTTP协议POST与GET的区别 GET是从服务器上获取数据，POST是向服务器传送数据 GET是通过发送HTTP协议通过URl参数传递进行接收，而POST是实体数据，通过表单提交 GET传送的数据量较小，不能大于2KB。POST传送的数据量较大，一般被默认为不受限制。 GET安全性非常低，POST安全性较高]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>职场</category>
        <category>简历内容应答</category>
      </categories>
      <tags>
        <tag>简历内容应答</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10添加指定程序到开机自启动]]></title>
    <url>%2F2018%2F04%2F03%2FWin10%E6%B7%BB%E5%8A%A0%E6%8C%87%E5%AE%9A%E7%A8%8B%E5%BA%8F%E5%88%B0%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[打开运行对话框（win键+R），输入命令 shell:startup 会直接弹出启动项对应的目录，然后像把应用程序快捷方式(需要对该执行文件右键创建快捷方式)复制或者剪切到启动目录 注意：该方式的启动项对应的目录是个人目录，也就是说不是针对系统上的所有用户。]]></content>
      <categories>
        <category>IT基础知识</category>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Win10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Http Server 网络处理模型的进化之路]]></title>
    <url>%2F2018%2F04%2F02%2FHttp-Server-%E7%BD%91%E7%BB%9C%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96%E4%B9%8B%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[缘起我刚毕业那会儿，国家还是包分配工作的，我的死党小明被分配到了一个叫数据库的大城市，天天都可以坐在高端大气上档次的机房里，在那里专门执行 SQL查询优化，工作稳定又舒适； 隔壁宿舍的小白被送到了编译器镇，在那里专门把 C 源文件编译成 EXE 程序，虽然累，但是技术含量非常高，工资高，假期多。 我成绩不太好，典型的差生，四级补考了两次才过，被发配到了一个不知道什么名字的村庄，据说要处理什么 HTTP请求，这个村庄其实就是一个破旧的电脑，令我欣慰的是可以上网，时不时能和死党们通个信什么的。 不过辅导员说了，我们都有光明的前途。 Http Server 1.0HTTP是个新鲜的事物，能够激起我一点点工作的兴趣，不至于沉沦下去。 一上班，操作系统老大扔给我一大堆文档： “这是 HTTP协议， 两天看完！” 我这样的英文水平，这几十页的英文 HTTP协议我不吃不喝不睡两天也看不完， 死猪不怕开水烫，慢慢磨吧。 两个星期以后，我终于大概明白了这 HTTP是怎么回事：无非是有些电脑上的浏览器向我这个破电脑发送一个预先定义好的文本（Http request）, 然后我这边处理一下（通常是从硬盘上取一个后缀名是 html的文件，然后再把这个文件通过文本方式发回去（http response），就这么简单。 唯一麻烦的实现，我得请操作系统给我建立 Http 层下面的 TCP 连接通道， 因为所有的文本数据都得通过这些 TCP通道接收和发送，这个通道是用 socket建立的。 弄明白了原理，我很快就搞出了第一版程序，这个程序长这个样子： 看看， 这些 socket, bind, listen , accept… 都是操作系统老大提供的接口， 我能做的也就是把他们组装起来：先在 80端口监听，然后进入无限循环，如果有连接请求来了，就接受 (accept)，创建新的 socket，最后才可以通过这个 socket来接收，发送 http 数据。 老大给我的程序起了个名称，Http Server 版本 1.0 。 这个名字听起来挺高端的，我喜欢。 我兴冲冲的拿来实验，程序启动了，在 80端口“蹲守”，过了一会儿就有连接请求了， 赶紧 Accept ,建立新的 socket ，成功 ！接下来就需要从 socket 中读取 Http Request 了。 可是这个 receive 调用好慢，我足足等了 100 毫秒还没有响应！我被阻塞 (block) 住了！ 操作系统老大说：“别急啊，我也在等着从网卡那里读数据，读完以后就会复制给你。” 我乐的清闲，可以休息一下。 可是操作系统老大说：“别介啊，后边还有很多浏览器要发起连接，你不能在这儿歇着啊。” 我说不歇着怎么办？receive调用在你这里阻塞着，我除了加入阻塞队列，让出 CPU 让别人用还能干什么？ 老大说： “唉，大学里没听说过多进程吗？你现在很明显是单进程，一旦阻塞就完蛋了，想办法用下多进程，每个进程处理一个请求！” 老大教训的是，我忘了多进程并发编程了。 Http Server 2.0 ：多进程多进程的思路非常简单，当 accept连接以后，对于这个新的 socket ，不在主进程里处理，而是新创建子进程来接管。这样主进程就不会阻塞在 receive 上，可以继续接受新的连接了。 我改写了代码，把 Http server 升级为 V2.0，这次运行顺畅了很多，能并发的处理很多连接了。 这个时候 Web 刚刚兴起，我这个 Http Server 访问的人还不多，每分钟也就那么几十个连接发过来，我轻松应对。 由于是新鲜事物，我还有资本给搞数据库的小明和做编译的小白吹吹牛，告诉他们我可是网络高手。 没过几年，Web迅速发展，我所在的破旧机器也不行了，换成了一个性能强悍的服务器，也搬到了四季如春的机房里。 现在每秒中都有上百个连接请求了，有些连接持续的时间还相当的长，所以我经常得创建成百上千的进程来处理他们，每个进程都得耗费大量的系统资源，很明显操作系统老大已经不堪重负了。 他说：“咱们不能这么干了，这么多进程，光是做进程切换就把我累死了。” “要不对每个 Socket 连接我不用进程了，使用线程？ ” “可能好一点，但我还是得切换线程啊，你想想办法限制一下数量吧。” 我怎么限制？我只能说同一时刻，我只能支持 x个连接，其他的连接只能排队等待了。 这肯定不是一个好的办法。 Http Server 3.0 : Select模型老大说：“我们仔细合计合计，对我来说，一个 Socket连接就是一个所谓的文件描述符（File Descriptor ,简称 fd , 是个整数），这个 fd 背后是一个简单的数据结构，但是我们用了一个非常重量级的东西 – 进程 –来表示对它的读写操作，有点浪费啊。” 我说：“要不咱们还切换回单进程模型？但是又会回到老路上去，一个 receive 的阻塞就什么事都干不了了。” “单进程也不是不可以，但是我们要改变一下工作方式。” “改成什么？” 我想不透老大在卖什么关子。 “你想想你阻塞的本质原因，还不是因为人家浏览器还没有把数据发过来，我自然也没法给你，而你又迫不及待的想去读，我只好把你阻塞。在单进程情况下，一阻塞，别的事儿都干不了。“ “对，就是这样” “所以你接受了客户端连接以后，不能那么着急的去读，咱们这么办，你的每个 socket fd 都有编号，你把这些编号告诉我，就可以阻塞休息了 。” 我问道：“这不和以前一样吗？原来是调用 receive 时阻塞，现在还是阻塞。” “听我说完，我会在后台检查这些编号的 socket，如果发现这些 socket 可以读写，我会把对应的 socket 做个标记，把你唤醒去处理这些 socket 的数据，你处理完了，再把你的那些 socket fd 告诉我，再次进入阻塞，如此循环往复。” 我有点明白了：“ 这是我们俩的一种通信方式，我告诉你我要等待什么东西，然后阻塞，如果事件发生了，你就把我唤醒，让我做事情。” “对，关键点是你等我的通知，我把你从阻塞状态唤醒后，你一定要去遍历一遍所有的 socket fd，看看谁有标记，有标记的做相应处理。我把这种方式叫做 select 。” 我用 select 的方式改写了 Http server，抛弃了一个 socket 请求对于一个进程的模式，现在我用一个进程就可以处理所有的 socket了。 Http Server4.0 : epoll这种称为 select 的方式运行了一段时间，效果还不错，我只管把 socket fd 告诉老大，然后等着他通知我就行了。 有一次我无意中问老大：“我每次最多可以告诉你多少个 socket fd？” “1024个。” “那就是说我一个进程最多只能监控 1024 个 socket 了？ ” “是的，你可以考虑多用几个进程啊！” 这倒是一个办法，不过”select”的方式用的多了，我就发现了弊端，最大的问题就是我从阻塞中恢复以后，需要遍历这 1000 多个 socket fd，看看有没有标志位需要处理。 实际的情况是， 很多 socket 并不活跃， 在一段时间内浏览器并没有数据发过来， 这 1000 多个 socket 可能只有那么几十个需要真正的处理，但是我不得不查看所有的 socket fd，这挺烦人的。 难道老大不能把那些发生了变化的 socket 告诉我吗？ 我把这个想法给老大说了下，他说：“嗯，现在访问量越来越大， select 方式已经不满足要求，我们需要与时俱进了，我想了一个新的方式，叫做 epoll。” “看到没有，使用 epoll 和 select 其实类似“ 老大接着说 ：”不同的地方是第 3 步和第 4 步，我只会告诉你那些可以读写的 socket , 你呢只需要处理这些 ‘ready’ 的 socket 就可以了“ “看来老大想的很周全， 这种方式对我来说就简单的多了。 ” 我用 epoll 把 Http Server 再次升级，由于不需要遍历全部集合，只需要处理哪些有变化的，活跃的 socket 文件描述符，系统的处理能力有了飞跃的提升。 我的 Http Server 受到了广泛的欢迎，全世界有无数人在使用，最后死党数据库小明也知道了，他问我：“ 大家都说你能轻松的支持好几万的并发连接， 真是这样吗？ ” 我谦虚的说：“过奖，其实还得做系统的优化啦。” 他说：“厉害啊，你小子走了狗屎运了啊。” 我回答： “毕业那会儿辅导员不是说过吗， 每个人都有光明的前途。”]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识</category>
        <category>epoll模型</category>
      </categories>
      <tags>
        <tag>epoll模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[终端常用快捷键]]></title>
    <url>%2F2018%2F04%2F01%2F%E7%BB%88%E7%AB%AF%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[常用快捷键Tab键： 命令、文件名等自动补全功能。 Ctrl+a： 光标回到命令行首。 （a：ahead） Ctrl+e： 光标回到命令行尾。 （e：end） Ctrl+b： 光标向行首移动一个字符。 （b：backwards） Ctrl+f： 光标向行尾移动一个字符。 （f：forwards） Ctrl+w： 删除光标处到行首的字符，也就是删除光标前面的所有内容。 Ctrl+k： 删除光标处到行尾的字符，也就是删除光标后面的所有内容。 Ctrl+u： 删除整个命令行文本字符，删除整行命令。 Ctrl+h： 向行首删除一个字符，向前删除一个字符，相当于Backspace。 Ctrl+d： 向行尾删除一个字符，向后删除一个字符，相当于Delete。 Ctrl+y： 粘贴Ctrl+u，Ctrl+k，Ctrl+w删除的文本。 Ctrl+p： 上一个使用的历史命令。 （p：previous） Ctrl+n： 下一个使用的历史命令。（n：next ） Ctrl+t： 交换光标所在字符和其前的字符。 Ctrl+i： 相当于Tab键。 Shift+Insert： 粘贴鼠标所复制的内容 Ctrl+d: 在空命令行的情况下可以退出终端。 Shift+c： 删除之后的所有内容并进入编辑模式 Ctrl+c： 中断终端中正在执行的任务。 Ctrl+z： 使正在运行在终端的任务，运行于后台。 （可用fg恢复到前台） 非常用快捷键Ctrl+s： 使终端发呆，静止，可以使快速输出的终端屏幕停下来。 Ctrl+q： 退出Ctrl+s引起的发呆。 Ctrl+[： 相当于Esc键。 Esc键： 连续按3次显示所有的支持的终端命令，相当于Tab键。 Ctrl+r： 快速检索历史命令。（r：retrieve）。 Ctrl+o： =Ctrl+m：相当Enter键。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>Linux基础知识</category>
        <category>终端常用快捷键</category>
      </categories>
      <tags>
        <tag>终端常用快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux运维工程师面试常见问题]]></title>
    <url>%2F2018%2F03%2F28%2FLinux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[redhat、centos、suse、ubuntu等发行版本的区别这些发行版本本质上没有什么区别，都是类unix系统。redhat的系统本身是免费的，但是它的服务和一些特定的组件是收费的而centos是redhat社区版本，国内使用较多，社区相当活跃。SUSE也是分为两种，一种企业版本的SLES，一种是opensuse，sles主要是一些金融国企在使用，安全性较好，opensuse是社区版本，suse使用起来和centos相差不多，软件包形式使用源自redhat的rpm，但是管理工具使用的是zypper。ubuntu源自于debain，国外使用的较多，也是相当热门的一个发型版本，在桌面领域有绝对技术优势，适合开发人员使用。 OSI7层模型和TCP/IP模型的区别联系OSI7层模型OSI 7层模式主要是由国际标准化组织（ISO）创建的，是一个国际通用的标准，它被开发出作为一个参照标准，用于指导如何设计网络通信系统。说的简单一点就是统一网络设备商的协议标准，实现多网络设备商环境（主机，路由器，交换机等等都是网络设置，都要遵循同一套的通信标准）它一共分为7层，每一层在网络通信数据传输过程中都定义了不同的功能。 OSI7层模型主要分为(从下到上)：物理层，数据链路层，网络层，传输层，会话层，表示层，应用层。每一层说明： layer function Application data flow；离用户最近的一层，它使一个网络应用能和另一个网络应用相互通信 Presentation 定义数据格式；数据压缩、加解密等。 Session 定义如何建立和终止连接，是告诉4层怎么做，4层只管被动的接受然后去做。 Transport 数据段；将上层的数据流进行分段；建立和终止网络连接；常用于流量控制和数据恢复 Network 数据包；使用IP地址在Internet上唯一确定一台设备；定义设备路由，寻址 Data link 数据帧；将上层数据封装成数据帧，其中包含源目MAC地址以及帧校验字段（用于检测传输错误）；它包含2个子层（LLC和MAC） Physical 比特流；定义了比特流如何在两台设备之间流通；主要涉及线缆，网卡， 下面是每一层常见的对应协议 layer protocol Application HTTP,FTP,Telnet,SMTP,SNMP Presentation MIME,TIFF,GIF,JPEG,PICT,ASCII,EBCDIC,encryption,MPEG,MIDI,HTML Session SSl/TLS,NetBIOS,RPC Transport TCP,UDP Network IP,ICMP,ARP,RARP Data link PPP,HDLC,IEEE 802.3/802.2,FDDI,ATM,IEEE 802.5/802.2 Physical Ethernet TCP/IP协议族TCP/IP模型类似OSI模型，作用也是描述一套指导标准，实现网络设备之间的通信，它被设计成4层 Application Transport Internet Network Access 其对应关系为： TCP/IP model OSI model Application Application Presentation Session Transport Transport Internet Network Network Access Data link Physical 区别除了层数的区别之外，它们之间最大的区别就是： OSI模型规定了在一个网络上传输数据所需要的步骤，并且它是非常具体的，定义了每一层使用什么协议以及如何使用；而TCP/IP模型是不特定的。另外一个区别就是，目前TCP/IP是所有网络设备上既定的协议事实，一般分析问题使用OSI模型。 路由，交换技术的基本原理路由技术我们在这里说的路由技术一般是指，路由转发。主要涉及设备为路由器或者三层交换机。这些设备上会维护一张路由表，其中的信息可以是通过动态路由协议（例如OSPF，EIGRP，ISIS，RIP，BGP，静态路由，默认路由等）获取组成路由表的内容是：出口接口和对应网段 路由设备接收到一个数据包之后，会解封装，获取其中的目的IP地址信息（网段信息），然后查找路由表，选择最优路由去转发。路由设备上也会有一张ARP表，根据广播域 交换技术在一个局域网内，也就是一个广播域内使用的技术。通常会涉及到的设备就是交换机。交换机上会维护一张MAC地址转发表，其中的信息是MAC地址和端口的映射关系。交换机根据数据帧中的目的MAC地址进行数据包的转发 注意：在一个广播域内的数据流动是依靠二层MAC来实现的，因为在第一次会涉及到ARP，有了记录之后，交换机会记录他的MAC地址表，后续的速度就会非常快 PS：有关网络模型和路由技术可以结合一个小案例在白板上演示一下，效果会更好，也就是将上述两部分的内容有机的整合成为一个整体 脚本部分运维知识优化运维思想运维核心是什么稳定性-网站/平台不宕机【核心】集群 负载均衡 高可用 解耦，微服务 数据不丢失【核心】数据备份，异地容灾 避免人为错误 避免人为错误，主要分为两个方面，一个是开发不严谨产生的错误，这部分通过流程可以控制。【比如测试不严谨，开发人员的代码有问题，直接把服务器资源跑没了】 一个是自己操作产生的问题，这部分通过一些智能化的自动化工具来尽量避免【避免在执行命令的时候误操作等等】 解决：建立完善的流程制度，对运维来说，包括标准化，对开发测试来说，包括上线的流程化 运维效率管理平台运维脚本化，工具化，自动化，人工智能化 如何做好运维工作运维职业规划你为什么离职你对加班的看法个人最大的优点和缺点]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>职场</category>
        <category>Linux运维面试问题</category>
      </categories>
      <tags>
        <tag>Linux运维面试问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker常用命令记录]]></title>
    <url>%2F2018%2F03%2F19%2Fdocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[镜像命令下载/获取镜像运行镜像查看镜像信息images命令使用docker images命令可以列出本地主机上已有镜像的基本信息。 1$ docker images [option] 常用参数如下： -a –all=true|false 列出所有的镜像文件（包括临时文件），默认为否 –digests=true|false 列出镜像的数字摘要值，默认为否 -f –filter=[] 过滤列出的镜像 具体可以通过man docker-images 进行查看。 inspect命令镜像操作tag命令删除镜像]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>虚拟化</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix的历史数据与趋势数据]]></title>
    <url>%2F2018%2F02%2F06%2Fzabbix%E7%9A%84%E5%8E%86%E5%8F%B2%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%B6%8B%E5%8A%BF%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[参考链接官方文档 zabbix history trends历史与趋势数据详解 zabbix配置操作详解（三） Zabbix系统中的历史数据和趋势数据 正文历史与趋势历史数据和趋势数据是Zabbix系统中对采集到的监控项数据进行存储的两种方式。 历史根据设定的时间间隔保持每个收集的值， 而趋势是每个小时产生一个值（一条信息），内容为历史数据在一个小时内的平均值、最大值、最小值以及这一个小时内该监控项目所采集到的数据的个数。 在zabbix中的配置在监控项配置页面进行定义，在这里，我的配置是历史数据保留15天，趋势数据保留90天。如下图所示： 区别联系详解历史和趋势数据它们既有区别又有联系。 历史数据： Zabbix系统针对每个监控项目在每次采集时所收集到的数据，这个数据保存Zabbix系统数据库的历史表中，这就是所谓的历史数据。 因为每次所采集到的数据都保存在历史表中，所以如果监控项目的更新间隔越小，则在固定时间内所保存到历史表中的数据就越多。如果每个监控项目的更新间隔是30秒的话，则两个小时，该监控项目在Zabbix数据库的历史表中就会产生240条记录，一天就会产生2880条记录。 如果我们的Zabbix系统只监控一台被监控主机，且这台被监控主机只有一个被监控项目，那么每天产生2880条记录确实不值得一提的。但是，当我们监控系统所监控的项目比较多时，则这个数据量是非常大的。 比如说，如果我们监控系统监控1000个监控项目，且每个监控项目的更新间隔都是30秒，则每天历史表中就会产生2880*1000=2880000条记录，也即近300万条记录。而1000个监控项目可以监控多少主机呢？我们以48口的交换机为例，单监控每台交换机的每个端口的流量，则一台48口的交换机就有96个监控项目。所以，如果我们仅监控这样的48口的交换机，1000个监控项目只差不多只够监控10台这样交换机。由此可见，如果我们所监控主机的数量稍微多一点，或者更确切的来说，我们所监控的项目稍微多点，则Zabbix系统每天在其数据库中所产生的记录是非常大的。 因此，我们建议，如非必须的，我们在配置监控项目时，应尽量减小历史数据的保留天数，以免给数据库系统带来很大的压力。 趋势数据： 而趋势数据则不同，对于相同的更新间隔，系统所产生的趋势数据的数量远远没有历史数据那么庞大。对同一个监控项目，之所以趋势数据的数据量要远远小于历史数据的数据量，是由趋势数据的取值方式决定的。 趋势数据取值方式是，它取对应监控项目的历史数据在一个小时内的平均值、最大值、最小值以及这一个小时内该监控项目所采集到的数据的个数。 因此，不管一个监控项目的更新间隔是多少，它所对应的趋势数据在数据库中的记录都只有一条。更新间隔越小，仅可能导致数据个数增大，而不会影响该监控项目在趋势表里的记录条数的。 由此，或许你觉得趋势数据很不准确，你还是愿意保留更长时间的历史数据，以便查看较长时间的数据图。其实不是这样的，因为在Zabbix系统数据库的趋势表里不但保留一个小时内历史数据的最大值、最小值和平均值，而还保存这一个小时内所采集到的数据个数。因此，在要求并不是很高的场合，使用趋势数据绘出的监控项目的数据图的走势与用历史数据绘出的数据图的走势差别不会很大的。 不管是历史数据还是趋势数据，都会周期性被Zabbix服务器端一种称之为“主妇（housekeeper）”进程进行清理，它会周期性的删除过期的历史数据和趋势数据。 也正是因为这个进程的存在，才会使Zabbix系统数据的数据量不会一直的彭胀下去。而实际上，如果我们在保持Zabbix系统的被监控主机和被监控项目不变，且不更改监控项目的更新间隔的情况下，Zabbix系统的数据库的数据量会在增长到一定的数据量后不再增长，而是基本维持在这个数据量上不变。 “主妇”进程清理历史数据和趋势数据的频率可以在Zabbix服务器端组件(或服务器代理组件)的配置文件zabbix_server.conf中进行配置，它的配置项是HousekeepingFrequency。 特别注意： 1、 如果监控项目的“保留历史数据(天)”配置项被设置成0时，则数据库历史表中仅保留该监控项目所采集的最后一条数据，其它历史数据将不会被会保留。而且，引用该监控项目的触发器也只能使用该项目所采集的最后数据。因此，此时如果在触发器里引用该项目时使用max、avg、min等函数，其将没有意义。 2、 如果监控项目的“保留趋势数据(天)”配置项被设置成0时，则该项目在系统数据库的趋势表里将不保留任何数据。 配置建议具体该配置成什么样的周期，需要根据监控项以及数据库的配置以及对数据查看的要求程度来决定。这里只给出相关建议。 历史数据配置首先我们需要知道当前mysql的存储情况。在zabbix的前端页面上，我们可以看到如下图所示信息： 这个数值就是NVPS，也就是每秒处理平均数量（Number of processed values per second) 计算公式如下： 历史数据大小=NVPSx3600x24x365(天数)x50B 每个监控项大小约为50B，每秒条数为NVPS，一小时3600秒，一天24小时，一年365年。 具体单个监控项大小取决于数据库引擎，通常为50B 例如： 假设有6W个监控项，刷新周期都为60秒（我这里为30秒），那么每秒将会产生1000条数据，也就是每秒会向数据库写入1000条数据。如果我的历史数据保留天数为90天，那么需要的空间大小如下： 1000x3600x24x90x50=388 800 000 000(B) (约为362G，如果保存一年则为：362x4=1448G) 趋势数据配置因为趋势数据是每小时每个监控项一条记录，因此可以计算出大致所占的空间，其计算公式如下： 趋势数据大小=监控项个数x24x365(天数)x128B 每一个监控项的大小约为128B，每小时产生一条记录，一天24小时，一年365天 具体单个监控项大小取决于数据库引擎，通常为128B 例如： 假设有6W个监控项，保存一年的趋势数据，那么需要的空间如下： 60000x24x265x128=67 276 800 000(B) （约为67GB） 总结通过上面的计算对比，相信可以很直观的看到差别，在同样一年的情况下，历史与趋势所占存储空间的比例为：1448/67。 所以，具体选择什么周期需要根据公司的业务及实际情况（硬件配置等）来决定，并没有一个统一的标准，遵循这个公式，都可以很明确的计算预估出数据量情况。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维监控体系</category>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[autohotkeye常用操作]]></title>
    <url>%2F2018%2F02%2F06%2Fautohotkeye%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言AutoHotkey是一个windows下的开源、免费、自动化软件工具。它由最初旨在提供键盘快捷键的脚本语言驱动(称为：热键)，随着时间的推移演变成一个完整的脚本语言。但你不需要把它想得太深，你只需要知道它可以简化你的重复性工作，一键自动化启动或运行程序等等；以此提高我们的工作效率，改善生活品质；通过按键映射，鼠标模拟，定义宏等。 参考资料官方https://autohotkey.com/docs/AutoHotkey.htm 民间https://jeffjade.com/2016/03/11/2016-03-11-autohotkey/https://ahkcn.github.io/docs/AutoHotkey.htm 下载安装下载地址autohotkey下载地址 使用说明 AutoHotkey doesn’t do anything on its own; it needs a script to tell it what to do. A script is simply a plain text file with the .ahk filename extension containing instructions for the program, like a configuration file, but much more powerful. A script can do as little as performing a single action and then exiting, but most scripts define a number of hotkeys, with each hotkey followed by one or more actions to take when the hotkey is pressed. 也就是说，在实际使用的时候，是通过autohotkey去调用脚本，然后再去执行一系列的操作 脚本是自己定义个一个后缀为.ahk的文件 然后双击启动Ahk2Exe.exe，选择自己编写的这个ahk文件，执行convert，之后会生成一个ahk.exe的可执行文件。启动这个ahk.exe文件，就将配置加载，之后就可以使用这些热键进行一系列的操作 一个脚本中对应一系列热键 脚本符号这里简单说明下脚本中常用符号代表的含义： # 号 代表 Win 键； ! 号 代表 Alt 键； ^ 号 代表 Ctrl 键； + 号 代表 shift 键； :: 号(两个英文冒号)起分隔作用； run， 非常常用 的 AHK 命令之一; ; 号 代表注释后面一行内容； *通配符 即使附加的修饰键被按住也能激发热键. 这常与 重映射 按键或按钮组合使用. 例如: *#c::Run Calc.exe 表示：Win+C、Shift+Win+C、Ctrl+Win+C 等都会触发此热键。 run它的后面是要运行的程序完整路径（比如我的Sublime的完整路径是：D:\Program Files (x86)\Sublime Text 3\sublime_text.exe）或网址。为什么第一行代码只是写着“notepad”，没有写上完整路径？因为“notepad”是“运行”对话框中的命令之一。 如果你想按下“Ctrl + Alt + Shift + Win + Q”（这个快捷键真拉风啊。(￣▽￣)）来启动 QQ 的话，可以这样写： ^!+#q::run QQ所在完整路径地址。 AutoHotKey的强大，有类似Mac下的Alfred2之风，可以自我定制(当然啦，后者还是强大太多)。所以可以说，它强大与否，在于使用者的你爱或者不爱折腾。学以致用，如果简单的折腾下，可以使得我们工作效率大幅提升，何乐不为？况且，在见识的增长中，这可以给我们思维带来极大的营养。以下是笔者常用功能的脚本配置： 温馨提示： 以下几个系统默认的 Win 快捷键，请自行确认是否覆盖 Win + E：打开资源管理器； Win + D：显示桌面； Win + F：打开查找对话框； Win + R：打开运行对话框； Win + L：锁定电脑； Win + PauseBreak：打开系统属性对话框; Win + Q: 本地文件/网页等搜索; Win + U: 打开控制面板－轻松使用设置中心; 配置使用这是我自行编写的脚本的内容 #q::Run https://wx.qq.com/ #w::Run http://watchmen.xin/ #e::Run E:\software\tcmd\totalcmd\TOTALCMD64.EXE #r::Run, E:\software\ss\Shadowsocks.exe #t::Run, E:\software\Snipaste\Snipaste.exe #y::Run, E:\software\TIMqq\Bin\QQScLauncher.exe #u::Run, E:\software\foxmail\Foxmail.exe #i::Run, E:\software\xmanager\Xshell.exe 进阶单热键多命令类似下面的这种设置被称为单行热键, 因为它们只包含单个命令. #n::Run Notepad ^!c::Run calc.exe 要在一个热键中执行多个命令，请把首行放在热键定义的下面，且在最后行命令的下一行添加 return。例如： #n:: Run http://www.google.com Run Notepad.exe return 如果要运行的程序或文档没有在环境变量中, 那么需要指定它的完整路径才能运行: Run %A_ProgramFiles%\Winamp\Winamp.exe 在上面的例子中, %A_ProgramFiles% 是 内置变量. 使用它而不使用像 C:\Program Files 这样的, 脚本可以有更好的移植性, 这表示它在其他电脑上能执行的可能性更大. 注意: 命令和变量的名称是不区分大小写的. 例如, “Run” 等同于 “run”, 而 “A_ProgramFiles” 等同于 “a_programfiles”. 要让脚本等到程序或文档关闭后才继续执行, 请使用 RunWait 代替 Run. 在下面的例子中, 一直到用户关闭记事本后 MsgBox 命令才会继续执行. RunWait Notepad MsgBox The user has finished (Notepad has been closed).]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>AutoHotKey</category>
      </categories>
      <tags>
        <tag>autohotkey</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[七牛云-qshell工具常用命令]]></title>
    <url>%2F2018%2F02%2F05%2F%E4%B8%83%E7%89%9B%E4%BA%91-qshell%E5%B7%A5%E5%85%B7%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[前言/简介qshell是利用七牛文档上公开的API实现的一个方便开发者测试和使用七牛API服务的命令行工具。 该工具设计和开发的主要目的就是帮助开发者快速解决问题。 目前该工具融合了七牛存储，CDN，以及其他的一些七牛服务中经常使用到的方法对应的便捷命令，比如b64decode，就是用来解码七牛的URL安全的Base64编码用的，所以这是一个面向开发者的工具。 官方资料文档https://developer.qiniu.com/kodo/tools/1302/qshell 视频教程http://notdelete.echohu.top/spjc/qshell-win.mp4 安装/环境准备目前在windows上使用qshell需要执行以下几个步骤添加命令到系统 下载qshell，存储到指定文件夹，例如我这里是：E:\software\qshell 重命名，将qshell_windows_x64.exe重命名为qshell.exe 添加系统环境变量，将E:\software\qshell追加到环境变量中 命令选项参数 描述 -d 设置是否输出DEBUG日志，如果指定这个选项，则输出DEBUG级别的日志 -m 切换到多用户模式，这样所有的临时文件写入都在命令运行的目录下 -h 打印命令列表帮助信息，遇到参数忘记的情况下，可以使用该命令 -v 打印工具版本，反馈问题的时候，请提前告知工具对应版本号 命令列表 实际操作我们使用qupload来进行文件的管理 官方文档 命令参数展示 命令语法： 1qshell qupload [&lt;ThreadCount&gt;] &lt;LocalUploadConfig&gt; 命令参数： 配置参数展示qupload 功能需要配置文件的支持，配置文件支持的全部参数如下： { &quot;src_dir&quot; : &quot;&lt;LocalPath&gt;&quot;, &quot;bucket&quot; : &quot;&lt;Bucket&gt;&quot;, &quot;file_list&quot; : &quot;&lt;FileList&gt;&quot;, &quot;key_prefix&quot; : &quot;&lt;Key Prefix&gt;&quot;, &quot;up_host&quot; : &quot;&lt;Upload Host&gt;&quot;, &quot;ignore_dir&quot; : false, &quot;overwrite&quot; : false, &quot;check_exists&quot; : false, &quot;check_hash&quot; : false, &quot;check_size&quot; : false, &quot;rescan_local&quot; : true, &quot;skip_file_prefixes&quot; : &quot;test,demo,&quot;, &quot;skip_path_prefixes&quot; : &quot;hello/,temp/&quot;, &quot;skip_fixed_strings&quot; : &quot;.svn,.git&quot;, &quot;skip_suffixes&quot; : &quot;.DS_Store,.exe&quot;, &quot;log_file&quot; : &quot;upload.log&quot;, &quot;log_level&quot; : &quot;info&quot;, &quot;log_rotate&quot; : 1, &quot;log_stdout&quot; : false, &quot;file_type&quot; : 0 } 参数具体含义如下： 密钥设置单用户 1qshell account ak sk 多用户 1qshell -m account ak sk 这里的ak、sk在个人面板中的密钥管理中查看，点击显示，然后进行复制粘贴 如下图所示： 上传图片这里我们选择qupload方式来进行图片的上传，在windows本地创建一个文件夹用户放置图片数据，每次同步该文件夹即可，不用再单独每张上传 步骤1：创建本地图片文件夹如下图所示，在指定位置下创建一个文件夹用于存放图片，在这里，我把它和我的博客文件夹放在同级 步骤2：创建配置文件如下图所示，在指定目录下创建配置文件，注意，这里需要使用编辑打开，不要用notpad++这些编辑器 步骤3：执行命令进行上传准备工作都做好后，执行如下命令直接上传： qshell qupload 1 c:\Users\56810\blog\config.txt qshell qupload 1 C:\Users\Administrator\blog\config.txt 如下图所示 其他配置下载文件刷新缓存官方资料 使用七牛云提供的 qshell 命令行工具，参考使用文档，先设置密钥，然后执行 cdnrefresh 命令来刷新缓存。 具体操作为： 步骤1：修改配置文件 步骤2：执行命令]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>七牛云-qshell</category>
      </categories>
      <tags>
        <tag>qshell</tag>
        <tag>七牛云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RSS介绍及使用]]></title>
    <url>%2F2018%2F02%2F04%2FRSS%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[转载来源：http://www.ruanyifeng.com/blog/2006/01/rss.html RSS定义在解释RSS是什么之前，让我先来打一个比方。 读大学的时候，我有个习惯，就是每天要去看食堂后面的海报栏。在那里，会贴出各种各样最新的消息，比如哪个系要开讲座了、星期二晚上的电影放什么、二手货转让等等。只要看一下海报栏，就会对学校的各种活动心中有数。 如果没有海报栏的话，要想知道这些消息就会很麻烦。讲座消息会贴在各个系自己的公告栏里，电影排片表是贴在电影院里的，二手货消息则会贴在各幢宿舍的楼道里。我所在的大学有20几个系，一万多人，要想知道所有这些消息的话，即使是可能的话，也会相当的麻烦。 从这个例子出发，让我们来考虑一下互联网。 互联网是什么？最直观的说，就是一个杂乱无章的巨大信息源，其丰富和杂乱的程度，不仅是巨大的，而且几乎是无限的。 一个使用者，要想及时掌握的互联网上出现的最新信息，有办法吗？ 答案是没有办法，他只有一个网站一个网站的打开，去看有什么最新内容，就好比每天都必须去每一个系里走一遍，看有什么最新讲座。如果是几个网站，哪倒也不难，都去看一遍也花不了多少时间。但是随着你关注的网站数量上升，这项工作会迅速的变为”Mission Impossible”。想象一下，如果你每天关注几十个、甚至几百个网站，会是怎样的情景。光是打开它们的首页，就要花费多少时间啊，更别说浏览花去的时间了。 也许有人会说，普通人的话，谁会关心那么多网站啊？ 我要说，哪怕你只是一个网络的初级或最单纯的使用者，与你发生关系的网站数量也在急剧增加，因为Blog出现了。越来越多的人开始写作网络日志（Blog），把自己的想法和生活在网上展示，其中也必然包括你的朋友，或者其他你感兴趣的人。你想知道他/她的最新动向，就势必要留心他/她的Blog。所以，你的网站浏览清单总有一天会和你的电话本、MSN Message好友列表一样多，甚至更多。 那时，你会发现浏览网站会变成一种困难和低效率的行为。 有没有办法找到互联网上的”海报栏”，只去一个地方就知道你所想知道的所有最新内容？ 有，那就是RSS。 RSS内容和阅读器准确的说，RSS就像一个网站的海报，里面包括这个网站的最新内容，会自动更新。所以，我们只要订阅了RSS，就不会错过自己喜欢的网站的更新了。 但是光有海报还不行，还必须有海报栏，也就是说必须有RSS阅读器才行。因为RSS只是数据源，它本身是写给程序看的，必须经过阅读器转换，才能成为可以浏览的格式。 RSS阅读器多种多样，大致分为两种，一种是桌面型的，需要安装；另一种是在线型，直接使用浏览器进行阅读。 使用/订阅RSS在浏览器中订阅RSS，就必须先知道RSS的地址。一般来说，各个网站的首页都会用显著位置标明。名称可能会有些不同，比如RSS、XML、FEED，大家知道它们指的都是同样的东西就可以了。有时RSS后面还会带有版本号，比如2.0、1.0，甚至0.92，这个不必理会，它们只是内部格式不同，内容都是一样。 将RSS地址复制下来以后，你就可以在在线阅读器中添加。 以后，只用打开这一个网页，就可以看到所有你喜欢的网站的最新内容了。 推荐RSS阅读器个人目前在使用的RSS阅读器为：inoreader]]></content>
      <categories>
        <category>IT基础知识</category>
        <category>RSS</category>
      </categories>
      <tags>
        <tag>RSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown语法]]></title>
    <url>%2F2018%2F01%2F25%2FMarkdown%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Markdown介绍Markdown 是一种轻量级标记语言，它以纯文本形式(易读、易写、易更改)编写文档，常用的标记符号也不超过十个，并最终以HTML格式发布,让写作者专注于写作而不用关注样式。 划重点： 轻量级 标记语言 纯文本，所以兼容性极强，可以用所有文本编辑器打开。 让你专注于文字而不是排版。 格式式转换方便，Markdown 的文本你可以轻松转换为 html、电子书等。 Markdown 的标记语法有极好的可读性，常用的标记符号不过十来个 参考资料看完我这篇文章，再看完我下面推荐的这些内容，然后对比归纳总结，认真实践后，可以说在平常工作学习中完全够用。 官方资料 Markdown 语法说明 (简体中文版) Markdown 语法介绍 易读易写!-MarkDown语法说明 个人文章 献给写作者的 Markdown 新手指南 Markdown——入门指南 Markdown 基本语法 编辑器 个人在用的编辑器是MarkdownPad 2。各个工具之间相差不会很大，熟练掌握快捷键是提高效率的好方法 核心理念Markdown 的目标是实现「易读易写」，成为一种适用于网络的书写语言。。不管从任何角度来说，可读性，都是最重要的。Markdown 的语法全由一些符号所组成，这些符号经过精挑细选，其作用一目了然。比如：在文字两旁加上星号，看起来就像强调。 划重点： 语法是非常简单的符号 即写即读 兼容HTMLMarkdown 的构想不是要使得 HTML文档更容易书写。HTML 已经很容易写了。Markdown 的理念是，能让文档更容易读、写和随意改。 HTML是一种发布的格式，而Markdown 是一种书写的格式。也因此，Markdown 的格式语法只涵盖纯文本可以涵盖的范围。 常用操作标题（MarkdownPad中快捷键为Ctrl+1/2/3/4）：Markdown 支持两种标题的语法，类 Setext 和类 atx 形式。类 Setext 形式是用底线的形式，利用 = （最高阶标题）和 - （第二阶标题），例如： This is an H1 ============= This is an H2 ------------- 任何数量的 = 和 - 都可以有效果。但是这种形式只支持2层标题。 类 Atx 形式则是在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶，例如： # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 强调在Markdown中，可以使用 和 _ 来表示斜体和加粗。*单个为斜体，2个为加粗 加粗（MarkdownPad中快捷键为Ctrl+b）：加粗部分使用方式如下： **Coding，让开发更简单** __Coding，让开发更简单__ 实际展示效果如下： Coding，让开发更简单 Coding，让开发更简单 斜体（MarkdownPad中快捷键为Ctrl+l）：斜体部分的使用如下： *Coding，让开发更简单* _Coding，让开发更简单_ 实际展示效果展示如下： Coding，让开发更简单 Coding，让开发更简单 列表无序列表（MarkdownPad中快捷键为Ctrl+u）：* list1 前面使用*号 - list2 前面使用-号 + list3 前面使用+号 效果如下： list1 list2 list3 有序列表(MarkdownPad中快捷键为Ctrl+shift+o）：1. list1 使用数字+英文的点号，空格后接数据 2. list2 效果如下： list1 list2 区块引用（MarkdownPad中快捷键为Ctrl+q）在我们写作的时候经常需要引用他人的文字，这个时候引用这个格式就很有必要了，在 Markdown 中，你只需要在你希望引用的文字前面加上 &gt; 就好了。注意&gt;和文本之间要保留一个字符的空格。 &gt; 数据1 使用&gt;号 &gt; 数据2 &gt; &gt; 二级引用 注意区块引用可以包含多级引用 &gt; 实际效果展示： 数据1 数据2 这是二级引用 三级引用 代码区块（MarkdownPad中快捷键为Ctrl+k）：代码区块包括3种，文字内和单独一行以及指定代码格式的区块行 文字内加区块，不会加空白处底纹使用``（数字1左边，ESC下面的按键） 实际效果展示：在文件中含有代码区块是什么样子 整行的代码区块行，会加空白处底纹（快捷操作：全部选中然后敲Tab）缩进4个空格或者一个制表符（tab键）或者将代码块包裹在代码块包裹在 “/` 之间（避免无休止的缩进）。 123require 'redcarpet'markdown = Redcarpet.new("Hello World!")puts markdown.to_html 实际效果展示 123require 'redcarpet'markdown = Redcarpet.new("Hello World!")puts markdown.to_html 实际效果展示： 现在的效果就是整整一个的区块行，如果这段代码比较长的话，那么markdown就会在下面生成一个查看条，供用户左右拉取调整，就是如现在所示。 指定代码格式的区块行12$ line1-test1$ line2-test2 实际效果展示： 12$ line1-test1$ line2-test2 分割线/分隔线（MarkdownPad中快捷键为Ctrl+r）：一行中用三个以上的星号、减号、底线来建立一个分隔线，可以在字符之间加入空格，也可以不加空格 * * * *** ***** --- - - - 实际效果展示如下： 网页链接网页链接有2种方式，一种是直接显示链接，一种是通过文字进行跳转 直接显示&lt;https://www.baidu.com&gt; 用&lt;&gt;尖括号将内容包起来，markdown就会自动把它转成链接。网页链接、邮箱链接等都采用这种方式 实际效果展示如下：这段话中将要插入百度https://www.baidu.com的链接 文字跳转More info: [Server](https://hexo.io/docs/server.html) 前面是解释性说明，[]内是可以跳转的文字，()内是真正访问的地址。 实际效果展示如下： 请点击百度调整到百度页面 图片链接图片链接分为2部分，一种是在文字中，通过文字来链接到图片位置，用户需要点击这个文字链接去查看图片，优点是使文字更简约，缺点是无法直观的看到图。因此，第二种方式是直接在文章中显示图片。 我们把这两种方式分别称之为：行内式和参考式 行内式行内式的图片语法看起来像是： ![Alt text](/path/to/img.jpg) 参考案例：![Alt text](/path/to/img.jpg &quot;Optional title&quot;) 详细叙述如下： 一个惊叹号 ! 接着一个方括号，里面放上图片的替代文字 接着一个普通括号，里面放上图片的网址， 最后还可以用引号包住并加上 选择性的 ‘title’ 文字。 参考式参考式的图片语法则长得像这样： ![Alt text][id] 「id」是图片参考的名称，图片参考的定义方式则和连结参考一样： 参考案例：[id]: url/to/image &quot;Optional title attribute&quot; 参考式超链接一般用在学术论文上面，或者另一种情况，如果某一个链接在文章中多处使用，那么使用引用 的方式创建链接将非常好，它可以让你对链接进行统一的管理。 参考式同样适用于网页链接的使用 表格普通表格： First Header | Second Header | Third Header ------------ | ------------- | ------------ Content Cell | Content Cell | Content Cell Content Cell | Content Cell | Content Cell 设置表格两边内容对齐，中间内容居中，例如： First Header | Second Header | Third Header :----------- | :-----------: | -----------: Left | Center | Right Left | Center | Right 实际效果展示： First Header Second Header Third Header Left Center Right Left Center Right 文本居中居中使用html方式添加，格式如下： 1&lt;center&gt;这一行需要居中&lt;/center&gt; 文本居中的引用先看下实际效果： 主要用于主页等显示，和上面的文本场景有点不一样。 具体实现： &lt;!-- HTML方式: 直接在 Markdown 文件中编写 HTML 来调用 --&gt; &lt;!-- 其中 class=&quot;blockquote-center&quot; 是必须的 --&gt; &lt;blockquote class=&quot;blockquote-center&quot;&gt;blah blah blah&lt;/blockquote&gt; &lt;!-- 标签 方式，要求NexT版本在0.4.5或以上 --&gt; {% centerquote %} content {% endcenterquote %} &lt;!-- 标签别名 --&gt; {% cq %} content {% endcq %} 添加空行&lt;br /&gt; 使用该方法进行插入 反斜杠转义\*literal asterisks\* 使用这种方式来输出*号 实际效果展示： *literal asterisks* 字体与字号字体，字号和颜色编辑如下代码 &lt;font face=&quot;黑体&quot;&gt;我是黑体字&lt;/font&gt; &lt;font face=&quot;微软雅黑&quot;&gt;我是微软雅黑&lt;/font&gt; &lt;font face=&quot;STCAIYUN&quot;&gt;我是华文彩云&lt;/font&gt; &lt;font color=#0099ff size=7 face=&quot;黑体&quot;&gt;color=#0099ff size=72 face=&quot;黑体&quot;&lt;/font&gt; &lt;font color=#00ffff size=72&gt;color=#00ffff&lt;/font&gt; &lt;font color=gray size=72&gt;color=gray&lt;/font&gt; Size：规定文本的尺寸大小。可能的值：从 1 到 7 的数字。浏览器默认值是 3 效果如下： 我是黑体字我是微软雅黑我是华文彩云color=#0099ff size=72 face=”黑体”color=#00ffffcolor=gray 字体颜色语法格式：&lt;font color=指定颜色的英文单词&gt;内容&lt;/font&gt;，例如 例如将字体颜色修改为红色： 代码为：&lt;font color=red&gt;内容&lt;/font&gt; 内容 背景色&lt;table&gt;&lt;tr&gt;&lt;td bgcolor=orange&gt;背景色是：orange&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt; 实际效果如下： 背景色是：orange 删除线文本两端加上两个~~即可 删除我 复选框列表在列表符号后面加上[]或者[x]代表选中或者未选中情况 - [x] C - [x] C++ - [x] Java - [x] Qt - [x] Android - [ ] C# - [ ] .NET 实际效果为 C C++ Java Qt Android C# .NET 生成目录-TOC插件首先下载和安装 Visual Studio Code 锚点网页中，锚点其实就是页内超链接，也就是链接本文档内部的某些元素，实现当前页面中的跳转。比如我这里写下一个锚点，点击回到目录，就能跳转到目录。 在目录中点击这一节，就能跳过来。还有下一节的注脚。这些根本上都是用锚点来实现的。 语法描述： 代码： 这里使用截图的方式展示，因为直接编写的话，hexo会检测报错(因为%没有对应的%结尾) emoji表情Github的Markdown语法支持添加emoji表情，输入不同的符号码（两个冒号包围的字符）可以显示出不同的表情。 比如:blush:，可以显示: :blush: 注释注释是写作者自己的标注记录，不被浏览器解析渲染。HTML 以 结尾的闭包定义注释（支持跨行），不在正文中显示。 Markdown 沿用 HTML Comment 注释格式： &lt;!-- This text will not appear in the browser window. --&gt; 折叠块代码如下： &lt;details&gt; &lt;summary&gt;点击展开答案&lt;/summary&gt; &lt;p&gt; 象&lt;/p&gt; &lt;/details&gt; 效果如下： 你和猪，打一种动物 点击展开答案 象 代码高亮与原来使用缩进来添加代码块的语法不同，这里使用 来包含多行代码： 三个 ``` 要独占一行。 指定图片大小Markdown 不支持指定图片的显示大小，不过可以通过直接插入标签来指定相关属性： &lt;img src=&quot;https://avatars2.githubusercontent.com/u/3265208?v=3&amp;s=100&quot; alt=&quot;GitHub&quot; title=&quot;GitHub,Social Coding&quot; width=&quot;50&quot; height=&quot;50&quot; /&gt; 效果如下： 在单元格里换行借助于 HTML 里的 实现。 示例代码： | Header1 | Header2 | |---------|----------------------------------| | item 1 | 1. one&lt;br /&gt;2. two&lt;br /&gt;3. three | 示例效果： Header1 Header2 item 1 1. one2. two3. three]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TotalCommander常用快捷键]]></title>
    <url>%2F2018%2F01%2F25%2FTotalCommander%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[基础知识参考资料可以先看下相关资料，这些资料对概念介绍等做的非常详细也写的非常好，因此这里不再赘述，本文主要是针对实际的运用。 官方资料 https://www.ghisler.com/官网上没有相对应的文档，需要额外去搜寻 优秀个人文章 TC学堂——最易读的Total Commander教程-强烈推荐通过该网站进行学习 Total Commander快捷键 实际操作常用目录这部分设置可以说是TC操作的精华，效率直接甩开windows资源管理器几条街。 快速添加ctrl+d，添加，然后a直接添加 常用目录高级配置通过自定义配置，可以自定义调整常用目录的名称、顺序等，后续的增删改查也在此页面进行。 ctrl+d，添加，进去之后按c进入常用目录配置对话框。在里面配置的时候，需要再最前面人为添加&amp;。 名称设置： &amp;1 test $b blog 命令参考设置： cd C:\Users\56810\blog\blog 直达组合键通过直达组合键，可以直接切换到指定目录下。 设置：alt+s 调出窗口，再按s进行配置。一共可以使用的个数是一般都是类似ctrl+alt+F1/F2..F11这么11个组合键 名称设置： &amp;1 desktop $b blog 命令参考设置： cd C:\Users\56810\blog\blog 配置完成之后，切换到桌面只需要：alt+s+1 切换磁盘分区Alt+F1调出分区选项之后，按D则进入D盘，E则进入E盘。 目录内容查看Alt+1 详细的列表信息 Alt+2 图形信息显示 Alt+3 目录树显示 多Tab标签操作ctrl+t 新建tab ctrl+上箭头 新建父目录tab ctrl+w 关闭标签 ctrl+shift+w 关闭所有非活动标签 ctrl+tab, ctrl+shift+tab 在同侧的tab间切换 改变tab排列顺序（包括在两个窗口间移动）：鼠标左键拖动。 自定义快捷键，直接切换到第N个标签可以在 wincmd.ini 中 [Shortcuts] 段，增加如下内容， C+1=cm_SrcActivateTab1 C+2=cm_SrcActivateTab2 C+3=cm_SrcActivateTab3 效果： ctrl+1～3 激活第 1～3 个标签，依次类推 压缩操作压缩： 选中文件之后，执行Alt+F5 查看压缩文件内容（不解压缩）： ctrl+右箭头或者直接回车 解压缩：Alt+F9 文件搜索Alt+F7 创建操作F7/Shift+F7 新建一个或多层文件夹。可以像DOS那样新建多层的目录，比如c:\file\a\b\c Shift+F4 新建文本文件，调用记事本编辑（自定义编辑器） 其他快捷键ctrl +e 进入资源管理器 alt + f1 选择第一个窗口的磁盘 +f2就是选择第二个窗口的磁盘 alt+下箭头 历史记录 alt+左箭头 返回上一个操作目录（历史目录） alt+右箭头 返回下一个操作目录（历史目录） ctrl+\ 返回到当前目录的根目录 Ctrl+Shift+Enter 查看当前的路径 shift+F10 右键 F3 文件内容预览 ctrl+M 批量重命名 Shift+F4 新建文本文件，调用记事本编辑（自定义编辑器） Ctrl+加号 全部选择同一类型的文件（例如压缩文件，目录文件） Ctrl+减号 全部取消同一类型的文件（例如压缩文件，目录文件）]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>TotalCommander</category>
      </categories>
      <tags>
        <tag>TC操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F23%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is my first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask author on GitHub. 前言之前使用阿里云结合wordpress的方式搭博客，但是自己维护起来不是太方便，云服务器一旦攻击，数据是个问题。之后在51cto和csdn上写，但是要受到平台的限制。最近发现github有博客功能(几年前就推出了，竟然现在才发现)，完美解决这些问题。github提供空间，用户自行选择博客框架，专注于内容，大部分人应该还是喜欢这种简约风主题。目前这个博客使用github-pages+Hexo来实现。 参考资料搭建 https://zhuanlan.zhihu.com/p/26625249 http://eleveneat.com/2015/04/24/Hexo-a-blog/ 进阶 主要参考官方资料 Hexo文档 https://hexo.io/zh-cn/docs/ Next主题使用手册 http://theme-next.iissnan.com/ 根据官方资料，按图索骥，基本上都能很好的把所有功能实现出来。使用问题可以随时沟通交流 markdown语法 关于markdown的使用，可以看我的这篇博文 Markdown语法 Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy Generate static files + Deploy to remote sites1$ hexo g -d More info: Deployment]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>个人博客</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
