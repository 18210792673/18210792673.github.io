<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[zabbix清理历史数据]]></title>
    <url>%2F2018%2F05%2F17%2Fzabbix%E6%B8%85%E7%90%86%E5%8E%86%E5%8F%B2%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[参考文献： 概述zabbix一般都是这几个表太大， history， history_uint，history_log zabbix里面的时间是用的时间戳方式记录，我们可以转换一下，然后根据时间戳来删除； 比如要删除2014年的1月1号以前的数据，先将标准时间转换为时间戳 # date +%s -d &quot;2018-05-15 00:00:01&quot; 1526313601 实际操作停止应用停止zabbix_server [root@dwb-dev1 ~]$ /etc/init.d/zabbix_server stop 清理数据启动mysql /bin/sh /usr/bin/mysqld_safe --datadir=/var/lib/mysql --socket=/var/lib/mysql/mysql.sock --pid-file=/var/run/mysqld/mysqld.pid --basedir=/usr --user=mysql /usr/libexec/mysqld --basedir=/usr --datadir=/var/lib/mysql --user=mysql --log-error=/var/log/mysqld.log --pid-file=/var/run/mysqld/mysqld.pid --socket=/var/lib/mysql/mysql.sock mysql&gt; DELETE FROM history_uint WHERE clock &lt; 1526313601; mysql&gt; delete from history where clock &lt; 1526313601; mysql&gt; optimize table history; mysql&gt; optimize table history_uint; 注：执行过优化命令之后可能会需要很长的一段时间，中间不要中断，否则容易丢失数据。 启动应用启动zabbix_server rider-contract-service app465app476]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维监控体系</category>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDS]]></title>
    <url>%2F2018%2F05%2F14%2FRDS%2F</url>
    <content type="text"><![CDATA[官方文档 基础知识阿里云关系型数据库（Relational Database Service，简称 RDS）是一种稳定可靠、可弹性伸缩的在线数据库服务。基于阿里云分布式文件系统和SSD盘高性能存储，RDS 支持 MySQL、SQL Server、PostgreSQL 和 PPAS（Postgre Plus Advanced Server，一种高度兼容 Oracle 的数据库）引擎，并且提供了容灾、备份、恢复、监控、迁移等方面的全套解决方案，彻底解决数据库运维的烦恼。 RDS的特点云数据库RDS作为一个公共的关系型数据库，高可用和高安全是其首要优势，其次才是高性能，因为没人会使用既不稳定又不安全的服务。RDS的优势主要体现在如下几点： RDS提供了主备双节点的实例，双节点可以在同一地域的不同可用区，MySQL实例的双节点还可以在不同地域，当主实例出现故障时可快速切换到备实例，保障了RDS的稳定性。 RDS在数据的存取上加入了中间层，所有请求都会经过中间层，而且有SQL注入的请求都会被中间层拦截掉。在底层数据写入上，RDS采用了最高安全级别的写入，保证在主机异常掉电的情况下数据不会出现丢失。以此来保障数据库的高安全性。 RDS源码团队持续对MySQL进行源码优化，在标准的基准测试中性能和稳定性上都是高于社区版本的。 关于这部分内容可以查看：对比ECS自建数据库与RDS性能时的注意事项 访问控制数据库账号 当用户创建实例后，RDS并不会为用户创建任何初始的数据库账号。 有如下两种方式来创建数据库帐号： 用户可以通过控制台或者API来创建普通数据库账号，并设置数据库级别的读写权限。 如果用户需要更细粒度的权限控制，比如表、视图，字段级别的权限，也可以通过控制台或者API先创建高权限数据库账号，并使用数据库客户端和高权限数据库账号来创建普通数据库账号。高权限数据库账号可以为普通数据库账号设置表级别的读写权限。 说明：通过高权限数据库账号创建的普通数据库账号，无法通过控制台或者API进行管理。 IP白名单 虽然RDS不支持ECS的安全组功能，但是RDS提供了IP白名单来实现网络安全访问控制。 默认情况下，RDS实例被设置为不允许任何IP访问，即127.0.0.1。 用户可以通过控制台的数据安全性模块或者API来添加IP白名单规则。IP白名单的更新无需重启RDS实例，因此不会影响用户的使用。 IP白名单可以设置多个分组，每个分组可配置1000个IP或IP段。 设置白名单后，只有以下服务器才能访问RDS实例： 白名单中 IP 地址所属的服务器 白名单中 ECS 安全组内的 ECS 实例 注意事项： 系统会给每个实例创建一个默认的default白名单分组，该白名单分组只能被修改或清空，但不能被删除。 对于新建的RDS实例，系统默认会将回送地址127.0.0.1添加到default白名单分组中，IP地址127.0.0.1代表禁止所有IP地址或IP段访问该RDS实例。所以，在您设置白名单时，需要先将127.0.0.1删除，然后再添加您允许访问该RDS实例的IP地址或IP段。 若将白名单设置为%或者0.0.0.0/0，代表允许任何IP访问RDS实例。该设置将极大降低数据库的安全性，如非必要请勿使用。 安全组 目前仅杭州、青岛、香港地域支持 ECS 安全组。 目前仅支持添加一个安全组。 对白名单中的 ECS 安全组的更新将实时应用到白名单中。 系统安全 RDS 处于多层防火墙的保护之下，可以有力地抗击各种恶意攻击，保证数据的安全。 RDS 服务器不允许直接登录，只开放特定的数据库服务所需要的端口。 RDS 服务器不允许主动向外发起连接，只能接受被动访问。 数据链路服务阿里云数据库提供全数据链路服务，包括 DNS、负载均衡、Proxy 等。因为 RDS 使用原生的 DB Engine，对数据库的操作高度类似，基本没有学习成本。 DNS DNS 模块提供域名到 IP 的动态解析功能，以便规避 RDS 实例 IP 地址改变带来的影响。在连接池中设置域名后，即使对应的IP地址发生了变化，仍然可以正常访问 RDS 实例。 例如，某 RDS 实例的域名为 test.rds.aliyun.com，对应的 IP 地址为 10.10.10.1。某程序连接池中设置为 test.rds.aliyun.com 或 10.10.10.1 都可以正常访问 RDS 实例。 一旦该 RDS 实例发生了可用区迁移或者版本升级后，IP 地址可能变为 10.10.10.2。如果程序连接池中设置的是域名 test.rds.aliyun.com，则仍然可以正常访问 RDS 实例。但是如果程序连接池中设置的是IP地址 10.10.10.1，就无法访问 RDS 实例了。 负载均衡 负载均衡 模块提供实例 IP 地址（包括内网 IP 和外网 IP），以便屏蔽物理服务器变化带来的影响。 例如，某 RDS 实例的内网 IP 地址为 10.1.1.1，对应的 Proxy 或者 DB Engine 运行在 192.168.0.1 上。在正常情况下，负载均衡 模块会将访问 10.1.1.1 的流量重定向到 192.168.0.1 上。当 192.168.0.1 发生了故障，处于热备状态的 192.168.0.2 接替了 192.168.0.1 的工作。此时 负载均衡 模块会将访问 10.1.1.1 的流量重定向到 192.168.0.2 上，RDS 实例仍旧正常提供服务。 Proxy Proxy 模块提供数据路由、流量探测和会话保持等功能。 数据路由功能：支持大数据场景下的分布式复杂查询聚合和相应的容量管理。 流量探测功能：降低 SQL 注入的风险，在必要情况下支持 SQL 日志的回溯。 会话保持功能：解决故障场景下的数据库连接中断问题。 高可用服务高可用服务由 Detection、Repair、Notice 等模块组成，主要保障数据链路服务的可用性，除此之外还负责处理数据库内部的异常。 另外，RDS 还通过迁移到支持多可用区的地域和采用适当的高可用策略，提升 RDS 的高可用服务。 Detection Detection 模块负责检测 DB Engine 的主节点和备节点是否提供了正常的服务。通过间隔为 8~10 秒的心跳信息，HA 节点可以轻易获得主节点的健康情况，结合备节点的健康情况和其它 HA 节点的心跳信息，Detection 模块可以排除网络抖动等异常引入的误判风险，在 30 秒内完成异常切换操作。 Repair Repair 模块负责维护 DB Engine 的主节点和备节点之间的复制关系，还会修复主节点或者备节点在日常运行中出现的错误。 例如： 主备复制异常断开的自动修复 主备节点表级别损坏的自动修复 主备节点 Crash 的现场保存和自动修复 Notice Notice 模块负责将主备节点的状态变动通知到 负载均衡 或者 Proxy，保证用户访问正确的节点。 例如：Detection 模块发现主节点异常，并通知 Repair 模块进行修复。Repair 模块进行了尝试后无法修复主节点，通知 Notice 进行流量切换。Notice 模块将切换请求转发至 负载均衡 或者Proxy，此时用户流量全部指向备节点。与此同时，Repair 在别的物理服务器上重建了新的备节点，并将变动同步给 Detection 模块。Detection 模块开始重新检测实例的健康状态。 多可用区 RDS在特定地域提供了多可用区部署的能力，也就是将RDS的主备实例分别部署于同一地域的不同可用区。相对于单可用区 RDS 实例，多可用区 RDS 实例可以承受更高级别的灾难。 目前多可用区 RDS 不额外收取任何费用，用户可以直接在已开通多可用区的地域购买多可用区 RDS 实例，也可以通过跨可用区迁移将单可用区 RDS 实例转化成多可用区 RDS 实例。 注意： 因为多可用区之间存在一定的网络延迟，因此多可用区 RDS 实例在采用半同步数据复制方案的时候，对于单个更新的响应时间会比单可用区实例长。这种情况最好通过提高并发量的方式来实现整体吞吐量的提高。 高可用策略 高可用策略是根据用户自身业务的特点，采用服务优先级和数据复制方式之间的不同组合，以组合出适合自身业务特点的高可用策略。 服务优先级有以下两个级别： RTO（Recovery Time Objective）优先：数据库应该尽快恢复服务，即可用时间最长。对于数据库在线时间要求比较高的用户应该使用 RTO 优先策略。 RPO（Recovery Point Objective）优先：数据库应该尽可能保障数据的可靠性，即数据丢失量最少。对于数据一致性要求比较高的用户应该使用 RPO 优先策略。 数据复制方式有以下三种方式： 异步复制（Async）：应用发起更新（含增加、删除、修改操作）请求，Master 完成相应操作后立即响应应用，Master 向 Slave 异步复制数据。因此异步复制方式下，Slave 不可用不影响主库上的操作，而 Master 不可用有较小概率会引起数据不一致。 强同步复制（Sync）：应用发起更新（含增加、删除、修改操作）请求，Master 完成操作后向 Slave 复制数据，Slave 接收到数据后向 Master 返回成功信息，Master 接到 Slave 的反馈后再响应应用。Master 向 Slave 复制数据是同步进行的，因此 Slave 不可用会影响 Master 上的操作，而 Master 不可用不会引起数据不一致。 半同步复制（Semi-Sync）：正常情况下数据复制方式采用强同步复制方式，当 Master 向 Slave 复制数据出现异常的时候（Slave 不可用或者双节点间的网络异常），Master 会暂停对应用的响应，直到复制方式超时退化成异步复制。如果允许应用在此时更新数据，则 Master 不可用会引起数据不一致。当双节点间的数据复制恢复正常（Slave 恢复或者网络恢复），异步复制会恢复成强同步复制。恢复成强同步复制的时间取决于半同步复制的实现方式，阿里云数据库 MySQL 5.5 版和 MySQL 5.6 版有所不同。 实际操作MySQL数据库版本阿里云上的MySQL提供基础版、高可用版和金融版三种版本 基础版一般就是用于个人学习、或开发测试时使用。目前基础版只提供MySQL 5.7版本，并且只提供单节点部署，性价比非常高。基础版采用计算节点与存储分离的实现方式，也就是说假如计算节点宕机，MySQL就不可用啦，但数据都存在云盘里面不会丢，数据一致性还是可以得到保证，不用担心数据丢失。可用性不高这是基础版的最大问题，反正只是用于不重要的场景，生产环境大家是不会选用基础版的。 高可用版顾名思义，为应用提供了数据库的高可用保障，也就是说至少要用双节点。RDS MySQL高可用版采用一主一备的经典高可用架构，采用基于binlog的数据复制技术维护数据库的可用性和数据一致性。同时，高可用版从性能上也可以保障业务生产环境的需求，配置上采用物理服务器部署，本地SSD硬盘，提供最佳性能，各方面表现均衡。 最高级的是金融版，针对像金融、证券、保险等行业的核心数据库，他们对数据安全性、可用性要求非常高。金融版采用三节点，实现一主两备的部署架构，通过binlog日志多副本多级别复制，确保数据的强一致性，可提供金融级的数据可靠性和跨机房容灾能力。 规格阿里云上MySQL有三种规格类型：通用型、独享型和独占型。 其中通用型和独享型都是在一台物理服务器上划分多个资源隔离的区域，为不同用户提供MySQL数据库实例。他们的不同点在于，通用型对于CPU和存储空间采用了复用的技术。当部署在同一台服务器上的所有MySQL 实例都很繁忙的情况下，有可能会出现实例间的CPU争抢，或存储的争抢；而独享型虽然也是多个数据库实例共享一台物理服务器，但资源隔离策略上确保每个用户都可以独享所分配到的CPU、内存、I/O、存储，不会出现多个实例发生资源争抢的情况。 最高级别的一种是独占型，是指一个MySQL实例独占一台服务器，会获得最好的性能，当然价格也最贵。最求极致性能但对价格不敏感的客户一般会在重要业务系统采用独占型实例。 使用流程通常，从新购实例到可以开始使用实例，需要完成如下操作： 使用限制高权限账号数据库连接注意：目前只支持同一个可用区的连接，不同可用区无法连接，如果需要跨越可用区，需要进行设置 目前RDS连接可以使用DMS连接或者第三方工具连接 跨可用区访问管理工具-DMSDMS 是一款访问管理云端数据库的Web服务，支持Redis、 MySQL、SQL Server、PostgreSQL和MongoDB等数据源。DMS提供了数据管理、对象管理、数据流转和实例管理四部分功能。DMS使用也非常简单： 数据迁移-DTS相关资料： 文档中心 帮助中心 ECS自建数据库迁移到RDS DTS概述数据传输(Data Transmission)服务DTS是阿里云提供的一种支持RDBMS(关系型数据库)、NoSQL、OLAP等多种数据源之间数据交互的数据服务。它提供了数据迁移、实时数据订阅及数据实时同步等多种数据传输能力。通过数据传输可实现不停服数据迁移、数据异地灾备、跨境数据同步、缓存更新策略等多种业务应用场景，助您构建安全、可扩展、高可用的数据架构。 数据传输服务DTS的目标是帮用户将复杂的数据交互工作承担下来，让用户可以专注于上层的业务开发，数据传输服务承诺99.95%的链路稳定性。 数据传输服务DTS支持多种数据源类型，例如： 关系型数据库：Oracle、MySQL、SQLServer、PostgreSQL NoSQL: Redis OLAP: 分析型数据库AnalyticDB 迁移服务主要帮助用户把数据从本地数据库迁移到阿里云数据库，或者把阿里云数据库的一个实例迁移到另一实例中。阿里云数据库提供了数据传输服务DTS（Data Transfer Service）工具，方便用户快速的迁移数据库。 DTS是一个云上的数据传输服务，能快速的将本地数据库或者RDS中的实例迁移到另一个RDS实例中。关于DTS简介，请参见DTS产品概述。 DTS提供了三种迁移模式，分别为结构迁移、全量迁移和增量迁移： 结构迁移：DTS会将迁移对象的结构定义迁移到目标实例，目前支持结构迁移的对象有表、视图、触发器、存储过程和存储函数。 全量迁移：DTS会将源数据库迁移对象已有数据全部迁移到目标实例中。 注意：在全量迁移过程中，为了保证数据一致性，无主键的非事务表会被锁定。锁定期间这些表无法写入，锁定时长依赖于这些表的数据量大小。在这些无主键非事务表迁移完成后，锁才会释放。 增量迁移：DTS会将迁移过程中数据变更同步到目标实例。 , 注意：如果迁移期间进行了DDL操作，这些结构变更不会同步到目标实例。 源及目标数据迁移支持的源实例类型包括: (1) RDS实例 (2) 本地自建数据库 (3) ECS自建数据库 数据迁移支持的目标实例包括： (1) RDS实例 (2) ECS自建数据库 (3) Redis实例 Mysql迁移限制对于本地 MySQL-&gt;RDS for MySQL 的数据迁移，DTS 支持结构迁移、全量数据迁移及增量数据迁移，各迁移类型的功能及限制如下： 迁移过程中，不支持 DDL 操作。 结构迁移不支持 event 的迁移。 如果使用了对象名映射功能后，依赖这个对象的其他对象可能迁移失败。 当选择增量迁移时，源端的本地 MySQL 实例需要按照要求开启 binlog。 当选择增量迁移时，源库的 binlog_format 需要设置为 row。 当选择增量迁移且源 MySQL 实例如果为 5.6 或以上版本时，它的 binlog_row_image 必须为 full。 数据同步数据实时同步功能旨在帮助用户实现两个数据源之间的数据实时同步。通过数据实时同步功能可实现数据异地灾备、本地数据灾备、跨境数据同步及在线离线数据打通(OLTP-&gt;OLAP数据同步)等多种业务场景。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>公有云产品</category>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>RDS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP请求]]></title>
    <url>%2F2018%2F05%2F13%2FHTTP%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[本文转载自：HTTP深入浅出 http请求 概述HTTP(HyperText Transfer Protocol)是一套计算机通过网络进行通信的规则。 计算机专家设计出HTTP，使HTTP客户（如Web浏览器）能够从HTTP服务器(Web服务器)请求信息和服务，HTTP目前协议的版本是1.1。 HTTP是一种无状态的协议，无状态是指Web浏览器和Web服务器之间不需要建立持久的连接，这意味着当一个客户端向服务器端发出请求，然后Web服务器返回响应(response)，连接就被关闭了，在服务器端不保留连接的有关信息.HTTP遵循请求(Request)/应答(Response)模型。Web浏览器向Web服务器发送请求，Web服务器处理请求并返回适当的应答。所有HTTP连接都被构造成一套请求和应答。 HTTP使用内容类型，是指Web服务器向Web浏览器返回的文件都有与之相关的类型。所有这些类型在MIME Internet邮件协议上模型化，即Web服务器告诉Web浏览器该文件所具有的种类，是HTML文档、GIF格式图像、声音文件还是独立的应用程序。大多数Web浏览器都拥有一系列的可配置的辅助应用程序，它们告诉浏览器应该如何处理Web服务器发送过来的各种内容类型。 通信过程在一次完整的HTTP通信过程中，Web浏览器与Web服务器之间将完成下列7个步骤： 1. 建立TCP连接 在HTTP工作开始之前，Web浏览器首先要通过网络与Web服务器建立连接，该连接是通过TCP来完成的，该协议与IP协议共同构建Internet，即著名的TCP/IP协议族，因此Internet又被称作是TCP/IP网络。HTTP是比TCP更高层次的应用层协议，根据规则，只有低层协议建立之后才能，才能进行更层协议的连接，因此，首先要建立TCP连接，一般TCP连接的端口号是80 2. Web浏览器向Web服务器发送请求命令 一旦建立了TCP连接，Web浏览器就会向Web服务器发送请求命令 例如：GET/sample/hello.jsp HTTP/1.1 3. Web浏览器发送请求头信息 浏览器发送其请求命令之后，还要以头信息的形式向Web服务器发送一些别的信息，之后浏览器发送了一空白行来通知服务器，它已经结束了该头信息的发送。 4. Web服务器应答 客户机向服务器发出请求后，服务器会客户机回送应答， HTTP/1.1 200 OK 应答的第一部分是协议的版本号和应答状态码 5. Web服务器发送应答头信息 正如客户端会随同请求发送关于自身的信息一样，服务器也会随同应答向用户发送关于它自己的数据及被请求的文档。 6. Web服务器向浏览器发送数据 Web服务器向浏览器发送头信息后，它会发送一个空白行来表示头信息的发送到此为结束，接着，它就以Content-Type应答头信息所描述的格式发送用户所请求的实际数据 7. Web服务器关闭TCP连接 一般情况下，一旦Web服务器向浏览器发送了请求数据，它就要关闭TCP连接，然后如果浏览器或者服务器在其头信息加入了这行代码Connection:keep-alive 添加之后，TCP连接在发送后将仍然保持打开状态，于是，浏览器可以继续通过相同的连接发送请求。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带宽。 HTTP请求HTTP请求格式当浏览器向Web服务器发出请求时，它向服务器传递了一个数据块，也就是请求信息，HTTP请求信息由3部分组成： 请求方法URI协议/版本 请求头(Request Header) 请求正文 下面是一个HTTP请求的例子： GET/sample.jspHTTP/1.1 Accept:image/gif.image/jpeg,*/* Accept-Language:zh-cn Connection:Keep-Alive Host:localhost User-Agent:Mozila/4.0(compatible;MSIE5.01;Window NT5.0) Accept-Encoding:gzip,deflate username=jinqiao&amp;password=1234 说明： （1）请求方法URI协议/版本 请求的第一行是“方法URL议/版本”：GET/sample.jsp HTTP/1.1 以上代码中“GET”代表请求方法，“/sample.jsp”表示URI，“HTTP/1.1代表协议和协议的版本。 根据HTTP标准，HTTP请求可以使用多种请求方法。例如：HTTP1.1支持7种请求方法：GET、POST、HEAD、OPTIONS、PUT、DELETE和TARCE。在Internet应用中，最常用的方法是GET和POST。 URL完整地指定了要访问的网络资源，通常只要给出相对于服务器的根目录的相对目录即可，因此总是以“/”开头，最后，协议版本声明了通信过程中使用HTTP的版本。 （2）请求头(Request Header) 请求头包含许多有关的客户端环境和请求正文的有用信息。例如，请求头可以声明浏览器所用的语言，请求正文的长度等。 Accept:image/gif.image/jpeg.*/* Accept-Language:zh-cn Connection:Keep-Alive Host:localhost User-Agent:Mozila/4.0(compatible:MSIE5.01:Windows NT5.0) Accept-Encoding:gzip,deflate. （3）请求正文 请求头和请求正文之间是一个空行，这个行非常重要，它表示请求头已经结束，接下来的是请求正文。 请求正文中可以包含客户提交的查询字符串信息：username=jinqiao&amp;password=1234 在以上的例子的HTTP请求中，请求的正文只有一行内容。当然，在实际应用中，HTTP请求正文可以包含更多的内容。 HTTP请求方法我这里只讨论GET方法与POST方法 GET方法 GET方法是默认的HTTP请求方法，我们日常用GET方法来提交表单数据，然而用GET方法提交的表单数据只经过了简单的编码，同时它将作为URL的一部分向Web服务器发送，因此，如果使用GET方法来提交表单数据就存在着安全隐患上。例如Http://127.0.0.1/login.jsp?Name=zhangshi&amp;Age=30&amp;Submit=%cc%E+%BD%BB从上面的URL请求中，很容易就可以辩认出表单提交的内容。（？之后的内容）另外由于GET方法提交的数据是作为URL请求的一部分所以提交的数据量不能太大 POST方法 POST方法是GET方法的一个替代方法，它主要是向Web服务器提交表单数据，尤其是大批量的数据。POST方法克服了GET方法的一些缺点。通过POST方法提交表单数据时，数据不是作为URL请求的一部分而是作为标准数据传送给Web服务器，这就克服了GET方法中的信息无法保密和数据量太小的缺点。因此，出于安全的考虑以及对用户隐私的尊重，通常表单提交时采用POST方法。 从编程的角度来讲，如果用户通过GET方法提交数据，则数据存放在QUERY＿STRING环境变量中，而POST方法提交的数据则可以从标准输入流中获取。 HTTP响应HTTP应答与HTTP请求相似，HTTP响应也由3个部分构成，分别是： 协议状态版本代码描述 响应头(Response Header) 响应正文 下面是一个HTTP响应的例子： HTTP/1.1 200 OK Server:Apache Tomcat/5.0.12 Date:Mon,6Oct2003 13:23:42 GMT Content-Length:112 &lt;html&gt; &lt;head&gt; &lt;title&gt;HTTP响应示例&lt;title&gt; &lt;/head&gt; &lt;body&gt; Hello HTTP! &lt;/body&gt; &lt;/html&gt; （1）协议状态版本代码描述 协议状态代码描述HTTP响应的第一行类似于HTTP请求的第一行，它表示通信所用的协议是HTTP1.1服务器已经成功的处理了客户端发出的请求（200表示成功）: HTTP/1.1 200 OK （2）响应头 响应头(Response Header)响应头也和请求头一样包含许多有用的信息，例如服务器类型、日期时间、内容类型和长度等： Server:Apache Tomcat/5.0.12 Date:Mon,6Oct2003 13:13:33 GMT Content-Type:text/html Last-Moified:Mon,6 Oct 2003 13:23:42 GMT Content-Length:112 （3）响应正文 响应正文响应正文就是服务器返回的HTML页面： &lt;html&gt; &lt;head&gt; &lt;title&gt;HTTP响应示例&lt;title&gt; &lt;/head&gt; &lt;body&gt; Hello HTTP! &lt;/body&gt; &lt;/html&gt; 响应头和正文之间也必须用空行分隔。 HTTP应答码 HTTP应答码也称为状态码，它反映了Web服务器处理HTTP请求状态。HTTP应答码由3位数字构成，其中首位数字定义了应答码的类型： 1XX－信息类(Information),表示收到Web浏览器请求，正在进一步的处理中 2XX－成功类（Successful）,表示用户请求被正确接收，理解和处理例如：200 OK 3XX-重定向类(Redirection),表示请求没有成功，客户必须采取进一步的动作。 4XX-客户端错误(Client Error)，表示客户端提交的请求有错误 例如：404 NOT Found，意味着请求中所引用的文档不存在。 5XX-服务器错误(Server Error)表示服务器不能完成对请求的处理：如 500 常见请求方法 GET 通过请求URI得到资源 POST用于添加新的内容 PUT用于修改某个内容 DELETE删除某个内容 CONNECT,用于代理进行传输，如使用SSL OPTIONS询问可以执行哪些方法 PATCH,部分文档更改 PROPFIND, (wedav)查看属性 PROPPATCH, (wedav)设置属性 MKCOL, (wedav)创建集合（文件夹） COPY, (wedav)拷贝 MOVE, (wedav)移动 LOCK, (wedav)加锁 UNLOCK (wedav)解锁 TRACE用于远程诊断服务器 HEAD类似于GET, 但是不返回body信息，用于检查对象是否存在，以及得到对象的元数据]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识</category>
        <category>HTTP请求</category>
      </categories>
      <tags>
        <tag>HTTP请求</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper常用命令]]></title>
    <url>%2F2018%2F05%2F02%2FZookeeper%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[zk客户端命令zookeeper命令行工具类似于Linux的shell环境，使用它我们可以简单的对zookeeper进行访问、数据创建、数据修改等操作 语法： $ sh zkCli.sh -server 127.0.0.1:2181 一些简单操作： 显示根目录下、文件： ls / 使用 ls 命令来查看当前 ZooKeeper 中所包含的内容 显示根目录下、文件： ls2 / 查看当前节点数据并能看到更新次数等数据 创建文件，并设置初始内容： create /zk “test” 创建一个新的 znode节点“ zk ”以及与它关联的字符串 获取文件内容： get /zk 确认 znode 是否包含我们所创建的字符串 修改文件内容： set /zk “zkbak” 对 zk 所关联的字符串进行设置 删除文件： delete /zk 将刚才创建的 znode 删除 退出客户端： quit 帮助命令： help 四字命令： ZooKeeper 支持某些特定的四字命令字母与其的交互。它们大多是查询命令，用来获取 ZooKeeper 服务的当前状态及相关信息。用户在客户端可以通过 telnet 或 nc 向 ZooKeeper 提交相应的命令 echo stat|nc 127.0.0.1 2181 来查看哪个节点被选择作为follower或者leader echo ruok|nc 127.0.0.1 2181 测试是否启动了该Server，若回复imok表示已经启动。 echo dump| nc 127.0.0.1 2181 ,列出未经处理的会话和临时节点。 echo kill | nc 127.0.0.1 2181 ,关掉server echo conf | nc 127.0.0.1 2181 ,输出相关服务配置的详细信息。 echo cons | nc 127.0.0.1 2181 ,列出所有连接到服务器的客户端的完全的连接 / 会话的详细信息。 echo envi |nc 127.0.0.1 2181 ,输出关于服务环境的详细信息（区别于 conf 命令）。 echo reqs | nc 127.0.0.1 2181 ,列出未经处理的请求。 echo wchs | nc 127.0.0.1 2181 ,列出服务器 watch 的详细信息。 echo wchc | nc 127.0.0.1 2181 ,通过 session 列出服务器 watch 的详细信息，它的输出是一个与 watch 相关的会话的列表。 echo wchp | nc 127.0.0.1 2181 ,通过路径列出服务器 watch 的详细信息。它输出一个与 session 相关的路径。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL普通用户无法本地登录的解决方法及MySQL的用户认证算法]]></title>
    <url>%2F2018%2F05%2F02%2FMySQL%E6%99%AE%E9%80%9A%E7%94%A8%E6%88%B7%E6%97%A0%E6%B3%95%E6%9C%AC%E5%9C%B0%E7%99%BB%E5%BD%95%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%E5%8F%8AMySQL%E7%9A%84%E7%94%A8%E6%88%B7%E8%AE%A4%E8%AF%81%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[参考文献： MySQL普通用户无法本地登录的解决方法及MySQL的用户认证算法 问题在启动cachecloud项目的时候，发现日志中出现大量的连接数据库报错 我的授权命令为： mysql&gt; grant all privileges on cache_cloud.* to &apos;cachecloud&apos;@&apos;%&apos; identified by &apos;Cache_cloud123&apos;; Query OK, 0 rows affected (0.00 sec) 这样的配置，按道理来说，是不应该出现连不上的（%代表任意的主机来源，并且已经排查了防火墙等因素） 在本地登录发现发现存在如下问题： 当输入之前设置的密码时，将会一直提示：“ERROR 1045 (28000): Access denied”，而当我们不输入密码，也就是说输入密码为空时则能正常进入数据库。 我们使用USER()和CURRENT_USER()两个函数查看所使用的用户。 mysql&gt; SELECT USER(), CURRENT_USER(); +----------------------+----------------+ | USER() | CURRENT_USER() | +----------------------+----------------+ | cachecloud@localhost | @localhost | +----------------------+----------------+ 1 row in set (0.00 sec) mysql&gt; USER()函数返回你在客户端登陆时指定的用户名和主机名。 CURRENT_USER()函数返回的是MySQL使用授权表中的哪个用户来认证你的登录请求。 这里发现，之前设置的授权规则并没有生效，是数据库使用的是’’@’localhost’这个来源信息来进行登录认证，而’’@’localhost’这个匿名用户是没有密码的，因此我输入空密码登录成功了。但是登录后，所对应的用户的匿名用户。 一般在MySQL在安装完毕后，我们使用mysql_install_db这个脚本生成授权表，会默认创建’’@’localhost’这个匿名用户。正是因为这个匿名用户，影响了其他用户从本地登录的认证。 原因那么MySQL是如何进行用户身份认证呢？ MySQL的简要认证算法如下： 当用户从客户端请求登陆时，MySQL将授权表中的条目与客户端所提供的条目进行比较，包括用户的用户名，密码和主机。 授权表中的Host字段是可以使用通配符作为模式进行匹配的，如test.example.com, %.example.com, %.com和%都可以匹配test.example.com这个主机。 授权表中的User字段不允许使用模式匹配，但是可以有一个空字符的用户名代表匿名用户，并且空字符串可以匹配所有的用户名，就像通配符一样。 当user表中的Host和User有多个值可以匹配客户端提供的主机和用户名时，MySQL将user表读入内存，并且按照一定规则排序，按照排序规则读取到的第一个匹配客户端用户名和主机名的条目对客户端进行身份验证。 排序规则： 对于Host字段，按照匹配的精确程度进行排序，越精确的排序越前，例如当匹配test.example.com这个主机时, %.example.com比%.com更精确，而test.example.com比%.example.com更精确。 对于User字段，非空的字符串用户名比空字符串匹配的用户名排序更靠前。 User和Host字段都有多个匹配值，MySQL使用主机名排序最前的条目，在主机名字段相同时再选取用户名排序更前的条目。 因此，如果User和Host字段都有多个匹配值，主机名最精确匹配的条目被用户对用户进行认证。 了解了这个规则之后，我们就知道为什么cachecloud登录失败了。 在使用该用户进行本机登录的时候，mysql中有2个匹配条目 ‘cachecloud’@’%’ ‘’@’localhost’ 匿名用户能够匹配的原因上面说过，空字符串可以匹配所有的用户名，就像通配符一样。 根据MySQL认证时的排序规则，第一个条目的用户名排序更前，第二个条目的主机名更精确，排序更前。 而MySQL会优先使用主机名排序第一的条目进行身份认证，因此’’@’localhost’被用户对客户端进行认证。因此，只有使用匿名用户的空密码才能登录进数据库。就会出现刚才上面的情况了。 解决删除匿名用户【仅仅是为了安全也有这个必要】 为什么root用户不会受影响，而只有普通用户不能从本地登录？ 因为mysql_install_db脚本会在授权表中生成’root’@’localhost’这个账户。同样的，使用root登录MySQL时，’root’@’localhost’和’’@’localhost’都能匹配登录的账户，但是根据排序规则，主机名相同，而用户名非空字符串优先，因此’roo’@’localhost’这个条目的排序更靠前。使用root本地登录是不会被匿名用户遮盖。 [root@qa1-common004 ~]# mysql -u root -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 942 Server version: 5.6.40 MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. mysql&gt; mysql&gt; select user,host from mysql.user; +------------+---------------------------+ | user | host | +------------+---------------------------+ | cachecloud | % | | root | 127.0.0.1 | | root | ::1 | | | localhost | | root | localhost | | | qa1-common004.ecs.east1-b | | root | qa1-common004.ecs.east1-b | +------------+---------------------------+ 7 rows in set (0.00 sec) mysql&gt; delete from mysql.user where user=&apos;&apos;; Query OK, 2 rows affected (0.00 sec) mysql&gt; select user,host from mysql.user; +------------+---------------------------+ | user | host | +------------+---------------------------+ | cachecloud | % | | root | 127.0.0.1 | | root | ::1 | | root | localhost | | root | qa1-common004.ecs.east1-b | +------------+---------------------------+ 5 rows in set (0.00 sec) mysql&gt; flush privileges; Query OK, 0 rows affected (0.00 sec) mysql&gt; exit 退出之后再次登录，问题得到解决。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>数据库</category>
        <category>MySQL</category>
        <category>MySQL问题</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql密码设置修改及恢复]]></title>
    <url>%2F2018%2F05%2F02%2FMysql%E5%AF%86%E7%A0%81%E8%AE%BE%E7%BD%AE%E4%BF%AE%E6%94%B9%E5%8F%8A%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[mysql 5.6版本mysql 5.6版本中，安装完毕之后不会设置初始密码，服务启动之后，直接输入mysql即可进入数据库 安装之后的第一次密码设置 # mysql 在终端直接输入mysql进入数据库 mysql&gt; set password = password(&apos;Mysql_password123&apos;); mysql&gt; flush privileges; 修改密码 忘记密码之后的恢复]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>数据库</category>
        <category>MySQL</category>
        <category>Mysql密码</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下JDK的安装配置]]></title>
    <url>%2F2018%2F04%2F28%2FLinux%E4%B8%8BJDK%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在Linux中JDK的配置主要分为以下几个步骤： 下载 解压 软链接 配置系统/用户环境变量 下载：官方下载链接：下载 JAVA环境的配置主要分为两种，一种是由root用户操作，针对所有用户全局生效的配置，一种是由具体普通用户操作，仅针对该用户生效的配置 因此，以下的配置根据实际需求。 全局生效-管理员权限操作解压+软链接 # tar -zxvf jdk-7u75-linux-x64.tar.gz -C /usr/local/ # cd /usr/local/ # ln -s /usr/local/jdk1.7.0_75/ /usr/local/JDK 创建软链接目的：灵活的版本升级切换 配置系统环境变量： # vim /etc/profile 在文件末尾添加以下内容 export JAVA_HOME=/usr/local/JDK export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib export PATH=$JAVA_HOME/bin:$JRE_HOME/bin::$CLASSPATH:$PATH # source /etc/profile 用户局部生效-用户环境变量**注意提示符的变化，这里以appdev用户为例 解压+软链接 $ tar -zxvf jdk-7u75-linux-x64.tar.gz $ ln -s /home/appdev/jdk1.7.0_75/ /home/appdev/JDK 创建软链接目的：灵活的版本升级切换 配置系统环境变量： $ vim .bash_profile 在文件末尾添加以下内容 export JAVA_HOME=/home/appdev/JDK export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib export PATH=$JAVA_HOME/bin:$JRE_HOME/bin::$CLASSPATH:$PATH $ source .bash_profile]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>基础环境配置</category>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>JDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cachecloud-Redis云平台]]></title>
    <url>%2F2018%2F04%2F28%2FCachecloud%2F</url>
    <content type="text"><![CDATA[Cachecloud介绍有关cachecloud的一些基础知识，官方都有非常详细的文档，这里不再花费篇幅进行复述，下面是相关的资料链接，请自行查看。 github官网： https://github.com/sohutv/cachecloud Wiki: https://github.com/sohutv/cachecloud/wiki 博客： https://cachecloud.github.io/ 官方视频： http://my.tv.sohu.com/pl/9100280/index.shtml 简介： CacheCloud提供一个Redis云管理平台：实现多种类型(Redis Standalone、Redis Sentinel、Redis Cluster)自动部署、解决Redis实例碎片化现象、提供完善统计、监控、运维功能、减少开发人员的运维成本和误操作，提高机器的利用率，提供灵活的伸缩性，提供方便的接入客户端。 提供的功能： 监控统计： 提供了机器、应用、实例下各个维度数据的监控和统计界面。 一键开启： Redis Standalone、Redis Sentinel、Redis Cluster三种类型的应用，无需手动配置初始化。 Failover： 支持哨兵,集群的高可用模式。 伸缩： 提供完善的垂直和水平在线伸缩功能。 完善运维： 提供自动运维和简化运维操作功能，避免纯手工运维出错。 方便的客户端 方便快捷的客户端接入。 元数据管理： 提供机器、应用、实例、用户信息管理。 流程化： 提供申请，运维，伸缩，修改等完善的处理流程 一键导入： 一键导入已经存在Redis 须知： Redis集群、redis哨兵集群、Redis单实例等在CacheCloud中都是以应用的形式存在，一个应用对应一个appid 一个redis集群是一个应用，分配一个appid（不管其中有几个节点） 一个哨兵集群是一个应用，分配一个appid（不管其中有几个主从节点和哨兵节点） 一个单实例是一个应用，分配一个appid 如何使用： 我们在平台上的执行任何操作都需要**账号**，创建的单节点、哨兵、集群等都是以用户申请的应用形式存在的。普通用户的主要工单有 注册用户申请 应用申请 应用扩容 应用配置修改 管理员的界面可操作的选项较多，此处不做详细说明。 客户端如何连接： 客户端在第一次启动的时候去CacheCloud通过appId拿到Redis的节点信息，之后不会与CacheCloud打交道了。 流程图如下所示： 安装部署这里只说单机环境，高可用环境将在下面章节说明：CacheCloud高可用架构 环境要求： JDK 7+ Maven 3+ MySQL 5.5+ Redis 3+ 基础环境JDK+MavenJDK： 步骤： 下载 解压 软链接 配置系统环境变量 操作如下： [root@qa1-common004 local]# java -version java version &quot;1.8.0_77&quot; Java(TM) SE Runtime Environment (build 1.8.0_77-b03) Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode) [root@qa1-common004 local]# which java /usr/local/jdk/bin/java [root@qa1-common004 local]# ll /usr/local/jdk lrwxrwxrwx 1 root root 11 Apr 17 17:01 /usr/local/jdk -&gt; jdk1.8.0_77 这里我使用的是1.8版本。 详细操作请看文章：Linux下JDK的安装配置 Maven 步骤： 下载 下载链接 解压 软链接 配置系统环境变量 操作如下： # wget http://www-eu.apache.org/dist/maven/maven-3/3.5.3/binaries/apache-maven-3.5.3-bin.tar.gz # tar -zxvf apache-maven-3.5.3-bin.tar.gz -C /usr/local/ # cd /usr/local/ # ln -s /usr/local/apache-maven-3.5.3/ /usr/local/maven # vim /etc/profile 在文件末尾添加以下内容，保存退出 M3_HOME=/usr/local/maven export PATH=$M3_HOME/bin:$PATH [root@host-192-168-8-37 ~]# source /etc/profile 下载CacheCloud项目# yum -y install git # git clone https://github.com/sohutv/cachecloud.git # ls cachecloud/ cachecloud-open-client cachecloud-open-common cachecloud-open-web LICENSE pom.xml README.md script MySQL这里安装mysql5.7版本 配置yum源并安装 centos6.8 【6.8安装5.6版本，安装5.7时涉及依赖关系过多】 # wget http://repo.mysql.com/mysql-community-release-el6-5.noarch.rpm # rpm -ivh mysql-community-release-el6-5.noarch.rpm # yum -y install mysql-community-server centos 7.x 【5.7版本】 # wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm # rpm -ivh mysql57-community-release-el7-11.noarch.rpm # yum -y install mysql-server 修改mysql配置文件 # vim /etc/my.cnf [mysqld] character-set-server=utf8 启动 # /etc/init.d/mysqld start 数据库配置创建数据库 mysql&gt; create database cache_cloud default charset utf8; Query OK, 1 row affected (0.00 sec) 创建cachecloud用户 mysql&gt; grant all privileges on cache_cloud.* to &apos;cachecloud&apos;@&apos;%&apos; identified by &apos;Cache_cloud123&apos;; Query OK, 0 rows affected (0.00 sec) 导入初始化数据注意，这里已经不是在数据库中了 [root@qa1-common004 script]# pwd /root/software/cachecloud/script [root@qa1-common004 script]# mysql -u root -p cache_cloud &lt; cachecloud.sql Enter password: 修改cachecloud配置数据库设置 [root@qa1-common004 swap]# pwd /root/software/cachecloud/cachecloud-open-web/src/main/swap [root@qa1-common004 swap]# cat online.properties cachecloud.db.url = jdbc:mysql://172.24.64.132:3306/cache_cloud?useUnicode=true&amp;amp;characterEncoding=UTF-8 cachecloud.db.user = cachecloud cachecloud.db.password = Cache_cloud123 cachecloud.maxPoolSize = 20 isClustered = true isDebug = false spring-file=classpath:spring/spring-online.xml log_base=/opt/cachecloud-web/logs web.port=8585 log.level=WARN 注意这里需要提前在数据库中删除匿名用户 开启机器监控功能 # pwd /root/software/cachecloud/cachecloud-open-web/src/main/java/com/sohu/cache/schedule/jobs # vim ServerJob.java 将稳中的注释去掉，修改之后的文件如下所示： 如果公司已经有完善的监控，那么不建议开启机器监控，能够一定程度上减小数据库的压力。 cachecloud构建及启动项目构建 在cachecloud的根目录下执行以下maven命令，该命令会进行项目的构建 [root@qa1-common004 cachecloud]# pwd /root/software/cachecloud [root@qa1-common004 cachecloud]# [root@host-192-168-8-37 cachecloud]# mvn clean compile install -Ponline [root@host-192-168-8-37 cachecloud]# cd script/ [root@host-192-168-8-37 script]# sh deploy.sh /root/software/ 启动 # sh /opt/cachecloud-web/start.sh 启动成功之后的web页面如下图所示： 实际使用redis数据节点初始化执行初始化脚本 sh cachecloud-init.sh cachecloud 添加主机redis应用模板配置注意：在部署redis相关应用之前，一定要先进行模板的配置，因为默认配置下，redis的守护进程模式为关系，保护模式也是开启的 修改配置： 配置名称：daemonize；配置值：yes;配置说明：是否守护进程 新增配置： 配置名称：protected-mode；配置值：no;配置说明：保护模式 配置名称：bind；配置值：0.0.0.0;配置说明：绑定ip 注意：哨兵的配置模板中只需要新增protected-mode参数即可。 部署哨兵应用导入已经存在的redis实例redis哨兵cachecloud使用优化哨兵复用问题： 使用cachecloud部署哨兵集群时，每次生成的哨兵节点都是不一样的，这种情况，会造成一定的资源浪费（每一对主从都需要至少3个哨兵节点，对服务器的端口资源、内存资源等都会造成一定的浪费） 因此，我们采取复用哨兵节点的方式来实现redis的主从高可用 实现步骤： 哨兵模板中设置端口，将端口固定，为了后续的配置方便 手动创建主从节点 哨兵中添加新建的主从节点 在cachecloud平台上导入这个应用 相当于其实是导入redis哨兵的方式 哨兵配置： redis-cli -p 6388 sentinel monitor master-test-qa1 172.24.64.134 6385 2&amp;&amp;redis-cli -p 6388 SENTINEL set master-test-qa1 auth-pass redis123&amp;&amp;redis-cli -p 6388 sentinel set master-test-qa1 down-after-milliseconds 20000&amp;&amp;redis-cli -p 6388 sentinel set master-test-qa1 failover-timeout 60000 关闭master节点的持久化 AOF持久化：appendonly 配置为no cachecloud坑- CacheCloud安装部署及使用常见问题及注意事项 数据库版本问题： 如果使用mysql5.7，则需要进行针对sql文件做一些设置（only_full_group_by模式设置等） cachecloud平台乱码问题： 需要修改online.properties配置文件中的连接串（使用这种方式：jdbc:mysql://127.0.0.1:3306/cache_cloud?useUnicode=true&amp;characterEncoding=UTF-8） cachecloud后台配置模板：默认配置下，redis没有开启守护进程运行方式、开启了保护模式等，需要做一些配置修改之后才可以正常启动 机器监控数据无法展示问题：除了在程序文件中去掉相应的代码注释，还需要将cachecloud-open-web/nmon下指定系统版本的nmon文件放到/opt/cachecloud/soft/目录下 密码配置问题：密码配置栏中，输入密码之后，还需要点击更新才可以生效 Jedis支持redis版本问题：Jedis暂时无法稳定支持redis4.x版本，因此涉及到的集群水平扩容等功能是无法实现的（集群创建等还是可以支持的），因此我们建议使用3版本，后续关注Jedis的版本发布情况。 应用导入时提示：节点不是存活的 cachecloud节点上需要安装redis，因为他会使用redis-cli 去ping指定的节点，没有返回pong时，则会报错 哨兵导入问题： 如果哨兵是复用的，也就是说一组哨兵节点监听了多对主从节点，那么在导入的时候回出现问题，目前导入功能只支持一个mastername 主机名设置问题： 注意hosts文件需要进行配置]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>数据库</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>cachecloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile编写]]></title>
    <url>%2F2018%2F04%2F28%2FDockerfile%E7%BC%96%E5%86%99%2F</url>
    <content type="text"><![CDATA[在docker中，创建镜像的方法主要有三种： 基于已有镜像的容器创建 基于本地模板导入 基于dockerfile创建 dockerfile是一个文本格式的配置文件，用户可以使用dockerfile来快速创建自定义镜像。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>虚拟化</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker基础知识]]></title>
    <url>%2F2018%2F04%2F28%2FDocker%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[基础知识]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>虚拟化</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo]]></title>
    <url>%2F2018%2F04%2F26%2FDubbo%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>分布式</category>
        <category>Dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RPC框架]]></title>
    <url>%2F2018%2F04%2F26%2FRPC%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[参考文献： 知乎 基础知识远程过程调用（英语：Remote Procedure Call，缩写为 RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用，例：Java RMI。 RPC就是要像调用本地的函数一样去调远程函数。 传统的本地调用方式： 代码文件中定义和调用 或者import的方式导入模块之后再进行调用 而远程调用，指的是，我们要调用执行的函数是在远程的机器上的，]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>分布式</category>
        <category>RPC</category>
      </categories>
      <tags>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML]]></title>
    <url>%2F2018%2F04%2F25%2FHTML%2F</url>
    <content type="text"><![CDATA[基础知识常用指令段落标识标签&lt;p&gt;&lt;/p&gt;标识一整个段落； 其中标签&lt;p&gt;指出了段落的开头位置， 而标签&lt;/p&gt;指出了段落的结束位置。 代码为： &lt;p&gt;&lt;/p&gt;标识段落；标签 &lt;p&gt;指出了段落的开头位置，而标签&lt;/p&gt;指出了段落的结束位置。 效果： 标识段落；标签指出了段落的开头位置，而标签指出了段落的结束位置。### 颜色标识 ###代码为： 内容效果：内容### 链接 ###在HTML页面中，链接是使用锚（mao）标签来定义了： 格式为： link text 这里也就是类似markdown的链接使用方式，也可以说是类似Linux中软链接的方式，可以隐藏后端真实的URL串，并且让对外的链接保持最新也要容易得多。实际案例： Learning Log]]></content>
      <categories>
        <category>编程语言</category>
        <category>HTML</category>
      </categories>
      <tags>
        <tag>HTML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[disconf]]></title>
    <url>%2F2018%2F04%2F25%2Fdisconf%2F</url>
    <content type="text"><![CDATA[官方资料： github主页 简介disconf专注于各种「分布式系统配置管理」的「通用组件」和「通用平台」, 提供统一的「配置管理服务」 工作流程如下图所示： 实现目标： 部署极其简单：同一个上线包，无须改动配置，即可在 多个环境中(RD/QA/PRODUCTION) 上线 【每个应用中只需要配置相关的参数即可。】 部署动态化：更改配置，无需重新打包或重启，即可 实时生效 统一管理：提供web平台，统一管理 多个环境(RD/QA/PRODUCTION)、多个产品 的所有配置 核心目标：一个jar包，到处运行,涉及到配置不会存储在项目文件中 部署及使用]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>分布式</category>
        <category>disconf</category>
      </categories>
      <tags>
        <tag>disconf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python零碎知识及版本变化记录]]></title>
    <url>%2F2018%2F04%2F23%2Fpython%E9%9B%B6%E7%A2%8E%E7%9F%A5%E8%AF%86%E5%8F%8A%E7%89%88%E6%9C%AC%E5%8F%98%E5%8C%96%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[零碎知识 CharField类型必须添加max_length来指定长度上限，如果要想没有限制，则考虑使用TextField类型 版本变化##django## 外键： Django2.0版本之后，创建外键时需要在后面加上on_delete topic = models.ForeignKey(Topic) 应该修改为： topic = models.ForeignKey(Topic,on_delete=models.CASCADE) django.core.urlresolvers变化 Django 2.0 removes the django.core.urlresolvers module, which was moved to django.urls in version 1.10. You should change any import to use django.urls instead.]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python版本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ignoring option PermSize=512m; support was removed in 8.0]]></title>
    <url>%2F2018%2F04%2F23%2Fignoring-option-PermSize-512m-support-was-removed-in-8-0%2F</url>
    <content type="text"><![CDATA[使用jdk1.8的时候设置了vm参数：-Xmx2048m -XX:PermSize=512m -XX:MaxPermSize=768m -Xss2m此时运行java程序时VM提示如下警告： Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=512m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=768m; support was removed in 8.0 程序能正常兼容启动，不会产生影响，但这个提示还是引起了注意 问题原因： -XX:PermSize和-XX:MaxPermSize在jdk1.8中被弃用了，使用-XX:MetaspaceSize和-XX:MaxMetaspaceSize替代。 所以此时VM参数正确应为：-Xmx2048m -XX:MetaspaceSize=512m -XX:MaxMetaspaceSize=768m -Xss2m 根本原因： 在jdk1.8之前之前我们将储存类信息、常量、静态变量的方法区称为持久代(Permanent Generation)，PermSize和MaxPermSize是设置持久代大小的参数。 但是在jdk1.8中，持久代被完全移除了，所以这两个参数也被移除了，多了一个元数据区(Metadata Space)，所以设置元数据区大小的参数也变成对应的MetaspaceSize和MaxMetaspaceSize了。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维工作日常问题</category>
      </categories>
      <tags>
        <tag>持久代被废弃</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中方法与函数的区别]]></title>
    <url>%2F2018%2F04%2F22%2Fpython%E4%B8%AD%E5%B1%9E%E6%80%A7%E4%B8%8E%E6%96%B9%E6%B3%95%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[定义: function(函数) —— A series of statements which returns some value toa caller. It can also be passed zero or more arguments which may beused in the execution of the body. method(方法) —— A function which is defined inside a class body. Ifcalled as an attribute of an instance of that class, the methodwill get the instance object as its first argument (which isusually called self). Function包含一个函数头和一个函数体, 支持0到n个形参 而Method则是在function的基础上, 多了一层类的关系, 正因为这一层类, 所以区分了 function 和 method.而这个过程是通过 PyMethod_New实现的 也就是说，函数可以脱离于类单独存在，在使用的时候，需要往函数中传入参数（实参） 而方法是与某个对象紧密联系的，不能脱离于类而存在方法的作用域只是在一个类中，只能在该类实例化后被该类使用 方法的绑定, 肯定是伴随着class的实例化而发生,我们都知道, 在class里定义方法, 需要显示传入self参数, 因为这个self是代表即将被实例化的对象。 定义角度： 从定义的角度上看，我们知道函数(function)就相当于一个数学公式，它理论上不与其它东西关系，它只需要相关的参数就可以。所以普通的在module中定义的称谓函数是很有道理的。 那么方法的意思就很明确了，它是与某个对象相互关联的，也就是说它的实现与某个对象有关联关系。这就是方法。虽然它的定义方式和函数是一样的。也就是说，在Class定义的函数就是方法。 总结： 函数是一段代码，通过名字来进行调用。它能将一些数据（参数）传递进去进行处理，然后返回一些数据（返回值），也可以没有返回值。所有传递给函数的数据都是显式传递的。 方法也是一段代码，也通过名字来进行调用，但它跟一个对象相关联。方法和函数大致上是相同的，但有两个主要的不同之处： 方法中的数据是隐式传递的； 方法可以操作类内部的数据（请记住，对象是类的实例化–类定义了一个数据类型，而对象是该数据类型的一个实例化）]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python方法与函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[湿气的产生及预防治疗]]></title>
    <url>%2F2018%2F04%2F22%2F%E6%B9%BF%E6%B0%94%E7%9A%84%E4%BA%A7%E7%94%9F%E5%8F%8A%E9%A2%84%E9%98%B2%E6%B2%BB%E7%96%97%2F</url>
    <content type="text"><![CDATA[文章结构： 第一部分：是什么-湿气的概念 &amp;&amp; 为什么-湿气的产生原因 第二部分：怎么做-如何预防及治疗 参考文献： 知乎-湿气是怎么回事，人为什么会有湿气？ 湿气（中医理论概念） 1. 湿气概念及产生原因 1.1 概念1.1.1 湿 我们在日常生活中，感受到湿的时候一般是物体含水量超出一定范围，这个水分可以依附到很多物体上，比如湿巾、湿木头、湿衣服等等（无法正常排出的水）。 但含水量也不能超过一定限度，依附不住的水就不叫做湿，而是自由的水，比如湿衣服滴下来的水，这就不称为湿。 1.1.2 湿气 湿气是一种中医理论中的概念。通俗的来说，就是人体内有多余的水份无法正常代谢排出，堆积在身体之内，从而影响身体健康。（具体原因可能是个人体质、疾病或生活习惯不良，造成体内水分调控系统失衡） 就像我们日常生活中所看见的：食物放在潮湿的地方相比干燥的地方，很快就会发霉，类比到人上，当人的湿气较重之后，会产生一系列疾病。 一般也把湿气称之为：湿邪 在致病的风、寒、暑、湿、燥、火这“六淫邪气” 中，中医最怕湿邪。 湿是最容易渗透的。湿邪从来不孤军奋战，总是要与别的邪气狼狈为奸。 湿气遇寒则成为寒湿，这就好比冬天的时候，如果气候干燥，不管怎么冷，人都还是能接受的，但如果湿气重，人就很难受了。南方的冬天比北方的冬天更令人难受，就是因为南方湿气比较重，寒湿袭人。 湿气遇热则成为湿热，这就好比夏天的桑拿天，又热又湿，让人喘不过气来，明显不如烈日当空、气候干燥的时候来得痛快。 湿气遇风则成为风湿，驱风很容易，但一旦成了风湿，就往往是慢性疾病，一时半会儿治不好了。 湿气在皮下，就形成肥胖，也是不好处理的健康问题…… 为什么现代人的病那么复杂，那么难治？因为他们体内有湿，体外的邪气总是和体内的湿气里应外合，纠缠不清！以前仅仅盛行于我国西南的川菜，风行全国，就是因为川味是辛辣的，以前只有生活在湿邪比较重的西南一带人需要用它来化解体内的湿气；全国人体内都有湿气了，这就需要辛辣来化解。 主导湿气的人体器官是：脾 因此，湿气问题的根本原因是各种原因导致的脾功能下降（也有可能是其他器官导致，因为脾在工作时要需要借助胃肝肾等器官），具体见下文。 1.2 产生原因-湿气是怎么来的？1.2.1 外在原因 一个是因为外在的环境，也就是湿邪进入到了身体。 比如长期居住在湿气重的地方，比如淋了雨还不及时擦干，比如晚上洗头没吹干就睡觉，让外界的湿气进入到体内。 湿气进入身体后常常奔着脾胃去，导致脾的运化能力下降，而这又会容易导致体内生湿。 1.2.2 内因 另外一个就是饮食习惯差，导致脾运化能力下降而生湿。【饮食不当，伤害脾胃，这是产生湿气的罪归祸首】 此外夏天的时候狂开空调，狂吃冷饮，硬生生的把要出来的水份给逼回去了。还有缺乏运动，没有及时的增强脾的工作能力。 脾主运化，吃进来的食物通过它来运化出精微物质，剩下的糟粕排出体外。当因为各种原因导致脾虚、运化能力下降的时候，精微物质就没法完全提炼出来。 1.2.3 原因解析 从微观的角度讲，物质没有完全被消化时，就成了携带营养物质的“垃圾”，成分复杂且分子比较大，没法被人体吸收，但又不像糟粕那么大块头好分辨，那么容易把它们驱逐。 它们的分子量和体积远大于水分子，潜伏着，聚集起来，极其容易把周围的水分子吸附住、束缚住，使含水量超出正常的生理水平，于是形成了湿。 脾被湿气困住，更加影响它的运化工作，导致湿气加重。湿一直凝聚不化，时间长了就成为痰，身体出于自保自救，把其中一部分水、二氧化碳和营养垃圾打包成了脂肪。所以中医常说胖人多痰湿，就是这个道理。【So，减肥先去湿气】 1.3 湿气的特点1.3.1 笨重并且混浊 湿气依附在身体某些地方，和身边的物体紧紧结合，难舍难离。物体湿的状态时会比干燥的时候重很多,所以体内有湿气的时候，我们往往觉得身体或头部沉重；湿气浊会导致身体气血流通不畅，长期聚集身体又没法整治它,导致有湿气的地方脏乱差，滋生各种毒害。 1.3.2 难缠粘人 什么东西被湿邪盯上，就好像被缠上了粘液，各种不爽，比如小便不畅，大便黏腻不爽等。此外它还很难去除，经常和你缠缠绵绵，病程较长，比如风湿病、温湿病。 1.3.3 阻遏气机、损伤阳气 湿气本质上属于阴邪，靠着它黏腻难缠的劲头，赖在脏腑经络上不走，导致气机升降无能，于是阳气就没法正常生发了。所以一般被湿邪困住的人，阳气都不太旺，会有脸色淡白，精力不济的现象。 1.4 湿气重的表现 头发爱出油、面部油亮, 小肚子大(常有胀气)，身体浮肿。 身体发沉、发重，浑身无力。 皮肤上会有湿疹，胃口不好，嘴里发黏。 常感到疲倦，精力不集中睡觉打呼噜，痰多，咳嗽,睡觉留口水、口臭、身体有异味，耳内湿（耳禅湿）毛发粗糙，易脱落。 舌质很胖，颜色偏淡。症状严重的，舌头边上会有齿痕，这叫“裙边舌”。 眼袋下垂，黑圆圈严重，肥胖，减肥后反弹，机能衰退，对房事不感兴趣质量不高等。 大便溏稀不成型，正常的大便是光滑的呈圆柱体，每次大便之后，不会粘在光滑的马桶壁上，如果你每次上完厕所，大便冲不干净，那么一定是体内湿气在作怪。而且，湿气会让便秘如影随形。下一次，当你大便的时候，很可能就会出现便秘。 等等等等 当湿气演变成为顽固性湿气的时候，身体会出现数十种不适： 以上症状，如果你占了2种以上，要引起注意了，这说明体内有湿气。湿气不除，是引发及恶化疾病的关键。 并且现代人由于工作强度、压力等都更大，因此运动量也原来越小，体内阴盛阳虚从而湿邪内郁。这也是当前越来越多的年轻人有湿气相关疾病的原因。 2. 如何预防及治疗 在这里，我们将预防和治疗两者结合在一起说明，因为光靠预防不能完全杜绝，或多或少肯定都还是会产生湿气。 湿气很重，不要只会傻傻拔罐。 2.1 药物目前没有什么比较好的药物，一般采用饮食结合运动的方式来预防和治疗湿气。 2.2 饮食这里只说该吃什么，至于不该吃什么，请看日常生活章节 薏米赤小豆桂圆粥 薏米：性寒。因此要用赤小豆来中和，并且每次的量不宜太多 赤小豆：性，。注意赤小豆是扁的，红豆是圆的 桂圆/枣：桂圆甘温。有的人体质偏寒，里面可以加一点温补的食物，像桂圆、大枣都可以 如果着凉感冒了，或是体内有寒，胃中寒痛，食欲不佳，可在薏米赤小豆汤中加几片生姜。生姜性温，能温中祛寒，健脾和胃。 肾虚的人，可在薏米赤小豆汤中加一些黑豆。因为黑色入肾，豆的形状也跟肾十分相似，以形补形，是补肾的佳品。 人们常说的脚气病，是典型的湿热下注。可在薏米赤小豆汤中加点碎黄豆，用熬出来的汤泡脚，这是治脚气的一个小秘方。 学会薏米赤小豆汤的加减变化，使用得当可以对生活中大部分常见病起到很好的治疗效果。 如下图所示： 2.3 运动现代人动脑多、体力消耗少，加上长期待在密闭空调内，很少流汗，身体调控湿度的能力变差。因此这也为产生湿气创造了条件。 运动出汗是很好的去湿气方式 2.4 日常生活2.4.1 不宜 不过食生冷肥甘厚腻甜辛辣， 避开生冷食物。这里说的生冷食物指的是冷饮、凉拌菜等，而不是水果。【这一点在夏天的时候最为明显，一些人在夏天时喝冷饮、和冰镇啤酒、吃冰镇西瓜、吃凉菜等毫无节制】 夏天尽量不吹空调 睡前务必吹干头发 饮食口味重，日常饮食口味经常过重的话，由于细胞渗透压的作用，浓度低的会向浓度高的一方渗透，力求平衡，从而会使身体处于不正常状态 不宜久坐，一小时不动两小时不动三小时不动，身体以为你不会动了，它的运行也会慢下来慢下来 不宜大量吃水果。 2.4.2 宜 晚上用热水泡脚。 每天晚上坚持用热水泡脚半小时（注意：时间是半小时），泡到微微出汗。 泡脚的同时敲打肘窝、腘和腋窝各5分钟。这三个地方是排湿气的重要部位。腋窝都知道，肘窝就是手肘后面弯曲部位，腘就是膝盖后面弯曲部位。 天气好的日子，勤晒衣物和被子，减少病菌，降低生病的可能。 夏天时家中易闷热潮湿，每天要适度开窗换气，新鲜的空气可以减少细菌病毒的滋生，以傍晚最适宜。 清淡饮食 保持衣物干爽,不要穿潮湿未干的衣服、盖潮湿的被子，被子(垫絮)要经常晒。 夏天不要贪凉睡地板 2.5 总结食疗、运动最多只能暂时缓解症状，找到自身湿气产生的原因，才能从根上断绝它。 我们不能怪罪脾胃太虚弱，吃那么多它累死也消化不完啊；不要怪它懒罢工不干活，湿气困着它，它也很无奈；别说它工作不到位，身体消耗少，营养物质只能不断堆积。 不形成良好的生活习惯，喝再多薏米粥、吃再多健脾祛湿的方药都是白搭！所以与其总是寻医问药寻找除湿气的方法，不如老老实实先好好吃饭、合理饮食、不贪凉不贪酒、加强体育锻炼.多动少吃清淡平衡饮食,这才是正确的姿势。]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>极简身体之道</category>
        <category>湿气</category>
      </categories>
      <tags>
        <tag>湿气</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[X-Pack]]></title>
    <url>%2F2018%2F04%2F19%2FX-Pack%2F</url>
    <content type="text"><![CDATA[参考资料： 官网 概述x-pack是Elastic Stack的扩展包，实现了如下的一系列功能： Security Monitoring Alerting and Notification Reporting Graph Machine Learning 安装]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>X-Pack</category>
      </categories>
      <tags>
        <tag>X-Pack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[su: cannot set user id: Resource temporarily unavailable]]></title>
    <url>%2F2018%2F04%2F18%2Fsu-cannot-set-user-id-Resource-temporarily-unavailable%2F</url>
    <content type="text"><![CDATA[切换用户时出现如下提示符： [root@qa3-app018 ~]# su - appqa su: cannot set user id: Resource temporarily unavailable 常见的能够控制用户资源的文件一般有两个，一是/etc/profile,二是/etc/security/limits.conf。 除此之外，在Centos6.x版本后，还有一个配置文件对ulimit设置生效，就是/etc/security/limits.d/90-nproc.conf /etc/security/limits.conf文件如下图所示： domain: 是指限制的对象，可以是个人，也可以是组，组前面要加@符号，也可以设置为除root用户外的 任何人，用*号表示； type: 是指类型，soft是当前系统生效的值，hard是系统可以设置的最大值； item: 项目，是可以对什么项目做限制，如最大进程数，文件最大值； value: 值，所设置的值的大小。 这里我的设置是： root soft nofile 65535 root hard nofile 65535 * soft nofile 65535 * hard nofile 65535 可以看到是没有问题的 /etc/security/limits.d/90-nproc.conf[appqa@qa3-app018 ~]$ cat /etc/security/limits.d/90-nproc.conf # Default limit for number of user&apos;s processes to prevent # accidental fork bombs. # See rhbz #432903 for reasoning. * soft nproc 1024 root soft nproc unlimited 这这里发现默认最大进程数只有1024 将1024修改为10240之后，再次执行su即可恢复正常，问题得到解决。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维工作日常问题</category>
      </categories>
      <tags>
        <tag>运维工作日常问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器硬件知识]]></title>
    <url>%2F2018%2F04%2F18%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%A1%AC%E4%BB%B6%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[##]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>服务器硬件</category>
        <category>服务器硬件知识</category>
      </categories>
      <tags>
        <tag>服务器硬件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程管理-进程性能分析]]></title>
    <url>%2F2018%2F04%2F18%2F%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86-%E8%BF%9B%E7%A8%8B%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[分为两部分 服务器即时数据查看 监控趋势数据查看 主要分为以下4个部分 - CPU 系统整体CPU情况，具体进程占用CPU情况，需要精确进程或者线程调用的系统调用，过去5次/5秒等时间内的进程占用情况 - 内存 系统整体占用情况，具体进程占用内存情况，需要精确进程或者线程调用的系统调用 - 磁盘 整体磁盘IO情况，具体进程占用磁盘IO情况，需要精确进程或者线程调用的系统调用 - 网络 系统整体网络IO情况，带宽使用情况；具体进程占用的网络IO情况，需要精确进程或者线程调用的系统调用 - 系统 - 系统负载 系统整体占用情况 本文主要讲述在服务器端的即时数据查看。 #CPU性能分析# 即时数据查看磁盘磁盘IO 网络端口 系统应用进程]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>进程管理</category>
        <category>进程性能分析</category>
      </categories>
      <tags>
        <tag>进程管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVS从入门到精通]]></title>
    <url>%2F2018%2F04%2F18%2FLVS%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>负载均衡</category>
        <category>LVS</category>
      </categories>
      <tags>
        <tag>LVS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-django项目]]></title>
    <url>%2F2018%2F04%2F17%2FPython-django%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[基础环境： Python3系列（针对环境变量做好软链接，将python3链接为python） sudo apt-get -y install python3 ln -s /usr/bin/python3 /usr/bin/python pip3(pip2不支持Python3.x，因此我们要安装pip来支持python3) sudo apt-get install python3-pip ln -s /usr/bin/pip3 /usr/bin/pip pip的升级：pip install –upgrade pip 依赖关系（python3-venv） sudo apt-get -y install python3-venv 环境准备创建激活虚拟环境要使用django，首先需要建立一个虚拟工作环境。虚拟环境是系统的一个位置，你可以在其中安装包，并将其与其他python包隔离。将项目的库与其他项目分离是有益的。 创建虚拟环境： wxh@wxh-virtual-machine:/opt$ python -m venv ll_env 激活虚拟环境 (ll_env) wxh@wxh-virtual-machine:/opt$ source ll_env/bin/activate 停止虚拟环境 (ll_env) wxh@wxh-virtual-machine:/opt$ deactivate 安装django安装django(python3之后对应的django版本为1.8.1，因此在安装的时候需要额外注意)： (ll_env) wxh@wxh-virtual-machine:/opt$ pip install Django django项目创建项目：(ll_env) wxh@wxh-virtual-machine:/opt$ sudo django-admin.py startproject learning_log . 创建完毕之后的目录结构如下所示： 注意事项： 命令末尾有有一个句点的存在，如果遗忘，可能出现一些问题。 manage.py文件是一个简单的程序，它接受命令并将其交给django的相关部分去运行，我们将会使用这些命令来管理诸如使用数据库和运行服务器等任务 目录learning_log有4个文件，其中最重要的的是setting.py、urls.py、wsgi.py。 setting.py指定django如何与系统交互以及如何管理项目。在开发项目的过程中，我们将修改其中的一些设置，并添加一些设置。 urls.py告诉django应该创建哪些网页来响应浏览器请求。 wsgi.py帮助django提供它创建的文件。（web server gateway interface）web服务器网关接口的缩写 在一个目录下，只能创建一个django项目，因为一个目录下不允许存在2个manage.py 创建数据库django将大部分与项目有关的信息都存储在数据库中，因此我们需要创建一个供django使用的数据库。为给项目“学习笔记”创建数据库，在处于活动虚拟环境中的情况下执行下面的命令： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py migrate 启动/停止项目核实django项目是否正确的创建，执行下面的命令启动： python manage.py runserver [port] 创建应用程序django由一系列应用程序组成（也可以看成是一系列的功能组件），他们协同工作，让项目成为一个整体。现在我们暂时只创建一个应用程序，它将完成项目的大部分工作。在后面，我们还将再添加一个管理用户账户的应用程序 (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py startapp learning_logs 命令startapp让django创建应用程序所需的基础设施。如果现在查看项目目录，将看到其中新增了一个文件夹learning_logs 其中最重要的文件是models、admin和views，我们将使用models来定义我们要在应用程序中管理的数据 注意：learning_logs是learning_log项目中的一个应用程序，这个概念要分清。 定义模型每位用户都需要在学习笔记中创建很多的主题（例如：数学、英语、语文等等）。用户输入的每个条目（文章，每篇笔记）都要与特定的主题相关联。这些条目将以文本的形式显示。我们还需要存储每个目录的时间戳，以便能够告诉用户每个条目都是什么时候创建的。 (ll_env) wxh@wxh-virtual-machine:/opt$ cat learning_logs/models.py from django.db import models # Create your models here. 我们可以看到这个文件的内容。django事先导入了模块models，让我们自己创建模型。 模型告诉django如何处理应用程序中存储的数据。在代码层面，模型就是一个类，就像前面讨论的每个类一样，包含属性和方法。 我们编写完毕之后的代码如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim models.py from django.db import models class Topic(models.Model): &quot;&quot;&quot;用户学习的主题&quot;&quot;&quot; text = models.CharField(max_length=200) date_added = models.DateTimeField(auto_now_add=True) def __str__(self): &quot;&quot;&quot;返回模型的字符串表示&quot;&quot;&quot; return self.text 我们创建了一个名为Topic的类，它继承了models模块中的类Model-django中一个定义了模型基本功能的类。新建的这个Topic类中，我们只定义了两个属性：text和data_added（也就是拿来保存主题和时间戳） 属性text是一个CharField——由字符或文本组成的数据（见）。需要存储少量的文本，如名称、标题或城市时，可使用CharField。定义CharField属性时，必须告诉Django该在数据库中预留多少空间。在这里，我们将max_length设置成了200（即200个字符），这对存储大多数主题名来说足够了。 属性date_added是一个DateTimeField——记录日期和时间的数据（见）。我们传递了实参auto_add_now=True，每当用户创建新主题时，这都让Django将这个属性自动设置成当前日期和时间 我们需要告诉django，默认应该使用哪个属性来显示有关主题的信息。django调用方法str()来显示模型的简单表示，在这里，我们编写了方法str(),它返回存储在属性text中的字符串。 激活模型要使用模型，必须让Django将应用程序包含到项目中。为此，打开settings.py（它位于项目learning_log目录下），你将看到一个这样的片段，这一段的配置告诉Django使用哪些应用程序安装在项目中： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo vim learning_log/settings.py 33 INSTALLED_APPS = [ 34 &apos;django.contrib.admin&apos;, 35 &apos;django.contrib.auth&apos;, 36 &apos;django.contrib.contenttypes&apos;, 37 &apos;django.contrib.sessions&apos;, 38 &apos;django.contrib.messages&apos;, 39 &apos;django.contrib.staticfiles&apos;, 40 ] 我们将新建的应用程序加载进项目之中，修改之后的配置如下所示： 33 INSTALLED_APPS = [ 34 &apos;django.contrib.admin&apos;, 35 &apos;django.contrib.auth&apos;, 36 &apos;django.contrib.contenttypes&apos;, 37 &apos;django.contrib.sessions&apos;, 38 &apos;django.contrib.messages&apos;, 39 &apos;django.contrib.staticfiles&apos;, 40 # my appalication 41 &apos;learning_logs&apos; 42 ] 通过这种将应用程序编组的方式，在项目不断增大，包含更多的应用程序时，可以有效的对应用程序进行跟踪。 数据库配置： 接下来，需要让django修改数据库，使其能够存储与模型Topic相关的信息，执行以下命令： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py makemigrations learning_logs Migrations for &apos;learning_logs&apos;: learning_logs/migrations/0001_initial.py - Create model Topic (ll_env) wxh@wxh-virtual-machine:/opt$ 命令makemigrations让Django确定该如何修改数据库，使其能够存储与我们定义的新模型相关联的数据。 输出表明Django创建了一个名为0001_initial.py的迁移文件，这个文件将在数据库中为模型Topic创建一个表。 下面来应用这种迁移，让Django替我们修改数据库： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py migrate Operations to perform: Apply all migrations: admin, auth, contenttypes, learning_logs, sessions Running migrations: Applying learning_logs.0001_initial... OK 注意：每当需要修改该应用程序管理的数据时（在这里是learning_logs应用程序），都采取如下三个步骤： 修改models.py； 对learning_logs调用makemigrations; 让django迁移项目** django管理网站一个网站，需要有管理员来管理网站，django提供的管理网站（admin site）能够轻松的实现。 接下来，我们将建立管理网站，并通过它使用模型Topic来添加一些主题 创建超级用户创建具备所有权限的用户—超级用户，执行以下命令： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py createsuperuser Username (leave blank to use &apos;root&apos;): ll_admin Email address: Password: Password (again): Superuser created successfully. 注意：django并不存储实际输入的明文密码，而是存储该密码的散列值，每当你输入密码的时候，django都将计算其散列值，并将结果与存储的散列值进行比较。 向管理网站注册模型Django自动在管理网站中添加了一些模型，如User和Group，但对于我们创建的模型，必须手工进行注册。 我们创建应用程序learning_logs时， Django在models.py所在的目录中创建了一个名为admin.py的文件： (ll_env) wxh@wxh-virtual-machine:/opt$ cat learning_logs/admin.py from django.contrib import admin # Register your models here. 修改之后的文件为： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim admin.py from django.contrib import admin from learning_logs.models import Topic admin.site.register(Topic) 导入我们要注册的模型Topic，再使用admin.site.register方法让django通过管理网站管理我们的模型 接下来访问：http://127.0.0.1:8000/admin可以直接使用我们刚才创建的超级管理员用户的用户名和密码进行登录。 登录之后的页面如下所示： 这个网页能够让你添加和修改用户和用户组，还可以管理刚才定义的模型Topic相关的数据。 我们能够看到刚才定义的模型Topic及其相关的数据（当前没有数据） 添加主题向管理网站注册了topic之后，我们需要添加主题，这里添加Chess和Rock Climbing主题。添加完毕之后，如下图所示： 定义模型Entry当前的模型只是定义了2个属性（主题和时间戳），并不能实际的保存数据，因此我们需要添加模型Entry 关系：每个条目都会与特定的主题相关联，即多对一的关系。 修改之后的代码如下图所示： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim models.py from django.db import models class Topic(models.Model): &quot;&quot;&quot;用户学习的主题&quot;&quot;&quot; text = models.CharField(max_length=200) date_added = models.DateTimeField(auto_now_add=True) def __str__(self): &quot;&quot;&quot;返回模型的字符串表示&quot;&quot;&quot; return self.text class Entry(models.Model): &quot;&quot;&quot;学到的有关某个主题的具体知识&quot;&quot;&quot; topic = models.ForeignKey(Topic,on_delete=models.CASCADE) text = models.TextField() date_added = models.DateTimeField(auto_now_add=True) class Meta: verbose_name_plural = &apos;entries&apos; def __str__(self): &quot;&quot;&quot;返回模型的字符串表示&quot;&quot;&quot; return self.text[:50] + &quot;...&quot; 像Topic一样， Entry也继承了Django基类Model。 第一个属性topic是一个ForeignKey实例。外键是一个数据库术语，它引用了数据库中的另一条记录；这些代码将每个条目关联到特定的主题。每个主题创建时，都给它分配了一个键（或ID）。需要在两项数据之间建立联系时，Django使用与每项信息相关联的键。稍后我们将根据这些联系获取与特定主题相关联的所有条目。 接下来是属性text，它是一个TextField实例（见）。这种字段不需要长度限制，因为我们不想限制条目的长度。 属性date_added让我们能够按创建顺序呈现条目，并在每个条目旁边放置时间戳。 我们在Entry类中嵌套了Meta类。 Meta存储用于管理模型的额外信息，在这里，它让我们能够设置一个特殊属性，让Django在需要时使用Entries来表示多个条目。如果没有这个类，Django将使用Entrys来表示多个条目。 最后，方法str()告诉Django，呈现条目时应显示哪些信息。由于条目包含的文本可能很长，我们让Django只显示text的前50个字符（见）。我们还添加了一个省略号，指出显示的并非整个条目。 迁移模型Entry由于我们添加了一个新模型，因此需要再次迁移数据库 步骤： 修改models.py makemigrations参数 mkigrate参数 命令及输出如下所示： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py makemigrations learning_logs Migrations for &apos;learning_logs&apos;: learning_logs/migrations/0002_entry.py - Create model Entry (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py migrate Operations to perform: Apply all migrations: admin, auth, contenttypes, learning_logs, sessions Running migrations: Applying learning_logs.0002_entry... OK 生成了一个新的迁移文件0002_entry.py，它告诉django如何修改数据库，使其能够存储与模型Entry相关的信息。 执行命令migrate，我们发现django应用了这种迁移且一切顺利。 向管理网站注册Entry首先需要修改admin.py (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim admin.py from django.contrib import admin from learning_logs.models import Topic,Entry admin.site.register(Topic) admin.site.register(Entry) 然后刷新页面，可以看到新的内容 接下来，我们添加条目 为当前两个主题都添加相应的条目： django shell输入一些数据后，就可通过交互式终端会话以编程方式查看这些数据了。这种交互式环境称。为Django shell，是测试项目和排除其故障的理想之地。下面是一个交互式shell会话示例： (ll_env) wxh@wxh-virtual-machine:/opt$ sudo python manage.py shell [sudo] password for wxh: Python 3.5.2 (default, Nov 23 2017, 16:37:01) [GCC 5.4.0 20160609] on linux Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. (InteractiveConsole) &gt;&gt;&gt; from learning_logs.models import Topic &gt; &gt;&gt;&gt; Topic.objects.all() &lt;QuerySet [&lt;Topic: Chess&gt;, &lt;Topic: Rock Climbing&gt;]&gt; &gt;&gt;&gt; topics = Topic.objects.all() &gt;&gt;&gt; for topic in topics: ... print (topic.id,topic) ... 8 Chess 9 Rock Climbing &gt;&gt;&gt; t = Topic.objects.get(id=8) &gt;&gt;&gt; t.text &apos;Chess&apos; &gt;&gt;&gt; t.date_added datetime.datetime(2018, 4, 23, 12, 51, 16, 430242, tzinfo=&lt;UTC&gt;) 命令python manage.py shell启动一个Python解释器，可使用它来探索存储在项目数据库中的数据 在这里，我们导入了模块learning_logs.models中的模型Topic，然后使用方法Topic.objects.all()来获取模型Topic的所有实例；它返回的是一个列表，称为查询集（queryset）。 我们可以像遍历列表一样遍历查询集。 我们将返回的查询集存储在topics中，然后打印每个主题的id属性和字符串表示。从输出可知，主题Chess的ID为2，而Rock Climbing的ID为9。 知道对象的ID后，就可获取该对象并查看其任何属性。 我们还可以查看与主题相关联的条目。前面我们给模型Entry定义了属性topic，这是一个ForeignKey，将条目与主题关联起来。利用这种关联， Django能够获取与特定主题相关联的所有条目，如下所示： &gt;&gt;&gt; t.entry_set.all() &lt;QuerySet [&lt;Entry: The opening is the first part of the game, roughly...&gt;]&gt; 为通过外键关系获取数据，可使用相关模型的小写名称、下划线和单词set。例如，假设你有模型Pizza和Topping，而Topping通过一个外键关联到Pizza；如果你有一个名为my_pizza的对象，表示一张比萨，就可使用代码my_pizza.topping_set.all()来获取这张比萨的所有配料。 编写用户可请求的网页时，我们将使用这种语法。确认代码能获取所需的数据时， shell很有帮助。如果代码在shell中的行为符合预期，那么它们在项目文件中也能正确地工作。如果代码引发了错误或获取的数据不符合预期，那么在简单的shell环境中排除 故障要比在生成网页的文件中排除故障容易得多。我们不会太多地使用shell，但应继续使用它来熟悉对存储在项目中的数据进行访问的Django语法。 注意：每次修改模型后，都需要重启shell,执行Ctr + D django APIOnce you’ve created your data models, Django automatically gives you a database-abstraction API that lets you create, retrieve, update and deleteobjects. Creating objects &gt;&gt;&gt; from learning_logs.models import Topic &gt;&gt;&gt; b = Topic(text=&apos;badminton&apos;) &gt;&gt;&gt; b.save() 注意： This performs an INSERT SQL statement behind the scenes. Django doesn’t hit the database until you explicitly call save() The save() method has no return value 其他操作，查看：官方资料 创建网页：学习笔记主页使用django创建网页的过程通常分为四个阶段： 定义模型 这里我们有Topic模型和Entry模型，模型我们可以理解为就是一个数据库，其中定义了几个字段以及其对应的数据类型，后续我们都是根据这些数据进行操作。 定义URL URL模式描述了URL是如何设计的，让Django知道如何将浏览器请求与网站URL匹配，以确定返回哪个网页 编写视图 每个URL都被映射到特定的视图——视图函数获取并处理网页所需的数据。【也就是URL与内容的映射关系】 编写模板 视图函数通常调用一个模板，后者生成浏览器能够理解的网页。【也就是内容数据的显示方式，视图前端显示模板】 接下来，我们来创建学习笔记的主页。我们将定义该主页的URL、编写其视图函数并创建一个简单的模板 逻辑关系： 项目中定义调用的应用程序的模块，然后在具体应用程序中配置URL和视图 映射URL用户通过在浏览器中输入URL以及单击链接来请求网页，因此我们需要确定项目需要哪些URL主 页 的 URL 最 重 要 ， 它 是 用 户 用 来 访 问 项 目 的 基 础 URL 。 当 前 ， 基 础 URL（http://localhost:8000）返回默认的Django网站，让我们知道正确地建立了项目。我们将修改这一点，将这个基础URL映射到“学习笔记”的主页。 打开项目主目录中的urls.py文件（该文件针对整个项目的url配置），默认的内容为： 前两行导入了为项目和管理网站管理URL的函数和模块，在这个针对整个项目的urls.py文件中，变量urlpatterns包含项目中的应用程序的URL。 admin.site.urls模块定义了可在管理网站中请求的所有URL。 修改之后的配置文件如下图所示： 现在，我们需要在learning_logs目录下创建urls.py文件文件内容如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim urls.py (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ cat urls.py &quot;&quot;&quot;定义learning_logs的URL模式&quot;&quot;&quot; from django.conf.urls import url from . import views app_name = &apos;learning_logs&apos; urlpatterns = [ #主页 url(r&apos;^$&apos;,views.index,name=&apos;index&apos;) ] 讲解： 实际的URL模式其实是对函数url()的调用，这个函数接受3个实参 这里的正则表达式让python查找开头和末尾之间没有任何东西的url。 python忽略项目基础的URL（在这里是:http://localhost:8000/），因此这个正则表达式与基础URL相匹配，其他的非基础URL都不与这个正则表达式匹配，如果请求的是其他的URL页面，django将会返回一个错误页面。 url()的第2个实参指定了要调用的视图函数。请求的URL与前面的正则表达式匹配时，django将会调用views.index（这个index视图函数稍后编写） 第3个实参，将这个url模式的名称指定为index(相当于是alias别名的形式)，让我们在代码的其他地方引用它。每当我们需要提供这个主页的链接时，我们可以直接使用这个名称，而不用编写URL。 编写视图每个URL都被映射到特定的视图函数——视图函数获取并处理网页所需的数据。【也就是URL与内容的映射关系】 视图函数接受请求中的信息，准备好生成网页所需的数据，再讲这些数据发送给浏览器—这通常还涉及到网页的模板 learning_logs中的文件views.py是执行命令python manage.py startapp时自动生成的，当前的文件内容如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ cat views.py from django.shortcuts import render # Create your views here. 现在，这个文件只导入了函数render()，它根据视图提供的数据渲染响应。修改之后的文件内容如下如所示： from django.shortcuts import render def index(request): &quot;&quot;&quot;学习笔记的主页&quot;&quot;&quot; return render(request,&apos;learning_logs/index.html&apos;) 当URL被刚才定义的模式匹配之后，django会在views.py文件中查找函数index()，再将请求对象传递给这个视图函数（也就是这里的request）。 接下来向函数render进行传2个实参，原始的请求对象以及一个用于创建网页的模板（模板目录下的learning_logs目录下的index.html文件） 下面我们来编写这个模板。 编写模板模板定义了网页的结构，也就是说模板指定了网页是什么样子的。 每当网页被请求时，django将会填入相关的数据。模板让你能够访问视图提供的任何数据。我们主页视图没有提供任何数据，因此相应的模板非常简单。 创建目录在目录learning_logs下创建templates目录，用户保存网页模板文件 然后创建子目录learninig_logs，并在该子目录下新建文件index.html 文件内容为： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo mkdir -p templates/learning_logs (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ cd templates/learning_logs (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim index.html &lt;p&gt;Learning Log&lt;/p&gt; &lt;p&gt;Learning Log helps you keep track of your learning, for any topic you&apos;re learning about.&lt;/p&gt; 现在重新访问网站，会发现显示的是刚才自定义的这个网页，不再显示django网页。 如下图所示： 总结创建网页的过程看起来会有点复杂，但是将URL、视图和模板分离，效果实际上会更好，这让我们可以分别考虑项目的不同方面。 例如：数据库专家可以专注于模型，程序员可以专注于视图代码，而web涉及人员可以专注于模板。 关系： 访问请求–&gt;url（入：定义url匹配规则，出：视图函数。同时还定义这个url的调用别名） –&gt;视图（入：url请求对象，出：创建网页的模板，包含路径和名称信息。动态内容在这里生成） –&gt;模板（html网页，其中包含具体的网页内容，这里只是静态内容，接受视图中的动态数据然后组装显示） 创建其他网页接下来，我们创建两个显示数据的网页。 其中一个列出所有的主题 另一个显示特定主题的所有条目 对于每一个网页，我们都将指定URL模式，编写一个视图函数，并编写一个模板。 为了效率，我们可以先编写一个父模板 模板继承父模板：base.html 我们在/opt/learning_logs/templates/learning_logs目录下创建base.html【在项目中，每个网页都将继承base.html】 内容如下： &lt;p&gt; &lt;a href=&quot;{ % url &apos;learning_logs:index&apos;% }&quot;&gt;Learning Log&lt;/a&gt; &lt;/p&gt; { % block content % }{ % endblock content % } 实际效果为： Learning Log { % block content % }{ % endblock content % } 说明： 在当前，所有页面都包含的元素只有顶端的标题。我们将在每个页面中包含这个模板，因此我们将这个标题设置到主页的链接。 模板标签是用大括号和百分号{ %% }表示的。模板标签是一小段代码，生成要在网页中显示的信息。 在这里，模板标签{ % url ‘learning_logs:index’% }生成一个URL，该URL与learning_logs/urls.py中定义的名为index的URL模式匹配，也就是说learning_logs是一个命名空间，而index是该命名空间中一个名称独特的URL模式。 在HTML页面中，链接是使用锚（mao）标签来定义了： 格式为： &lt;a href=&quot;link_url&quot;&gt;link text &lt;/a&gt; 这里也就是类似markdown的链接使用方式，也可以说是类似Linux中软链接的方式，可以隐藏后端真实的URL串，并且让对外的链接保持最新也要容易得多。 子模板现在我们需要重新编写index.html，使其继承base.html，修改之后的代码如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim index.html { % extends &quot;learning_logs/base.html&quot; % } { % block content % } &lt;p&gt;Learning Log helps you keep track of your learning, for any topic you&apos;re learning about.&lt;/p&gt; { % endblock content % } 子模板的第一行必须包含标签{ % extends % }，让Django知道它继承了哪个父模板。 注意：这时候虽然两个文件的目录结构都是一样，但是还是需要制定目录learning_logs。 这行代码导入了base.html页面的所有内容，让index.html能够指定要在content块中预留的空间中添加的内容。 在子模板中，只需要包含当前网页特有的内容，这不仅简化了每个模板，还使得网站修改起来容易得多，要修改很多网页都包含的元素，只需要在父模板中修改该元素。 在大型项目中，我们可以定义多级父模板，有一个用于整个网站的父模板，并且网站的每个主要部分都有一个父模板，每个部分中又去继承这个模板。 接下来，我们专注于另外两个网页： 显示全部所有主题的网页 显示特定主题中条目的网页 显示所有主题的页面URL模式首先定义显示所有主题页面的URL。在这里我们使用topics来表示 完整的URL应该是：http://localhost:8000/topics 我们修改urls.py &quot;&quot;&quot;定义learning_logs的URL模式&quot;&quot;&quot; from django.conf.urls import url from . import views app_name = &apos;learning_logs&apos; urlpatterns = [ #主页 url(r&apos;^$&apos;,views.index,name=&apos;index&apos;), url(r&apos;^topics/$&apos;,views.topics,name=&apos;topics&apos;) ] 视图函数修改views.py，添加上面指定的topics视图函数 from django.shortcuts import render from .models import Topic def index(request): &quot;&quot;&quot;学习笔记的主页&quot;&quot;&quot; return render(request,&apos;learning_logs/index.html&apos;) def topics(request): &quot;&quot;&quot;显示所有的主题&quot;&quot;&quot; topics = Topic.object.order_by(&apos;date_added&apos;) context = {&apos;topics&apos;:topics} return render(request,&apos;learning_logs/topics.html&apos;,context) 说明： 需要导入与所需数据相关联的模型，也就是导入Topic类 视图函数中包含一个形参（django从服务器收到的访问request对象） 第一行中，我们查询数据库，请求提供Topic对象，并且按照属性date_added进行排序，然后将返回的查询集存储在topics中 context定义了一个将要发送给模板的上下文。上下文是一个字典类型，其中的键是我们将在模板中用来访问数据的名称，而值是我们要发送给模板的数据。在这里，我们暂时只是定义了一个键值对，包含我们将要在网页中显示的主题。 创建使用数据的网页时，除了对象request和模板的路径之外，我们还将变量context传递给render() 模板在这里，我们需要定义上面指定的topics网页。页面接受字典context，以便能使用topics()提供的数据。 创建的topics.html页面内容如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim topics.html { % extends &quot;learning_logs/base.html&quot; % } { % block content % } &lt;p&gt;Topics&lt;/p&gt; &lt;ul&gt; { % for topic in topics % } &lt;li&gt;{{ topic }}&lt;/li&gt; { % empty % } &lt;li&gt; No topics have been added yet.&lt;/li&gt; { % endfor % } &lt;/ul&gt; { % endblock content % } 说明： 就像index.html一样，我们首先使用标签{ % extends % }来继承base.html，再开始定义content块。这个网页的主体是一个项目列表，其中列出了用户输入的主题。在标准的HTML中，项目列表被称为无序列表，用标签&lt;ul&gt;&lt;/ul&gt;表示。 我们使用了一个for循环的模板标签【注意：pytho使用缩进来指出哪些代码是for循环的组成部分，而在模板中，每个for循环都必须使用{ % endfor %标签来显示地指出其结束位置}】 在这里topics这个变量是从视图函数中传递过来的，因此不需要我们再指定 在HTML中，for循环的格式为： { % for item in list % } do something with each item { % endfor % } 在循环中，我们要将每个主题转换为一个项目列表项。要在模板中打印变量，需要将变量名用双花括号括起来。每次循环时，代码都被替换为topic的当前值。这些花括号不会出现在网页中，它们只是用于告诉Django我们使用了一个模板变量。 HTML标签表示一个项目列表项，在标签对内部，位于标签和之间的内容都是一个项目列表项。 修改父模板 我们现在需要修改父模板(base.html)，使其包含到显示所有主题的页面的链接修改之后的内容如下所示： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ cat base.html &lt;p&gt; &lt;a href=&quot;{%url 'learning_logs:index'%}&quot;&gt;Learning Log&lt;/a&gt; - &lt;a href=&quot;{%url 'learning_logs:topics'%}&quot;&gt;Topics&lt;/a&gt; &lt;/p&gt; {% block content%}{% endblock content%} 这个时候刷新页面，显示如下： ![topics](http://picture.watchmen.xin/python-django/alltopics.png) ### 显示特定主题主所有条目的页面 ### 接下来，我们需要创建一个专注于显示特定主题的页面-**`显示该主题的名称以及该主题的所有条目`** 同样，我们的顺序还是： 1. 定义URL模式 2. 编写视图函数 3. 创建网页模板 此外，我们还需要修改上一个网页（显示所有主题的网页），让每个项目列表都是一个链接，点击之后可以显示相应主题的所有条目 #### URL模式 #### (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim urls.py """定义learning_logs的URL模式""" from django.conf.urls import url from . import views app_name = 'learning_logs' urlpatterns = [ #主页 url(r'^$',views.index,name='index'), url(r'^topics/$',views.topics,name='topics'), url(r'^topics/(?P\d+)/$',views.topic,name='topic') ] **说明：** r让django将这个字符串视为原始字符串，并指出正则表达式包含在引号内。这个表达式的第二部分**`/(?P\d+)/`**与包含在两个//内的整数内容进行匹配，并将这个整数存储在一个名为topic_id的实参中，这部分表达式捕获URL中的值； ?P将匹配到的值存储到topic_id中，而表达式\d+与包含在两个斜杆内的任何数字都匹配，不管这个数字为多少位 当发现URL与这个模式匹配的时候，django将会调用视图函数topiic()，并将存储在topic_id中的值作为实参传递给它。在这个函数中，我们使用topic_id的值来获取相应的主题 #### 视图 #### 视图topic()需要从数据库中获取指定的主题以及与之相关联的所有条目。如下所示： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim views.py from django.shortcuts import render from .models import Topic def index(request): """学习笔记的主页""" return render(request,'learning_logs/index.html') def topics(request): """显示所有的主题""" topics = Topic.objects.order_by('date_added') context = {'topics':topics} return render(request,'learning_logs/topics.html',context) def topic(request,topic_id): """显示单个主题及其所有的条目""" topic = Topic.objects.get(id=topic_id) entries = topic.entry_set.order_by('-date_added') context = {'topic':topic,'entries':entries} return render(request,'learning_logs/topic.html',context) 在这个视图函数中，我们有两个形参。这个函数接受正则表达式(?P\d+)捕获的值，并将其存储到topic_id这个形参当中 接下来，我们使用get()来获取指定的主题 下一行中，我们获取与该主题相关联的条目并将它们按照date_add进行排序：date_added前面的减号（-）指定按照降序进行排序，也就是先显示最近的条目，我们将主题和条目都存储在字典context当中，再将这个字典发送给模板topic.html。 注意： > 第一行和第二行（get和order_by）的代码都被称之为查询，因为它们会向数据中查询特定的信息，在自己的项目中编写这样的查询时，可以现在django shell中先进行试验 > 相比于直接编写视图和网页模板，再在浏览器中检查结果，在shell中执行代码可以更加快速的获得反馈 #### 模板 #### 这个模板需要显示每个主题的名称和其对应条目的内容，如果当前主题不包含任何条目，我们还需要向用户指出这一点。 代码如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim topic.html {% extends "learning_logs/base.html" %} {% block content %} Topic:{{ topic }} Entries: {% for entry in entries%} {{ entry.date_added|date:'M d, Y H:i'}} {{ entry.text|linebreaks }} { % empty % } There are no entries for this topic yet. {% endfor %} {% endblock content %} 说明： 是一个模板变量，在这里变量都是这种表示方式，这里的变量topic，是存储在字典context中的 在&lt;ul&gt;&lt;/ul&gt;中我们利用for循环定义一个显示每个条目的项目列表（无序列表） 每个列表项目都将列出两项信息：条目的时间戳和完整的文本。 第一行时间戳：在django中的模板中，竖线（|）表示模板过滤器–【对模板变量的值进行修改的函数】。后面的部分我们称之为：过滤器 过滤之后的时间戳显示格式将是：’M d, Y H:i’以这样的格式显示时间戳： January 1, 2015 23:00 第二行显示text的完整值，而不仅仅是entry的前50个字符。过滤器linebreaks将包含换行符的长条目转换为浏览器能够理解的格式，避免显示一个不间断的文本块。 在最后，我们使用模板标签{ % empty % }打印一条消息，告诉用户当前主题还没有条目 注意：代码和说明中的{和%中其实是没有空格的，在这里故意加上空格是因为hexo博客框架无法正常处理。 将显示所有主题的页面中的每个主题都设置为链接在浏览器中查看显示特定主题的页面前，我们需要修改模板topics.html，让每个主题都链接到相应的网页 如下所示： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim topics.html 这个时候我们再次刷新页面，再单击其中的一个主题，将会看到类似于下图的页面： 用户账户在这一章节，我们将创建对用户友好而直观的网页，让用户无需通过管理网站就能添加新的主题和条目，以及编辑既有的条目。 我们还将添加一个用户注册系统，让用户能够创建账户和自己的学习笔记。让任意数量的用户都能与之交互，是Web应用程序的核心所在。 让用户能够输入数据当前，只有超级用户能够通过管理网站输入数据。我们不想让用户与管理网站进行交互，因此我们将使用django的表单创建工具来创建工具让用户能够输入数据的页面 这部分，主要会涉及以下几种数据 添加新主题 添加新条目 编辑条目 添加新主题在添加新主题时，需要使用到一个输入框【在这里是一个基于表单的页面】 用户添加主题的表单我们需要让用户输入并提交信息的页面是表单 并且在用户输入数据的时候，我们还需要进行验证，确认提供的信息是否是正确的数据类型 而且信息不是恶意的信息，例如终端服务器的代码。 然后，我们再对这些有效信息进行处理，并将其保存到数据库的合适地方，这些工作都是由django自动完成。 在django中，创建表单最简单的方式是使用ModeForm，它根据我们在第18章定义的模型中的信息自动创建表单。 创建一个forms.py文件，并存储到models.py所在目录中 代码如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim forms.py from django import forms from .models import Topic class TopicForm(forms.ModelForm): class Meta: model = Topic fields = [&apos;text&apos;] labels = {&apos;text&apos;:&apos;&apos;} 最简单的ModelForm版本只包含一个内嵌的Meta类，它告诉django根据哪个模型创建表单，以及在表单中包含哪些字段。 在这里，我们根据模型Topic创建一个表单，该表单只包含字段text，下面的代码让django不要为字段text生产标签。 URL定义在这里我们创建的url是：http://localhost:8000/new_topic/ 代码如下： &quot;&quot;&quot;定义learning_logs的URL模式&quot;&quot;&quot; from django.conf.urls import url from . import views app_name = &apos;learning_logs&apos; urlpatterns = [ #主页 url(r&apos;^$&apos;,views.index,name=&apos;index&apos;), url(r&apos;^topics/$&apos;,views.topics,name=&apos;topics&apos;), url(r&apos;^topics/(?P&lt;topic_id&gt;\d+)/$&apos;,views.topic,name=&apos;topic&apos;), # 用户添加新主题的网页 url(r&apos;^new_topic/$&apos;,views.&apos;new_topic&apos;,name=&apos;new_topic&apos;) ] 这个url模式将请求交给视图函数new_topic(),接下来我们将编写这个函数 视图函数new_topic()函数new_topic()需要处理两种情形 第一种情况是刚进入new_topic网页（在这种情况下，它应该显示一个空表单） 第二种情况是对提交的表单数据进行处理，并将用户重定向到网页topics 代码如下： from django.shortcuts import render from django.http import HttpResponseRedirect from django.urls import reverse from .models import Topic from .forms import TopicForm def index(request): &quot;&quot;&quot;学习笔记的主页&quot;&quot;&quot; return render(request,&apos;learning_logs/index.html&apos;) def topics(request): &quot;&quot;&quot;显示所有的主题&quot;&quot;&quot; topics = Topic.objects.order_by(&apos;date_added&apos;) context = {&apos;topics&apos;:topics} return render(request,&apos;learning_logs/topics.html&apos;,context) def topic(request,topic_id): &quot;&quot;&quot;显示单个主题及其所有的条目&quot;&quot;&quot; topic = Topic.objects.get(id=topic_id) entries = topic.entry_set.order_by(&apos;-date_added&apos;) context = {&apos;topic&apos;:topic,&apos;entries&apos;:entries} return render(request,&apos;learning_logs/topic.html&apos;,context) def new_topic(request): &quot;&quot;&quot;添加新主题&quot;&quot;&quot; if request.method != &apos;POST&apos;: # 未提交数据：创建一个新表单 form = TopicForm() else: # POST提交的数据，对数据进行处理 form = TopicForm(request.POST) if form.is_valid(): form.save() return HttpResponseRedirect(reverse(&apos;learning_logs:topics&apos;)) context = {&apos;form&apos;: form} return render(request,&apos;learning_logs/new_topic.html&apos;,context) 说明： 在这里，我们我们导入了HttpResponseRedirect类，用户提交主题后，我们会使用这个类将用户重定向到网页topics。 函数reverse()根据指定的URL模型确定URL，这意味着django将在页面被请求时生成URL。我们还导入了刚才创建的表单TopicFrom 补充信息：GET请求与POST创建Web应用程序时，将用到的两种主要请求类型是GET请求和POST请求。对于只是从服务器读取数据的页面，使用GET请求；在用户需要通过表单提交信息时，通常使用POST请求。 处理所有表单时，我们都将指定使用POST方法。还有一些其他类型的请求，但这个项目没有使用。 函数new_topic()将请求对象作为参数。 用户初次请求该网页时，其浏览器将发送GET请求； 用户填写并提交表单时，其浏览器将发送POST请求。 根据请求的类型，我们可以确定用户请求的是空表单（GET请求）还是要求对填写好的表单进行处理（POST请求）。 如果请求方法不是POST，请求就可能是GET，因此我们需要返回一个空表单（即便请求是其他类型的，返回一个空表单也不会有任何问题）。 我们创建一个TopicForm实例，将其存储在变量form中，再通过上下文字典将这个表单发送给模板。由于实例化TopicForm时我们没有指定任何实参， Django将创建一个可供用户填写的空表单。 如果请求方法为POST，将执行else代码块，对提交的表单数据进行处理。我们使用用户输入的数据（它们存储在request.POST中）创建一个TopicForm实例，这样对象form将包含用户提交的信息。 要将提交的信息保存到数据库，必须先通过检查确定它们是有效的。函数is_valid()核实用户填写了所有必不可少的字段（表单字段默认都是必不可少的），且输入的数据与要求的字段类型一致（例如，字段text少于200个字符，这是我们在第18章中的models.py中指定的）。 这种自动验证避免了我们去做大量的工作。如果所有字段都有效，我们就可调用save()，将表单中的数据写入数据库。保存数据后，就可离开这个页面了。 我们使用reverse()获取页面topics的URL，并将其传递给HttpResponseRedirect()，后者将用户的浏览器重定向到页面topics。在页面topics中，用户将在主题列表中看到他刚输入的主题。 模板new_topic下面来创建模板new_topic.html 代码如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim new_topic.html 链接到页面new_topic接下来，我们在页面topics中添加一个到页面new_topic的链接： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs/templates/learning_logs$ sudo vim topics.html 这个时候访问指定页面，可以看到有输入框出现 添加新条目现在用户可以添加新主题了，但是他们还想添加新的条目。 我们再次定义URL，编写视图函数和模板，并连接到添加新条目的网页，但在此之前，我们需要好在forms.py中再添加一个类 1. 用于添加新条目的表单 我们需要创建一个与模型Entry相关联的表单，但是这个表单的定制程度比TopicForm要高些 (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim forms.py from django import forms from .models import Topic,Entry class TopicForm(forms.ModelForm): class Meta: model = Topic fields = [&apos;text&apos;] labels = {&apos;text&apos;:&apos;&apos;} class EntryForm(forms.ModelForm): class Meta: model = Entry fields = [&apos;text&apos;] labels = {&apos;text&apos;:&apos;&apos;} widgets = {&apos;text&apos;:forms.Textarea(attrs={&apos;cols&apos;:80})} 说明： 在这里，再导入了Entry。我们定义了属性widgets。小部件（widgets）是一个HTML表单元素，例如单行文本框、多行文本区域或者下拉列表。 通过设置属性widgets，可覆盖Django选择的默认小部件。通过让Django使用forms.Textarea，我们定制了字段’text’的输入小部件，将文本区域的宽度设置为80列，而不是默认的40列。这给用户提供了足够的空间，可以编写有意义的条目。 2. URL模式new_entry 在用户添加新条目的页面的URL模式中，需要包含实参topic_id,因为条目必须与特定的主题相关联。该URL模式如下： (ll_env) wxh@wxh-virtual-machine:/opt/learning_logs$ sudo vim urls.py &quot;&quot;&quot;定义learning_logs的URL模式&quot;&quot;&quot; from django.conf.urls import url from . import views app_name = &apos;learning_logs&apos; urlpatterns = [ #主页 url(r&apos;^$&apos;,views.index,name=&apos;index&apos;), url(r&apos;^topics/$&apos;,views.topics,name=&apos;topics&apos;), url(r&apos;^topics/(?P&lt;topic_id&gt;\d+)/$&apos;,views.topic,name=&apos;topic&apos;), # 用户添加新主题的网页 url(r&apos;^new_topic/$&apos;,views.new_topic,name=&apos;new_topic&apos;), # 用户添加新条目的页面 url(r&apos;^new_entry/(?P&lt;topic_id&gt;\d+)/$&apos;,views.new_entry,name=&apos;new_entry&apos;) ] 3. 视图函数new_entry]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VPC+专线+IDC网络]]></title>
    <url>%2F2018%2F04%2F17%2FVPC-%E4%B8%93%E7%BA%BF-IDC%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[参考文献： #VPC 专有网络VPC（Virtual Private Cloud）是基于阿里云构建的一个隔离的网络环境。每个专有网络（也就是每个VPC）之间逻辑上彻底隔离。 专有网络是独有的的云上私有网络。用户可以完全掌控自己的专有网络，例如选择IP地址范围、配置路由表和网关等，可以在自己定义的专有网络中使用阿里云资源如ECS、RDS、SLB等。 将不同的云资源部署在不同的交换机内，可以提高服务的可用性。 同一个VPC内的云资源可以相互访问，不同的VPC之间通过隧道进行隔离。 可以将专有网络连接到其他专有网络，或本地网络（这里的本地网络包括IDC和公司内部机房），形成一个按需定制的网络环境，实现应用的平滑迁移上云和对数据中心的扩展。【默认情况下，VPC内主机是无法与外网连接的。这里需要借助阿里云的公网介入产品，例如弹性公网IP、NAT网关、负载均衡等】 VPC网络有以下特点： 不同用户的云服务器部署在不同的专有网络里。 不同专有网络之间通过隧道ID进行隔离。专有网络内部由于交换机和路由器的存在，所以可以像传统网络环境一样划分子网，每一个子网内部的不同云服务器使用同一个交换机互联，不同子网间使用路由器互联。 不同专有网络之间内部网络完全隔离，只能通过对外映射的IP（弹性公网IP和NAT IP）互联。 由于使用隧道封装技术对云服务器的IP报文进行封装，所以云服务器的数据链路层（二层MAC地址）信息不会进入物理网络，实现了不同云服务器间二层网络隔离，因此也实现了不同专有网络间二层网络隔离。 专有网络内的ECS使用安全组防火墙进行三层网络访问控制。 VPC中的路由器和交换机一个专有网络是一个大网段，例如：192.168.0.0/16每一个可用区是其中的子网，例如可用区A的网段：192.168.1.0/24 在当前的环境下，可以划分出来2^8=256个可用区（可用的为256个，0-255，剔除了256） 每一个可用区的可用主机数量为：2^8=256个（可用的为254个，1-255，剔除了0和256) 一个交换机连接的是一个可用区（也就是一个子网），路由器连接的是每个可用区，对外的网段是VPC的网段。 如下图所示： 路由器（VRouter）是专有网络的枢纽。作为专有网络中重要的功能组件，它可以连接VPC内的各个交换机，同时也是连接VPC和其他网络的网关设备。每个专有网络创建成功后，系统会自动创建一个路由器。每个路由器关联一张路由表。 交换机（VSwitch）是组成专有网络的基础网络设备，用来连接不同的云产品实例。创建专有网络之后，您可以通过创建交换机为专有网络划分一个或多个子网。同一专有网络内的不同交换机之间内网互通。您可以将应用部署在不同可用区的交换机内，提高应用的可用性。 注意：交换机不支持组播和广播。您可以通过阿里云提供的组播代理工具实现组播代理。 VPC可以使用的私网地址范围： 网段 可用私网IP数量 （不包括系统保留） 192.168.0.0/16 65532 172.16.0.0/12 1048572 10.0.0.0/8 16777212 交换机的网段不能和所属的专有网络的网段重叠，可以是其子集或者相同，网段大小在16位网络掩码与29位网络掩码之间。 注意：VPC的私网地址范围和我们实际认知的有所差异 在创建VPC时，系统会自动添加一条目标网段为100.64.0.0/10的系统路由用于VPC内的云产品通信。 问题：不同可用区之间通信是直接到达还是需要路由？ 我们也可以通过抓包来进行分析 总结： 一个VPC有且只会分配一个路由器 该路由器会关联一张路由表（其中包括不可更改的系统路由，用于VPC内部通信） 实现原理每个VPC都有一个独立的隧道号，一个隧道号对应着一个虚拟化网络。 一个VPC内的ECS（Elastic Compute Service）实例之间的传输数据包都会加上隧道封装，带有唯一的隧道ID标识，然后送到物理网络上进行传输。 不同VPC内的ECS实例因为所在的隧道ID不同，本身处于两个不同的路由平面，所以不同VPC内的ECS实例无法进行通信，天然地进行了隔离。 基于隧道技术和软件定义网络（Software Defined Network，简称SDN）技术，阿里云的研发在硬件网关和自研交换机设备的基础上实现了VPC产品。 以下是VPC实现的逻辑架构： VPC包含交换机、网关和控制器三个重要的组件。 VPC应用场景SNAT：在NAT网关上操作，接受到后端ECS主机发送来的数据包时，将源IP地址，修改为自身的IP地址，堆外隐藏内部的信息 DNAT：在NAT网关上操作，接受到internet发送过来的数据包时，将目的IP地址进行修改（这个时候是NAT网关的地址），修改成为内部ECS主机的IP地址。用于后端多套系统共享带宽。 VPC通信专有网络是完全隔离的网络环境。默认情况下，相同专有云网络内的ECS和云服务可以进行私网通信，但VPC与VPC之间、VPC与经典网络或公网不能互通。您可以使用弹性公网IP、高速通道、NAT、VPN网关或公网负载均衡等公网产品实现专有网络间的通信。 VPC与VPC通信 VPC与经典网络通信 VPC与Internet通信 VPC与本地IDC通信 VPC与VPC通信]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识</category>
        <category>VPC+专线+IDC网络</category>
      </categories>
      <tags>
        <tag>VPC</tag>
        <tag>专线</tag>
        <tag>IDC网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper从入门到精通]]></title>
    <url>%2F2018%2F04%2F17%2FZookeeper%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logstash]]></title>
    <url>%2F2018%2F04%2F17%2FLogstash%2F</url>
    <content type="text"><![CDATA[概述logstash由以下几个部分组成 input filter output How Logstash Works每个input都拥有一个线程 input生成的event存储在内存或者磁盘中]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Logstash</category>
      </categories>
      <tags>
        <tag>Logstash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kibana]]></title>
    <url>%2F2018%2F04%2F17%2FKibana%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Kibana</category>
      </categories>
      <tags>
        <tag>Kibana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Filebeat]]></title>
    <url>%2F2018%2F04%2F17%2FFilebeat%2F</url>
    <content type="text"><![CDATA[本文基于Filebeat 6.2.3版本 参考文献： 官网 1. Filebeat基础知识 Filebeat consists of two main components: prospectors and harvesters. These components work together to tail files and send event data to the output that you specify. prospectors:勘探者；探矿者【也就是数据变化的探测者，也就是入向配置】 harvesters：收割机；收获者【也就是数据的下游接收端，也就是出向配置】 spooler：处理程序；【处理程序会集合这些事件，最后filebeat会发送集合的数据到你指定的地点。】 Filebeat工作流程： 当你开启filebeat程序的时候，它会启动一个或多个探测器（prospectors）去检测你指定的日志目录或文件，对于探测器找出的每一个日志文件，filebeat启动收割进程（harvester），每一个收割进程读取一个日志文件的新内容，并发送这些新的日志数据到处理程序（spooler），处理程序会集合这些事件，最后filebeat会发送集合的数据到你指定的地点。 流程图如下： 1.1 What is harvester? A harvester is responsible for reading the content of a single file. The harvester reads each file, line by line, and sends the content to the output. One harvester is started for each file. The harvester is responsible for opening and closing the file, which means that the file descriptor remains open while the harvester is running. If a file is removed or renamed while it’s being harvested, Filebeat continues to read the file. This has the side effect that the space on your disk is reserved until the harvester closes. By default, Filebeat keeps the file open until close_inactive is reached. Closing a harvester has the following consequences:【关闭时按以下顺序依`次执行，也就是关闭顺序】 The file handler is closed, freeing up the underlying resources if the file was deleted while the harvester was still reading the file.The harvesting of the file will only be started again after scan_frequency has elapsed. If the file is moved or removed while the harvester is closed, harvesting of the file will not continue. To control when a harvester is closed, use the close_* configuration options. harvester负责打开和关闭文件 当harvester捕获到一个文件之后，这个文件被删除或者重命名，它将继续读取这个文件。【副作用是占用磁盘空间直到harvester进程关闭】 1.2 What is prospector? A prospector is responsible for managing the harvesters and finding all sources to read from. If the input type is log, the prospector finds all files on the drive that match the defined glob paths and starts a harvester for each file. Each prospector runs in its own Go routine. The following example configures Filebeat to harvest lines from all log files that match the specified glob patterns: filebeat.prospectors: - type: log paths: - /var/log/*.log - /var/path2/*.log prospector负责管理harvesters进程以及寻找需要去读取的资源（input设置） Filebeat的input type当前有两种设置：log和stdin The log prospector checks each file to see whether a harvester needs to be started, whether one is already running, or whether the file can be ignored (see ignore_older).New lines are only picked up if the size of the file has changed since the harvester was closed. 注意，Filebeat只能读取本地的文件，也就是需要在每个日志产生端都安装：Filebeat prospectors can only read local files. There is no functionality to connect to remote hosts to read stored files or logs. 1.3 How does Filebeat keep the state of filesfilebeat通过定期去刷新，将状态落地到磁盘的注册文件中，以这种形式来保持文件检测状态 Filebeat keeps the state of each file and frequently flushes the state to disk in the registry file The state is used to remember the last offset a harvester was reading from and to ensure all log lines are sent filebeat的状态信息记录的是最新的读取偏移量，如果下游的接受者（ES、kafka、logstash等不可达），filebeat将会保持这种状态，当检测后可达之后将会重新发送 当filebeat重启时，将会重新构建这个注册文件 每个prospector针对每个文件都会保持一个状态，也就是一个注册文件，因为文件可能会被删除或者重命名 针对每个文件，filebeat存储一个唯一的标识符去发现该文件之前是否有被收集过 For each file, Filebeat stores unique identifiers to detect whether a file was harvested previously. 1.4 how does Filebeat ensure at-least-once delivery?Filebeat保证一个事件的完整及正确性，它将发送最少一次给设置的下游输出，以保证没有数据丢失。 Filebeat guarantees that events will be delivered to the configured output at least once and with no data loss. Filebeat is able to achieve this behavior because it stores the delivery state of each event in the registry file. Filebeat能够保证这种特性的原因是因为它将分发状态也存储在这个注册文件中。 当下游因为阻塞或者其他原因，没有对某一个事件进行确认的时候，filebeat将一直尝试去发送这个事件，直到收到ACK If Filebeat shuts down while it’s in the process of sending events, it does not wait for the output to acknowledge all events before shutting down. Any events that are sent to the output, but not acknowledged before Filebeat shuts down, are sent again when Filebeat is restarted. This ensures that each event is sent at least once, but you can end up with duplicate events being sent to the output. You can configure Filebeat to wait a specific amount of time before shutting down by setting theshutdown_timeout option. 当Filebeat异常关闭，再次启动的时候，它将会重新发送没有接受到ACk的事件。通过这种机制来保证每个事件至少发送一次。 因此，为了减少重新发送event事件的次数，可以通过设置shutdown_timeout参数来设置当filebeat关闭时，等待多少时间之后再关闭进程，以保证收到尽量多的ACK 注意：虽然拥有这种机制来保证数据的不丢失，但还是存在一些可能的情况导致数据的丢失（日志轮转和删除文件时） 例如： If log files are written to disk and rotated faster than they can be processed by Filebeat 【当日志刚好触发到日志轮转条件时，并且此时filebeat还没有来得及收集的时候，原因是inode节点发生变化】 if files are deleted while the output is unavailable 【当该文件被删除时，并且输出不可用时】 总结：filebeat会维护一个注册文件【是落地到磁盘中的】，该注册文件中包含2个信息 所发送事件的偏移量，精确记录当前的发送情况。 下游断开时，将保持直到连接后再发送 发送事件的ACk，记录发送事件的接受情况。 没有收到，将一直持续发送。 2. Filebeat安装部署配置启动2.1 安装curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.2.3-x86_64.rpm rpm -vih filebeat-6.2.3-x86_64.rpm 2.2 启动service filebeat start ./filebeat -e -c ./bigdata.yml 2.3 命令主要提供了8个命令API [root@master ~]# filebeat --help Usage: filebeat [flags] filebeat [command] Available Commands: export Export current config or index template #导出ES的索引模板 help Help about any command keystore Manage secrets keystore modules Manage configured modules #Filebeat的模块相关 run Run filebeat setup Setup index template, dashboards and ML jobs #设置初始化环境，包括索引模板，kibana的仪表盘等 test Test config version Show current version info Flags: -E, --E setting=value Configuration overwrite -M, --M setting=value Module configuration overwrite -N, --N Disable actual publishing for testing -c, --c string Configuration file, relative to path.config (default &quot;filebeat.yml&quot;) --cpuprofile string Write cpu profile to file -d, --d string Enable certain debug selectors -e, --e Log to stderr and disable syslog/file output -h, --help help for filebeat --httpprof string Start pprof http server --memprofile string Write memory profile to this file --modules string List of enabled modules (comma separated) --once Run filebeat only once until all harvesters reach EOF --path.config string Configuration path (default &quot;&quot;) --path.data string Data path (default &quot;&quot;) --path.home string Home path (default &quot;&quot;) --path.logs string Logs path (default &quot;&quot;) --plugin pluginList Load additional plugins --setup Load the sample Kibana dashboards --strict.perms Strict permission checking on config files (default true) -v, --v Log at INFO level Use &quot;filebeat [command] --help&quot; for more information about a command. 注意： filebeat有：Config File Ownership and Permissions On systems with POSIX file permissions, all Beats configuration files are subject to ownership and file permission checks. The purpose of these checks is to prevent unauthorized users from providing or modifying configurations that are run by the Beat. The owner of the configuration files must be either root or the user who is executing the Beat process. The permissions on each file must disallow writes by anyone other than the owner. 也就是说只有root用户或者文件的属主才有权限执行命令 2.4 编辑配置文件配置文件生成规则 安装完毕之后，配置文件路径：/etc/filebeat/filebeat.yml参考配置文件为：filebeat.reference.yml 这里采用的是rpm包方式安装，因此生成规则如下： filebeat.yml整个配置文件分为几个部分，分别是： - Modules configuration - Filebeat prospectors 【上游输入设置。这部分包含内容为：type设置|path路径设置，】 - Filebeat autodiscover - Filebeat global options - General - Elastic Cloud - Outputs 【下游输出设置。这部分内容为：ES|kafka|logstash|】 - Paths - Dashboards - Template 【ES模板配置】 - Kibana - Logging - X - - - Monitoring 【x - - - 监控】 - HTTP Endpoint 让我们来配置filebeat： modules模块设置filebeat的模块实现了快速部署的方式。该部分的设置是可选的，你可以选择在后面自己定义prospectors设置相关设置。可以通过以下3种方式开启modules Enable module configs in the modules.d directoryedit Enable modules when you run Filebeatedit Enable module configs in the filebeat.yml file 使用了模块之后，仍然可以指定变量设置来覆盖模块中的定义【最小局部生效】。并且，可以使用高级设置来覆盖prospector中的相关设置 模块命令： 第一种方式： ./filebeat modules enable apache2 mysql 开启modules.d下的指定模块 ./filebeat modules list 查看启动的模块情况 第二种方式： ./filebeat -e --modules nginx,mysql,system 在启动filebeat的时候启动模块 第三种方式： filebeat.modules: - module: nginx - module: mysql - module: system 模块的高级设置： 能够覆盖prospector中的设置 Behind the scenes, each module starts a Filebeat prospector. Advanced users can add or override any prospector settings. For example, you can set close_eof to true in the module configuration: - module: nginx access: prospector: close_eof: true Or at the command line like this: ./filebeat -M &quot;nginx.access.prospector.close_eof=true&quot; Here you see how to use the -M flag along with the –modules flag: ./filebeat --modules nginx -M &quot;nginx.access.prospector.close_eof=true&quot; You can use wildcards to change variables or settings for multiple modules/filesets at once. For example, the following command enables close_eof for all the filesets in the nginx module: ./filebeat -M &quot;nginx.*.prospector.close_eof=true&quot; The following command enables close_eof for all prospectors created by any of the modules: ./filebeat -M &quot;*.*.prospector.close_eof=true&quot; Filebeat global options 设置这部分可以设置filebeat自动去探测检测文件 filebeat.config.prospectors: enabled: true path: configs/*.yml reload.enabled: true reload.period: 10s prospectors设置对于大多数的基本filebeat配置，你可以定义一个单一探测器针对一个单一的路径，例如： filebeat.prospectors: - input_type: log paths: - /var/log/*.log 在这个例子中，探测器会收集/var/log/*.log的所有匹配文件，这意味这filebeat会手机所有的/var/log下以.log结尾的文件，此处还支持Golang Glob支持的所有模式。 在预定义级别的子目录中获取所有文件，可以使用这个配置：/var/log//.log，这会找到/var/log下所有子目录中所有的以.log结尾的文件。但它并不会找到/var/log文件夹下的以.log结尾的文件。现在它还不能递归的在所有子目录中获取所有的日志文件。 Outputs设置如果你设置输出到elasticsearch中，那么你需要在filebeat的配置文件中设置elasticsearch的IP地址与端口。 output.elasticsearch: hosts: [&quot;192.168.1.42:9200&quot;] 如果要使用Logstash对Filebeat收集的数据执行附加处理，则需要将Filebeat配置为使用Logstash。 ＃----------------------------- Logstash输出------------------ -------------- output.logstash： hosts：[“127.0.0.1:5044”] 如果您打算使用随Filebeat提供的示例Kibana仪表板，需要配置Kibana，这段是属于kibana设置，不输出output设置 #============================== Kibana ===================================== setup.kibana： host：“localhost：5601” 如果设置ES和kibana的安全性设置，使用以下的配置 output.elasticsearch: hosts: [&quot;myEShost:9200&quot;] username: &quot;elastic&quot; password: &quot;elastic&quot; setup.kibana: host: &quot;mykibanahost:5601&quot; username: &quot;elastic&quot; password: &quot;elastic&quot; Template设置这一部分主要设置ES的模板 在Elasticsearch中，索引模板用于定义设置和映射，以确定如何分析字段。通过使用ES模板，可以有效的减轻存储压力ES模板：通过对索引中的每个字段做事先的预定义数据类型（例如ID，name等分别使用存储空间最小的数据类型） 在安装完毕Filebeat之后，会生成fields.yml这个ES模板文件下游如果是ES的话，Filebeat在启动的时候会自动的加载这个模板文件如果要关闭自动加载功能，则将以下参数设置为false setup.template.enabled: false 注意：如果该模板已经存在，则不会覆盖它，除非您配置Filebeat来指定执行此操作。 如果下游连接的不是ES而是logstash，那么需要手动导入模板 filebeat setup --template -E output.logstash.enabled=false -E &apos;output.elasticsearch.hosts=[&quot;localhost:9200&quot;]&apos; 强制Kibana使用最新的Filebeat索引信息 如果当前ES中已经有了filebeat的索引信息，那么修改模板之后，因此模板不会被覆盖，因此需要强制刷新生效。 curl -XDELETE &apos;http://localhost:9200/filebeat-*&apos; 中转方式导入ES模板 如果Filebeat没有直接连接到ES，那么可以将模板文件先导出到可以连接到ES的主机上，再通过这台去导入 filebeat export template &gt; filebeat.template.json curl -XPUT -H &apos;Content-Type: application/json&apos; http://localhost:9200/_template/filebeat-6.2.3 -d@filebeat.template.json 相关命令 ./filebeat setup -e 导入ES的索引末班 ./filebeat -e --modules system 导入模块的命令，这里是导入system模块 ./filebeat -e --modules system,nginx,mysql 一次运行多个模块 Kibana设置再kibana上显示filebeat的索引信息之前，you need to create the index pattern, filebeat-*， and load the dashboards into Kibana.不过在filebeat的6.0.0版本之后，这部分操作通过配置文件中的kibana配置部署来实现也就是上面说到的这一段的配置： #============================== Kibana ===================================== setup.kibana： host：“localhost：5601” 在配置之前请确保kibana已经处于运行状态，然后执行如下命令 filebeat setup --dashboards 多行日志处理处理多行日志，主要包括JAVA的堆栈内存，程序语言的类似\换行功能，时间戳引导的一段日志，应用指定的start–end等日志段 默认情况下JAVA堆栈日志是有多行组成的，例如： Exception in thread &quot;main&quot; java.lang.NullPointerException at com.example.myproject.Book.getTitle(Book.java:16) at com.example.myproject.Author.getBookTitles(Author.java:25) at com.example.myproject.Bootstrap.main(Bootstrap.java:14) 因此在filebeat中要把这些多行的日志组合成为一个event。这就需要以下的配置： multiline.pattern: &apos;^[[:space:]]&apos; multiline.negate: false multiline.match: after This configuration merges any line that begins with whitespace up to the previous line. 如果还涉及到更复杂的JAVA堆栈日志格式，例如： Exception in thread &quot;main&quot; java.lang.IllegalStateException: A book has a null property at com.example.myproject.Author.getBookIds(Author.java:38) at com.example.myproject.Bootstrap.main(Bootstrap.java:14) Caused by: java.lang.NullPointerException at com.example.myproject.Book.getId(Book.java:22) at com.example.myproject.Author.getBookIds(Author.java:35) ... 1 more 那么需要如下的配置： multiline.pattern: &apos;^[[:space:]]+(at|\.{3})\b|^Caused by:&apos; multiline.negate: false multiline.match: after In this example, the pattern matches the following lines: a line that begins with spaces followed by the word at or … a line that begins with the words Caused by:]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Filebeat</category>
      </categories>
      <tags>
        <tag>Filebeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka]]></title>
    <url>%2F2018%2F04%2F17%2FKafka%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume]]></title>
    <url>%2F2018%2F04%2F17%2FFlume%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch]]></title>
    <url>%2F2018%2F04%2F17%2FElasticsearch%2F</url>
    <content type="text"><![CDATA[更多内容请看：官方文档 基础知识ES在整个ELk架构中提供的功能： 数据存储 数据搜索 数据分析 索引模板在Elasticsearch中，索引模板用于定义设置和映射，以确定如何分析字段。（例如ID，name等分别使用存储空间最小的数据类型） 索引模板允许您定义创建新索引时将自动应用的模板。 模板仅适用于索引创建时。更改模板将不会影响现有的索引。在使用创建索引的API时，如果设置了数据类型，那么将会覆盖模板的设置，也就是最小局部生效]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>大数据相关组件</category>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK架构之Filebeat+kafka+logstash+Elasticsearch]]></title>
    <url>%2F2018%2F04%2F17%2FELK%E6%9E%B6%E6%9E%84%E4%B9%8BFilebeat-kafka-logstash-Elasticsearch%2F</url>
    <content type="text"><![CDATA[整体架构 组件关系说明： logstash 和filebeat都具有日志收集功能，filebeat更轻量，占用资源更少，但logstash 具有filter功能，能过滤分析日志。一般结构都是filebeat采集日志，然后发送到消息队列，redis，kafaka。然后logstash去获取，利用filter功能过滤分析，然后存储到elasticsearch中 filebeat–&gt;kafka集群–&gt;logstash–&gt;(file server文件系统|kafka集群|ES集群) Filebeat 日志收集 kafka 日志接受消息队列 logstash 将日志进行过滤分析后存储到ES ES 存储，检索，分析 FilebeatFilebeat配置文件： filebeat.prospectors: - input_type: log paths: - /home/appdeploy/deploy/logs/pinpoint/*-pinpoint.log document_type: tools.pinpoint.data scan_frequency: 5s tail_files: true output.kafka: hosts: [&quot;10.10.10.92:9092&quot;, &quot;10.10.10.93:9092&quot;, &quot;10.10.10.94:9092&quot;] topic: &apos;%{[type]}&apos; partition.round_robin: reachable_only: false required_acks: 1 compression: gzip max_message_bytes: 1000000 codec.format: string: &apos;%{[message]}&apos; 说明 filebeat.prospectors: - input_type: log #类型选择log|stdin中的log，在filebeat最新版本中input_type写成type paths: - /home/appdeploy/deploy/logs/pinpoint/*-pinpoint.log #指定探测的文件 document_type: tools.pinpoint.data scan_frequency: 5s #prospector每隔5秒去检测日志的生成情况 tail_files: true #设置之后，filebeat读取新文件时从文件末尾开始读取，而不是开头，如果设置了日志轮转，那么新文件的第一行将会被忽略 output.kafka: #输出策略采用kafka hosts: [&quot;10.10.10.92:9092&quot;, &quot;10.10.10.93:9092&quot;, &quot;10.10.10.94:9092&quot;] #kafka集群的配置信息 topic: &apos;%{[type]}&apos; #设置topic使用文件类型 partition.round_robin: #Topic的分区算法 reachable_only: false # 如果设置为true，那么event将只会推送到leader上，默认设置就是false required_acks: 1 #ACk可靠性级别，0表示不响应，1表示本地commit，-1表示所有的副本commit。默认为1 compression: gzip # 设置输出压缩编解码器，可选snappy and gzip。默认就是gzip max_message_bytes: 1000000 # JSON格式的信息一个传输允许的最大大小上限 codec.format: string: &apos;%{[message]}&apos; Kafka在这里，kafka的作用是将日志接受到消息队列中，以备后续的logstash收取 kafka的配置【每台的配置除了id不同之外，其他均类似】 [root@qa-bigdata002 config]# egrep -v ‘^#|^$’ server.properties broker.id=0 port=9092 host.name=172.24.80.87 num.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 log.dirs=/home/kafka/kafka-logs num.partitions=2 num.recovery.threads.per.data.dir=1 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 log.cleaner.enable=false zookeeper.connect=172.24.80.87:2181,172.24.80.88:2181,172.24.80.89:2181 zookeeper.connection.timeout.ms=600000 说明 broker.id=0 # The id of the broker. This must be set to a unique integer for each broker，在一个kafka集群中，每个broker的ID必须不同 port=9092 host.name=172.24.80.87 num.network.threads=3 #The number of threads handling network requests num.io.threads=8 # The number of threads doing disk I/O，将数据落地到磁盘的线程数量，一般为CPU核数的倍数 socket.send.buffer.bytes=102400 #发送缓冲区的大小，单位是字节，这里是1M socket.receive.buffer.bytes=102400 #接受缓冲区大小，这里是1M socket.request.max.bytes=104857600 #最大接受的请求数量，防止OOM的出现，out of memory,这里设置为104M log.dirs=/home/kafka/kafka-logs # 落地到磁盘的文件的存储路径 num.partitions=2 # 每个Topic的分区数量 num.recovery.threads.per.data.dir=1 #在日志数据恢复时， log.retention.hours=168 #日志保留小时数，这里的168小时，也就是保留7天。 log.segment.bytes=1073741824 #单个日志的文件的最大大小，现在配置为1G log.retention.check.interval.ms=300000 #每隔5分钟检测日志文件是否可以被删除 log.cleaner.enable=false #设置flase之后，日志保留策略将会采用上面的分段及超时设置，如果为true， zookeeper.connect=172.24.80.87:2181,172.24.80.88:2181,172.24.80.89:2181 #ZK的配置 zookeeper.connection.timeout.ms=600000 #连接ZK的超时时间 consumer.properties # Zookeeper connection string # comma separated host:port pairs, each corresponding to a zk # server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot; zookeeper.connect=172.24.80.87:2181,172.24.80.88:2181,172.24.80.89:2181 # timeout in ms for connecting to zookeeper zookeeper.connection.timeout.ms=6000 #consumer group id group.id=test-consumer-group producer.properties # list of brokers used for bootstrapping knowledge about the rest of the cluster # format: host1:port1,host2:port2 ... metadata.broker.list=172.24.80.87:9092,172.24.80.88:9092,172.24.80.89:9092 zookeeper.properties # the directory where the snapshot is stored. dataDir=/tmp/zookeeper # the port at which the clients will connect clientPort=2181 # disable the per-ip limit on the number of connections since this is a non-production config maxClientCnxns=0 LogstashElasticsearchES模板设置通过使用ES模板，可以有效的减轻存储压力ES模板：通过对索引中的每个字段做事先的预定义数据类型（例如ID，name等分别使用存储空间最小的数据类型）]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>大数据</category>
        <category>ELk日志处理平台</category>
        <category>Filebeat+kafka+logstash+Elasticsearch</category>
      </categories>
      <tags>
        <tag>ELK架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异地多活机房建设]]></title>
    <url>%2F2018%2F04%2F17%2F%E5%BC%82%E5%9C%B0%E5%A4%9A%E6%B4%BB%E6%9C%BA%E6%88%BF%E5%BB%BA%E8%AE%BE%2F</url>
    <content type="text"><![CDATA[异地多活参考文献 DTS使用场景 1. 前言概述随着业务的快速发展，对于很多公司来说，构建于单地域的技术体系架构，会面临诸如下面的多种问题： (1) 由于单地域底层基础设施的有限性限制了业务的可扩展性，例如城市供电能力，网络带宽建设能力等。 (2) 出现城市级别的故障灾害时，无法保证服务的可持续性，服务难以实现高可用。 (3) 用户分布比较广的业务，远距离访问延迟高，严重影响用户体验。 为解决企业遇到的这些问题，用户可以选择构建异地多活架构，在同城/异地构建多个单元(业务中心)。根据业务的某个维度将业务流量切分到各个单元(例如：电商的买家维度)。各个业务单元可以分布在不同的地域，从而有效解决了单地域部署带来的基础设施的扩展限制问题。 各个单元之间的数据层通过DTS的双向同步进行全局同步，保证全局数据一致。当任何一个单元出现故障时，只要将这个单元的流量切到其他单元即可在完成业务的秒级恢复，从而有效保证了服务的可持续性。 异地多活架构的单元可以根据用户分布选择部署区域，业务上可以按照用户所属区域划分单元流量，实现用户就近访问，避免远距离访问，降低访问延迟，提升用户访问体验。 当前公司的核心应用都是运行在阿里云上，这种方式存在以下几个缺点： 星型拓扑结构，所有压力都集中在一套系统之上，整体系统的高可用性还不够充分。新增IDC机房之后，可以将一部分流量引入到就近的云下机房。为核心系统减负，也就是说，同一个应用对应有多个生产环境。 的 1.1 思路整体思路： 为什么？ 是什么？ 怎么做？ 1.2 为什么？为什么需要异地机房？1.3 是什么？ 异地机房应该是什么样子，能实现什么功能，对当前架构有什么影响？1.4 怎么做？ 如何实现？这部分内容将会在下面的章节进行具体的阐述。 2. 具体实施]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维架构</category>
        <category>异地多活</category>
      </categories>
      <tags>
        <tag>异地多活机房建设</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用软件激活密钥]]></title>
    <url>%2F2018%2F04%2F16%2F%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E6%BF%80%E6%B4%BB%E5%AF%86%E9%92%A5%2F</url>
    <content type="text"><![CDATA[vmware workstations 14 pro CG54H-D8D0H-H8DHY-C6X7X-N2KG6 【亲测可用】 ZC3WK-AFXEK-488JP-A7MQX-XL8YF AC5XK-0ZD4H-088HP-9NQZV-ZG2R4 ZC5XK-A6E0M-080XQ-04ZZG-YF08D ZY5H0-D3Y8K-M89EZ-AYPEG-MYUA8]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>常用软件激活密钥</category>
      </categories>
      <tags>
        <tag>软件激活密钥</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令之curl命令]]></title>
    <url>%2F2018%2F04%2F16%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%B9%8Bcurl%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>Linux基础知识</category>
        <category>Linux常用命令</category>
      </categories>
      <tags>
        <tag>curl命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IT常用英语记录]]></title>
    <url>%2F2018%2F04%2F16%2F%E5%B8%B8%E7%94%A8%E8%8B%B1%E8%AF%AD%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[术语对照表 单词短语 释义 CURD CRUD (创建：Create， 读取：Read，更新：Update，删除： Delete) 是对于存储的信息可以进行操作的同义词。是一个对四种操作持久化信息的基本操作的助记符。CRUD 通常是指适用于存于数据库或数据存储器上的信息的操作，不过也可以应用在高层级的应用操作，例如通过在设置状态字段并标记删除的而并非移除数据的伪删除。 SDK RDS Remote Data Services。远程数据服务。云数据库 ECS 云服务器 ECS（Elastic Compute Service）是一种弹性可伸缩的计算服务 VPC 专有网络VPC（Virtual Private Cloud）是基于阿里云构建的一个隔离的网络环境，专有网络之间逻辑上彻底隔离。 PM Product Manager，产品经理 RD Research and Development 研究与开发 QA Qualtiy Assurance 品质保证。QA的主要职责就是质量保证工作。 OP Operator，操作员，管理员。 href hypertext reference 超文本连接 DML 数据变更是指DML(包含insert/delete/update) DDL 结构变更是指DDL(例如：create/drop/alter table) 单词对照表 英语 释义 side effect 副作用 properties 性能，属性，性质，特性，财产 involves 包含，牵涉 at-least-once 至少一次 deprecated 弃用，废弃，不赞成的 shipper 托运人；发货人；货主 prospectors 勘探者；探矿者 harvesters 收割机；收获者 layout 布局；设计；安排；陈列 keystore 密钥库;文件;密码;签名文件 permitted 被允许的；允许 individual 个人的；个别的；独特的；个体 seamlessly 无缝地 compatibility 兼容性 capabilities 能力；权限；功能；责任 migration 迁移；移民；移动 plural 复数 retrieve 检索，重新得到 Throughout 自始至终，到处；全部；贯穿，遍及 represent 代表；表现；描绘；回忆；再赠送 high-performance 高性能的；高效能的 objective 客观 Subjective 主观]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>英语</category>
      </categories>
      <tags>
        <tag>IT常用英语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jumpserver安装部署及使用]]></title>
    <url>%2F2018%2F04%2F13%2Fjumpserver%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. 安装部署【注意：务必保证版本及操作一致】 参考链接：一步一步安装文档 1.1 环境准备1.1.1 安装依赖关系yum -y install wget sqlite-devel xz gcc automake zlib-devel openssl-devel epel-release git libffi-devel python-devel 注意：python-deve需要安装对应的版本，我这里安装的是python36-devel 1.1.2 建立python虚拟环境使用原因：因为 CentOS 6/7 自带的是 Python2，而 Yum 等工具依赖原来的 Python，为了不扰乱原来的环境我们来使用 Python 虚拟环境 如果服务器上没有python3.6.1+环境，则需要手动安装 $ wget https://www.python.org/ftp/python/3.6.1/Python-3.6.1.tar.xz $ tar xvf Python-3.6.1.tar.xz &amp;&amp; cd Python-3.6.1 $ ./configure &amp;&amp; make &amp;&amp; make install $ cd /opt $ python3 -m venv py3 $ source /opt/py3/bin/activate 看到下面的提示符代表成功，以后运行 Jumpserver 都要先运行以上 source 命令，以下所有命令均在该虚拟环境中运行 (py3) [root@localhost py3] 在源码安装python3时可能会出现报错，关键字：“ake: * [Objects/unicodeobject.o] Error 4”。这个时候，修改Makefile文件，把‘-DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes’中的‘03’改为‘02’，然后再重新编译安装即可。 1.2 安装启动 jumpserver1.2.1 下载jumpservercd /opt git clone --depth=1 https://github.com/jumpserver/jumpserver.git &amp;&amp; cd jumpserver &amp;&amp; git checkout master 注意：不要安装在/root、/home 等目录下，以免权限问题 1.2.2 安装RPM依赖包cd requirements yum -y install $(cat rpm_requirements.txt) 1.2.3 安装python库依赖pip install -r requirements.txt # 不要指定-i参数，因为镜像上可能没有最新的包，如果没有任何报错请继续 1.2.4 安装 Redis, Jumpserver 使用 Redis 做 cache 和 celery broke$ yum -y install redis $ service redis start 1.2.5 安装配置MySQLcreate database jumpserver default charset &apos;utf8&apos;; grant all on jumpserver.* to &apos;jumpserver&apos;@&apos;127.0.0.1&apos; identified by &apos;Jumpserver_password_123&apos;; 1.2.6 修改jumpserver配置文件$ cd /opt/jumpserver $ cp config_example.py config.py $ vi config.py # 我们计划修改 DevelopmentConfig中的配置，因为默认jumpserver是使用该配置，它继承自Config 注意: 配置文件是 Python 格式，不要用 TAB，而要用空格 在该文件中新添加一个类 class DevelopmentConfig(Config): DEBUG = True DB_ENGINE = &apos;mysql&apos; DB_HOST = &apos;127.0.0.1&apos; DB_PORT = 3306 DB_USER = &apos;jumpserver&apos; DB_PASSWORD = &apos;somepassword&apos; DB_NAME = &apos;jumpserver&apos; config = DevelopmentConfig() # 确保使用的是刚才设置的配置文件，该行默认在文件末尾就存在。 1.2.7 生成数据库表结构和初始化结构$ cd /opt/jumpserver/utils $ bash make_migrations.sh 1.2.8 运行jumpserver$ cd /opt/jumpserver $ python3 run_server.py all 运行不报错，请浏览器访问 http://192.168.244.144:8080/ (这里只是 Jumpserver, 没有 Web Terminal，所以访问 Web Terminal 会报错) 账号: admin 密码: admin 2. jumpserver配置2.1 安装 SSH Server 和 WebSocket Server: Coco【此时还是在虚拟环境下】 $ cd /opt $ git clone https://github.com/jumpserver/coco.git &amp;&amp; cd coco &amp;&amp; git checkout master 2.2 安装 Web Terminal 前端: Luna3. 常用命令启动 jumpserver /opt/jumpserver/service.sh start 停止 jumpserver /opt/jumpserver/service.sh stop 重启 jumpserver /opt/jumpserver/service.sh restart 查看 jumpserver 状态 /opt/jumpserver/service.sh status 4. 配置优化/注意事项配置文件路径：/opt/jumpserver/ jumpserver.conf、 配置如下： \[base] url = access_url（安装时配置） key = o57ev5oc1nwe44r4 ip = 0.0.0.0 port = 8000 log = debug \[db] engine = mysql host = mysql_addr（安装时配置） port = 3306 user = jumpserver password = password（安装时配置） database = jumpserver \[mail] mail_enable = 1 email_host = smtp.163.com email_port = 25 email_host_user = name@163.com（安装时配置） email_host_password = password（安装时配置） email_use_tls = False email_use_ssl = False \[connect] nav_sort_by = ip 问题/总结日志压缩删除 访问服务器记录日志生成路径：/opt/jumpserver/logs/tty 需要定时压缩文件夹，并保留一段时间的历史日志 压缩文件夹命令： for i in `ls -d 201802*`; do tar czvf ${i}.tar.gz ${i}; done 5. 使用连接之后的终端页面如下所示： 添加用户并为之授权步骤1：web平台添加用户 步骤2：SSH登录跳板机，进入home目录下的对应用户名称目录下，编辑/home/wangping/.ssh/authorized_keys文件 步骤3：将事先生成好的公钥追加如该文件之中，授权完成。 添加备注信息控制台–&gt;资产管理–&gt;查看资产–&gt;输入主机名称点击搜索–&gt;编辑–&gt;页面末尾–&gt;备注（多个名称之间以/进行分割）–&gt;提交 jumpserver常见问题##用户推送失败## 问题 报错:”系统用户 ops 推送失败 [ redis037_cachecloud ], 推送成功 [ ] 进入系统用户详情，查看失败原因”如下图所示： 问题解决： 可能是在批量添加的时候，IP地址前面多加了一个空格]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维安全</category>
        <category>堡垒机</category>
      </categories>
      <tags>
        <tag>jumpserver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix结合Grafana实现炫酷动态可视化监控]]></title>
    <url>%2F2018%2F04%2F04%2FZabbix%E7%BB%93%E5%90%88Grafana%E5%AE%9E%E7%8E%B0%E7%82%AB%E9%85%B7%E5%8A%A8%E6%80%81%E5%8F%AF%E8%A7%86%E5%8C%96%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[来源：公众号-运维军团-《10分钟打造炫酷的监控大屏》 说明Grafana是一个开源的数据展示工具，是一个开箱即用的可视化工具，具有功能齐全的度量仪表盘和图形编辑器，有灵活丰富的图形化选项，可以混合多种风格，支持多个数据源，例如Graphite、InfluxDB、Mysql、Zabbix等等。虽然zabbix监控性能毋庸置疑，但zabbix图形显示过于简单、丑，因此利用zabbix作为数据源，结合Grafana作前端展示再好不过了。 重要的是Grafana的使用也超级简单，安装完成后登陆添加数据源即可，后面的事情就是添加图表等工作了。 实操未更新完，待续]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维监控体系</category>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>Grafana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ditto常用操作]]></title>
    <url>%2F2018%2F04%2F04%2Fditto%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[概述Ditto 是一款开源、免费、强大的剪贴板增强工具。可以把复制过的所有内容保存起来（可以设定保存日期或条目总数），快捷地供后续调用。还可以合并粘贴，纯文本粘贴，支持分组、置顶、快速搜索、热键粘贴功能。并且，还可以通过网络共享剪贴板内容。 主页：http://ditto-cp.sourceforge.net/ 教程：http://xbeta.info/ditto.htm 操作平常情况下，Ditto只是系统托盘中的图标。按下热键（默认` ctrl+``）后，会出现的粘贴主界面；再点击右键会弹出功能丰富的菜单。 详细请参看教程]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>Ditto</category>
      </categories>
      <tags>
        <tag>Ditto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fences桌面]]></title>
    <url>%2F2018%2F04%2F04%2Ffences%E6%A1%8C%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[下载安装目前的fences版本都是收费版本，可以下载免费版使用30天之后才酌情是否购买。官网 早期Fences免费版本连接：下载链接【该版本不是太好用】 使用fences的使用比较简单，这里介绍3点吧。 1、右键框选桌面空白处，即可出现桌面小区域。 这个应该大家都懂吧，基本功能。 2、桌面空白处双击鼠标左键，隐藏全部桌面图标 对桌面有洁癖的同学来说，这是个好福音。 3、设置好你的桌面区域后，可以锁定它们 当我们已经设置好桌面的区域后，可以将这些区域锁定，这样就无法再调整它们。 在桌面空白处右键鼠标，“查看”里选择“锁定fences”即可，这样你在桌面设置的这些fences小区域，就跟桌面“结合”在一起了，如果想要调整，取消勾选即可。]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>Fences</category>
      </categories>
      <tags>
        <tag>fences</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[listary常用操作]]></title>
    <url>%2F2018%2F04%2F04%2Flistary%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[参考资料官网 官网链接 网友文章 Listary——让文件在指尖流动 Listary Pro - 能极大幅度提高你 Windows 文件浏览与搜索速度效率的「超级神器」 实际操作常用快捷操作 ctrl敲2下 调出Listary窗口【可以在设置中设置第二种方式】 左键双击 在文件夹中双击会调出Listary的收藏，最近文档等。 在电脑页面输入e 即可定位到E盘，以此类推 目录页面输入名称一部分 定位到该子目录或者文件 ctrl+右键 针对选中内容进行动作【例如打开文件夹等】 智能匹配只要输入文件名的一部分就可以找到这个文件，支持中文与英文。 比如，我输入测试 md就可以搜索到测XX试OO.md这个文件。 自然，输入的越多，返回的结果越精确。随着使用记录的积累，常用的文件或程序会获得更高的优先级。 打开保存文件浏览对话框增强Ctrl+G 在打开框中切换到上一次打开的目录 Ctrl+O 直接打开上一次打开目录中的文件]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>Listary</category>
      </categories>
      <tags>
        <tag>Listary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简历内容应答]]></title>
    <url>%2F2018%2F04%2F04%2F%E7%AE%80%E5%8E%86%E5%86%85%E5%AE%B9%E5%BA%94%E7%AD%94%2F</url>
    <content type="text"><![CDATA[0. 自我介绍面试官 早上/下午好 我叫汪小华 大学就读于晋中学院 网络工程专业。目前一共有三年工作经验， 实习和第一份工作都是在亿阳信通在这期间从最基础的桌面运维干起，一直到最后独立接手负责了一个项目从0到1的这么一个整体过程，有很多的收获。这一期间对如何做好运维工作有了一些的感悟。 第二份工作是被内推到创世漫道，主要负责公司linux平台的调整，主要和我对接的是公司的架构师，因此在这个过程中对运维思想这方面有了很大的提升。 有关这两部分具体的内容我会在后面和您聊如何通过运维思想做好运维工作时谈及 我的优点是有一定的网络基础，平时喜欢问为什么，和同事交流谈论的时候喜欢拿纸笔写写画画。最强的技能部分应该是进程管理，这一点我觉得对运维工作来说，至关重要，关于这部分稍后我们可以一块交流探讨。 我的缺点目前是如何将所学知识有机的结合成为一个整体，构成一张知识之网这种能力还不够，这一点也是在后续的工作中需要去刻意修炼的。 以上是我的自我介绍，今天我要应聘的岗位是Linux运维工程师，谢谢！ 1. 发行版本区别及shell、python这些发行版本本质上没有什么区别，都是类unix系统。redhat的系统是免费的，但是他服务是收费的，并且有一些类似RHCS等服务只有收费版才支持。而centos是redhat社区版本，国内使用较多，社区相当活跃。SUSE也是分为两种，一种企业版本的SLES，一种是opensuse，sles主要是一些金融国企在使用，安全性较好，opensuse是社区版本，suse使用起来和centos相差不多，软件包形式使用源自redhat的rpm，但是管理工具使用的是zypper。ubuntu源自于debain，国外使用的较多，也是相当热门的一个发型版本，在桌面领域有绝对技术优势，适合开发人员使用。 **有关shell的问题，做面试题，看abs【每天看一点】。** python目前正在学习，目前基础部分已经学完了，正在学习django项目，学完之后，要花钱买一套马哥或者老男孩的python视频来补充，预计下半年能够做项目。 2. OSI模型、TCP/IP部分；路由交换基本原理OSI7层模型OSI 7层模式主要是由国际标准化组织（ISO）创建的，是一个国际通用的标准，它被开发出作为一个参照标准，用于指导如何设计网络通信系统。说的简单一点就是统一网络设备商的协议标准，实现不同网络设备和谐共存的环境（主机，路由器，交换机等等都是网络设置，都要遵循同一套的通信标准）它一共分为7层，每一层在网络通信数据传输过程中都定义了不同的功能。 OSI7层模型主要分为(从下到上)：物理层，数据链路层，网络层，传输层，会话层，表示层，应用层。每一层说明： layer function Application data flow；离用户最近的一层，它使一个网络应用能和另一个网络应用相互通信 Presentation 定义数据格式；数据压缩、加解密等。 Session 定义如何建立和终止连接，是告诉4层怎么做，4层只管被动的接受然后去做。 Transport 数据段；将上层的数据流进行分段；建立和终止网络连接；常用于流量控制和数据恢复 Network 数据包；使用IP地址在Internet上唯一确定一台设备；定义设备路由，寻址 Data link 数据帧；将上层数据封装成数据帧，其中包含源目MAC地址以及帧校验字段（用于检测传输错误）；它包含2个子层（LLC和MAC） Physical 比特流；定义了比特流如何在两台设备之间流通；主要涉及线缆，网卡， 下面是每一层常见的对应协议 layer protocol Application HTTP,FTP,Telnet,SMTP,SNMP Presentation MIME,TIFF,GIF,JPEG,PICT,ASCII,EBCDIC,encryption,MPEG,MIDI,HTML Session SSl/TLS,NetBIOS,RPC Transport TCP,UDP Network IP,ICMP,ARP,RARP Data link PPP,HDLC,IEEE 802.3/802.2,FDDI,ATM,IEEE 802.5/802.2 Physical Ethernet TCP/IP协议族TCP/IP模型类似OSI模型，作用也是描述一套指导标准，实现网络设备之间的通信，它被设计成4层 Application Transport Internet Network Access 其对应关系为： TCP/IP model OSI model Application Application Presentation Session Transport Transport Internet Network Network Access Data link Physical OSI和TCP/IP的区别除了层数的区别之外，它们之间最大的区别就是： OSI模型规定了在一个网络上传输数据所需要的步骤，并且它是非常具体的，定义了每一层使用什么协议以及如何使用；而TCP/IP模型是不特定的。另外一个区别就是，目前TCP/IP是所有网络设备上既定的协议事实，一般分析问题使用OSI模型。 路由技术我们在这里说的路由技术一般是指，路由转发。主要涉及设备为路由器或者三层交换机。这些设备上会维护一张路由表，其中的信息可以是通过动态路由协议（例如OSPF，EIGRP，ISIS，RIP，BGP，静态路由，默认路由等）获取组成路由表的内容是：出口接口和对应网段 路由设备接收到一个数据包之后，会解封装，获取其中的目的IP地址信息（网段信息），然后查找路由表，选择最优路由去转发。路由设备上也会有一张ARP表，根据广播域 交换技术在一个局域网内，也就是一个广播域内使用的技术。通常会涉及到的设备就是交换机。交换机上会维护一张MAC地址转发表，其中的信息是MAC地址和端口的映射关系。交换机根据数据帧中的目的MAC地址进行数据包的转发 注意：在一个广播域内的数据流动是依靠二层MAC来实现的，因为在第一次会涉及到ARP，有了记录之后，交换机会记录他的MAC地址表，后续的速度就会较快 这个时候可以在白板上进行讲解，大致的讲解百度页面打开的整个过程。 3. 高可用+负载均衡keepalived 2种角色：master和backup； 4种状态：stop,master,backup,fault 检测脚本2种触发机制 当VRRP检测脚本检测到自身所承载应用的返回值不为0的时候，就会触发角色变化，这个时候，VRRP脚本中如果没有设置weight权重值，那么直接进入fault状态，在vrrp组中发送组播通告，宣告自己进入异常状态，让出master角色并且不参与竞选如果脚本中设置了weight权重值，这个时候又会分为两种情况。 当weight权重值大于0时，master的优先级不变，backup的优先级为weight+现在优先级在wight权重值小于0时，master的优先级为目前的优先级减去weight的绝对值，backup的优先级保持不变。经过我多次的实验，目前保证最佳切换效果的配置是Vrrp检测脚本组中不配置weight，并且所有主机都设置为backup，设置不抢占参数，这种情况下，能有效避免优先级设置不当导致的切换不成功。 Nginx Nginx工作在应用层（使用location，通过正则表达表达式进行相关匹配），负载均衡是基于upstream模块实现的，因此配置比较简单。但是对后端服务器的健康检测只能支持端口Nginx的负载均衡算法可以分为两类：内置策略和扩展策略，内置的有轮询，ip_hash等。扩展的有fair，通用hash，一致性hash等。 Nginx的负载均衡目前支持5种调度算法： rr轮询【默认算法】；接受到请求之后，按照时间顺序逐一分配到后端不同的服务器上 wrr加权轮询；权重值越大，被分配访问的概率就越大，主要用于后端服务器性能不一致的情况 ip_hash；每个请求按访问IP的哈希结果分配，计算之后，nginx内部会维护一张哈希表，这样每个访客固定访问一个后端服务器，可以有效的解决动态网页存在的session共享问题。 fair;【第三方算法，需要通过额外安装upstream_fair模块实现】。更智能的一个负载均衡算法，此算法可以根据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。这种策略具有很强的自适应性，但是实际的网络环境往往不是那么简单，因此要慎用。 url_hash；【第三方算法，需要通过额外安装hash模块实现】。也是哈希算法，只不过不是基于源IP，而是基于访问的URL来生成这张哈希表。每个URL定向到同一台后端服务器，可以进一步提高后端缓存服务器的效率。 注意：当算法是ip_hash的时候，后端服务器不能被添加weight和backup 我们在location中配置nginx负载均衡的时候，还需要添加proxy_next_upstream http_500 http_502 error timeout invalid_header; 这一行参数。用于定义故障转移策略。当后端服务器节点返回500、502和执行超时等错误时，自动将请求转发到upstream负载均衡器中的另一台服务器，实现故障转移。 Nginx负载均衡工作流： 当客户端访问 xxx 域名时请求会最先到达负载均衡器,负载均衡器就会去读取自己server标签段中的配置 到location里面一看,原来这是一个要往后端web节点抛的请求 而后,nginx通过 proxy_pass的配置项 在自己的主配置文件找到了事先定义好的后端web节点 最后,按照事先设置好的调度算法,把请求带上主机头和客户端原始ip一起抛给后端准备好的web服务器 nginx负载均衡较适合用于日pv 2000W以下的站点 HAProxy Haproxy能实现基于4层和7层的负载均衡， HAproxy的8中负载均衡算法1、roundrobin表示简单的轮询，每个服务器根据权重轮流使用，在服务器的处理时间平均分配的情况下这是最流畅和公平的算法。该算法是动态的，对于实例启动慢的服务器权重会在运行中调整。 2、leastconn连接数最少的服务器优先接收连接。leastconn建议用于长会话服务，例如LDAP、SQL、TSE等，而不适合短会话协议。如HTTP.该算法是动态的，对于实例启动慢的服务器权重会在运行中调整。 3、static-rr每个服务器根据权重轮流使用，类似roundrobin，但它是静态的，意味着运行时修改权限是无效的。另外，它对服务器的数量没有限制。 该算法一般不用； 4、source对请求源IP地址进行哈希，用可用服务器的权重总数除以哈希值，根据结果进行分配。只要服务器正常，同一个客户端IP地址总是访问同一个服务器。如果哈希的结果随可用服务器数量而变化，那么客户端会定向到不同的服务器； 该算法一般用于不能插入cookie的Tcp模式。它还可以用于广域网上为拒绝使用会话cookie的客户端提供最有效的粘连； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 5、uri表示根据请求的URI左端（问号之前）进行哈希，用可用服务器的权重总数除以哈希值，根据结果进行分配。只要服务器正常，同一个URI地址总是访问同一个服务器。一般用于代理缓存和反病毒代理，以最大限度的提高缓存的命中率。该算法只能用于HTTP后端； 该算法一般用于后端是缓存服务器； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 6、url_param在HTTP GET请求的查询串中查找中指定的URL参数，基本上可以锁定使用特制的URL到特定的负载均衡器节点的要求； 该算法一般用于将同一个用户的信息发送到同一个后端服务器； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 7、hdr(name)在每个HTTP请求中查找HTTP头，HTTP头将被看作在每个HTTP请求，并针对特定的节点； 如果缺少头或者头没有任何值，则用roundrobin代替； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 8、rdp-cookie（name）为每个进来的TCP请求查询并哈希RDP cookie； 该机制用于退化的持久模式，可以使同一个用户或者同一个会话ID总是发送给同一台服务器。如果没有cookie，则使用roundrobin算法代替； 该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据“hash-type”的变化做调整。 2种配置方式指的是在1.3版本之前，ha的负载均衡配置主要是在listen部分中进行配置在1.3版本之后，为了更好的维护和管理，将负载均衡的配置拆分成为了frotend和backend这两部分，为了保证兼容性，listen部分依然保留，目前主要使用listen部分配置HA的监控页面 HA通过ACL实现一些7层的功能例如通过path_end的ACl方法实现动静资源分离 通过hdr_dom(host)和hdr_reg(host)和hdr_beg(host)的方法实现虚拟主机 LVS关于LVS，它本身只是支持负载均衡，没有检测机制，因此要结合keepalived来使用【keepalived的诞生原因就是为了给LVS提供后端节点检测功能，到后面才添加了高可用的功能】。在这里需要明确一点，它只能转发4层数据包【IP+port】但是检测是能通过7层url进行监测的。LVS的8种算法：1.轮叫调度（Round Robin）调度器通过“轮叫”调度算法将外部请求按顺序轮流分配到集群中的真实服务器上，它均等地对待每一台服务器，而不管服务器上实际的连接数和系统负载。大锅饭调度：rr - 纯轮询方式，比较垃圾。把每项请求按顺序在真正服务器中分派 2.加权轮叫（Weighted Round Robin）调度器通过“加权轮叫”调度算法根据真实服务器的不同处理能力来调度访问请求。这样可以保证处理能力强的服务器能处理更多的访问流量。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。带权重的大锅饭调度：wrr -带权重轮询方式。把每项请求按顺序在真正服务器中循环分派，但是给能力较大的服务器分派较多的作业。 3.最少链接（Least Connections）调度器通过“最少连接”调度算法动态地将网络请求调度到已建立的链接数最少的服务器上。如果集群系统的真实服务器具有相近的系统性能，采用“最小连接”调度算法可以较好地均衡负载。谁不干活就给谁分配：lc - 根据最小连接数分派 4.加权最少链接（Weighted Least Connections）在集群系统中的服务器性能差异较大的情况下，调度器采用“加权最少链接”调度算法优化负载均衡性能，具有较高权值的服务器将承受较大比例的活动连接负载。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。带权重的谁不干活就给谁分配：wlc - 带权重的。机器配置好的权重高 5.基于局部性的最少链接（Locality-Based Least Connections）“基于局部性的最少链接”调度算法是针对目标IP地址的负载均衡，目前主要用于Cache集群系统。该算法根据请求的目标IP地址找出该目标IP地址最近使用的服务器，若该服务器是可用的且没有超载，将请求发送到该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的工作负载，则用“最少链接” 的原则选出一个可用的服务器，将请求发送到该服务器。基于地区的最少连接调度：lblc - 缓存服务器集群。基于本地的最小连接。把请求传递到负载小的服务器上 6.带复制的基于局部性最少链接（Locality-Based Least Connections with Replication）“带复制的基于局部性最少链接”调度算法也是针对目标IP地址的负载均衡，目前主要用于Cache集群系统。它与LBLC算法的不同之处是它要维护从一个目标 IP地址到一组服务器的映射，而LBLC算法维护从一个目标IP地址到一台服务器的映射。该算法根据请求的目标IP地址找出该目标IP地址对应的服务器组，按“最小连接”原则从服务器组中选出一台服务器，若服务器没有超载，将请求发送到该服务器；若服务器超载，则按“最小连接”原则从这个集群中选出一台服务器，将该服务器加入到服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的程度。带有复制调度的基于地区的最少连接调度：lblcr - 带复制调度的缓存服务器集群。某页面缓存在服务器A上，被访问次数极高，而其他缓存服务器负载较低，监视是否访问同一页面，如果是访问同一页面则把请求分到其他服务器。 7.目标地址散列（Destination Hashing）“目标地址散列”调度算法根据请求的目标IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。目标散列调度：realserver中绑定两个ip。ld判断来者的ISP商，将其转到相应的IP。 8.源地址散列（Source Hashing）“源地址散列”调度算法根据请求的源IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。了解这些算法原理能够在特定的应用场合选择最适合的调度算法，从而尽可能地保持Real Server的最佳利用性。当然也可以自行开发算法，不过这已超出本文范围，请参考有关算法原理的资料。源散列调度：源地址散列。基于client地址的来源区分。（用的很少） 补充：为什么4层性能比7层更好？在7层，因为负载均衡器要获取报文内部的内容，因此要先和客户端建立连接，才能收到客户发过来的报文内容，然后获取报文内容之后，再根据调度算法进行负载。也就是说7层负载会和客户端和后端服务器分别建立一个TCP连接，而4层负载均衡只需要一次，因此性能肯定比4层差。 三种负载均衡产品之间的对比HAProxy和LVS的4层负载对比因为LVS是基于Linux内核的，但是HAProxy是属于第三方应用，因此在性能上，LVS占据绝对优势。因此，如果只是做纯4层转发，则使用LVS HAProxy对比NginxHAProxy支持更为丰富的后端节点检测机制，并且性能比Nginx好，因此在并发量较大的情况下，使用HAproxy，日PV并发量较小的情况下可以使用Nginx，配置也较为简单。 4. Redis持久化策略数据持久化策略主要分为RDB和AOF两种 RDB方式：数据文件内记录的是实际的数据。因此在进行数据恢复的时候，速度较快。适合全量备份。在进行RDB持久化时，会fork出一个单独的进行，因此会CPU的开销较大。 AOF方式：数据文件内记录的是产生数据变化的命令。因此在进行数据恢复的时候，速度较慢，并且其中的内容可以编辑，因此适合在执行了一些类似flushall或者flushdb等命令时进行数据恢复 混合持久化：Redis4.0版本之后的持久化，结合了RDB和AOF的有点，当进行AOF重写的时候，将会把当前的数据转变成为RDB形式进行保存，重写之后的数据继续以AOF的格式保存 主从复制在Redis2.6版本之前，主从复制时，每次传输的都是全量数据，因此会非常占用网络带宽和相关资源。在这之后，在Redis master节点上可以设置复制缓存区，来实现差异的增量复制。但是当缓冲区满了之后，还是会执行全量复制。 淘汰策略淘汰策略是指当Redis进行即将使用到设置的最大内存量，执行的一个策略，避免出现内存溢出的问题，也就是一种内存回收机制。一般在使用到maxmemory的90%时触发，默认策略是不回收。 在redis中可以配置的策略主要有以下几种： noeviction policy 【默认策略，永不过期策略。】不会删除任何数据，拒绝任何写入操作并返回客户端错误信息（error）OOM command not allowed when used memory，此时Redis只响应读操作 volatile-lru 根据LRU算法删除设置了超时属性（expire）的键，直到腾出足够空间为止，如果没有可以删除的键对象，则回退到noeviction策略 allkeys-lru 根据LRU算法删除键，不管数据有没有设置超时属性，直到腾出足够空间为止。 volatile-lfu 根据LFU算法删除设置了超时属性（expire）的键，直到腾出足够空间为止，如果没有可以删除的键对象，则回退到noeviction策略 allkeys-lfu 根据LFU算法删除键，不管数据有没有设置超时属性，直到腾出足够空间为止。 volatile-random 随机删除设置了超时属性（expire）的键，直到腾出足够的空间 allkeys-random 随机删除所有键，知道腾出足够空间为止 volatile-ttl 根据键值对象的ttl属性，删除最近将要过期的数据，如果没有，则回退到noeviction策略 常见性能问题 常见性能问题主要为： 内存设置不合理 大量的慢查询 key值（名称）设置过大 单个key的value过大 没有使用Redis的流水线功能 命令使用不合理，例如可以使用mset等或者禁止使用monitor等命令 客户端最大连接数设置【需要设置最大描述符，Redis默认会占用32个fd，因此可用的是1024-32】 TCP积压队列 定义AOF重写大小 客户端输出缓冲区 复制积压缓冲区 swap优化等等 哨兵模式 哨兵模式也就是Redis的高可用模式。一般的配置模式为一对主从，然后配置3个哨兵实例哨兵实例的设置原则：当有(n/2)+1个哨兵宣告需要进行切换时，才进行切换，这一点同样适用于zk等集群选举。因此最好3个以上的奇数个实例，偶数个会浪费一个。【这在5个以上节点时能看出明显的效果】 分布式集群 集群采用哈希槽的分配方式，一共有0-16383个槽最小建议配置为3主3从。Redis集群使用的是gossip协议。 cachecloud云平台 这是我从github上引入的Redis运维项目 5. Mysql+OracleMysql基础知识 Mysql主从复制原理 整体上来说，复制有3个步骤： A.master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events）； B.slave将master的binary log events拷贝到它的中继日志(relay log)； C.slave重做中继日志中的事件，将改变反映它自己的数据。 Mysql读写分离 Mysql高可用和集群有几种高可用方案：Mysql双主+keepalived【优点：架构简单，】 Mysql备份与恢复 逻辑备份：备份的是产生数据变化的sql语句。mysqldump能直接进行这个操作，但是因为它在备份过程中会锁表，并且备份的速度也非常的慢因此我们需要选择第三方工具。 物理备份：备份的是实际的数据，直接拷贝mysql的数据目录。 直接拷贝只适用于myisam类型的表。这种类型的表是与机器独立的。【这种备份的粒度较粗，不能实现更细粒度的数据恢复，特别是对于更新较为频繁的系统。】 实际生产环境中一般使用完整备份+增量备份每周日凌晨2点进行一次全量备份，之后的每天凌晨2点进行一次增量备份 然后再每天备份binlog日志【为了粒度更细致的数据恢复】 Mysql优化 mysql优化包括其他所有的网络服务优化，思路都是一致的。 分层次的来进行。【普遍规律+应用需求】 普遍规律： 首先是硬件层面 再次是操作系统层面的基础优化，例如文件描述符的数量，swap使用限制，文件系统（目前主流xfs） 再次是c/s架构方面的优化，例如TCP的连接队列大小，队列的缓存大小。tcp连接超时时间，tcp滑动窗口（发送和接受） 应用需求：数据库通用：最大连接数，索引（优先在where,order group等涉及的列上创建索引），sort，group等排序结果的缓冲区大小，慢查询，sql语句优化（减少使用like等开销大的语句），命令规范等Mysql：存储引擎 JAVA类：JVM设置，是否设置锁内存策略，堆内还是堆外内存，线程数量等。web类：压缩，静态文件缓存，CDN加速等 Mysql特殊：存储引擎等 Mysql常见问题 慢查询，sql写的有问题 mha的时候VIP漂移有问题 连接数问题 版本不一致问题 Oracle oracle没什么数据库概念上，oracle是只有一个数据库，然后里面有多用户，每个用户多表mysql是多个数据库，多个用户，采取授权的形式来访问 6. Ansible等自动化工具Ad-Hoc Ad-Hoc指的是一般性的临时操作 日常运维中主要使用的模块有： shell模块 yum模块 copy模块 service模块 PlaybookAnsible使用YAML语法描述配置文件，这个配置文件就被成为playbook，剧本 Ansible的核心理念是：极致的简单高效并且Ansbile是使用python编写的，因此在后续的二次开发上更占据优势。另一个趋势是python的运行方式，它和区块链一致，采用的是去中心化的部署方式，不需要安装客户端即可，通过SSH来实现，并且目前还提供了SSH的加速模式，适用于大规模的环境中，可以说，Ansible绝对是未来的趋势主流。 puppet、chef、slatstackpuppet和chef都是使用ruby编写的，并且配置繁琐，都需要配置客户端目前不适合 slatstack也是通过python编写，但是slatstack适用于更大的规模，因为ansible使用ssh来传输命令，而它使用zeroMQ来传输数据在1000台主机的情况下，MQ用时2秒左右，而ansible的SSH则用时85秒。 对比ansible默认情况下适用于200台以内的主机，适合中小型企业，如果数量再多可以使用Ansible的加速模式去实现 选型标准：选择最合适，如果当前的运维环境主机在百台，则ansible是最好的选择，如果上千台，那么无疑使用slatstack。 cobbler和kickstartkickstart是传统的批量装机方式，配置比较繁琐 cobbler是较早前的kickstart的升级版本，有点是容易配置 并且cobbler具有高级功能，可以根据不同机器的MAC地址来进行设置装机 关闭自动装机这里之前还发生过一个问题，就是有一次在装机的时候使用的是百兆交换机，导致老是有几台装不上，后来都换成千兆之后，就解决了这个问题。 关闭这个批量装机，因此centos的网卡名称不再是ethx的形式，因此在安装的时候，我们需要再ks文件中添加命令，来调整网卡的命令规则 7. Nginx，Httpd，tomcat，weblogic，php，gitlab，Jenkins这部分和web相关，主要是和电商，互联网公司等核心为web的紧密相关，也就是主要是LNMP这一套 Nginx基础知识基础知识 Nginx主要分为几个模块 全局配置【worker数量，worker的最大打开数量，CPU指定等】 Event模块配置【worker的最大连接数等，网络IO处理模型等】 Http模块【其中包括upstream段,server段,server中的location段等】 主要配置的地方就是HTTP模块中的upstream，server中的location段【动静分离等都是在这里进行配置】 注意：nginx的模块是静态的，在编译时就已经完全编译进去，而不是像Httpd是动态链接的形式 Nginx常见问题日志文件将磁盘存储空间占满了。 Nginx常见应用场景web服务器【一般会做动静分离，rewrite功能（重定向302是临时，301是永久，地址栏都改变，主要看爬虫变不变），防盗链】 负载均衡服务器 Nginx优化全局优化 工作进程数量（worker_processes数量）一般等于CPU的核数，因为每个进程是单线程的模式，使用epoll网络IO模型来进行处理。 worker_rlimit_nofile 60000；每个work进程最大打开文件数量。【这里需要跟操作系统的文件描述符相对应】 Event模块优化 worker进程最大连接优化，官方数据是能支持到5W【那么所有的连接数=5W*几个worker】 网络模型【通常使用epoll模型】 HTTP模块优化 不显示版本 关闭TCP延迟发送数据 keepalive的超时时间等 压缩传输的设置【压缩级别，压缩的触发大小】 Nginx和Httpd在这里主要说web，不说nginx的负载均衡，这部分已经在第3条说了。 tomcat常见问题**数据库连接问题，后端数据库异常，没有连接到tomcat乱码tomcat日志大小问题，权限问题JAVA_HOME没有设置正确 tomcat优化**主要分为2块，tomcat的JVM内存优化和tomcat的并发优化 内存优化：Tomcat内存优化主要是对 tomcat 启动参数优化，我们可以在 tomcat 的启动脚本 catalina.sh 中设置 java_OPTS 参数 JAVA_OPTS参数说明 -server 启用jdk 的 server 版； -Xms Java虚拟机初始化时的最小堆内存； -Xmx java虚拟机可使用的最大堆内存； 【堆内存建议设置一致，避免GC回收后再次动态分配，增大系统的开销】 -XX: PermSize 内存永久保留区域 -XX:MaxPermSize 内存最大永久保留区域 【这部分，默认64位的是256M】 JAVA_OPTS=’-Xms1024m -Xmx2048m -XX: PermSize=256M -XX:MaxNewSize=256m -XX:MaxPermSize=256m’ 并发优化/线程优化+缓存优化： 在Tomcat 配置文件 server.xml 中的 参数说明 maxThreads 客户请求最大线程数 表示最多同时处理多少个连接 minSpareThreads **Tomcat初始化时创建的 socket 线程数** maxSpareThreads **Tomcat连接器的最大空闲 socket 线程数 ** enableLookups 若设为true, 则支持域名解析，可把 ip 地址解析为主机名 redirectPort 在需要基于安全通道的场合，把客户请求转发到基于SSL 的 redirectPort 端口 acceptAccount 监听端口队列最大数，满了之后客户请求会被拒绝（不能小于maxSpareThreads ） connectionTimeout 连接超时 minProcessors 服务器创建时的最小处理线程数 maxProcessors 服务器同时最大处理线程数 URIEncoding URL统一编码 compression 打开压缩功能 compressionMinSize 启用压缩的输出内容大小，这里面默认为2KB compressableMimeType 压缩类型 connectionTimeout 定义建立客户连接超时的时间. 如果为 -1, 表示不限制建立客户连接的时间 参考配置： tomcat多实例部署http://blog.51cto.com/watchmen/1955972 传统方式复制目录的话，会造成资源浪费，因为lib和bin等公共资源会被多次加载，造成在内存中不必要的重复 思路：将bin下的文件和lib文件单独拆分出来 weblogicweblogic最开始bea公司的一个JAVA中间件产品，现在归属于oracle功能非常的强大，支持EJB比如在配置程序连接数据库时，不需要再代码中通过jdbc的方式去人工手动指定，而是通过后台管理页面的数据源配置中，进行配置。所以说，在一般的环境中，使用tomcat即可，如果涉及到大型的java应用开发，就要使用weblogic PHPPHP主要对接Nginx，处理php文件【通过php-fpm来处理】PHP-CGI 解释器每进程消耗 7 至 25 兆内存所以它的优化是进程数量的设置【包括启动时分配的，最小空闲的，最大空闲的，最大值】一般启动时分配5个，最小空闲为5个，最大空闲为32个，最大值为32个 GitlabJenkinsJDK支持tomcat支持maven支持Jenkins支持 Jenkins的安装一共有3个步骤 首先是下载war包到tomcat的webapps目录并将其重命名为ROOT.war，之后就是对其环境变量进行配置。 设定jenkins的目录及管理用户及编码修改tomcat目录下./conf/context.xml：增加jenkins环境变量 修改tomcat目录下的./conf/server.xml,是编码符合jenkins 步骤四：在第一次登陆jenkins页面时，需要输入一串加密数据这串数据位于其家目录下的./secrets/initialAdminPassword之中。 流程：JDK+tomcat部署Jenkins添加git 源码仓库使用maven进行构建【需要编写触发脚本，当有源码发生变化时，在2分钟后进行构建部署等操作】 8. 消息队列MQ产品使用MQ产品的原因 程序异步解耦 数据冗余 扩展性，不需要改变程序的代码，就可以扩展性能。 灵活性，峰值处理能力。 消息的顺序保证 异步通信，允许用户把消息放入队列中，但是并不立即处理它 ActiveMQ；老牌的MQ产品，完全遵守JMS规范。是apache开源的一个MQ产品，比较重量级，没有什么特殊的亮点 ActiveMQ的高可用集群模式通过ZK来实现，为了保证数据的一致性，因此会严重影响性能。从ActiveMQ 5.9开始，它实现了通过ZooKeeper + LevelDB实现高可用集群的部署方式。这种方式，对外只有Master提供服务这种方式实现了可以称之为半事务特性的机制，Master 将会存储并更新然后等待 (2-1)=1 个Slave存储和更新完成，才汇报 success RabbitMQ；遵循AMQP协议，借助erlang的特性在可靠性、稳定性和实时性上比别的MQ做得更好，非常重量级，性能比较好，适合企业级的开发。但是不利于做二次开发和维护 由于 rabbitmq 是使用 erlang 开发的，而 erlang 就是为分布式而生的。所以 rabbitmq 便于集群。rabbitmq 集群有两种模式：普通模式、镜像模式。 普通模式：也是默认模式，对于 queue 来说，消息实体只存在与其中的一个节点，A、B 两个节点只有相同的元数据，即队列的结构。当消息在A时，消费中从B中取消息时，消息会从A中传递到B中。所以consumer应尽量连接每一个节点，从中取消息。即对于同一个逻辑队列，要在多个节点建立物理Queue。否则无论consumer连rabbit01或rabbit02，出口总在rabbit01，会产生瓶颈。 镜像模式：镜像模式是 rabbitmq 的 HA 方案。其与普通模式的唯一不同之处在于消息会在 A，B 两个节点中同步。当然这种模式带来的副作用也是显而易见的。除了降低系统性能以外，如果队列数量过多，网络带宽将会受到影响。所以这种情况只运用到对高可靠性要求的场合上。 集群配置方式：安装erlang,然后同步三台机器上的.erlang.cookie文件内容因为RabbitMQ的集群是依赖erlang集群，而erlang集群是通过这个cookie进行通信认证的，因此我们做集群的第一步就是干cookie。注意：erlang.cookie文件中cookie值一致，且权限为owner只读。因此需要设置为600 注意： RabbitMQ单节点环境只允许是磁盘节点，防止重启RabbitMQ时丢失系统的配置信息。RabbitMQ集群环境至少要有一个磁盘节点，因为当节点加入或者离开集群时，必须要将该变更通知到至少一个磁盘节点。 kafka；也是apache基金会的一个MQ产品。高吞吐量，消息的接受和消费都是落地到磁盘，因此适用于大数据环境流处理，对实时性要求不是太高的环境，可以积压非常庞大的数据量（瓶颈在磁盘） kafka是一种分布式的，基于发布/订阅的消息系统。有主分区和副本分区的概念。并且kafka中的数据是追加的形式，保证了消息的有序性 rocketmq；阿里开发并开发的一个MQ产品，纯JAVA开发。具有高吞吐量、高可用性、适合大规模分布式系统应用的特点。RocketMQ思路起源于Kafka，但并不是Kafka的一个Copy，它对消息的可靠传输及事务性做了优化，目前在阿里集团被广泛应用于交易、充值、流计算、消息推送、日志流式处理、binglog分发等场景。 9. flume，zk，es，logstash，kibana系列flume flume：是一个日志收集软件。flume的agent设计实现这一系列的操作，一个agent就是一个java进程，运行在日志收集节点-也就是日志收集服务器节点。 agent里面包含3个核心的组件：source—-&gt;channel—–&gt;sink,类似生产者、仓库、消费者的架构。source:收集数据，可以处理各种类型sink：该组件是用于把数据发送到目的地的组件，目的地包括有：hdfs，kafka等等文件系统 工作流：flume的核心是把数据从数据源(source)收集过来，在将收集到的数据送到指定的目的地(sink)。为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel),待数据真正到达目的地(sink)后，flume在删除自己缓存的数据。 也就是说flume提供了一种类似事务机制。 flume的2种工作模式：主动模式和被动模式【主要是针对客户端来说】。这两种模式和zabbix的两种模式一样 在进行配置的时候，每个agent实例是通过别名来进行区分的。 kafka流式消息队列产品，接受flume发送过来的消息，或者日常产生端直接将JSON格式的数据发送到Kafka中。详见上方MQ产品 zookeeperzk:是一个分布式应用程序协调组件，用于为哪些原生没有提供集群功能的服务实现分布式集群。提供的功能包括：配置维护、域名服务、分布式同步、组服务等。zk的工作流：1、选举Leader。（选举zk集群中的leader）2、同步数据。3、选举Leader过程中算法有很多，但要达到的选举标准是一致的。4、Leader要具有最高的执行ID，类似root权限。5、集群中大多数的机器得到响应并接受选出的Leader。 注意：zk在3.5.0以上的版本会有一个内嵌的web服务，通过访问http://localhost:8080/commands来访问以上的命令列表。 一旦Zk集群启动之后，它将等待客户端的连接 esEs的主要功能是将收集的数据建立索引，方便日后数据的存储于检索。ES不止是一个全文本引擎，他还是一个分布式实时文档存储系统。这里，KCE的数据目的地和ES的数据来源设置成了一个分区，因此避免了磁盘IO的二次开销 logstash日志收集，需要在日志产生端配置，收集日志，再进行发送，目前使用flume来代替了。 kibanakibana不多说了，主要是提供了一个连接ES的入口 10. dockerdocker的核心三大组件是 镜像 容器 仓库 镜像主要分为几种，一个是官方的或者别人已经写好的镜像文件另一个可以自己产生镜像文件。 自己产生的镜像文件可以分为两种 在现有镜像的基础之上commit出来一个新的镜像 编写dockerfile文件，然后build出来一个镜像 建议通过dockerfile的形式产生镜像，因为使用commit出来的镜像会存在很多的缓存文件等。 容器是镜像的运行态，和程序及进程的概念比较像。 仓库主要分为两种，一个是存储镜像的仓库【里面的 镜像通过tag标签来尽心区分，默认是latest】，另一个是存储仓库名称的注册仓库 公网上的仓库可以是docker hub，也可以通过官方提供的registry镜像来简单搭建一套本地私有仓库环境: dockerfile编写dockerfile主要分为4个部分 基础镜像信息 from字段，也就是这个应用是以那个镜像为基础的 维护者信息，maintainer，也就是作者信息 镜像的操作指令，也就是在制作镜像是要执行的一系列操作，add加入一系列的文件，例如JDK，war包等 容器启动时执行指令-CMD，在启动时要执行的操作，例如启动项目等 在cachecloud中，基础镜像是使用的centos7.4-内核基础3.0-1811系统维护者是我，镜像的操作指令是JDK环境等等；容器启动时执行的命令是启动cachecloud项目 k8s k8s是谷歌开源的一个容器集群管理项目k8s对集群中的资源进行了不同级别的抽象，每个资源都是一个rest对象，通过API进行操作，通过JSON/YAML格式的模板文件进行定义要注意的是，k8s并不是直接对容器操作，它的操作最小单位是容器组。容器组由一个或多个容器组成。k8s围绕着容器组进行创建，调度，停止等生命周期管理。 ESXI,vsphere,xen,kvm 这些是第一家公司所使用的产品exsi和vsphere是vmware公司的企业虚拟化产品，相比于kvm，它有更好的性能，因此它是直接在物理上安装虚拟化操作系统，不需要第三方软件的实现。esxi是单机版本，vsphere是集中管理版本，支持在线迁移等高级功能。xenserver是思杰公司的一个虚拟化产品，单机的操作比vmware的esxi好，但是在涉及到多机环境时不是太好kvm需要linux系统的支持，然后还要安装一系列的组件，相对来说，更方便，但是不够专业，一般企业使用的相对较少。 11. 监控软件及JMX，JVMzabbix 我们的生产是怎么监控的 首先是监控模板，监控一些基础指标，例如CPU，内存，磁盘等 一些类似HAproxy，activemq等有web页面的应用我们通过web监控来实现【创建web场景，60秒内，尝试连接3次，如果3次都失败，则报警，这里还会涉及到一些有认证的页面，也是可以实现的。】 更高级一点的例如redis等应用，需要监控一些特定的指标，我们通过自定义监控项来实现。 JAVA类的应用，在后期慢慢的开放了JMX端口的情况下，陆续加入了JMX的监控。 自定义监控项为了简单高效，我们自己编写的脚本，判断引用的状态，将采用所能想到的一切来判断，然后再最后只输出一个0,如果服务不正常的话，则输出为1。 zabbix的一些优化操作采取zabbix的主动模式来进行监控使用自动发现的功能。 自动发现等操作 各监控产品的区别zabbix是一款商业的开源软件，涉及到的东西非常之多，因此官方能够靠咨询，技术服务等来收费运作。而cacti，nagios等是普通的开源软件，自然没有zabbix这么强大。 nagios的可视化功能非常弱，zabbix是有自己的可视化界面的【一般我们都是通过最新数据哪里查看，为了给zabbix减负，不是非必要的情况下，一般不会给监控项添加图形】它不支持自动发现，并且缺少图形展示工具，也没有历史数据，追查起来非常困难。 cacti是一个PHP程序它通过使用SNMP 协议获取远端网络设备和相关信息，（其实就是使用Net-SNMP 软件包的snmpget 和snmpwalk 命令获取）并通过RRDTOOL 工具绘图， 通过SNMP采集数据，并且自定义监控项等非常繁琐，报警方式需要添加插件等。 JMX监控 前提条件：需要JAVA类程序开放JMX端口【也就是开放API接口】 工作流：（1）zabbix_server需要知道一台主机上的特定端口的JMX值时，它会向Zabbix-Java-gateway进程去询问。这个连接进程叫做StartJavaPollers （2）Zabbix-Java-gateway使用JMXmanagementAPI这个API去查询特定的应用程序 注意：在配置的时候，StartJavaPollers线程数量要小于等于START_POLLERS设置的线程数量 这些操作操作完毕之后，在web页面上进行操作，添加JMX监控模板即可。 JVM调优 提到虚拟机的内存结构，可能首先想起来的就是堆栈。对象分配到堆上，栈上用来分配对象的引用以及一些基本数据类型相关的值。 JAVA虚拟机的内存结构是分了好几个区域的。分区域的好处是： 便于查找 便于内存回收【如果不分，回收内存就要全部内存扫描】 JVM内存分区（5部分）： 方法区 线程共享【这部分常被成为永久代，除了编译后的字节码之外，方法区中还会存放常量，静态变量以及及时编译器编译后的代码等数据。】 堆 线程共享【这部分一般是Java虚拟机中最大的一块内存区域，这块存储对象的实例。堆内存是垃圾收集器主要光顾的区域，一般来讲根据使用的垃圾收集器的不同，堆中还会划分为一些区域，比如新生代和老年代。新生代还可以再划分为Eden，Survivor等区域。另外为了性能和安全性的角度，在堆中还会为线程划分单独的区域，称之为线程分配缓冲区。更细致的划分是为了让垃圾收集器能够更高效的工作，提高垃圾收集的效率。】 Java栈 线程独享【每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。在Java虚拟机规范中，对于此区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展，当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。】 本地方法栈 线程独享【本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。】 程序计数器 线程独享【这部分内存不会内存溢出，字节码行号提示器】 堆（新生代和老年代等）： Xms256m 代表堆内存初始值为256MB Xmx1024m 代表堆内存最大值为1024MB如果-Xmx不指定或者指定偏小，应用可能会导致java.lang.OutOfMemory错误 方法区（永久代） PermSize和MaxPermSize指明虚拟机为java永久生成对象（Permanate generation）例如：class对象、方法对象这些可反射（reflective）对象分配内存限制，这些内存不包括在Heap（堆内存）区之中。-XX:PermSize=64MB 最小尺寸，初始分配XX:MaxPermSize=256MB 最大允许分配尺寸，按需分配这部分设置过小会导致：java.lang.OutOfMemoryError: PermGen space MaxPermSize缺省值和-server -client选项相关。-server选项下默认MaxPermSize为64m。 -client选项下默认MaxPermSize为32m 设置-Xms、-Xmx 相等以避免在每次GC 后调整堆的大小 在jdk1.8之前之前我们将储存类信息、常量、静态变量的方法区称为持久代(Permanent Generation)，PermSize和MaxPermSize是设置持久代大小的参数，在jdk1.8中持久代被完全移除了，所以这两个参数也被移除了，多了一个元数据区(Metadata Space)，所以设置元数据区大小的参数也变成对应的MetaspaceSize和MaxMetaspaceSize了 -XX:PermSize和-XX:MaxPermSize在jdk1.8中使用-XX:MetaspaceSize和-XX:MaxMetaspaceSize替代了现在能兼容正常启动，没有产生影响，知悉。 云产品说起阿里云，这期间还发生了一个人为事故。当初京东金融本来是通过我们的平台发送的，但是它要我们给他拉专线直接连接运营商。但是这边没有给他拉，而是买了一台阿里云服务器，暴露出一个公网IP地址让它连接，在这台服务器上面部署HAproxy，还是调整到我们的平台。【上边领导们的决定，我就不评论是非对错了 哈哈】 然后有一天，突然HAProxy的web监测报警，页面打不开。马上上服务器看，CPU爆了【买的服务器配置一般】检查进程。内存正常，磁盘正常，CPU爆了，然后再查看网络连接，发现有大量的CLOSE_WAIT（400个close_wait;100多个establish） 在TCP关闭时，主动关闭的一方发出 FIN 包，被动关闭的一方响应 ACK 包，此时，被动关闭的一方就进入了 CLOSE_WAIT 状态。如果一切正常，稍后被动关闭的一方也会发出 FIN 包，然后迁移到 LAST_ACK 状态。 导致产生大量close_wait的原因是突然遭遇大量的请求，即便响应速度不慢，但是也来不及消费，导致多余的请求还在队列里就被对方关闭了。（因为对方设置了超时时间）。但是linux没有对close_wait做类似超时控制的设置，如果不重启进程，这个状态很可能会永远的持续下去， AWS主要是当初想搭VPN，但是一大堆的限制，最终没成功，所以现在是直接买的商业的，稳定，速度也有保证。七牛云，产品主要是数据存储和CDN加速，我自己的博客目前也是在用七牛云。瑞江云，是公司在做什么业务时和人家合作时，人家送的，具体什么我就不知道了 13 自身素质关于这三个人的管理经验，是在第一家公司公司的时候。亿阳分为很多个部门，其中就有一个对外产品部门，当时是准备和人保合作，进入金融行业。因此拿下了一个标，但是招运维主管的时候的比较难招，差不不行，好的知道是外包驻场的形式一般也不愿意来，到最后实在没办法只能从公司内部要人了，然后就把我派过去了。在那边呆了有7个月左右。当时工作非常艰辛【上一家被换掉是因为政治原因，具体是谁就不知道了】，因此过去需要接受上一家的工作，然后开发二代新产品，中间不能停，也就是起承上启下的作用。当时1个月直接就瘦了10斤，天天加班。 高效办公系列软件TC:资源管理器Autohotey：热键管理器Listary：文件搜索浏览增强工具evernote:云笔记Fences:桌面管理工具Ditto：剪切板增强工具Snipaste：截图工具Everything:文件搜索工具 运维职业规划-如何通过运维思想做好运维工作运维思想-运维核心稳定性-网站/平台不宕机【这是运维的核心】一般通过以下方式来实现 架构使用集群+负载均衡+高可用+应用解耦，微服务等部署方式来保证性能 安全【】 运营推广不能在白天高峰期推广，需要和运维打招呼 前端图片的优化，不能使用大图等，尽量使用缩略图 数据库优化【加入Redis数据缓存层，sql语句优化等】 避免随时上线的操作【减少次数】 测试生产等环境保持一致【系统，软件版本，路径等等】 流程操作【运维标准和流程】 等等等等 数据不丢失 应用配置数据管理-【考虑使用CMDB等平台】 数据库数据 避免人为问题避免人为错误，主要分为两个方面， 一个是他人(主要是开发不严谨)产生的错误，这部分通过运维流程并结合工具控制。【比如测试不严谨，或者开发人员的代码有问题，直接把服务器资源跑没了】 一个是自己操作产生的问题，这部分通过一些智能化的自动化工具来尽量避免【避免在执行命令的时候误操作等等，常见的有】 解决：建立完善的流程制度，对运维来说，包括标准化，对开发测试来说，包括上线的流程化【通过运维制度和一些工具来实现】 提升运维效率这一点是放在最后的，是在上面都做好的前提下，然后再有这么一层，什么自动化，CICD，devops，不是说招几个运维开发就能解决的。一定是需要一个过程的。运维效率很重要，但是不能盲目的只盯住这个上面 个人如何做好运维工作主动性很多东西如果不主动去找系统负责人去推进，进度根本没法完成。 划重点的能力写文档，研究技术，培训讲解等，需要将其中最重要的东西给讲述出来。就比如在看书的时候，有时候一些大部头的书，可能一句话非常长，你要从中快速挑出这句话的重点。然后建立知识体系。【这就需要能快速的找出重点，快速浏览说明性的内容，因为有可能这些说明性内容对你目前的水平来说完全来说可以忽略】 全局观 比如像我当初对接那么多的系统，在出问题的时候，可能是后面某一个系统出问题，但是导致你直接无法使用，所以你需要根据症状， 态度某一项技术不会非常正常，要摆正心态，虚心向人学习，比如像开发学习，像DBA学习，等等。构建完善的知识体系。一个技术不会到会其实有时候就是一个月的事，根本没有大家想的那么恐怖，不要怕，大胆的去问。 换位思考，自身作则 上面的任务怎么说话去分派下去。怎么安排任务， 流程制度 流程化，制度化为了便于管理，减少出错的概率。因此要有流程和制度 新员工刚进来，可以适当的较少压力，因为有 分配任务的时候，要求下面的人去重复描述下，确保正确无误 优秀的思维去分享给团队，让团队一起成长 比如烧开水理论， 优秀的团队应该是一列高铁 个人职业规划个人现阶段的努力方向是能够快速解决问题这个要求就非常高，需要具备一定的开发能力。比如开发开发出来的程序，在测试上正常，但是一到生产上，服务器的负载就持续飙升，CPU资源被消耗殆尽，这个时候要能够快速的定位到进程。然后要能分析进程内部的资源消耗情况，比如调用内核的哪些系统调用的情况引起的异常等等，找到之后能不能定位到相应的程序代码，这样才能解决问题，而不是找到进程之后，简单的重启。【这一阶段基本上就是资深运维开发工程师级别，预计3年时间】 在这之后下一个阶段目标是未雨绸缪，在源头将问题遏制住因此需要具备开发能力，在软件需求评审和软件设计阶段就要参与进来。【在这一阶段基本上就达到了架构师的水平】 补充HTTP协议POST与GET的区别 GET是从服务器上获取数据，POST是向服务器传送数据 GET是通过发送HTTP协议通过URl参数传递进行接收，而POST是实体数据，通过表单提交 GET传送的数据量较小，不能大于2KB。POST传送的数据量较大，一般被默认为不受限制。 GET安全性非常低，POST安全性较高]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>职场</category>
        <category>简历内容应答</category>
      </categories>
      <tags>
        <tag>简历内容应答</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10添加指定程序到开机自启动]]></title>
    <url>%2F2018%2F04%2F03%2FWin10%E6%B7%BB%E5%8A%A0%E6%8C%87%E5%AE%9A%E7%A8%8B%E5%BA%8F%E5%88%B0%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[打开运行对话框（win键+R），输入命令 shell:startup 会直接弹出启动项对应的目录，然后像把应用程序快捷方式(需要对该执行文件右键创建快捷方式)复制或者剪切到启动目录 注意：该方式的启动项对应的目录是个人目录，也就是说不是针对系统上的所有用户。]]></content>
      <categories>
        <category>IT基础知识</category>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Win10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Http Server 网络处理模型的进化之路]]></title>
    <url>%2F2018%2F04%2F02%2FHttp-Server-%E7%BD%91%E7%BB%9C%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96%E4%B9%8B%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[缘起我刚毕业那会儿，国家还是包分配工作的，我的死党小明被分配到了一个叫数据库的大城市，天天都可以坐在高端大气上档次的机房里，在那里专门执行 SQL查询优化，工作稳定又舒适； 隔壁宿舍的小白被送到了编译器镇，在那里专门把 C 源文件编译成 EXE 程序，虽然累，但是技术含量非常高，工资高，假期多。 我成绩不太好，典型的差生，四级补考了两次才过，被发配到了一个不知道什么名字的村庄，据说要处理什么 HTTP请求，这个村庄其实就是一个破旧的电脑，令我欣慰的是可以上网，时不时能和死党们通个信什么的。 不过辅导员说了，我们都有光明的前途。 Http Server 1.0HTTP是个新鲜的事物，能够激起我一点点工作的兴趣，不至于沉沦下去。 一上班，操作系统老大扔给我一大堆文档： “这是 HTTP协议， 两天看完！” 我这样的英文水平，这几十页的英文 HTTP协议我不吃不喝不睡两天也看不完， 死猪不怕开水烫，慢慢磨吧。 两个星期以后，我终于大概明白了这 HTTP是怎么回事：无非是有些电脑上的浏览器向我这个破电脑发送一个预先定义好的文本（Http request）, 然后我这边处理一下（通常是从硬盘上取一个后缀名是 html的文件，然后再把这个文件通过文本方式发回去（http response），就这么简单。 唯一麻烦的实现，我得请操作系统给我建立 Http 层下面的 TCP 连接通道， 因为所有的文本数据都得通过这些 TCP通道接收和发送，这个通道是用 socket建立的。 弄明白了原理，我很快就搞出了第一版程序，这个程序长这个样子： 看看， 这些 socket, bind, listen , accept… 都是操作系统老大提供的接口， 我能做的也就是把他们组装起来：先在 80端口监听，然后进入无限循环，如果有连接请求来了，就接受 (accept)，创建新的 socket，最后才可以通过这个 socket来接收，发送 http 数据。 老大给我的程序起了个名称，Http Server 版本 1.0 。 这个名字听起来挺高端的，我喜欢。 我兴冲冲的拿来实验，程序启动了，在 80端口“蹲守”，过了一会儿就有连接请求了， 赶紧 Accept ,建立新的 socket ，成功 ！接下来就需要从 socket 中读取 Http Request 了。 可是这个 receive 调用好慢，我足足等了 100 毫秒还没有响应！我被阻塞 (block) 住了！ 操作系统老大说：“别急啊，我也在等着从网卡那里读数据，读完以后就会复制给你。” 我乐的清闲，可以休息一下。 可是操作系统老大说：“别介啊，后边还有很多浏览器要发起连接，你不能在这儿歇着啊。” 我说不歇着怎么办？receive调用在你这里阻塞着，我除了加入阻塞队列，让出 CPU 让别人用还能干什么？ 老大说： “唉，大学里没听说过多进程吗？你现在很明显是单进程，一旦阻塞就完蛋了，想办法用下多进程，每个进程处理一个请求！” 老大教训的是，我忘了多进程并发编程了。 Http Server 2.0 ：多进程多进程的思路非常简单，当 accept连接以后，对于这个新的 socket ，不在主进程里处理，而是新创建子进程来接管。这样主进程就不会阻塞在 receive 上，可以继续接受新的连接了。 我改写了代码，把 Http server 升级为 V2.0，这次运行顺畅了很多，能并发的处理很多连接了。 这个时候 Web 刚刚兴起，我这个 Http Server 访问的人还不多，每分钟也就那么几十个连接发过来，我轻松应对。 由于是新鲜事物，我还有资本给搞数据库的小明和做编译的小白吹吹牛，告诉他们我可是网络高手。 没过几年，Web迅速发展，我所在的破旧机器也不行了，换成了一个性能强悍的服务器，也搬到了四季如春的机房里。 现在每秒中都有上百个连接请求了，有些连接持续的时间还相当的长，所以我经常得创建成百上千的进程来处理他们，每个进程都得耗费大量的系统资源，很明显操作系统老大已经不堪重负了。 他说：“咱们不能这么干了，这么多进程，光是做进程切换就把我累死了。” “要不对每个 Socket 连接我不用进程了，使用线程？ ” “可能好一点，但我还是得切换线程啊，你想想办法限制一下数量吧。” 我怎么限制？我只能说同一时刻，我只能支持 x个连接，其他的连接只能排队等待了。 这肯定不是一个好的办法。 Http Server 3.0 : Select模型老大说：“我们仔细合计合计，对我来说，一个 Socket连接就是一个所谓的文件描述符（File Descriptor ,简称 fd , 是个整数），这个 fd 背后是一个简单的数据结构，但是我们用了一个非常重量级的东西 – 进程 –来表示对它的读写操作，有点浪费啊。” 我说：“要不咱们还切换回单进程模型？但是又会回到老路上去，一个 receive 的阻塞就什么事都干不了了。” “单进程也不是不可以，但是我们要改变一下工作方式。” “改成什么？” 我想不透老大在卖什么关子。 “你想想你阻塞的本质原因，还不是因为人家浏览器还没有把数据发过来，我自然也没法给你，而你又迫不及待的想去读，我只好把你阻塞。在单进程情况下，一阻塞，别的事儿都干不了。“ “对，就是这样” “所以你接受了客户端连接以后，不能那么着急的去读，咱们这么办，你的每个 socket fd 都有编号，你把这些编号告诉我，就可以阻塞休息了 。” 我问道：“这不和以前一样吗？原来是调用 receive 时阻塞，现在还是阻塞。” “听我说完，我会在后台检查这些编号的 socket，如果发现这些 socket 可以读写，我会把对应的 socket 做个标记，把你唤醒去处理这些 socket 的数据，你处理完了，再把你的那些 socket fd 告诉我，再次进入阻塞，如此循环往复。” 我有点明白了：“ 这是我们俩的一种通信方式，我告诉你我要等待什么东西，然后阻塞，如果事件发生了，你就把我唤醒，让我做事情。” “对，关键点是你等我的通知，我把你从阻塞状态唤醒后，你一定要去遍历一遍所有的 socket fd，看看谁有标记，有标记的做相应处理。我把这种方式叫做 select 。” 我用 select 的方式改写了 Http server，抛弃了一个 socket 请求对于一个进程的模式，现在我用一个进程就可以处理所有的 socket了。 Http Server4.0 : epoll这种称为 select 的方式运行了一段时间，效果还不错，我只管把 socket fd 告诉老大，然后等着他通知我就行了。 有一次我无意中问老大：“我每次最多可以告诉你多少个 socket fd？” “1024个。” “那就是说我一个进程最多只能监控 1024 个 socket 了？ ” “是的，你可以考虑多用几个进程啊！” 这倒是一个办法，不过”select”的方式用的多了，我就发现了弊端，最大的问题就是我从阻塞中恢复以后，需要遍历这 1000 多个 socket fd，看看有没有标志位需要处理。 实际的情况是， 很多 socket 并不活跃， 在一段时间内浏览器并没有数据发过来， 这 1000 多个 socket 可能只有那么几十个需要真正的处理，但是我不得不查看所有的 socket fd，这挺烦人的。 难道老大不能把那些发生了变化的 socket 告诉我吗？ 我把这个想法给老大说了下，他说：“嗯，现在访问量越来越大， select 方式已经不满足要求，我们需要与时俱进了，我想了一个新的方式，叫做 epoll。” “看到没有，使用 epoll 和 select 其实类似“ 老大接着说 ：”不同的地方是第 3 步和第 4 步，我只会告诉你那些可以读写的 socket , 你呢只需要处理这些 ‘ready’ 的 socket 就可以了“ “看来老大想的很周全， 这种方式对我来说就简单的多了。 ” 我用 epoll 把 Http Server 再次升级，由于不需要遍历全部集合，只需要处理哪些有变化的，活跃的 socket 文件描述符，系统的处理能力有了飞跃的提升。 我的 Http Server 受到了广泛的欢迎，全世界有无数人在使用，最后死党数据库小明也知道了，他问我：“ 大家都说你能轻松的支持好几万的并发连接， 真是这样吗？ ” 我谦虚的说：“过奖，其实还得做系统的优化啦。” 他说：“厉害啊，你小子走了狗屎运了啊。” 我回答： “毕业那会儿辅导员不是说过吗， 每个人都有光明的前途。”]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>网络知识</category>
        <category>epoll模型</category>
      </categories>
      <tags>
        <tag>epoll模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[终端常用快捷键]]></title>
    <url>%2F2018%2F04%2F01%2F%E7%BB%88%E7%AB%AF%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[常用快捷键Tab键： 命令、文件名等自动补全功能。 Ctrl+a： 光标回到命令行首。 （a：ahead） Ctrl+e： 光标回到命令行尾。 （e：end） Ctrl+b： 光标向行首移动一个字符。 （b：backwards） Ctrl+f： 光标向行尾移动一个字符。 （f：forwards） Ctrl+w： 删除光标处到行首的字符，也就是删除光标前面的所有内容。 Ctrl+k： 删除光标处到行尾的字符，也就是删除光标后面的所有内容。 Ctrl+u： 删除整个命令行文本字符，删除整行命令。 Ctrl+h： 向行首删除一个字符，向前删除一个字符，相当于Backspace。 Ctrl+d： 向行尾删除一个字符，向后删除一个字符，相当于Delete。 Ctrl+y： 粘贴Ctrl+u，Ctrl+k，Ctrl+w删除的文本。 Ctrl+p： 上一个使用的历史命令。 （p：previous） Ctrl+n： 下一个使用的历史命令。（n：next ） Ctrl+t： 交换光标所在字符和其前的字符。 Ctrl+i： 相当于Tab键。 Shift+Insert： 粘贴鼠标所复制的内容 Ctrl+d: 在空命令行的情况下可以退出终端。 Shift+c： 删除之后的所有内容并进入编辑模式 Ctrl+c： 中断终端中正在执行的任务。 Ctrl+z： 使正在运行在终端的任务，运行于后台。 （可用fg恢复到前台） 非常用快捷键Ctrl+s： 使终端发呆，静止，可以使快速输出的终端屏幕停下来。 Ctrl+q： 退出Ctrl+s引起的发呆。 Ctrl+[： 相当于Esc键。 Esc键： 连续按3次显示所有的支持的终端命令，相当于Tab键。 Ctrl+r： 快速检索历史命令。（r：retrieve）。 Ctrl+o： =Ctrl+m：相当Enter键。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>Linux基础知识</category>
        <category>终端常用快捷键</category>
      </categories>
      <tags>
        <tag>终端常用快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux运维工程师面试常见问题]]></title>
    <url>%2F2018%2F03%2F28%2FLinux%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[redhat、centos、suse、ubuntu等发行版本的区别这些发行版本本质上没有什么区别，都是类unix系统。redhat的系统本身是免费的，但是它的服务和一些特定的组件是收费的而centos是redhat社区版本，国内使用较多，社区相当活跃。SUSE也是分为两种，一种企业版本的SLES，一种是opensuse，sles主要是一些金融国企在使用，安全性较好，opensuse是社区版本，suse使用起来和centos相差不多，软件包形式使用源自redhat的rpm，但是管理工具使用的是zypper。ubuntu源自于debain，国外使用的较多，也是相当热门的一个发型版本，在桌面领域有绝对技术优势，适合开发人员使用。 OSI7层模型和TCP/IP模型的区别联系OSI7层模型OSI 7层模式主要是由国际标准化组织（ISO）创建的，是一个国际通用的标准，它被开发出作为一个参照标准，用于指导如何设计网络通信系统。说的简单一点就是统一网络设备商的协议标准，实现多网络设备商环境（主机，路由器，交换机等等都是网络设置，都要遵循同一套的通信标准）它一共分为7层，每一层在网络通信数据传输过程中都定义了不同的功能。 OSI7层模型主要分为(从下到上)：物理层，数据链路层，网络层，传输层，会话层，表示层，应用层。每一层说明： layer function Application data flow；离用户最近的一层，它使一个网络应用能和另一个网络应用相互通信 Presentation 定义数据格式；数据压缩、加解密等。 Session 定义如何建立和终止连接，是告诉4层怎么做，4层只管被动的接受然后去做。 Transport 数据段；将上层的数据流进行分段；建立和终止网络连接；常用于流量控制和数据恢复 Network 数据包；使用IP地址在Internet上唯一确定一台设备；定义设备路由，寻址 Data link 数据帧；将上层数据封装成数据帧，其中包含源目MAC地址以及帧校验字段（用于检测传输错误）；它包含2个子层（LLC和MAC） Physical 比特流；定义了比特流如何在两台设备之间流通；主要涉及线缆，网卡， 下面是每一层常见的对应协议 layer protocol Application HTTP,FTP,Telnet,SMTP,SNMP Presentation MIME,TIFF,GIF,JPEG,PICT,ASCII,EBCDIC,encryption,MPEG,MIDI,HTML Session SSl/TLS,NetBIOS,RPC Transport TCP,UDP Network IP,ICMP,ARP,RARP Data link PPP,HDLC,IEEE 802.3/802.2,FDDI,ATM,IEEE 802.5/802.2 Physical Ethernet TCP/IP协议族TCP/IP模型类似OSI模型，作用也是描述一套指导标准，实现网络设备之间的通信，它被设计成4层 Application Transport Internet Network Access 其对应关系为： TCP/IP model OSI model Application Application Presentation Session Transport Transport Internet Network Network Access Data link Physical 区别除了层数的区别之外，它们之间最大的区别就是： OSI模型规定了在一个网络上传输数据所需要的步骤，并且它是非常具体的，定义了每一层使用什么协议以及如何使用；而TCP/IP模型是不特定的。另外一个区别就是，目前TCP/IP是所有网络设备上既定的协议事实，一般分析问题使用OSI模型。 路由，交换技术的基本原理路由技术我们在这里说的路由技术一般是指，路由转发。主要涉及设备为路由器或者三层交换机。这些设备上会维护一张路由表，其中的信息可以是通过动态路由协议（例如OSPF，EIGRP，ISIS，RIP，BGP，静态路由，默认路由等）获取组成路由表的内容是：出口接口和对应网段 路由设备接收到一个数据包之后，会解封装，获取其中的目的IP地址信息（网段信息），然后查找路由表，选择最优路由去转发。路由设备上也会有一张ARP表，根据广播域 交换技术在一个局域网内，也就是一个广播域内使用的技术。通常会涉及到的设备就是交换机。交换机上会维护一张MAC地址转发表，其中的信息是MAC地址和端口的映射关系。交换机根据数据帧中的目的MAC地址进行数据包的转发 注意：在一个广播域内的数据流动是依靠二层MAC来实现的，因为在第一次会涉及到ARP，有了记录之后，交换机会记录他的MAC地址表，后续的速度就会非常快 PS：有关网络模型和路由技术可以结合一个小案例在白板上演示一下，效果会更好，也就是将上述两部分的内容有机的整合成为一个整体 脚本部分运维知识优化运维思想运维核心是什么稳定性-网站/平台不宕机【核心】集群 负载均衡 高可用 解耦，微服务 数据不丢失【核心】数据备份，异地容灾 避免人为错误 避免人为错误，主要分为两个方面，一个是开发不严谨产生的错误，这部分通过流程可以控制。【比如测试不严谨，开发人员的代码有问题，直接把服务器资源跑没了】 一个是自己操作产生的问题，这部分通过一些智能化的自动化工具来尽量避免【避免在执行命令的时候误操作等等】 解决：建立完善的流程制度，对运维来说，包括标准化，对开发测试来说，包括上线的流程化 运维效率管理平台运维脚本化，工具化，自动化，人工智能化 如何做好运维工作运维职业规划你为什么离职你对加班的看法个人最大的优点和缺点]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>职场</category>
        <category>Linux运维面试问题</category>
      </categories>
      <tags>
        <tag>Linux运维面试问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker常用命令记录]]></title>
    <url>%2F2018%2F03%2F19%2Fdocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[镜像命令下载/获取镜像运行镜像查看镜像信息images命令使用docker images命令可以列出本地主机上已有镜像的基本信息。 1$ docker images [option] 常用参数如下： -a –all=true|false 列出所有的镜像文件（包括临时文件），默认为否 –digests=true|false 列出镜像的数字摘要值，默认为否 -f –filter=[] 过滤列出的镜像 具体可以通过man docker-images 进行查看。 inspect命令镜像操作tag命令删除镜像]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>虚拟化</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix的历史数据与趋势数据]]></title>
    <url>%2F2018%2F02%2F06%2Fzabbix%E7%9A%84%E5%8E%86%E5%8F%B2%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%B6%8B%E5%8A%BF%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[参考链接官方文档 zabbix history trends历史与趋势数据详解 zabbix配置操作详解（三） Zabbix系统中的历史数据和趋势数据 正文历史与趋势历史数据和趋势数据是Zabbix系统中对采集到的监控项数据进行存储的两种方式。 历史根据设定的时间间隔保持每个收集的值， 而趋势是每个小时产生一个值（一条信息），内容为历史数据在一个小时内的平均值、最大值、最小值以及这一个小时内该监控项目所采集到的数据的个数。 在zabbix中的配置在监控项配置页面进行定义，在这里，我的配置是历史数据保留15天，趋势数据保留90天。如下图所示： 区别联系详解历史和趋势数据它们既有区别又有联系。 历史数据： Zabbix系统针对每个监控项目在每次采集时所收集到的数据，这个数据保存Zabbix系统数据库的历史表中，这就是所谓的历史数据。 因为每次所采集到的数据都保存在历史表中，所以如果监控项目的更新间隔越小，则在固定时间内所保存到历史表中的数据就越多。如果每个监控项目的更新间隔是30秒的话，则两个小时，该监控项目在Zabbix数据库的历史表中就会产生240条记录，一天就会产生2880条记录。 如果我们的Zabbix系统只监控一台被监控主机，且这台被监控主机只有一个被监控项目，那么每天产生2880条记录确实不值得一提的。但是，当我们监控系统所监控的项目比较多时，则这个数据量是非常大的。 比如说，如果我们监控系统监控1000个监控项目，且每个监控项目的更新间隔都是30秒，则每天历史表中就会产生2880*1000=2880000条记录，也即近300万条记录。而1000个监控项目可以监控多少主机呢？我们以48口的交换机为例，单监控每台交换机的每个端口的流量，则一台48口的交换机就有96个监控项目。所以，如果我们仅监控这样的48口的交换机，1000个监控项目只差不多只够监控10台这样交换机。由此可见，如果我们所监控主机的数量稍微多一点，或者更确切的来说，我们所监控的项目稍微多点，则Zabbix系统每天在其数据库中所产生的记录是非常大的。 因此，我们建议，如非必须的，我们在配置监控项目时，应尽量减小历史数据的保留天数，以免给数据库系统带来很大的压力。 趋势数据： 而趋势数据则不同，对于相同的更新间隔，系统所产生的趋势数据的数量远远没有历史数据那么庞大。对同一个监控项目，之所以趋势数据的数据量要远远小于历史数据的数据量，是由趋势数据的取值方式决定的。 趋势数据取值方式是，它取对应监控项目的历史数据在一个小时内的平均值、最大值、最小值以及这一个小时内该监控项目所采集到的数据的个数。 因此，不管一个监控项目的更新间隔是多少，它所对应的趋势数据在数据库中的记录都只有一条。更新间隔越小，仅可能导致数据个数增大，而不会影响该监控项目在趋势表里的记录条数的。 由此，或许你觉得趋势数据很不准确，你还是愿意保留更长时间的历史数据，以便查看较长时间的数据图。其实不是这样的，因为在Zabbix系统数据库的趋势表里不但保留一个小时内历史数据的最大值、最小值和平均值，而还保存这一个小时内所采集到的数据个数。因此，在要求并不是很高的场合，使用趋势数据绘出的监控项目的数据图的走势与用历史数据绘出的数据图的走势差别不会很大的。 不管是历史数据还是趋势数据，都会周期性被Zabbix服务器端一种称之为“主妇（housekeeper）”进程进行清理，它会周期性的删除过期的历史数据和趋势数据。 也正是因为这个进程的存在，才会使Zabbix系统数据的数据量不会一直的彭胀下去。而实际上，如果我们在保持Zabbix系统的被监控主机和被监控项目不变，且不更改监控项目的更新间隔的情况下，Zabbix系统的数据库的数据量会在增长到一定的数据量后不再增长，而是基本维持在这个数据量上不变。 “主妇”进程清理历史数据和趋势数据的频率可以在Zabbix服务器端组件(或服务器代理组件)的配置文件zabbix_server.conf中进行配置，它的配置项是HousekeepingFrequency。 特别注意： 1、 如果监控项目的“保留历史数据(天)”配置项被设置成0时，则数据库历史表中仅保留该监控项目所采集的最后一条数据，其它历史数据将不会被会保留。而且，引用该监控项目的触发器也只能使用该项目所采集的最后数据。因此，此时如果在触发器里引用该项目时使用max、avg、min等函数，其将没有意义。 2、 如果监控项目的“保留趋势数据(天)”配置项被设置成0时，则该项目在系统数据库的趋势表里将不保留任何数据。 配置建议具体该配置成什么样的周期，需要根据监控项以及数据库的配置以及对数据查看的要求程度来决定。这里只给出相关建议。 历史数据配置首先我们需要知道当前mysql的存储情况。在zabbix的前端页面上，我们可以看到如下图所示信息： 这个数值就是NVPS，也就是每秒处理平均数量（Number of processed values per second) 计算公式如下： 历史数据大小=NVPSx3600x24x365(天数)x50B 每个监控项大小约为50B，每秒条数为NVPS，一小时3600秒，一天24小时，一年365年。 具体单个监控项大小取决于数据库引擎，通常为50B 例如： 假设有6W个监控项，刷新周期都为60秒（我这里为30秒），那么每秒将会产生1000条数据，也就是每秒会向数据库写入1000条数据。如果我的历史数据保留天数为90天，那么需要的空间大小如下： 1000x3600x24x90x50=388 800 000 000(B) (约为362G，如果保存一年则为：362x4=1448G) 趋势数据配置因为趋势数据是每小时每个监控项一条记录，因此可以计算出大致所占的空间，其计算公式如下： 趋势数据大小=监控项个数x24x365(天数)x128B 每一个监控项的大小约为128B，每小时产生一条记录，一天24小时，一年365天 具体单个监控项大小取决于数据库引擎，通常为128B 例如： 假设有6W个监控项，保存一年的趋势数据，那么需要的空间如下： 60000x24x265x128=67 276 800 000(B) （约为67GB） 总结通过上面的计算对比，相信可以很直观的看到差别，在同样一年的情况下，历史与趋势所占存储空间的比例为：1448/67。 所以，具体选择什么周期需要根据公司的业务及实际情况（硬件配置等）来决定，并没有一个统一的标准，遵循这个公式，都可以很明确的计算预估出数据量情况。]]></content>
      <categories>
        <category>IT科学技术知识体系结构-Linux运维方向</category>
        <category>运维监控体系</category>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[autohotkeye常用操作]]></title>
    <url>%2F2018%2F02%2F06%2Fautohotkeye%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言AutoHotkey是一个windows下的开源、免费、自动化软件工具。它由最初旨在提供键盘快捷键的脚本语言驱动(称为：热键)，随着时间的推移演变成一个完整的脚本语言。但你不需要把它想得太深，你只需要知道它可以简化你的重复性工作，一键自动化启动或运行程序等等；以此提高我们的工作效率，改善生活品质；通过按键映射，鼠标模拟，定义宏等。 参考资料官方https://autohotkey.com/docs/AutoHotkey.htm 民间https://jeffjade.com/2016/03/11/2016-03-11-autohotkey/https://ahkcn.github.io/docs/AutoHotkey.htm 下载安装下载地址autohotkey下载地址 使用说明 AutoHotkey doesn’t do anything on its own; it needs a script to tell it what to do. A script is simply a plain text file with the .ahk filename extension containing instructions for the program, like a configuration file, but much more powerful. A script can do as little as performing a single action and then exiting, but most scripts define a number of hotkeys, with each hotkey followed by one or more actions to take when the hotkey is pressed. 也就是说，在实际使用的时候，是通过autohotkey去调用脚本，然后再去执行一系列的操作 脚本是自己定义个一个后缀为.ahk的文件 然后双击启动Ahk2Exe.exe，选择自己编写的这个ahk文件，执行convert，之后会生成一个ahk.exe的可执行文件。启动这个ahk.exe文件，就将配置加载，之后就可以使用这些热键进行一系列的操作 一个脚本中对应一系列热键 脚本符号这里简单说明下脚本中常用符号代表的含义： # 号 代表 Win 键； ! 号 代表 Alt 键； ^ 号 代表 Ctrl 键； + 号 代表 shift 键； :: 号(两个英文冒号)起分隔作用； run， 非常常用 的 AHK 命令之一; ; 号 代表注释后面一行内容； *通配符 即使附加的修饰键被按住也能激发热键. 这常与 重映射 按键或按钮组合使用. 例如: *#c::Run Calc.exe 表示：Win+C、Shift+Win+C、Ctrl+Win+C 等都会触发此热键。 run它的后面是要运行的程序完整路径（比如我的Sublime的完整路径是：D:\Program Files (x86)\Sublime Text 3\sublime_text.exe）或网址。为什么第一行代码只是写着“notepad”，没有写上完整路径？因为“notepad”是“运行”对话框中的命令之一。 如果你想按下“Ctrl + Alt + Shift + Win + Q”（这个快捷键真拉风啊。(￣▽￣)）来启动 QQ 的话，可以这样写： ^!+#q::run QQ所在完整路径地址。 AutoHotKey的强大，有类似Mac下的Alfred2之风，可以自我定制(当然啦，后者还是强大太多)。所以可以说，它强大与否，在于使用者的你爱或者不爱折腾。学以致用，如果简单的折腾下，可以使得我们工作效率大幅提升，何乐不为？况且，在见识的增长中，这可以给我们思维带来极大的营养。以下是笔者常用功能的脚本配置： 温馨提示： 以下几个系统默认的 Win 快捷键，请自行确认是否覆盖 Win + E：打开资源管理器； Win + D：显示桌面； Win + F：打开查找对话框； Win + R：打开运行对话框； Win + L：锁定电脑； Win + PauseBreak：打开系统属性对话框; Win + Q: 本地文件/网页等搜索; Win + U: 打开控制面板－轻松使用设置中心; 配置使用这是我自行编写的脚本的内容 #q::Run https://wx.qq.com/ #w::Run http://watchmen.xin/ #e::Run E:\software\tcmd\totalcmd\TOTALCMD64.EXE #r::Run, E:\software\ss\Shadowsocks.exe #t::Run, E:\software\Snipaste\Snipaste.exe #y::Run, E:\software\TIMqq\Bin\QQScLauncher.exe #u::Run, E:\software\foxmail\Foxmail.exe #i::Run, E:\software\xmanager\Xshell.exe 进阶单热键多命令类似下面的这种设置被称为单行热键, 因为它们只包含单个命令. #n::Run Notepad ^!c::Run calc.exe 要在一个热键中执行多个命令，请把首行放在热键定义的下面，且在最后行命令的下一行添加 return。例如： #n:: Run http://www.google.com Run Notepad.exe return 如果要运行的程序或文档没有在环境变量中, 那么需要指定它的完整路径才能运行: Run %A_ProgramFiles%\Winamp\Winamp.exe 在上面的例子中, %A_ProgramFiles% 是 内置变量. 使用它而不使用像 C:\Program Files 这样的, 脚本可以有更好的移植性, 这表示它在其他电脑上能执行的可能性更大. 注意: 命令和变量的名称是不区分大小写的. 例如, “Run” 等同于 “run”, 而 “A_ProgramFiles” 等同于 “a_programfiles”. 要让脚本等到程序或文档关闭后才继续执行, 请使用 RunWait 代替 Run. 在下面的例子中, 一直到用户关闭记事本后 MsgBox 命令才会继续执行. RunWait Notepad MsgBox The user has finished (Notepad has been closed).]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>AutoHotKey</category>
      </categories>
      <tags>
        <tag>autohotkey</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[七牛云-qshell工具常用命令]]></title>
    <url>%2F2018%2F02%2F05%2F%E4%B8%83%E7%89%9B%E4%BA%91-qshell%E5%B7%A5%E5%85%B7%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[前言/简介qshell是利用七牛文档上公开的API实现的一个方便开发者测试和使用七牛API服务的命令行工具。 该工具设计和开发的主要目的就是帮助开发者快速解决问题。 目前该工具融合了七牛存储，CDN，以及其他的一些七牛服务中经常使用到的方法对应的便捷命令，比如b64decode，就是用来解码七牛的URL安全的Base64编码用的，所以这是一个面向开发者的工具。 官方资料文档https://developer.qiniu.com/kodo/tools/1302/qshell 视频教程http://notdelete.echohu.top/spjc/qshell-win.mp4 安装/环境准备目前在windows上使用qshell需要执行以下几个步骤添加命令到系统 下载qshell，存储到指定文件夹，例如我这里是：E:\software\qshell 重命名，将qshell_windows_x64.exe重命名为qshell.exe 添加系统环境变量，将E:\software\qshell追加到环境变量中 命令选项参数 描述 -d 设置是否输出DEBUG日志，如果指定这个选项，则输出DEBUG级别的日志 -m 切换到多用户模式，这样所有的临时文件写入都在命令运行的目录下 -h 打印命令列表帮助信息，遇到参数忘记的情况下，可以使用该命令 -v 打印工具版本，反馈问题的时候，请提前告知工具对应版本号 命令列表 实际操作我们使用qupload来进行文件的管理 官方文档 命令参数展示 命令语法： 1qshell qupload [&lt;ThreadCount&gt;] &lt;LocalUploadConfig&gt; 命令参数： 配置参数展示qupload 功能需要配置文件的支持，配置文件支持的全部参数如下： { &quot;src_dir&quot; : &quot;&lt;LocalPath&gt;&quot;, &quot;bucket&quot; : &quot;&lt;Bucket&gt;&quot;, &quot;file_list&quot; : &quot;&lt;FileList&gt;&quot;, &quot;key_prefix&quot; : &quot;&lt;Key Prefix&gt;&quot;, &quot;up_host&quot; : &quot;&lt;Upload Host&gt;&quot;, &quot;ignore_dir&quot; : false, &quot;overwrite&quot; : false, &quot;check_exists&quot; : false, &quot;check_hash&quot; : false, &quot;check_size&quot; : false, &quot;rescan_local&quot; : true, &quot;skip_file_prefixes&quot; : &quot;test,demo,&quot;, &quot;skip_path_prefixes&quot; : &quot;hello/,temp/&quot;, &quot;skip_fixed_strings&quot; : &quot;.svn,.git&quot;, &quot;skip_suffixes&quot; : &quot;.DS_Store,.exe&quot;, &quot;log_file&quot; : &quot;upload.log&quot;, &quot;log_level&quot; : &quot;info&quot;, &quot;log_rotate&quot; : 1, &quot;log_stdout&quot; : false, &quot;file_type&quot; : 0 } 参数具体含义如下： 密钥设置单用户 1qshell account ak sk 多用户 1qshell -m account ak sk 这里的ak、sk在个人面板中的密钥管理中查看，点击显示，然后进行复制粘贴 如下图所示： 上传图片这里我们选择qupload方式来进行图片的上传，在windows本地创建一个文件夹用户放置图片数据，每次同步该文件夹即可，不用再单独每张上传 步骤1：创建本地图片文件夹如下图所示，在指定位置下创建一个文件夹用于存放图片，在这里，我把它和我的博客文件夹放在同级 步骤2：创建配置文件如下图所示，在指定目录下创建配置文件，注意，这里需要使用编辑打开，不要用notpad++这些编辑器 步骤3：执行命令进行上传准备工作都做好后，执行如下命令直接上传： qshell qupload 1 c:\Users\56810\blog\config.txt qshell qupload 1 C:\Users\Administrator\blog\config.txt 如下图所示 其他配置下载文件刷新缓存官方资料 使用七牛云提供的 qshell 命令行工具，参考使用文档，先设置密钥，然后执行 cdnrefresh 命令来刷新缓存。 具体操作为： 步骤1：修改配置文件 步骤2：执行命令]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>七牛云-qshell</category>
      </categories>
      <tags>
        <tag>qshell</tag>
        <tag>七牛云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RSS介绍及使用]]></title>
    <url>%2F2018%2F02%2F04%2FRSS%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[转载来源：http://www.ruanyifeng.com/blog/2006/01/rss.html RSS定义在解释RSS是什么之前，让我先来打一个比方。 读大学的时候，我有个习惯，就是每天要去看食堂后面的海报栏。在那里，会贴出各种各样最新的消息，比如哪个系要开讲座了、星期二晚上的电影放什么、二手货转让等等。只要看一下海报栏，就会对学校的各种活动心中有数。 如果没有海报栏的话，要想知道这些消息就会很麻烦。讲座消息会贴在各个系自己的公告栏里，电影排片表是贴在电影院里的，二手货消息则会贴在各幢宿舍的楼道里。我所在的大学有20几个系，一万多人，要想知道所有这些消息的话，即使是可能的话，也会相当的麻烦。 从这个例子出发，让我们来考虑一下互联网。 互联网是什么？最直观的说，就是一个杂乱无章的巨大信息源，其丰富和杂乱的程度，不仅是巨大的，而且几乎是无限的。 一个使用者，要想及时掌握的互联网上出现的最新信息，有办法吗？ 答案是没有办法，他只有一个网站一个网站的打开，去看有什么最新内容，就好比每天都必须去每一个系里走一遍，看有什么最新讲座。如果是几个网站，哪倒也不难，都去看一遍也花不了多少时间。但是随着你关注的网站数量上升，这项工作会迅速的变为”Mission Impossible”。想象一下，如果你每天关注几十个、甚至几百个网站，会是怎样的情景。光是打开它们的首页，就要花费多少时间啊，更别说浏览花去的时间了。 也许有人会说，普通人的话，谁会关心那么多网站啊？ 我要说，哪怕你只是一个网络的初级或最单纯的使用者，与你发生关系的网站数量也在急剧增加，因为Blog出现了。越来越多的人开始写作网络日志（Blog），把自己的想法和生活在网上展示，其中也必然包括你的朋友，或者其他你感兴趣的人。你想知道他/她的最新动向，就势必要留心他/她的Blog。所以，你的网站浏览清单总有一天会和你的电话本、MSN Message好友列表一样多，甚至更多。 那时，你会发现浏览网站会变成一种困难和低效率的行为。 有没有办法找到互联网上的”海报栏”，只去一个地方就知道你所想知道的所有最新内容？ 有，那就是RSS。 RSS内容和阅读器准确的说，RSS就像一个网站的海报，里面包括这个网站的最新内容，会自动更新。所以，我们只要订阅了RSS，就不会错过自己喜欢的网站的更新了。 但是光有海报还不行，还必须有海报栏，也就是说必须有RSS阅读器才行。因为RSS只是数据源，它本身是写给程序看的，必须经过阅读器转换，才能成为可以浏览的格式。 RSS阅读器多种多样，大致分为两种，一种是桌面型的，需要安装；另一种是在线型，直接使用浏览器进行阅读。 使用/订阅RSS在浏览器中订阅RSS，就必须先知道RSS的地址。一般来说，各个网站的首页都会用显著位置标明。名称可能会有些不同，比如RSS、XML、FEED，大家知道它们指的都是同样的东西就可以了。有时RSS后面还会带有版本号，比如2.0、1.0，甚至0.92，这个不必理会，它们只是内部格式不同，内容都是一样。 将RSS地址复制下来以后，你就可以在在线阅读器中添加。 以后，只用打开这一个网页，就可以看到所有你喜欢的网站的最新内容了。 推荐RSS阅读器个人目前在使用的RSS阅读器为：inoreader]]></content>
      <categories>
        <category>IT基础知识</category>
        <category>RSS</category>
      </categories>
      <tags>
        <tag>RSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown语法]]></title>
    <url>%2F2018%2F01%2F25%2FMarkdown%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Markdown介绍Markdown 是一种轻量级标记语言，它以纯文本形式(易读、易写、易更改)编写文档，常用的标记符号也不超过十个，并最终以HTML格式发布,让写作者专注于写作而不用关注样式。 划重点： 轻量级 标记语言 纯文本，所以兼容性极强，可以用所有文本编辑器打开。 让你专注于文字而不是排版。 格式式转换方便，Markdown 的文本你可以轻松转换为 html、电子书等。 Markdown 的标记语法有极好的可读性，常用的标记符号不过十来个 参考资料看完我这篇文章，再看完我下面推荐的这些内容，然后对比归纳总结，认真实践后，可以说在平常工作学习中完全够用。 官方资料 Markdown 语法说明 (简体中文版) Markdown 语法介绍 易读易写!-MarkDown语法说明 个人文章 献给写作者的 Markdown 新手指南 Markdown——入门指南 Markdown 基本语法 编辑器 个人在用的编辑器是MarkdownPad 2。各个工具之间相差不会很大，熟练掌握快捷键是提高效率的好方法 核心理念Markdown 的目标是实现「易读易写」，成为一种适用于网络的书写语言。。不管从任何角度来说，可读性，都是最重要的。Markdown 的语法全由一些符号所组成，这些符号经过精挑细选，其作用一目了然。比如：在文字两旁加上星号，看起来就像强调。 划重点： 语法是非常简单的符号 即写即读 兼容HTMLMarkdown 的构想不是要使得 HTML文档更容易书写。HTML 已经很容易写了。Markdown 的理念是，能让文档更容易读、写和随意改。 HTML是一种发布的格式，而Markdown 是一种书写的格式。也因此，Markdown 的格式语法只涵盖纯文本可以涵盖的范围。 常用操作标题（MarkdownPad中快捷键为Ctrl+1/2/3/4）：Markdown 支持两种标题的语法，类 Setext 和类 atx 形式。类 Setext 形式是用底线的形式，利用 = （最高阶标题）和 - （第二阶标题），例如： This is an H1 ============= This is an H2 ------------- 任何数量的 = 和 - 都可以有效果。但是这种形式只支持2层标题。 类 Atx 形式则是在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶，例如： # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 强调在Markdown中，可以使用 和 _ 来表示斜体和加粗。*单个为斜体，2个为加粗 加粗（MarkdownPad中快捷键为Ctrl+b）：加粗部分使用方式如下： **Coding，让开发更简单** __Coding，让开发更简单__ 实际展示效果如下： Coding，让开发更简单 Coding，让开发更简单 斜体（MarkdownPad中快捷键为Ctrl+l）：斜体部分的使用如下： *Coding，让开发更简单* _Coding，让开发更简单_ 实际展示效果展示如下： Coding，让开发更简单 Coding，让开发更简单 列表无序列表（MarkdownPad中快捷键为Ctrl+u）：* list1 前面使用*号 - list2 前面使用-号 + list3 前面使用+号 效果如下： list1 list2 list3 有序列表(MarkdownPad中快捷键为Ctrl+shift+o）：1. list1 使用数字+英文的点号，空格后接数据 2. list2 效果如下： list1 list2 区块引用（MarkdownPad中快捷键为Ctrl+q）在我们写作的时候经常需要引用他人的文字，这个时候引用这个格式就很有必要了，在 Markdown 中，你只需要在你希望引用的文字前面加上 &gt; 就好了。注意&gt;和文本之间要保留一个字符的空格。 &gt; 数据1 使用&gt;号 &gt; 数据2 &gt; &gt; 二级引用 注意区块引用可以包含多级引用 &gt; 实际效果展示： 数据1 数据2 这是二级引用 三级引用 代码区块（MarkdownPad中快捷键为Ctrl+k）：代码区块包括3种，文字内和单独一行以及指定代码格式的区块行 文字内加区块，不会加空白处底纹使用``（数字1左边，ESC下面的按键） 实际效果展示：在文件中含有代码区块是什么样子 整行的代码区块行，会加空白处底纹（快捷操作：全部选中然后敲Tab）缩进4个空格或者一个制表符（tab键）或者将代码块包裹在代码块包裹在 “/` 之间（避免无休止的缩进）。 实际效果展示 123require 'redcarpet'markdown = Redcarpet.new("Hello World!")puts markdown.to_html 实际效果展示： 现在的效果就是整整一个的区块行，如果这段代码比较长的话，那么markdown就会在下面生成一个查看条，供用户左右拉取调整，就是如现在所示。 指定代码格式的区块行 实际效果展示： 12$ line1-test1$ line2-test2 分割线/分隔线（MarkdownPad中快捷键为Ctrl+r）：一行中用三个以上的星号、减号、底线来建立一个分隔线，可以在字符之间加入空格，也可以不加空格 * * * *** ***** --- - - - 实际效果展示如下： 网页链接网页链接有2种方式，一种是直接显示链接，一种是通过文字进行跳转 直接显示&lt;https://www.baidu.com&gt; 用&lt;&gt;尖括号将内容包起来，markdown就会自动把它转成链接。网页链接、邮箱链接等都采用这种方式 实际效果展示如下：这段话中将要插入百度https://www.baidu.com的链接 文字跳转More info: [Server](https://hexo.io/docs/server.html) 前面是解释性说明，[]内是可以跳转的文字，()内是真正访问的地址。 实际效果展示如下： 请点击百度调整到百度页面 图片链接图片链接分为2部分，一种是在文字中，通过文字来链接到图片位置，用户需要点击这个文字链接去查看图片，优点是使文字更简约，缺点是无法直观的看到图。因此，第二种方式是直接在文章中显示图片。 我们把这两种方式分别称之为：行内式和参考式 行内式行内式的图片语法看起来像是： ![Alt text](/path/to/img.jpg) 参考案例：![Alt text](/path/to/img.jpg &quot;Optional title&quot;) 详细叙述如下： 一个惊叹号 ! 接着一个方括号，里面放上图片的替代文字 接着一个普通括号，里面放上图片的网址， 最后还可以用引号包住并加上 选择性的 ‘title’ 文字。 参考式参考式的图片语法则长得像这样： ![Alt text][id] 「id」是图片参考的名称，图片参考的定义方式则和连结参考一样： 参考案例：[id]: url/to/image &quot;Optional title attribute&quot; 参考式超链接一般用在学术论文上面，或者另一种情况，如果某一个链接在文章中多处使用，那么使用引用 的方式创建链接将非常好，它可以让你对链接进行统一的管理。 参考式同样适用于网页链接的使用 表格普通表格： First Header | Second Header | Third Header ------------ | ------------- | ------------ Content Cell | Content Cell | Content Cell Content Cell | Content Cell | Content Cell 设置表格两边内容对齐，中间内容居中，例如： First Header | Second Header | Third Header :----------- | :-----------: | -----------: Left | Center | Right Left | Center | Right 实际效果展示： First Header Second Header Third Header Left Center Right Left Center Right 文本居中居中使用html方式添加，格式如下： 1&lt;center&gt;这一行需要居中&lt;/center&gt; 文本居中的引用先看下实际效果： 主要用于主页等显示，和上面的文本场景有点不一样。 具体实现： &lt;!-- HTML方式: 直接在 Markdown 文件中编写 HTML 来调用 --&gt; &lt;!-- 其中 class=&quot;blockquote-center&quot; 是必须的 --&gt; &lt;blockquote class=&quot;blockquote-center&quot;&gt;blah blah blah&lt;/blockquote&gt; &lt;!-- 标签 方式，要求NexT版本在0.4.5或以上 --&gt; {% centerquote %} content {% endcenterquote %} &lt;!-- 标签别名 --&gt; {% cq %} content {% endcq %} 添加空行&lt;br /&gt; 使用该方法进行插入 反斜杠转义\*literal asterisks\* 使用这种方式来输出*号 实际效果展示： *literal asterisks* 字体与字号字体，字号和颜色编辑如下代码 &lt;font face=&quot;黑体&quot;&gt;我是黑体字&lt;/font&gt; &lt;font face=&quot;微软雅黑&quot;&gt;我是微软雅黑&lt;/font&gt; &lt;font face=&quot;STCAIYUN&quot;&gt;我是华文彩云&lt;/font&gt; &lt;font color=#0099ff size=7 face=&quot;黑体&quot;&gt;color=#0099ff size=72 face=&quot;黑体&quot;&lt;/font&gt; &lt;font color=#00ffff size=72&gt;color=#00ffff&lt;/font&gt; &lt;font color=gray size=72&gt;color=gray&lt;/font&gt; Size：规定文本的尺寸大小。可能的值：从 1 到 7 的数字。浏览器默认值是 3 效果如下： 我是黑体字我是微软雅黑我是华文彩云color=#0099ff size=72 face=”黑体”color=#00ffffcolor=gray 字体颜色语法格式：&lt;font color=指定颜色的英文单词&gt;内容&lt;/font&gt;，例如 例如将字体颜色修改为红色： 代码为：&lt;font color=red&gt;内容&lt;/font&gt; 内容 背景色&lt;table&gt;&lt;tr&gt;&lt;td bgcolor=orange&gt;背景色是：orange&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt; 实际效果如下： 背景色是：orange 删除线文本两端加上两个~~即可 删除我 复选框列表在列表符号后面加上[]或者[x]代表选中或者未选中情况 - [x] C - [x] C++ - [x] Java - [x] Qt - [x] Android - [ ] C# - [ ] .NET 实际效果为 C C++ Java Qt Android C# .NET 生成目录-TOC插件首先下载和安装 Visual Studio Code 锚点网页中，锚点其实就是页内超链接，也就是链接本文档内部的某些元素，实现当前页面中的跳转。比如我这里写下一个锚点，点击回到目录，就能跳转到目录。 在目录中点击这一节，就能跳过来。还有下一节的注脚。这些根本上都是用锚点来实现的。 语法描述： 代码： 这里使用截图的方式展示，因为直接编写的话，hexo会检测报错(因为%没有对应的%结尾) emoji表情Github的Markdown语法支持添加emoji表情，输入不同的符号码（两个冒号包围的字符）可以显示出不同的表情。 比如:blush:，可以显示: :blush: 注释注释是写作者自己的标注记录，不被浏览器解析渲染。HTML 以 结尾的闭包定义注释（支持跨行），不在正文中显示。 Markdown 沿用 HTML Comment 注释格式： &lt;!-- This text will not appear in the browser window. --&gt; 折叠块代码如下： &lt;details&gt; &lt;summary&gt;点击展开答案&lt;/summary&gt; &lt;p&gt; 象&lt;/p&gt; &lt;/details&gt; 效果如下： 你和猪，打一种动物 点击展开答案 象 代码高亮与原来使用缩进来添加代码块的语法不同，这里使用 来包含多行代码： 三个 ``` 要独占一行。 指定图片大小Markdown 不支持指定图片的显示大小，不过可以通过直接插入标签来指定相关属性： &lt;img src=&quot;https://avatars2.githubusercontent.com/u/3265208?v=3&amp;s=100&quot; alt=&quot;GitHub&quot; title=&quot;GitHub,Social Coding&quot; width=&quot;50&quot; height=&quot;50&quot; /&gt; 效果如下： 在单元格里换行借助于 HTML 里的 实现。 示例代码： | Header1 | Header2 | |---------|----------------------------------| | item 1 | 1. one&lt;br /&gt;2. two&lt;br /&gt;3. three | 示例效果： Header1 Header2 item 1 1. one2. two3. three]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TotalCommander常用快捷键]]></title>
    <url>%2F2018%2F01%2F25%2FTotalCommander%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[基础知识参考资料可以先看下相关资料，这些资料对概念介绍等做的非常详细也写的非常好，因此这里不再赘述，本文主要是针对实际的运用。 官方资料 https://www.ghisler.com/官网上没有相对应的文档，需要额外去搜寻 优秀个人文章 TC学堂——最易读的Total Commander教程-强烈推荐通过该网站进行学习 Total Commander快捷键 实际操作常用目录这部分设置可以说是TC操作的精华，效率直接甩开windows资源管理器几条街。 快速添加ctrl+d，添加，然后a直接添加 常用目录高级配置通过自定义配置，可以自定义调整常用目录的名称、顺序等，后续的增删改查也在此页面进行。 ctrl+d，添加，进去之后按c进入常用目录配置对话框。在里面配置的时候，需要再最前面人为添加&amp;。 名称设置： &amp;1 test $b blog 命令参考设置： cd C:\Users\56810\blog\blog 直达组合键通过直达组合键，可以直接切换到指定目录下。 设置：alt+s 调出窗口，再按s进行配置。一共可以使用的个数是一般都是类似ctrl+alt+F1/F2..F11这么11个组合键 名称设置： &amp;1 desktop $b blog 命令参考设置： cd C:\Users\56810\blog\blog 配置完成之后，切换到桌面只需要：alt+s+1 切换磁盘分区Alt+F1调出分区选项之后，按D则进入D盘，E则进入E盘。 目录内容查看Alt+1 详细的列表信息 Alt+2 图形信息显示 Alt+3 目录树显示 多Tab标签操作ctrl+t 新建tab ctrl+上箭头 新建父目录tab ctrl+w 关闭标签 ctrl+shift+w 关闭所有非活动标签 ctrl+tab, ctrl+shift+tab 在同侧的tab间切换 改变tab排列顺序（包括在两个窗口间移动）：鼠标左键拖动。 自定义快捷键，直接切换到第N个标签可以在 wincmd.ini 中 [Shortcuts] 段，增加如下内容， C+1=cm_SrcActivateTab1 C+2=cm_SrcActivateTab2 C+3=cm_SrcActivateTab3 效果： ctrl+1～3 激活第 1～3 个标签，依次类推 压缩操作压缩： 选中文件之后，执行Alt+F5 查看压缩文件内容（不解压缩）： ctrl+右箭头或者直接回车 解压缩：Alt+F9 文件搜索Alt+F7 创建操作F7/Shift+F7 新建一个或多层文件夹。可以像DOS那样新建多层的目录，比如c:\file\a\b\c Shift+F4 新建文本文件，调用记事本编辑（自定义编辑器） 其他快捷键ctrl +e 进入资源管理器 alt + f1 选择第一个窗口的磁盘 +f2就是选择第二个窗口的磁盘 alt+下箭头 历史记录 alt+左箭头 返回上一个操作目录（历史目录） alt+右箭头 返回下一个操作目录（历史目录） ctrl+\ 返回到当前目录的根目录 Ctrl+Shift+Enter 查看当前的路径 shift+F10 右键 F3 文件内容预览 ctrl+M 批量重命名 Shift+F4 新建文本文件，调用记事本编辑（自定义编辑器） Ctrl+加号 全部选择同一类型的文件（例如压缩文件，目录文件） Ctrl+减号 全部取消同一类型的文件（例如压缩文件，目录文件）]]></content>
      <categories>
        <category>常用软件工具</category>
        <category>TotalCommander</category>
      </categories>
      <tags>
        <tag>TC操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F23%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is my first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask author on GitHub. 前言之前使用阿里云结合wordpress的方式搭博客，但是自己维护起来不是太方便，云服务器一旦攻击，数据是个问题。之后在51cto和csdn上写，但是要受到平台的限制。最近发现github有博客功能(几年前就推出了，竟然现在才发现)，完美解决这些问题。github提供空间，用户自行选择博客框架，专注于内容，大部分人应该还是喜欢这种简约风主题。目前这个博客使用github-pages+Hexo来实现。 参考资料搭建 https://zhuanlan.zhihu.com/p/26625249 http://eleveneat.com/2015/04/24/Hexo-a-blog/ 进阶 主要参考官方资料 Hexo文档 https://hexo.io/zh-cn/docs/ Next主题使用手册 http://theme-next.iissnan.com/ 根据官方资料，按图索骥，基本上都能很好的把所有功能实现出来。使用问题可以随时沟通交流 markdown语法 关于markdown的使用，可以看我的这篇博文 Markdown语法 Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy Generate static files + Deploy to remote sites1$ hexo g -d More info: Deployment]]></content>
      <categories>
        <category>个人知识体系</category>
        <category>个人博客</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
